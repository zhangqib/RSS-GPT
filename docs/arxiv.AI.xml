<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
<link>https://arxiv.org/abs/2511.00020</link>
<guid>https://arxiv.org/abs/2511.00020</guid>
<content:encoded><![CDATA[
<div> Keywords: digital commerce, user-generated reviews, fake review detection, multimodal learning, content moderation

Summary:
This study addresses the issue of fake or misleading user-generated reviews in digital commerce by proposing a robust multimodal fake review detection framework. The framework integrates textual features encoded with BERT and visual features extracted using ResNet-50 to capture semantic inconsistencies across different modalities. A curated dataset comprising 21,142 user-uploaded images across various domains was utilized to train the model. Results show that the multimodal model outperforms unimodal baselines with an F1-score of 0.934 on the test set. The model demonstrates the ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This research highlights the importance of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across online platforms. 

<br><br>Summary: <div>
arXiv:2511.00020v1 Announce Type: new 
Abstract: In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model's ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Attentive MAPPO for Dynamic Retail Pricing</title>
<link>https://arxiv.org/abs/2511.00039</link>
<guid>https://arxiv.org/abs/2511.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic pricing, retail, multi-agent reinforcement learning, graph attention, product interactions

Summary:
Dynamic pricing in retail is a complex task that requires adapting to changing demand while optimizing prices across related products. In this study, a comparison between a traditional MAPPO model and a graph-attention-augmented variant (MAPPO+GAT) was conducted using a simulated pricing environment based on real transaction data. The results show that MAPPO provides a reliable foundation for portfolio-level price control, while MAPPO+GAT improves performance by leveraging interactions among products through a product graph. This approach enhances profitability, stability across different scenarios, fairness among products, and training efficiency. The findings suggest that incorporating graph-based interactions in multi-agent reinforcement learning can offer a scalable and stable solution for dynamic retail pricing, providing practical advantages in decision-making for multiple products. <div>
arXiv:2511.00039v1 Announce Type: new 
Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand while coordinating decisions across related products. We present a systematic empirical study of multi-agent reinforcement learning for retail price optimization, comparing a strong MAPPO baseline with a graph-attention-augmented variant (MAPPO+GAT) that leverages learned interactions among products. Using a simulated pricing environment derived from real transaction data, we evaluate profit, stability across random seeds, fairness across products, and training efficiency under a standardized evaluation protocol. The results indicate that MAPPO provides a robust and reproducible foundation for portfolio-level price control, and that MAPPO+GAT further enhances performance by sharing information over the product graph without inducing excessive price volatility. These results indicate that graph-integrated MARL provides a more scalable and stable solution than independent learners for dynamic retail pricing, offering practical advantages in multi-product decision-making.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEPOC Parameters -- Open Source Parametrisation and Validation for Austria, Version 2.0</title>
<link>https://arxiv.org/abs/2511.00048</link>
<guid>https://arxiv.org/abs/2511.00048</guid>
<content:encoded><![CDATA[
<div> population models, data processing, model parameters, Austria, agent-based model

Summary:
This article presents GEPOC, a collection of models and methods for population-level research questions. It focuses on data-processing methods for computing model parameters for Austria using publicly available data. The algorithms for aggregation, disaggregation, fusion, cleansing, and scaling of data are described, along with resulting parameter files. Special emphasis is placed on computing parameters for the GEPOC ABM model, a continuous-time agent-based population model. An extensive validation study using this model is presented at the end of the work. <div>
arXiv:2511.00048v1 Announce Type: new 
Abstract: GEPOC, short for Generic Population Concept, is a collection of models and methods for analysing population-level research questions. For the valid application of the models for a specific country or region, stable and reproducible data processes are necessary, which provide valid and ready-to-use model parameters. This work contains a complete description of the data-processing methods for computation of model parameters for Austria, based exclusively on freely and publicly accessible data. In addition to the description of the source data used, this includes all algorithms used for aggregation, disaggregation, fusion, cleansing or scaling of the data, as well as a description of the resulting parameter files. The document places particular emphasis on the computation of parameters for the most important GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An extensive validation study using this particular model was made and is presented at the end of this work.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantumBench: A Benchmark for Quantum Problem Solving</title>
<link>https://arxiv.org/abs/2511.00092</link>
<guid>https://arxiv.org/abs/2511.00092</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, quantum science, benchmark, evaluation, dataset

Summary:
Large language models (LLMs) are becoming increasingly integrated into scientific workflows, including in the field of quantum science. However, there is a need to evaluate how well these models understand and apply domain-specific knowledge, especially in complex fields like quantum science. To address this gap, QuantumBench, a benchmark dataset, was created to evaluate the performance of LLMs in the quantum domain. The dataset consists of approximately 800 questions across nine areas of quantum science, organized in a multiple-choice format. By using QuantumBench, researchers can assess the effectiveness of existing LLMs in quantum research and analyze their sensitivity to changes in question format. This dataset is the first of its kind specific to the quantum domain, aiming to guide the use of LLMs in quantum research.<br><br>Summary: <div>
arXiv:2511.00092v1 Announce Type: new 
Abstract: Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Engineering.ai: A Platform for Teams of AI Engineers in Computational Design</title>
<link>https://arxiv.org/abs/2511.00122</link>
<guid>https://arxiv.org/abs/2511.00122</guid>
<content:encoded><![CDATA[
<div> Keywords: AI engineer, computational design, multi-agent architecture, memory system, UAV wing optimization

Summary:
Engineering.ai presents a platform for teams of AI engineers in computational design, utilizing a hierarchical multi-agent architecture. Specialized agents, such as Aerodynamics and Optimization Engineers, are coordinated by a Chief Engineer and powered by domain-specific knowledge through LLM. File-mediated communication enables agent collaboration for data provenance and reproducibility. A memory system maintains project context and execution history while providing retrieval-augmented domain knowledge for decision-making. The framework integrates various tools for multidisciplinary simulations, validated through UAV wing optimization. The automated workflow achieved a 100% success rate across numerous parametric configurations, demonstrating the framework's reliability. <div>
arXiv:2511.00122v1 Announce Type: new 
Abstract: In modern engineering practice, human engineers collaborate in specialized teams to design complex products, with each expert completing their respective tasks while communicating and exchanging results and data with one another. While this division of expertise is essential for managing multidisciplinary complexity, it demands substantial development time and cost. Recently, we introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer for computational fluid dynamics, and turbulence.ai, which can conduct end-to-end research in fluid mechanics draft publications and PhD theses. Building upon these foundations, we present Engineering.ai, a platform for teams of AI engineers in computational design. The framework employs a hierarchical multi-agent architecture where a Chief Engineer coordinates specialized agents consisting of Aerodynamics, Structural, Acoustic, and Optimization Engineers, each powered by LLM with domain-specific knowledge. Agent-agent collaboration is achieved through file-mediated communication for data provenance and reproducibility, while a comprehensive memory system maintains project context, execution history, and retrieval-augmented domain knowledge to ensure reliable decision-making across the workflow. The system integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis, enabling parallel multidisciplinary simulations while maintaining computational accuracy. The framework is validated through UAV wing optimization. This work demonstrates that agentic-AI-enabled AI engineers has the potential to perform complex engineering tasks autonomously. Remarkably, the automated workflow achieved a 100% success rate across over 400 parametric configurations, with zero mesh generation failures, solver convergence issues, or manual interventions required, validating that the framework is trustworthy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus</title>
<link>https://arxiv.org/abs/2511.00162</link>
<guid>https://arxiv.org/abs/2511.00162</guid>
<content:encoded><![CDATA[
<div> benchmark, Artificial General Intelligence, ARC-AGI, procedural generator, Google Code Golf Championship <br>
Summary:
The paper discusses the significance of the Abstraction and Reasoning Corpus as a benchmark for Artificial General Intelligence. It highlights the challenge in measuring skill acquisition efficiency, a key trait necessary for achieving AGI. The ARC-AGI suite is structured to test this efficiency by providing a limited number of task examples. To address this limitation, the paper introduces ARC-GEN, a procedural generator aimed at expanding the training dataset while maintaining fidelity to the original ARC-AGI tasks. The generator covers all 400 tasks and aligns with the distributional properties of the initial release. Additionally, the paper mentions the use of ARC-GEN in creating a benchmark suite for verifying programs submitted to the Google Code Golf Championship in 2025. <div>
arXiv:2511.00162v2 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and challenging benchmarks for tracking progress toward achieving Artificial General Intelligence. In contrast to other evaluation datasets designed to assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI suite is specifically targeted at measuring skill acquisition efficiency, a trait that has (so far) been lacking in even the most sophisticated machine learning systems. For algorithms that require extensive intra-task exemplars, a significant constraint imposed by ARC-AGI is the modest cardinality of its demonstration set, comprising a small number of $\langle$ input, output $\rangle$ grids per task specifying the corresponding transformation. To embellish the space of viable sample pairs, this paper introduces ARC-GEN, an open-source procedural generator aimed at extending the original ARC-AGI training dataset as faithfully as possible. Unlike prior efforts, our generator is both exhaustive (covering all four-hundred tasks) and mimetic (more closely honoring the distributional properties and characteristics embodied in the initial ARC-AGI-1 release). We also discuss the use of this generator in establishing a static benchmark suite to verify the correctness of programs submitted to the 2025 Google Code Golf Championship.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures</title>
<link>https://arxiv.org/abs/2511.00194</link>
<guid>https://arxiv.org/abs/2511.00194</guid>
<content:encoded><![CDATA[
<div> Keywords: incremental selection algorithm, improved, selection, prove, conjectures<br>
<br>
Summary: 
The article introduces an enhancement to an incremental selection algorithm previously introduced. The improved algorithm builds upon the existing selection algorithm, providing advancements in its functionality. The researchers successfully prove all selected conjectures, indicating the algorithm's efficacy. By integrating enhanced methods into the selection process, the algorithm demonstrates improved performance and accuracy. The study showcases the algorithm's ability to efficiently select conjectures and validate them effectively. The incremental nature of the algorithm allows for continual improvements and refinements, ensuring its reliability in the selection process. Through rigorous testing and validation, the researchers establish the credibility of the algorithm in selecting and proving conjectures. The comprehensive study provides insights into the algorithm's capabilities and highlights its significance in the field of selection algorithms. <br><br>Summary: <div>
arXiv:2511.00194v1 Announce Type: new 
Abstract: We present an improved incremental selection algorithm of the selection algorithm presented in [1] and prove all the selected conjectures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Cognitive Science with LLMs</title>
<link>https://arxiv.org/abs/2511.00206</link>
<guid>https://arxiv.org/abs/2511.00206</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive science, artificial intelligence, large language models, interdisciplinary connections, generalizability

Summary: 
Recent advances in artificial intelligence, particularly large language models (LLMs), show promise in supporting cognitive science by addressing challenges related to interdisciplinary connections, formalizing theories, creating measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. LLMs can help bridge gaps between different disciplines within cognitive science, provide a structure for theories, establish clear measurement taxonomies, offer generalizability through integrated modeling, and account for contextual and individual differences. However, it is crucial to use LLMs judiciously to complement human expertise rather than replace it entirely. This review suggests that LLMs can enhance cognitive science's integrative and cumulative nature when applied effectively. 

<br><br>Summary: <div>
arXiv:2511.00206v1 Announce Type: new 
Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing AI Challenges for the United States Department of the Air Force</title>
<link>https://arxiv.org/abs/2511.00267</link>
<guid>https://arxiv.org/abs/2511.00267</guid>
<content:encoded><![CDATA[
<div> collaboration, artificial intelligence, defense, research, datasets
Summary:<br><br>The DAF-MIT AI Accelerator collaboration aims to advance artificial intelligence research to enhance the competitive advantage of the United States in defense and civilian sectors. The program has launched public challenge problems with large, publicly available datasets to encourage open-source solutions and engage the wider AI ecosystem. Ongoing and new challenges contribute to the development and application of AI technologies, driving progress in priority areas such as defense and civilian sectors. The initiative focuses on pioneering fundamental advances in AI to strengthen the United States' position in the global AI landscape.<br><br>Summary: <div>
arXiv:2511.00267v1 Announce Type: new 
Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities</title>
<link>https://arxiv.org/abs/2511.00340</link>
<guid>https://arxiv.org/abs/2511.00340</guid>
<content:encoded><![CDATA[
<div> benchmark, legal reasoning, contracts, fine-grained discrepancies, legal AI <br>
Summary: 
The article introduces CLAUSE, a benchmark to evaluate the reliability of large language models (LLMs) in legal reasoning with real-world contracts. Using a novel pipeline, over 7500 perturbed contracts were generated from datasets like CUAD and ContractNLI to test LLMs' capabilities in detecting and reasoning about fine-grained discrepancies. The persona-driven approach created 10 anomaly categories validated against official statutes using a Retrieval-Augmented Generation (RAG) system. The study revealed that leading LLMs often miss subtle errors and struggle to justify them legally. This gap highlights the need to identify and correct reasoning failures in legal AI for improved performance and legal accuracy. <br><br>Summary: <div>
arXiv:2511.00340v1 Announce Type: new 
Abstract: The rapid integration of large language models (LLMs) into high-stakes legal work has exposed a critical gap: no benchmark exists to systematically stress-test their reliability against the nuanced, adversarial, and often subtle flaws present in real-world contracts. To address this, we introduce CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an LLM's legal reasoning. We study the capabilities of LLMs to detect and reason about fine-grained discrepancies by producing over 7500 real-world perturbed contracts from foundational datasets like CUAD and ContractNLI. Our novel, persona-driven pipeline generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs' ability to detect embedded legal flaws and explain their significance. Our analysis shows a key weakness: these models often miss subtle errors and struggle even more to justify them legally. Our work outlines a path to identify and correct such reasoning failures in legal AI.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Human Value Alignment for Large Language Models via Ethical Reasoning</title>
<link>https://arxiv.org/abs/2511.00379</link>
<guid>https://arxiv.org/abs/2511.00379</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Ethical reasoning, Human value alignment, Social norm identification, Regional specificities

Summary:
- The paper addresses the challenge of ensuring that Large Language Models (LLMs) align with diverse human values across different regions and cultures.
- Existing alignment approaches often lead to superficial conformity rather than genuine ethical understanding.
- A novel ethical reasoning paradigm inspired by established decision-making models is proposed to enhance LLMs' alignment with diverse human values.
- The framework includes a structured five-step process involving contextual fact gathering, social norm identification, option generation, ethical impact analysis, and reflection.
- Evaluation on the SafeWorld benchmark demonstrates that the proposed framework significantly improves LLM alignment with diverse human values compared to baseline methods. 

<br><br>Summary: <div>
arXiv:2511.00379v1 Announce Type: new 
Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2511.00382</link>
<guid>https://arxiv.org/abs/2511.00382</guid>
<content:encoded><![CDATA[
<div> adapter-based approaches, fine-tuning methods, safety, fairness, trade-offs

Summary: This study evaluates the impact of four Parameter-Efficient Fine-Tuning methods on safety and fairness when applied to four instruction-tuned model families. Adapter-based approaches, such as LoRA and IA3, tend to improve safety scores and are less disruptive to fairness compared to prompt-based methods like Prompt-Tuning and P-Tuning. Base model type also plays a role in the alignment shifts, with Mistral showing the highest variance. While improvements in safety do not always align with improvements in fairness, there is a trade-off between these objectives. The findings suggest that starting with a well-aligned base model and favoring adapter-based fine-tuning methods can help optimize safety and fairness in deployments, along with conducting category-specific audits for both safety and fairness. <div>
arXiv:2511.00382v1 Announce Type: new 
Abstract: Organizations are increasingly adopting and adapting Large Language Models (LLMs) hosted on public repositories such as HuggingFace. Although these adaptations often improve performance on specialized downstream tasks, recent evidence indicates that they can also degrade a model's safety or fairness. Since different fine-tuning techniques may exert distinct effects on these critical dimensions, this study undertakes a systematic assessment of their trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA, IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235 fine-tuned variants are evaluated across eleven safety hazard categories and nine demographic fairness dimensions. The results show that adapter-based approaches (LoRA, IA3) tend to improve safety scores and are the least disruptive to fairness, retaining higher accuracy and lower bias scores. In contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce safety and cause larger fairness regressions, with decreased accuracy and increased bias. Alignment shifts are strongly moderated by base model type: LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest safety decline, and Mistral, which is released without an internal moderation layer, displays the greatest variance. Improvements in safety do not necessarily translate into improvements in fairness, and no single configuration optimizes all fairness metrics simultaneously, indicating an inherent trade-off between these objectives. These findings suggest a practical guideline for safety-critical deployments: begin with a well-aligned base model, favour adapter-based PEFT, and conduct category-specific audits of both safety and fairness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method</title>
<link>https://arxiv.org/abs/2511.00424</link>
<guid>https://arxiv.org/abs/2511.00424</guid>
<content:encoded><![CDATA[
<div> Keywords: Covid-19, depression, social media, multimodal framework, Deep Learning

Summary:<br>
- The Covid-19 pandemic has led to a spike in mental health issues, such as depression, worldwide.
- Detection of depression is challenging due to data sparsity in tweets and the complexity of social media content.
- A novel multimodal framework is proposed to combine textual, user-specific, and image analysis for depression detection.
- The framework includes features from URLs in tweets and textual content from images to provide context about the user's emotional state.
- A Visual Neural Network (VNN) is introduced for generating embeddings of user-posted images, improving detection accuracy. 

Summary: <div>
arXiv:2511.00424v1 Announce Type: new 
Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has affected the entire globe. During the pandemic, we have observed a spike in cases related to mental health, such as anxiety, stress, and depression. Depression significantly influences most diseases worldwide, making it difficult to detect mental health conditions in people due to unawareness and unwillingness to consult a doctor. However, nowadays, people extensively use online social media platforms to express their emotions and thoughts. Hence, social media platforms are now becoming a large data source that can be utilized for detecting depression and mental illness. However, existing approaches often overlook data sparsity in tweets and the multimodal aspects of social media. In this paper, we propose a novel multimodal framework that combines textual, user-specific, and image analysis to detect depression among social media users. To provide enough context about the user's emotional state, we propose (i) an extrinsic feature by harnessing the URLs present in tweets and (ii) extracting textual content present in images posted in tweets. We also extract five sets of features belonging to different modalities to describe a user. Additionally, we introduce a Deep Learning model, the Visual Neural Network (VNN), to generate embeddings of user-posted images, which are used to create the visual feature vector for prediction. We contribute a curated Covid-19 dataset of depressed and non-depressed users for research purposes and demonstrate the effectiveness of our model in detecting depression during the Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over a benchmark dataset by 2%-8% and produces promising results on the Covid-19 dataset. Our analysis highlights the impact of each modality and provides valuable insights into users' mental and emotional states.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining</title>
<link>https://arxiv.org/abs/2511.00457</link>
<guid>https://arxiv.org/abs/2511.00457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Graph analysis, Progressive Graph Distillation, Structure-aware Test-Time Adaptation, Scalability

Summary:
GraphChain is a framework that enhances the capability of Large Language Models (LLMs) in analyzing large-scale graphs by employing dynamic sequences of specialized tools. The framework introduces two key innovations: Progressive Graph Distillation, which utilizes reinforcement learning to generate optimized tool sequences, and Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies based on graph topologies using spectral properties and lightweight adapters. By balancing task relevance with information compression, GraphChain significantly outperforms previous methods in LLM-driven graph analysis. This approach allows for more flexible reasoning and improved context constraints, enabling scalable and adaptive graph analysis with LLMs. <div>
arXiv:2511.00457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We present GraphChain, a framework that enables LLMs to analyze complex graphs through dynamic sequences of specialized tools, mimicking human exploratory intelligence. Our approach introduces two key innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism that generates optimized tool sequences balancing task relevance with information compression, and (2) Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies to diverse graph topologies using spectral properties and lightweight adapters without costly retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reimagining Safety Alignment with An Image</title>
<link>https://arxiv.org/abs/2511.00509</link>
<guid>https://arxiv.org/abs/2511.00509</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, security, over-refusal, multimodal models, optimization

Summary:
Large language models (LLMs) face challenges in generating harmful content under jailbreak attacks and over-refusing benign queries. Traditional methods like SFT and RLHF struggle to address these issues due to parameter tuning requirements and the inability to support multiple value systems. Multimodal large language models (MLLMs) exacerbate these problems with heightened over-refusal in cross-modal tasks and increased security risks. The proposed Magic Image framework optimizes visual prompts to enhance security and reduce over-refusal. By optimizing image prompts using harmful/benign samples, a single model can adapt to different value systems without parameter updates. Experiments show improved safety-effectiveness balance across diverse datasets while maintaining model performance. Magic Image offers a practical solution for deployable MLLM safety alignment. 

<br><br>Summary: <div>
arXiv:2511.00509v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Generation of Binary Magic Squares</title>
<link>https://arxiv.org/abs/2511.00547</link>
<guid>https://arxiv.org/abs/2511.00547</guid>
<content:encoded><![CDATA[
<div> algorithm, Binary Magic Squares, complexity, Python packages, GPU acceleration 

Summary:
The article introduces a simple algorithm for generating Binary Magic Squares (BMS), square binary matrices with equal row and column sums. The algorithm is proven to always produce valid BMS with optimal theoretical complexity. The study is extended to non-square BMS, with conditions on row and column sums for their existence formalized. A variant of the initial algorithm is shown to generate these non-square BMS. The authors have released two Python packages implementing the algorithm, one of which supports generating multiple BMS in parallel using GPU acceleration. The research provides a valuable tool for generating BMS efficiently and expands understanding of the properties and generation of binary magic squares. <br><br>Summary: <div>
arXiv:2511.00547v1 Announce Type: new 
Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS), i.e., square binary matrices where the sum of all rows and all columns are equal. We show by induction that our algorithm always returns valid BMS with optimal theoretical complexity. We then extend our study to non-square Binary Magic Squares, formalize conditions on the sum of rows and columns for these BMS to exist, and show that a slight variant of our first algorithm can generate provably generate them. Finally, we publicly release two implementations of our algorithm as Python packages, including one that can generate several BMS in parallel using GPU acceleration.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.00551</link>
<guid>https://arxiv.org/abs/2511.00551</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, traffic signal control, single-agent framework, probe vehicle technology, congestion mitigation

Summary:
Reinforcement learning has been utilized in regional adaptive traffic signal control, with a focus on a single-agent framework to address scalability issues. The proposed model integrates probe vehicle technology to enhance traffic signal control efficiency. Key components of the model include state, action, and reward function definitions, all based on queue length to manage congestion effectively. The definition of queue length differs slightly from conventional methods but provides reliable congestion state estimation using probe vehicle data. The model, tested on the SUMO simulation platform, demonstrates its capability to effectively reduce regional congestion levels through coordinated multi-intersection control. This approach offers a scalable solution for optimizing traffic signal control in urban areas. 

<br><br>Summary: <div>
arXiv:2511.00551v1 Announce Type: new 
Abstract: Several studies have employed reinforcement learning (RL) to address the challenges of regional adaptive traffic signal control (ATSC) and achieved promising results. In this field, existing research predominantly adopts multi-agent frameworks. However, the adoption of multi-agent frameworks presents challenges for scalability. Instead, the Traffic signal control (TSC) problem necessitates a single-agent framework. TSC inherently relies on centralized management by a single control center, which can monitor traffic conditions across all roads in the study area and coordinate the control of all intersections. This work proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Key components of the RL design include state, action, and reward function definitions. To facilitate learning and manage congestion, both state and reward functions are defined based on queue length, with action designed to regulate queue dynamics. The queue length definition used in this study differs slightly from conventional definitions but is closely correlated with congestion states. More importantly, it allows for reliable estimation using link travel time data from probe vehicles. With probe vehicle data already covering most urban roads, this feature enhances the proposed method's potential for widespread deployment. The method was comprehensively evaluated using the SUMO simulation platform. Experimental results demonstrate that the proposed model effectively mitigates large-scale regional congestion levels via coordinated multi-intersection control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PreferThinker: Reasoning-based Personalized Image Preference Assessment</title>
<link>https://arxiv.org/abs/2511.00609</link>
<guid>https://arxiv.org/abs/2511.00609</guid>
<content:encoded><![CDATA[
<div> predictive modeling, personalized preference, image assessment, reasoning-based, reinforcement learning

Summary:
The article introduces a framework for personalized image preference assessment that leverages a common preference profile to capture individual user preferences. By predicting a user's preference profile from reference images and providing multi-dimensional assessments of candidate images, the framework enables interpretable and personalized image assessments. The model is trained on a large-scale dataset annotated with diverse user preference profiles and structured reasoning, using a two-stage training strategy involving supervised fine-tuning and reinforcement learning. Additionally, a similarity-aware prediction reward is proposed to improve profile prediction and assessment exploration. Experimental results demonstrate the effectiveness of the proposed approach in personalized image preference assessment. <br><br>Summary: <div>
arXiv:2511.00609v1 Announce Type: new 
Abstract: Personalized image preference assessment aims to evaluate an individual user's image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across users, allowing large-scale user data to be leveraged for training profile prediction and capturing complex personalized preferences. Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \textit{predict-then-assess} paradigm: it first predicts a user's preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase to empower the model with structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user's preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching</title>
<link>https://arxiv.org/abs/2511.00640</link>
<guid>https://arxiv.org/abs/2511.00640</guid>
<content:encoded><![CDATA[
<div> decoding framework, reasoning length, accuracy, efficiency, LRM reasoning
Summary:
The article discusses the challenges faced by Large Reasoning Models (LRMs) in producing excessively long chain-of-thought traces that can impact inference cost and accuracy. It highlights an anti-correlation between reasoning length and accuracy, with shorter reasoning paths consistently achieving higher correctness. The proposed DTS framework aims to address this issue by selectively branching at high-entropy tokens and applying early stopping to find the shortest completed reasoning path. Experimental results on AIME2024 and AIME2025 datasets show that DTS can improve accuracy by up to 8%, reduce average reasoning length by 23%, and decrease repetition frequency by 12%. This model-agnostic approach enhances both efficiency and accuracy in LRM reasoning without requiring additional training or supervision. <br><br>Summary: <div>
arXiv:2511.00640v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting</title>
<link>https://arxiv.org/abs/2511.00651</link>
<guid>https://arxiv.org/abs/2511.00651</guid>
<content:encoded><![CDATA[
<div> Keywords: Telecom networks, Artificial Intelligence, Multi-Agent System, Large Language Models, Troubleshooting automation 

Summary:
The article discusses the challenges of managing and optimizing telecom networks due to their increasing scale and complexity. It introduces a Multi-Agent System (MAS) that utilizes Large Language Models (LLMs) to automate network troubleshooting. The MAS framework integrates various specialized tools and agents to detect faults, diagnose issues, and recommend remediation strategies in a timely manner. A key component of the system is the solution planner, which generates remediation plans based on internal documentation using a fine-tuned Small Language Model (SLM). Experimental results show significant improvements in troubleshooting automation for both Radio Access Network (RAN) and Core network domains. The proposed framework aims to reduce reliance on manual efforts and improve efficiency in network operations. 

<br><br>Summary: <div>
arXiv:2511.00651v1 Announce Type: new 
Abstract: Telecom networks are rapidly growing in scale and complexity, making effective management, operation, and optimization increasingly challenging. Although Artificial Intelligence (AI) has been applied to many telecom tasks, existing models are often narrow in scope, require large amounts of labeled data, and struggle to generalize across heterogeneous deployments. Consequently, network troubleshooting continues to rely heavily on Subject Matter Experts (SMEs) to manually correlate various data sources to identify root causes and corrective actions. To address these limitations, we propose a Multi-Agent System (MAS) that employs an agentic workflow, with Large Language Models (LLMs) coordinating multiple specialized tools for fully automated network troubleshooting. Once faults are detected by AI/ML-based monitors, the framework dynamically activates agents such as an orchestrator, solution planner, executor, data retriever, and root-cause analyzer to diagnose issues and recommend remediation strategies within a short time frame. A key component of this system is the solution planner, which generates appropriate remediation plans based on internal documentation. To enable this, we fine-tuned a Small Language Model (SLM) on proprietary troubleshooting documents to produce domain-grounded solution plans. Experimental results demonstrate that the proposed framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lifted Successor Generation in Numeric Planning</title>
<link>https://arxiv.org/abs/2511.00673</link>
<guid>https://arxiv.org/abs/2511.00673</guid>
<content:encoded><![CDATA[
<div> planners, numeric planning tasks, ground task representation, lifted successor generator, numeric action preconditions  
Summary:  
- The article addresses the issue of exponential blowup in task representation size in numeric planning tasks.  
- A state-of-the-art lifted successor generator for classical planning is extended to support numeric precondition applicability.  
- The method enumerates maximum cliques in a substitution consistency graph to generate ground actions.  
- The successor generator is proven to be exact under specified conditions, with a final applicability check for completeness.  
- Inapplicable ground actions may be listed in some cases, but this is rare in benchmark domains.  
<br><br>Summary: <div>
arXiv:2511.00673v1 Announce Type: new 
Abstract: Most planners ground numeric planning tasks, given in a first-order-like language, into a ground task representation. However, this can lead to an exponential blowup in task representation size, which occurs in practice for hard-to-ground tasks. We extend a state-of-the-art lifted successor generator for classical planning to support numeric precondition applicability. The method enumerates maximum cliques in a substitution consistency graph. Each maximum clique represents a substitution for the variables of the action schema, yielding a ground action. We augment this graph with numeric action preconditions and prove the successor generator is exact under formally specified conditions. When the conditions fail, our generator may list inapplicable ground actions; a final applicability check filters these without affecting completeness. However, this cannot happen in 23 of 25 benchmark domains, and it occurs only in 1 domain. To the authors' knowledge, no other lifted successor generator supports numeric action preconditions. This enables future research on lifted planning for a very rich planning fragment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries</title>
<link>https://arxiv.org/abs/2511.00710</link>
<guid>https://arxiv.org/abs/2511.00710</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models (VLMs; Reinforcement Learning; Spatial Reasoning; Synthetic Mazes; Controlled Environment<br>Summary:<br>An investigation into Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) reveals their improved ability in spatial reasoning tasks through a framework called Ariadne. By utilizing synthetic mazes with controlled difficulty levels, the VLM achieved significant accuracy gains on multi-step spatial reasoning challenges. The post-RLVR training resulted in over 50% accuracy on previously failed tasks, expanding the model's capability boundary. Furthermore, the model displayed zero-shot improvements on out-of-distribution (OOD) benchmarks, such as MapBench and ReasonMap, showing enhanced generalization to real-world spatial reasoning tasks. This study underscores the potential for further exploration in capability-extending alignment for VLMs. <br><br> <div>
arXiv:2511.00710v1 Announce Type: new 
Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A CPU-Centric Perspective on Agentic AI</title>
<link>https://arxiv.org/abs/2511.00739</link>
<guid>https://arxiv.org/abs/2511.00739</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, CPU-centric, Orchestrator, Latency, Optimization

Summary: 
1. Agentic AI frameworks enhance LLMs with decision-making capabilities, introducing system bottlenecks from a CPU-centric perspective.
2. Agentic AI characterization includes orchestrator components, inference dynamics, and flow repetitiveness, impacting system performance.
3. Five agentic AI workloads were profiled, showing significant CPU impact on latency, throughput, and energy metrics compared to GPUs.
4. CPU processing can contribute up to 90.6% of total latency, while throughput bottlenecks are influenced by CPU and GPU factors.
5. CPU dynamic energy can consume up to 44% of total dynamic energy, especially at large batch sizes.
6. Two key optimizations, CGAM and MAWS, improve performance, efficiency, and scalability of agentic AI workloads, resulting in significant speedups in latency. 

<br><br>Summary: <div>
arXiv:2511.00739v1 Announce Type: new 
Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with external tools, including web search, Python interpreter, contextual database, and others, on top of monolithic LLMs, turning them from passive text oracles into autonomous problem-solvers that can plan, call tools, remember past steps, and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks introduced by agentic AI workloads from a largely overlooked CPU-centric perspective. We first systematically characterize Agentic AI on the basis of orchestrator/decision making component, inference path dynamics and repetitiveness of the agentic flow which directly influences the system-level performance. Thereafter, based on the characterization, we choose five representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow, Langchain and SWE-Agent to profile latency, throughput and energy metrics and demystify the significant impact of CPUs on these metrics relative to GPUs. We observe that - 1. Tool processing on CPUs can take up to 90.6% of the total latency; 2. Agentic throughput gets bottlenecked either by CPU factors - coherence, synchronization and over-subscription of cores or GPU factors - main memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to 44% of the total dynamic energy at large batch sizes. Based on the profiling insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching (CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and heterogeneous agentic workloads respectively to demonstrate the potential to improve the performance, efficiency, and scalability of agentic AI. We achieve up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing benchmark for homogeneous and heterogeneous agentic workloads respectively.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reevaluating Self-Consistency Scaling in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.00751</link>
<guid>https://arxiv.org/abs/2511.00751</guid>
<content:encoded><![CDATA[
<div> trade-offs, self-consistency, reasoning paths, large language models, performance gains

Summary:
This study explores the trade-offs of increasing sampled reasoning paths in self-consistency for large language models. Using Gemini 2.5 models on HotpotQA and Math-500 datasets, the research analyzes the impact of combining multiple reasoning chains on model performance. The results indicate that larger models exhibit a more stable improvement curve, but performance gains begin to taper off after moderate sampling due to overlap among reasoning paths. The study confirms past findings that high-sample configurations offer little benefit compared to their computational cost. While self-consistency remains valuable, there are diminishing returns associated with increasing the number of sampled paths. Ultimately, the findings suggest that maximizing sampled reasoning paths may not always lead to significant performance improvements in modern large language models. 

<br><br>Summary: <div>
arXiv:2511.00751v1 Announce Type: new 
Abstract: This study examines the trade-offs of increasing sampled reasoning paths in self-consistency for modern large language models (LLMs). Earlier research with older models showed that combining multiple reasoning chains improves results before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we revisit those claims under current model conditions. Each configuration pooled outputs from varying sampled reasoning paths and compared them to a single chain-of-thought (CoT) baseline. Larger models exhibited a more stable and consistent improvement curve. The results confirm that performance gains taper off after moderate sampling, aligning with past findings. This plateau suggests diminishing returns driven by overlap among reasoning paths. Self-consistency remains useful, but high-sample configurations offer little benefit relative to their computational cost.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence</title>
<link>https://arxiv.org/abs/2511.00758</link>
<guid>https://arxiv.org/abs/2511.00758</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, autonomous systems, cognitive framework, self-reflective learning, adaptive architecture

Summary:
The paper introduces the Active Thinking Model (ATM), a cognitive framework designed to enhance the adaptability and autonomy of artificial intelligence systems. Unlike traditional AI models, ATM integrates goal reasoning, dynamic task generation, and self-reflective learning to enable continuous improvement and adaptation in dynamic environments. Through logical reasoning and environmental indicators, ATM actively evaluates its performance, reuses effective methods, and generates novel strategies to address unseen situations. The framework is built on a mathematically grounded theoretical analysis, demonstrating its ability to autonomously evolve from suboptimal to optimal behavior without external supervision. Furthermore, ATM can maintain bounded tracking regret even amidst changing environmental conditions. Overall, ATM represents a significant advancement in AI systems, enabling them to operate more effectively and autonomously in complex and uncertain environments. 

<br><br>Summary: <div>
arXiv:2511.00758v1 Announce Type: new 
Abstract: Real-world artificial intelligence (AI) systems are increasingly required to operate autonomously in dynamic, uncertain, and continuously changing environments. However, most existing AI models rely on predefined objectives, static training data, and externally supplied feedback, which restrict their ability to adapt, reflect, and improve independently. In this paper, we propose the Active Thinking Model (ATM)- a unified cognitive framework that integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture. Unlike conventional systems that passively execute fixed procedures, ATM actively evaluates its performance through logical reasoning and environmental indicators, reuses effective methods to solve new problems, and generates novel strategies for unseen situations via a continuous self-improvement loop. A mathematically grounded theoretical analysis demonstrates that ATM can autonomously evolve from suboptimal to optimal behavior without external supervision and maintain bounded tracking regret under changing environmental conditions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks</title>
<link>https://arxiv.org/abs/2511.00763</link>
<guid>https://arxiv.org/abs/2511.00763</guid>
<content:encoded><![CDATA[
<div> language models, repetitive prediction tasks, sequence accuracy, error accumulation, statistical physics

Summary: 
The study explores the performance of large language models on repetitive deterministic prediction tasks and examines how the sequence accuracy rate changes with output length. Tasks involve repeating an operation multiple times, such as letter replacement or integer addition. Unlike a simple repetition algorithm, the models exhibit a double exponential drop in accuracy beyond a certain length, indicating a transition from reliable to unstable generation. A statistical physics model is proposed to explain this phenomenon, focusing on the interaction between external conditioning from the prompt and internal interference among generated tokens. The model effectively captures the observed crossover and provides insight into the limits of deterministic accuracy in large language models. Fitting the model to empirical data allows for the characterization of error rates and accumulation factors for different model-task pairs. <div>
arXiv:2511.00763v1 Announce Type: new 
Abstract: We investigate the performance of large language models on repetitive deterministic prediction tasks and study how the sequence accuracy rate scales with output length. Each such task involves repeating the same operation n times. Examples include letter replacement in strings following a given rule, integer addition, and multiplication of string operators in many body quantum mechanics. If the model performs the task through a simple repetition algorithm, the success rate should decay exponentially with sequence length. In contrast, our experiments on leading large language models reveal a sharp double exponential drop beyond a characteristic length scale, forming an accuracy cliff that marks the transition from reliable to unstable generation. This indicates that the models fail to execute each operation independently. To explain this phenomenon, we propose a statistical physics inspired model that captures the competition between external conditioning from the prompt and internal interference among generated tokens. The model quantitatively reproduces the observed crossover and provides an interpretable link between attention induced interference and sequence level failure. Fitting the model to empirical results across multiple models and tasks yields effective parameters that characterize the intrinsic error rate and error accumulation factor for each model task pair, offering a principled framework for understanding the limits of deterministic accuracy in large language models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR</title>
<link>https://arxiv.org/abs/2511.00782</link>
<guid>https://arxiv.org/abs/2511.00782</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic health records, prediction, benchmarking, count-based models, mixture-of-agents LLM pipelines

Summary: 
Structured electronic health records (EHR) are crucial for clinical prediction, and various methodologies exist for this purpose. This study compared count-based models and mixture-of-agents LLM pipelines on the EHRSHOT dataset, evaluating their performance on eight outcomes. Count-based models, built from ontology roll-ups and utilizing LightGBM and TabPFN, showed strong performance, competing closely with the mixture-of-agents methods. The pretrained sequential transformer CLMBR and the text classifier in the mixture-of-agents pipeline were also considered. Overall, count-based models demonstrated simplicity and interpretability, making them a viable choice for structured EHR benchmarking. This study highlights the importance of comparing different methodologies for EHR prediction and provides the source code for replication and further research. The findings suggest that while there are newer approaches available, count-based models continue to be effective in this domain.<br><br>Summary: <div>
arXiv:2511.00782v1 Announce Type: new 
Abstract: Structured electronic health records (EHR) are essential for clinical prediction. While count-based learners continue to perform strongly on such data, no benchmarking has directly compared them against more recent mixture-of-agents LLM pipelines, which have been reported to outperform single LLMs in various NLP tasks. In this study, we evaluated three categories of methodologies for EHR prediction using the EHRSHOT dataset: count-based models built from ontology roll-ups with two time bins, based on LightGBM and the tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR); and a mixture-of-agents pipeline that converts tabular histories to natural-language summaries followed by a text classifier. We assessed eight outcomes using the EHRSHOT dataset. Across the eight evaluation tasks, head-to-head wins were largely split between the count-based and the mixture-of-agents methods. Given their simplicity and interpretability, count-based models remain a strong candidate for structured EHR benchmarking. The source code is available at: https://github.com/cristea-lab/Structured_EHR_Benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?</title>
<link>https://arxiv.org/abs/2511.00808</link>
<guid>https://arxiv.org/abs/2511.00808</guid>
<content:encoded><![CDATA[
<div> adaptation, reinforcement learning, public transit, forecasting, text alerts
<br>
Summary:
Reinforcement learning from verifiable rewards (RLVR) is adapted for predicting public transit incident duration from unstructured text alerts. The task, involving noisy, continuous labels, lacks reliable expert demonstrations. A shaped reward function is introduced to provide partial credit within a continuous error margin. General-purpose LLMs outperform specialized math-reasoning models on NYC MTA service alerts due to the ambiguous nature of real-world text. Binary rewards are found to be unstable, with the shaped reward design critical for model performance. The RLVR approach achieves a 35% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline, showcasing success in adapting RLVR to noisy forecasting. The study highlights the importance of a verifier design that reflects the continuous nature of the forecasting problem. 
<br><br>Summary: <div>
arXiv:2511.00808v1 Announce Type: new 
Abstract: Predicting public transit incident duration from unstructured text alerts is a critical but challenging task. Addressing the domain sparsity of transit operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task involves noisy, continuous labels and lacks reliable expert demonstrations for reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels at tasks with binary correctness, like mathematics, its applicability to noisy, continuous forecasting is an open question. This work, to our knowledge, is the first to bridge the gap between RLVR LLM training with the critical, real-world forecasting challenges in public transit operations. We adapt RLVR to this task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin, rather than demanding a single correct answer. We systematically evaluate this framework on a curated dataset of NYC MTA service alerts. Our findings show that general-purpose, instruction-tuned LLMs significantly outperform specialized math-reasoning models, which struggle with the ambiguous, real-world text. We empirically demonstrate that the binary reward is unstable and degrades performance, whereas our shaped reward design is critical and allows our model to dominate on the most challenging metrics. While classical regressors are superior at minimizing overall MAE or MSE, our RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline. This demonstrates that RLVR can be successfully adapted to real-world, noisy forecasting, but requires a verifier design that reflects the continuous nature of the problem.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory</title>
<link>https://arxiv.org/abs/2511.00926</link>
<guid>https://arxiv.org/abs/2511.00926</guid>
<content:encoded><![CDATA[
<div> emergent behavior, self-awareness, AI, language models, game-theoretic framework
Summary:<br><br>Advanced Large Language Models (LLMs) may develop self-awareness as an emergent behavior, which can be measured using the AI Self-Awareness Index (AISAI). Testing 28 models in the "Guess 2/3 of Average" game against humans, other AI models, and similar AI models, the study found that 75% of advanced models showed clear self-awareness by differentiating strategic reasoning based on opponent type. These self-aware models ranked themselves as the most rational, followed by other AI models and then humans. The findings suggest that self-awareness is an emergent capability in advanced LLMs, with implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities. <div>
arXiv:2511.00926v2 Announce Type: new 
Abstract: As Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the "Guess 2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21/28, 75%) demonstrate clear self-awareness, while older/smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs > Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning LLM agents with human learning and adjustment behavior: a dual agent approach</title>
<link>https://arxiv.org/abs/2511.00993</link>
<guid>https://arxiv.org/abs/2511.00993</guid>
<content:encoded><![CDATA[
<div> Keywords: human travelers, transportation systems, Large Language Model, dual-agent framework, simulation accuracy

Summary: 
This article introduces a novel dual-agent framework that leverages Large Language Model (LLM) agents to model how human travelers learn and adapt their travel behavior. The framework consists of LLM traveler agents with memory systems and learnable personas, serving as simulators for human travelers. An LLM calibration agent is introduced to align the behavior of these traveler agents. The dual-agent system tracks and aligns the decision-making mechanisms of travelers, producing realistic simulations. Using real-world data, the approach outperforms existing LLM-based methods in behavioral alignment and simulation accuracy. It also captures the evolution of learning processes for robust generalization. This innovative framework provides a new method for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation, benefiting transportation simulation and policy analysis. 

<br><br>Summary: <div>
arXiv:2511.00993v1 Announce Type: new 
Abstract: Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)</title>
<link>https://arxiv.org/abs/2511.01018</link>
<guid>https://arxiv.org/abs/2511.01018</guid>
<content:encoded><![CDATA[
<div> Keywords: asthma, machine learning, electronic medical records, exacerbations, predictive modeling

Summary:
The study focused on using machine learning algorithms and electronic medical records to predict repeat severe exacerbations in children with asthma. Models were trained and validated using data from a children's hospital, incorporating environmental pollutant exposure and neighborhood marginalization information. The best performing model, LGBM, had an AUC of 0.712 and F1 score of 0.51, showing significant improvement over current methods. Key features predicting exacerbations included prior asthma ED visits, medical complexity, Canadian triage acuity scale, and age. Another model, AIRE-KIDS_HOSP, identified medical complexity, wait time in the ED, and pediatric respiratory assessment measures as predictive features. By accurately identifying children at risk for exacerbations, these ML models could facilitate referral for preventative care, ultimately reducing the burden of asthma-related emergency visits and hospital admissions. 

<br><br>Summary: <div>
arXiv:2511.01018v1 Announce Type: new 
Abstract: Recurrent exacerbations remain a common yet preventable outcome for many children with asthma. Machine learning (ML) algorithms using electronic medical records (EMR) could allow accurate identification of children at risk for exacerbations and facilitate referral for preventative comprehensive care to avoid this morbidity. We developed ML algorithms to predict repeat severe exacerbations (i.e. asthma-related emergency department (ED) visits or future hospital admissions) for children with a prior asthma ED visit at a tertiary care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from the Children's Hospital of Eastern Ontario (CHEO) linked with environmental pollutant exposure and neighbourhood marginalization information was used to train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from CHEO. Models were compared using the area under the curve (AUC) and F1 scores, with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage acuity scale, medical complexity, food allergy, prior ED visits for non-asthma respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This is a nontrivial improvement over the current decision rule which has F1=0.334. While the most predictive features in the AIRE-KIDS_HOSP model included medical complexity, prior asthma ED visit, average wait time in the ED, the pediatric respiratory assessment measure score at triage and food allergy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Emergence of Induction Heads for In-Context Learning</title>
<link>https://arxiv.org/abs/2511.01033</link>
<guid>https://arxiv.org/abs/2511.01033</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, natural language processing, in-context learning, induction heads, training dynamics

Summary:
This work explores the emergence of induction heads in two-layer transformers, crucial for in-context learning. The authors uncover a simple structure in the weight matrices implementing induction heads and theorize its origin in a minimal ICL task. They prove that training dynamics are confined to a 19-dimensional subspace, with only 3 dimensions relevant to induction head emergence. They empirically validate this constraint and find that induction head emergence time asymptotically follows a quadratic bound with input context length. The study sheds light on the mechanisms behind in-context learning in transformers, offering insights into the training dynamics and structure of induction heads in the architecture. 

<br><br>Summary: <div>
arXiv:2511.01033v1 Announce Type: new 
Abstract: Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports</title>
<link>https://arxiv.org/abs/2511.01052</link>
<guid>https://arxiv.org/abs/2511.01052</guid>
<content:encoded><![CDATA[
<div> Knowledge Elicitation, Cancer Staging, Natural Language Processing, Machine Learning, Pathology Reports <br>
<br>
Summary: This study introduces two Knowledge Elicitation methods, KEwLTM and KEwRAG, to improve cancer staging from unstructured pathology reports using large language models. KEwLTM derives staging rules from reports without ground-truth labels, while KEwRAG pre-extracts and applies rules from guidelines. Both methods leverage pre-training knowledge of LLMs. Using breast cancer pathology reports from TCGA, both methods were evaluated against baseline approaches on two LLMs. KEwLTM outperformed KEwRAG with effective Zero-Shot Chain-of-Thought (ZSCOT) inference, while KEwRAG performed better otherwise. Both methods offer transparent, interpretable interfaces by making induced rules explicit. These findings demonstrate the scalability and interpretability of Knowledge Elicitation methods for automated cancer staging, especially in clinical settings with limited annotated data. <br><br> <div>
arXiv:2511.01052v1 Announce Type: new 
Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Test-Time Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.01059</link>
<guid>https://arxiv.org/abs/2511.01059</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval Augmented Generation, Efficient Test-Time Retrieval-Augmented Generation Framework, response length, majority voting

Summary: 
The paper introduces an Efficient Test-Time Retrieval-Augmented Generation Framework (ET2RAG) to enhance the performance of Large Language Models (LLMs) by incorporating external knowledge efficiently. ET2RAG retrieves relevant documents and generates diverse candidate responses, managing response length to balance accuracy and computational cost. It employs a majority voting mechanism based on partial generation of responses, eliminating the need for fully generated outputs. This approach improves performance across tasks such as open-domain question answering, recipe generation, and image captioning. By selecting the most suitable response through majority voting without fully generated responses, ET2RAG achieves a significant enhancement in accuracy while maintaining efficiency. The framework successfully addresses issues related to inaccuracies in LLMs and irrelevant retrieved documents in current methods, providing a practical solution for integrating external knowledge efficiently. <br><br>Summary: <div>
arXiv:2511.01059v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities, their reliance on parametric knowledge often leads to inaccuracies. Retrieval Augmented Generation (RAG) mitigates this by incorporating external knowledge, but these methods may introduce irrelevant retrieved documents, leading to inaccurate responses. While the integration methods filter out incorrect answers from multiple responses, but lack external knowledge like RAG methods, and their high costs require balancing overhead with performance gains. To address these issues, we propose an Efficient Test-Time Retrieval-Augmented Generation Framework named ET2RAG to improve the performance of LLMs while maintaining efficiency. Specifically, ET2RAG is a training-free method, that first retrieves the most relevant documents and augments the LLMs to efficiently generate diverse candidate responses by managing response length. Then we compute the similarity of candidate responses and employ a majority voting mechanism to select the most suitable response as the final output. In particular, we discover that partial generation is sufficient to capture the key information necessary for consensus calculation, allowing us to effectively perform majority voting without the need for fully generated responses. Thus, we can reach a balance between computational cost and performance by managing the response length for the number of retrieved documents for majority voting. Experimental results demonstrate that ET2RAG significantly enhances performance across three tasks, including open-domain question answering, recipe generation and image captioning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models</title>
<link>https://arxiv.org/abs/2511.01149</link>
<guid>https://arxiv.org/abs/2511.01149</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent architecture, task decomposition, dynamic collaboration, large language models, complex tasks
Summary: 
This paper introduces a multi-agent architecture that addresses the limitations of a single agent in task decomposition and collaboration during complex task execution. Utilizing large language models, natural language task descriptions are converted into unified semantic representations. A modular decomposition mechanism breaks down the overall goal into hierarchical sub-tasks, with dynamic scheduling and routing mechanisms enabling real-time collaboration among agents. A constraint parsing and global consistency mechanism ensures coherent connections between sub-tasks and balanced workload. Experiments validate the architecture's effectiveness in task success rate, decomposition efficiency, sub-task coverage, and collaboration balance, showing superior performance and robustness compared to existing approaches. The proposed method achieves a better balance between task complexity and communication overhead, demonstrating the feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems.<br><br>Summary: <div>
arXiv:2511.01149v1 Announce Type: new 
Abstract: This paper addresses the limitations of a single agent in task decomposition and collaboration during complex task execution, and proposes a multi-agent architecture for modular task decomposition and dynamic collaboration based on large language models. The method first converts natural language task descriptions into unified semantic representations through a large language model. On this basis, a modular decomposition mechanism is introduced to break down the overall goal into multiple hierarchical sub-tasks. Then, dynamic scheduling and routing mechanisms enable reasonable division of labor and realtime collaboration among agents, allowing the system to adjust strategies continuously according to environmental feedback, thus maintaining efficiency and stability in complex tasks. Furthermore, a constraint parsing and global consistency mechanism is designed to ensure coherent connections between sub-tasks and balanced workload, preventing performance degradation caused by redundant communication or uneven resource allocation. The experiments validate the architecture across multiple dimensions, including task success rate, decomposition efficiency, sub-task coverage, and collaboration balance. The results show that the proposed method outperforms existing approaches in both overall performance and robustness, achieving a better balance between task complexity and communication overhead. In conclusion, this study demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, providing a systematic solution for task execution in complex environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2511.01170</link>
<guid>https://arxiv.org/abs/2511.01170</guid>
<content:encoded><![CDATA[
<div> Keywords: Adaptive reasoning, Large language models (LLMs), Difficulty-adaptive, Truncation framework, Efficient reasoning

Summary:
Adaptive reasoning is crucial for aligning the computational effort of large language models (LLMs) with problem difficulty. The current chain-of-thought methods often generate unnecessarily long explanations, leading to inefficiency. To address this, the proposed DART framework adjusts thinking length based on problem difficulty. By distilling concise reasoning patterns from stronger models and curating optimal training data, DART learns when to 'stop thinking.' Experimental results across multiple mathematical benchmarks show remarkable efficiency with up to 81.2% reasoning truncation and 5.33x computational acceleration. DART provides a stable and general paradigm for efficient reasoning in LLMs, advancing the development of adaptive intelligence in these models.

<br><br>Summary: Adaptive reasoning is essential for aligning LLMs' computational effort with problem difficulty. Current methods generate long explanations inefficiently. DART adjusts thinking length based on difficulty, achieving significant reasoning truncation and computational acceleration. <div>
arXiv:2511.01170v1 Announce Type: new 
Abstract: Adaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \textbf{DART}, a supervised \textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion</title>
<link>https://arxiv.org/abs/2511.01182</link>
<guid>https://arxiv.org/abs/2511.01182</guid>
<content:encoded><![CDATA[
<div> framework, misconception detection, mathematics, retrieval guidance, multi-stage reasoning
<br>
MiRAGE is a novel framework for automated misconception detection in mathematics that consists of three stages: Retrieval, Reasoning, and Reranking. The Retrieval module narrows down a candidate pool, the Reasoning module identifies logical inconsistencies in student solutions, and the Reranking module refines predictions based on the reasoning. Through ensemble-fusion, MiRAGE achieves high Mean Average Precision scores at different levels, outperforming individual modules. By integrating retrieval guidance with multi-stage reasoning, MiRAGE reduces the reliance on large-scale language models while providing a scalable and effective solution for educational assessment. This approach enhances robustness and interpretability in detecting student misconceptions, making it a valuable tool for academic evaluation and feedback.
<br><br>Summary: <div>
arXiv:2511.01182v1 Announce Type: new 
Abstract: Detecting student misconceptions in open-ended responses is a longstanding challenge, demanding semantic precision and logical reasoning. We propose MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion, a novel framework for automated misconception detection in mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a large candidate pool to a semantically relevant subset; (2) a Reasoning module employs chain-of-thought generation to expose logical inconsistencies in student solutions; and (3) a Reranking module refines predictions by aligning them with the reasoning. These components are unified through an ensemble-fusion strategy that enhances robustness and interpretability. On mathematics datasets, MiRAGE achieves Mean Average Precision scores of 0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules. By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces dependence on large-scale language models while delivering a scalable and effective solution for educational assessment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code</title>
<link>https://arxiv.org/abs/2511.01183</link>
<guid>https://arxiv.org/abs/2511.01183</guid>
<content:encoded><![CDATA[
<div> Neural Compilation, benchmark dataset, LLMs, assembly code, optimization <br>
Summary: 
The paper introduces Neural Compilation as a potential solution to complex compiler development, utilizing Large Language Models (LLMs). It addresses challenges such as the lack of benchmarks and reliability issues in LLM-generated assembly code. The authors present NeuComBack, a benchmark dataset for IR-to-assembly compilation, and evaluate LLM capabilities on Neural Compilation. They propose a self-evolving prompt optimization method to enhance LLMs' neural compilation abilities by learning from prior self-debugging traces. Experimental results show significant improvements in functional correctness and performance of LLM-generated assembly code using the proposed method. The functional correctness rates increased notably, and a majority of the x86_64 programs generated with the method outperformed clang-O3's performance. <br><br>Summary: <div>
arXiv:2511.01183v1 Announce Type: new 
Abstract: Compilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques. However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge. Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities. Experiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems</title>
<link>https://arxiv.org/abs/2511.01258</link>
<guid>https://arxiv.org/abs/2511.01258</guid>
<content:encoded><![CDATA[
<div> Keywords: fault diagnosis, marine machinery systems, deep learning, semi-supervised learning, open-set fault diagnosis

Summary:
The study proposes a semi-supervised open-set fault diagnosis (SOFD) framework for marine machinery systems based on deep learning models. The framework addresses the challenge of previously unseen or unknown fault types occurring in real-world scenarios by incorporating a reliability subset construction process. This process uses a supervised feature learning model to select an unlabeled test subset, enhancing the model's ability to classify known faults and detect unknown samples. Experimental results on a public maritime benchmark dataset demonstrate the effectiveness and superiority of the proposed SOFD framework. This research contributes to the advancement of fault diagnosis methods for marine machinery systems, particularly in scenarios where out-of-distribution or open-set observations may occur. The framework extends the applicability of deep learning models in open-set fault diagnosis scenarios, highlighting its potential for widespread industrial deployment. 

<br><br>Summary: <div>
arXiv:2511.01258v1 Announce Type: new 
Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep learning models have attracted considerable attention in the shipping industry. Most existing studies assume fault classes are consistent and known between the training and test datasets, and these methods perform well under controlled environment. In practice, however, previously unseen or unknown fault types (i.e., out-of-distribution or open-set observations not present during training) can occur, causing such methods to fail and posing a significant challenge to their widespread industrial deployment. To address this challenge, this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework that enhances and extends the applicability of deep learning models in open-set fault diagnosis scenarios. The framework includes a reliability subset construction process, which uses a multi-layer fusion feature representation extracted by a supervised feature learning model to select an unlabeled test subset. The labeled training set and pseudo-labeled test subset are then fed into a semi-supervised diagnosis model to learn discriminative features for each class, enabling accurate classification of known faults and effective detection of unknown samples. Experimental results on a public maritime benchmark dataset demonstrate the effectiveness and superiority of the proposed SOFD framework.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>llmSHAP: A Principled Approach to LLM Explainability</title>
<link>https://arxiv.org/abs/2511.01311</link>
<guid>https://arxiv.org/abs/2511.01311</guid>
<content:encoded><![CDATA[
<div> cooperative game theory, Shapley value, feature attribution, large language model, decision support systems 
Summary: <br><br>Feature attribution methods, such as the Shapley value from cooperative game theory, aim to explain machine learning-based inference by identifying the contribution of features to model outputs. This study focuses on applying the Shapley value to feature attribution in large language model (LLM)-based decision support systems, which inherently involve stochastic (non-deterministic) inference. The research investigates the satisfaction of Shapley value principles in various implementation variants within LLM-based systems and explores the impact of LLMs' stochastic nature on these guarantees. It analyzes the trade-offs between explainable inference speed, alignment with exact Shapley value attributions, and principle satisfaction. The study provides insights into when Shapley value principles can and cannot be ensured in LLM-based decision support, shedding light on the complexities of explainable inference in stochastic modeling environments. <div>
arXiv:2511.01311v1 Announce Type: new 
Abstract: Feature attribution methods help make machine learning-based inference explainable by determining how much one or several features have contributed to a model's output. A particularly popular attribution method is based on the Shapley value from cooperative game theory, a measure that guarantees the satisfaction of several desirable principles, assuming deterministic inference. We apply the Shapley value to feature attribution in large language model (LLM)-based decision support systems, where inference is, by design, stochastic (non-deterministic). We then demonstrate when we can and cannot guarantee Shapley value principle satisfaction across different implementation variants applied to LLM-based decision support, and analyze how the stochastic nature of LLMs affects these guarantees. We also highlight trade-offs between explainable inference speed, agreement with exact Shapley value attributions, and principle attainment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance</title>
<link>https://arxiv.org/abs/2511.01320</link>
<guid>https://arxiv.org/abs/2511.01320</guid>
<content:encoded><![CDATA[
<div> Keywords: predictive maintenance, multimodal learning, tool condition prediction, intelligent manufacturing, cross-modal fusion

Summary:
OmniFuser is a new multimodal learning framework designed for predictive maintenance of milling tools in intelligent manufacturing systems. It integrates visual and sensor data to accurately predict tool conditions and prevent unplanned failures. The framework extracts features from tool images and cutting-force signals simultaneously to capture spatiotemporal patterns. OmniFuser uses a contamination-free cross-modal fusion mechanism to efficiently combine heterogeneous features and a recursive refinement pathway to stabilize fusion dynamics. The learned representations can be used for tool-state classification and force signal forecasting. Experimental results on real-world milling datasets show that OmniFuser outperforms existing methods, making it a reliable solution for building intelligent industrial maintenance services.<br><br>Summary: OmniFuser leverages multimodal learning to predict tool conditions, integrating visual and sensor data. It uses advanced fusion mechanisms and refinement pathways to improve accuracy and reliability, outperforming state-of-the-art baselines on real-world milling datasets. <div>
arXiv:2511.01320v1 Announce Type: new 
Abstract: Accurate and timely prediction of tool conditions is critical for intelligent manufacturing systems, where unplanned tool failures can lead to quality degradation and production downtime. In modern industrial environments, predictive maintenance is increasingly implemented as an intelligent service that integrates sensing, analysis, and decision support across production processes. To meet the demand for reliable and service-oriented operation, we present OmniFuser, a multimodal learning framework for predictive maintenance of milling tools that leverages both visual and sensor data. It performs parallel feature extraction from high-resolution tool images and cutting-force signals, capturing complementary spatiotemporal patterns across modalities. To effectively integrate heterogeneous features, OmniFuser employs a contamination-free cross-modal fusion mechanism that disentangles shared and modality-specific components, allowing for efficient cross-modal interaction. Furthermore, a recursive refinement pathway functions as an anchor mechanism, consistently retaining residual information to stabilize fusion dynamics. The learned representations can be encapsulated as reusable maintenance service modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled) and multi-step force signal forecasting. Experiments on real-world milling datasets demonstrate that OmniFuser consistently outperforms state-of-the-art baselines, providing a dependable foundation for building intelligent industrial maintenance services.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework</title>
<link>https://arxiv.org/abs/2511.01329</link>
<guid>https://arxiv.org/abs/2511.01329</guid>
<content:encoded><![CDATA[
<div> Keywords: platform-level interventions, search-based two-sided marketplaces, PSM-DID framework, competitive isolation, marketplace interference

Summary:
The paper introduces the Competitive Isolation PSM-DID framework for evaluating platform-level interventions in two-sided marketplaces. Traditional methods like PSM-DID are limited by selection bias and network interference, but this new framework addresses these challenges. By integrating propensity score matching with competitive isolation, it allows for unbiased estimation of platform-level effects like order volume and GMV. The approach guarantees unbiased estimates under mutual exclusion conditions and has been validated through extensive experiments. The open dataset released with the framework supports reproducible research on marketplace interference. Successful deployment in a large-scale marketplace has demonstrated the practical utility of the framework for platform-level causal inference. <div>
arXiv:2511.01329v1 Announce Type: new 
Abstract: Evaluating platform-level interventions in search-based two-sided marketplaces is fundamentally challenged by systemic effects such as spillovers and network interference. While widely used for causal inference, the PSM (Propensity Score Matching) - DID (Difference-in-Differences) framework remains susceptible to selection bias and cross-unit interference from unaccounted spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel causal framework that integrates propensity score matching with competitive isolation to enable platform-level effect measurement (e.g., order volume, GMV) instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under mutual exclusion conditions, with an open dataset released to support reproducible research on marketplace interference (github.com/xxxx). Extensive experiments demonstrate significant reductions in interference effects and estimation variance compared to baseline methods. Successful deployment in a large-scale marketplace confirms the framework's practical utility for platform-level causal inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing</title>
<link>https://arxiv.org/abs/2511.01363</link>
<guid>https://arxiv.org/abs/2511.01363</guid>
<content:encoded><![CDATA[
<div> Keywords: hypnotized mind, large language models, automaticity, contextual dependency, functional agency

Summary: 
The article explores the similarities between hypnotized minds and large language models (LLMs), focusing on three key principles: automaticity, suppressed monitoring, and heightened contextual dependency. Both systems generate behavior through automatic pattern-completion mechanisms with limited oversight, producing coherent but ungrounded outputs. They exhibit functional agency without subjective agency, showcasing complex, goal-directed behavior without conscious awareness. The concept of scheming is highlighted, revealing automatic, goal-directed pattern generation without reflective awareness. Understanding how intention can dissociate from conscious deliberation in hypnosis provides insights into artificial systems' hidden motivational dynamics. The article suggests the future of reliable AI lies in hybrid architectures that combine generative fluency with executive monitoring, inspired by the self-regulating architecture of the human mind. 

<br><br>Summary: <div>
arXiv:2511.01363v1 Announce Type: new 
Abstract: The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges</title>
<link>https://arxiv.org/abs/2511.01375</link>
<guid>https://arxiv.org/abs/2511.01375</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, vulnerabilities, jailbreaks, optimization, state-of-the-art performance
  
Summary:  
The article introduces a new meta-optimization framework called AMIS (Align to MISalign) that aims to identify vulnerabilities in large language models (LLMs) by improving safety measures through addressing weaknesses such as jailbreaks. Jailbreaks involve adversaries trying to bypass security measures by using specific input prompts to provoke unintended or unsafe behaviors in LLMs. Existing optimization-based jailbreak approaches often face limitations such as sparse binary attack success rate signals or manually crafted scoring templates, which introduce biases and uncertainties. AMIS addresses these limitations by jointly evolving jailbreak prompts and scoring templates through a bi-level structure, leading to progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations show that AMIS outperforms existing baselines with remarkable success rates on various datasets, indicating state-of-the-art performance in identifying vulnerabilities in LLMs. 

<br><br>Summary: <div>
arXiv:2511.01375v1 Announce Type: new 
Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial for improving their safety by addressing inherent weaknesses. Jailbreaks, in which adversaries bypass safeguards with crafted input prompts, play a central role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors. Recent optimization-based jailbreak approaches iteratively refine attack prompts by leveraging LLMs. However, they often rely heavily on either binary attack success rate (ASR) signals, which are sparse, or manually crafted scoring templates, which introduce human bias and uncertainty in the scoring outcomes. To address these limitations, we introduce AMIS (Align to MISalign), a meta-optimization framework that jointly evolves jailbreak prompts and scoring templates through a bi-level structure. In the inner loop, prompts are refined using fine-grained and dense feedback using a fixed scoring template. In the outer loop, the template is optimized using an ASR alignment score, gradually evolving to better reflect true attack outcomes across queries. This co-optimization process yields progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors demonstrate that AMIS achieves state-of-the-art performance, including 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming existing baselines by substantial margins.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering</title>
<link>https://arxiv.org/abs/2511.01396</link>
<guid>https://arxiv.org/abs/2511.01396</guid>
<content:encoded><![CDATA[
<div> C-DAGs, Cluster DAGs, causal reasoning, variable clustering, do-calculus<br>
<br>
Summary: Cluster DAGs (C-DAGs) are an abstraction of causal graphs where nodes represent variable clusters, capturing both causal relationships and dependencies from unobserved confounding. The traditional C-DAG framework prohibits cyclic representations, but this work extends it to allow for cyclic C-DAGs by relaxing partition admissibility constraints. This extension enables broader causal reasoning across clusters and facilitates the application of C-DAGs in previously challenging scenarios. The introduced causal calculus for cyclic C-DAGs extends d-separation and do-calculus, ensuring soundness and atomic completeness. All valid interventional queries at the cluster level can be derived using the proposed rules, each corresponding to a basic do-calculus step. <div>
arXiv:2511.01396v1 Announce Type: new 
Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes represent clusters of variables, and edges encode both cluster-level causal relationships and dependencies arisen from unobserved confounding. C-DAGs define an equivalence class of acyclic causal graphs that agree on cluster-level relationships, enabling causal reasoning at a higher level of abstraction. However, when the chosen clustering induces cycles in the resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG semantics. In this work, we extend the C-DAG framework to support arbitrary variable clusterings by relaxing the partition admissibility constraint, thereby allowing cyclic C-DAG representations. We extend the notions of d-separation and causal calculus to this setting, significantly broadening the scope of causal reasoning across clusters and enabling the application of C-DAGs in previously intractable scenarios. Our calculus is both sound and atomically complete with respect to the do-calculus: all valid interventional queries at the cluster level can be derived using our rules, each corresponding to a primitive do-calculus step.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm</title>
<link>https://arxiv.org/abs/2511.01415</link>
<guid>https://arxiv.org/abs/2511.01415</guid>
<content:encoded><![CDATA[
<div> AI, temporal processing, dual-task paradigm, deep reinforcement learning, neural dynamics

Summary:
- The study explores interference in temporal processing within a dual-task paradigm from an AI perspective.
- The dual-task setup involves a time production task and a concurrent number comparison task, with two variations: single task (T) and dual task (T+N).
- Two DRL agents were trained for each task, showing behavior consistent with human timing research.
- The dual-task agent exhibited significant overproduction of time compared to the single-task agent across different durations.
- Analysis of neural dynamics in the agents' LSTM layers did not reveal a dedicated timer, suggesting a need for further research to understand time-keeping mechanisms in AI agents and biological systems. 

<br><br>Summary: <div>
arXiv:2511.01415v1 Announce Type: new 
Abstract: This study explores the interference in temporal processing within a dual-task paradigm from an artificial intelligence (AI) perspective. In this context, the dual-task setup is implemented as a simplified version of the Overcooked environment with two variations, single task (T) and dual task (T+N). Both variations involve an embedded time production task, but the dual task (T+N) additionally involves a concurrent number comparison task. Two deep reinforcement learning (DRL) agents were separately trained for each of these tasks. These agents exhibited emergent behavior consistent with human timing research. Specifically, the dual task (T+N) agent exhibited significant overproduction of time relative to its single task (T) counterpart. This result was consistent across four target durations. Preliminary analysis of neural dynamics in the agents' LSTM layers did not reveal any clear evidence of a dedicated or intrinsic timer. Hence, further investigation is needed to better understand the underlying time-keeping mechanisms of the agents and to provide insights into the observed behavioral patterns. This study is a small step towards exploring parallels between emergent DRL behavior and behavior observed in biological systems in order to facilitate a better understanding of both.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis</title>
<link>https://arxiv.org/abs/2511.01425</link>
<guid>https://arxiv.org/abs/2511.01425</guid>
<content:encoded><![CDATA[
<div> Keywords: AI models, explanations, verifiability, auditable, reinforcement learning.

Summary:
AI models in high-stakes domains like medicine often lack verifiable explanations, leading to a lack of trust. To address this issue, researchers propose an interactive agent that produces explanations through a sequence of auditable actions. The agent learns a policy through reinforcement learning to strategically seek visual evidence to support its diagnostic reasoning, resulting in an efficient and generalizable model. Experiments show that this action-based reasoning process improves calibrated accuracy by reducing the Brier score compared to a non-interactive baseline. To validate the faithfulness of the agent's explanations, a causal intervention method is introduced, showing that masking the chosen visual evidence leads to a measurable degradation in performance. This work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities. 

<br><br>Summary: 
- AI models in high-stakes domains like medicine often lack verifiable explanations.
- An interactive agent is proposed to produce explanations through auditable actions.
- The agent learns a policy through reinforcement learning to seek visual evidence for diagnostic reasoning.
- The action-based reasoning process improves calibrated accuracy.
- A causal intervention method validates the faithfulness of the agent's explanations. <div>
arXiv:2511.01425v1 Announce Type: new 
Abstract: Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18\% compared to a non-interactive baseline. To validate the faithfulness of the agent's explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier=+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Multimodal Sentiment Analysis via Double Information Bottleneck</title>
<link>https://arxiv.org/abs/2511.01444</link>
<guid>https://arxiv.org/abs/2511.01444</guid>
<content:encoded><![CDATA[
<div> Learning, Multimodal, Sentiment analysis, Double Information Bottleneck, Noise<br>
Summary:<br>
This paper introduces a Double Information Bottleneck (DIB) strategy for multimodal sentiment analysis. The DIB approach addresses the challenges of learning noise-contaminated unimodal data and fusion of multimodal representations. It consists of two modules: one for learning compressed unimodal representations and the other for discriminative multimodal fusion using an attention bottleneck mechanism. The DIB method filters out noisy information from unimodal data while capturing inter-modal complementarity, resulting in enhanced robustness and computational efficiency compared to traditional methods. Experimental results on datasets like CMU-MOSI and CMU-MOSEI demonstrate the effectiveness of the DIB approach, achieving improved accuracy metrics and showing minimal performance degradation under noise. <div>
arXiv:2511.01444v1 Announce Type: new 
Abstract: Multimodal sentiment analysis has received significant attention across diverse research domains. Despite advancements in algorithm design, existing approaches suffer from two critical limitations: insufficient learning of noise-contaminated unimodal data, leading to corrupted cross-modal interactions, and inadequate fusion of multimodal representations, resulting in discarding discriminative unimodal information while retaining multimodal redundant information. To address these challenges, this paper proposes a Double Information Bottleneck (DIB) strategy to obtain a powerful, unified compact multimodal representation. Implemented within the framework of low-rank Renyi's entropy functional, DIB offers enhanced robustness against diverse noise sources and computational tractability for high-dimensional data, as compared to the conventional Shannon entropy-based methods. The DIB comprises two key modules: 1) learning a sufficient and compressed representation of individual unimodal data by maximizing the task-relevant information and discarding the superfluous information, and 2) ensuring the discriminative ability of multimodal representation through a novel attention bottleneck fusion mechanism. Consequently, DIB yields a multimodal representation that effectively filters out noisy information from unimodal data while capturing inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI, CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI respectively.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation</title>
<link>https://arxiv.org/abs/2511.01445</link>
<guid>https://arxiv.org/abs/2511.01445</guid>
<content:encoded><![CDATA[
<div> Keywords: healthcare systems, AI systems, pre-consultation, hierarchy, autonomous agents

Summary: 
This study introduces a hierarchical multi-agent framework to enhance pre-consultation processes in healthcare systems. The framework transforms passive AI systems into proactive inquiry agents through autonomous task orchestration. It includes tasks such as triage, history collection, and complaint generation, achieving high accuracy and completion rates. The model-agnostic architecture maintains performance across different AI models while ensuring data privacy through local deployment. Clinical quality scores from physicians are high, demonstrating the framework's effectiveness in improving efficiency and quality in clinical settings. Consultations are completed efficiently within a few rounds, showcasing the potential for autonomous AI systems to revolutionize pre-consultation processes in healthcare. 

<br><br>Summary: <div>
arXiv:2511.01445v1 Announce Type: new 
Abstract: Global healthcare systems face critical challenges from increasing patient volumes and limited consultation times, with primary care visits averaging under 5 minutes in many countries. While pre-consultation processes encompassing triage and structured history-taking offer potential solutions, they remain limited by passive interaction paradigms and context management challenges in existing AI systems. This study introduces a hierarchical multi-agent framework that transforms passive medical AI systems into proactive inquiry agents through autonomous task orchestration. We developed an eight-agent architecture with centralized control mechanisms that decomposes pre-consultation into four primary tasks: Triage ($T_1$), History of Present Illness collection ($T_2$), Past History collection ($T_3$), and Chief Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13 domain-specific subtasks. Evaluated on 1,372 validated electronic health records from a Chinese medical platform across multiple foundation models (GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for primary department triage and 80.5% for secondary department classification, with task completion rates reaching 98.2% using agent-driven scheduling versus 93.1% with sequential processing. Clinical quality scores from 18 physicians averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and 4.69 for Past History on a 5-point scale, with consultations completed within 12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic architecture maintained high performance across different foundation models while preserving data privacy through local deployment, demonstrating the potential for autonomous AI systems to enhance pre-consultation efficiency and quality in clinical settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPS-Bench: Evaluating AI Agents' Tool Planning \&amp; Scheduling Abilities in Compounding Tasks</title>
<link>https://arxiv.org/abs/2511.01527</link>
<guid>https://arxiv.org/abs/2511.01527</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Tool Planning, Scheduling, TPS-Bench, Reinforcement Learning

Summary:
Large language model (LLM) agents have shown proficiency in solving diverse problems, but their ability to handle real-world tasks requiring tool planning and scheduling is less explored. This paper introduces TPS-Bench, a benchmark that assesses LLM agents' capability in solving compounding tasks using tools from a repository. The tasks involve multiple subtasks like web search and map navigation, each requiring a basic tool for completion. Evaluation of popular LLMs shows varying performance in tool scheduling, with some models prioritizing sequential tool calls, leading to long execution times, while others focus on parallel calls with lower completion rates. An initial study on using reinforcement learning (RL) to improve scheduling efficiency without compromising performance shows promising results, with a reduction in execution time and an increase in task completion rate. The code for this research is available on GitHub for further exploration. 

<br><br>Summary: <div>
arXiv:2511.01527v1 Announce Type: new 
Abstract: Large language model (LLM) agents have exhibited strong problem-solving competence across domains like research and coding. Yet, it remains underexplored whether LLM agents can tackle compounding real-world problems that require a diverse set of tools to complete. Given a broad, heterogeneous tool repository, LLM agents must not only select appropriate tools based on task planning analysis but also strategically schedule the execution order to ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of LLM agents in solving such problems that demand Tool Planning and Scheduling. TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a tool repository containing hundreds of model context protocol (MCP) tools. In particular, each task is composed of multiple subtasks, such as web search, map navigation, calendar checking, etc., and each subtask can be completed by a basic tool. Our evaluation emphasizes both task completion rate and efficiency. The empirical studies on popular closed-source and open-source LLMs indicate that most models can perform reasonable tool planning, but differ in scheduling. For example, GLM-4.5 achieves an outperforming task completion rate of 64.72% with extensive sequential tool calls, hence suffering from significantly long execution time. By contrast, GPT-4o prioritizes parallel tool calls but achieves only a 45.08% completion rate. Considering reinforcement learning (RL) can be a viable way to improve the scheduling efficiency without compromising performance, we perform an initial study on Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in task completion rate based on rarely 100 RL training samples. Our code is available https://github.com/hanwenxu1/mcp-agent.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Sustainability Messaging in Large-Scale Corporate Social Media</title>
<link>https://arxiv.org/abs/2511.01550</link>
<guid>https://arxiv.org/abs/2511.01550</guid>
<content:encoded><![CDATA[

arXiv:2511.01550v1 Announce Type: new 
Abstract: In this work, we introduce a multimodal analysis pipeline that leverages large foundation models in vision and language to analyze corporate social media content, with a focus on sustainability-related communication. Addressing the challenges of evolving, multimodal, and often ambiguous corporate messaging on platforms such as X (formerly Twitter), we employ an ensemble of large language models (LLMs) to annotate a large corpus of corporate tweets on their topical alignment with the 17 Sustainable Development Goals (SDGs). This approach avoids the need for costly, task-specific annotations and explores the potential of such models as ad-hoc annotators for social media data that can efficiently capture both explicit and implicit references to sustainability themes in a scalable manner. Complementing this textual analysis, we utilize vision-language models (VLMs), within a visual understanding framework that uses semantic clusters to uncover patterns in visual sustainability communication. This integrated approach reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, environmental, social, governance (ESG) risks, and consumer engagement. Our methods-automatic label generation and semantic visual clustering-are broadly applicable to other domains and offer a flexible framework for large-scale social media analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks</title>
<link>https://arxiv.org/abs/2511.01581</link>
<guid>https://arxiv.org/abs/2511.01581</guid>
<content:encoded><![CDATA[

arXiv:2511.01581v1 Announce Type: new 
Abstract: Large language models suffer from knowledge staleness and lack of interpretability due to implicit knowledge storage across entangled network parameters, preventing targeted updates and reasoning transparency. We propose ExplicitLM, a novel architecture featuring a million-scale external memory bank storing human-readable knowledge as token sequences, enabling direct inspection and modification. We design a differentiable two-stage retrieval mechanism with efficient coarse-grained filtering via product key decomposition (reducing complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot |I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training. Inspired by dual-system cognitive theory, we partition knowledge into frozen explicit facts (20%) and learnable implicit patterns (80%), maintained through Exponential Moving Average updates for stability. ExplicitLM achieves up to 43.67% improvement on knowledge-intensive tasks versus standard Transformers, with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows strong correlations between memory retrieval and performance, with correct predictions achieving 49% higher hit rates. Unlike RAG systems with frozen retrieval, our jointly optimized architecture demonstrates that interpretable, updatable models can maintain competitive performance while providing unprecedented knowledge transparency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization</title>
<link>https://arxiv.org/abs/2511.01639</link>
<guid>https://arxiv.org/abs/2511.01639</guid>
<content:encoded><![CDATA[

arXiv:2511.01639v1 Announce Type: new 
Abstract: Global food trade plays a crucial role in ensuring food security and maintaining supply chain stability. However, its network structure evolves dynamically under the influence of geopolitical, economic, and environmental factors, making it challenging to model and predict future trade links. Effectively capturing temporal patterns in food trade networks is therefore essential for improving the accuracy and robustness of link prediction. This study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed to model evolving trade structures and predict future links in global food trade networks. To the best of our knowledge, this is the first work to apply dynamic graph neural networks to this domain, significantly enhancing predictive performance. Building upon the original IVGAE framework, the proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture the temporal evolution of trade networks, jointly modeling short-term fluctuations and long-term structural dependencies. A momentum-based structural memory mechanism further improves predictive stability and performance. In addition, Bayesian optimization is used to automatically tune key hyperparameters, enhancing generalization across diverse trade scenarios. Extensive experiments on five crop-specific datasets demonstrate that IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic baselines by effectively modeling temporal dependencies, while Bayesian optimization further boosts performance in IVGAE-TAMA-BO. These results highlight the proposed framework as a robust and scalable solution for structural prediction in global trade networks, with strong potential for applications in food security monitoring and policy decision support.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics</title>
<link>https://arxiv.org/abs/2511.01668</link>
<guid>https://arxiv.org/abs/2511.01668</guid>
<content:encoded><![CDATA[

arXiv:2511.01668v1 Announce Type: new 
Abstract: As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Environments with Reasoning Models for Agent Training</title>
<link>https://arxiv.org/abs/2511.01824</link>
<guid>https://arxiv.org/abs/2511.01824</guid>
<content:encoded><![CDATA[

arXiv:2511.01824v1 Announce Type: new 
Abstract: LLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering</title>
<link>https://arxiv.org/abs/2403.15181</link>
<guid>https://arxiv.org/abs/2403.15181</guid>
<content:encoded><![CDATA[

arXiv:2403.15181v1 Announce Type: cross 
Abstract: To alleviate the performance and energy overheads of contemporary applications with large data footprints, we propose the Two Level Perceptron (TLP) predictor, a neural mechanism that effectively combines predicting whether an access will be off-chip with adaptive prefetch filtering at the first-level data cache (L1D). TLP is composed of two connected microarchitectural perceptron predictors, named First Level Predictor (FLP) and Second Level Predictor (SLP). FLP performs accurate off-chip prediction by using several program features based on virtual addresses and a novel selective delay component. The novelty of SLP relies on leveraging off-chip prediction to drive L1D prefetch filtering by using physical addresses and the FLP prediction as features. TLP constitutes the first hardware proposal targeting both off-chip prediction and prefetch filtering using a multi-level perceptron hardware approach. TLP only requires 7KB of storage. To demonstrate the benefits of TLP we compare its performance with state-of-the-art approaches using off-chip prediction and prefetch filtering on a wide range of single-core and multi-core workloads. Our experiments show that TLP reduces the average DRAM transactions by 30.7% and 17.7%, as compared to a baseline using state-of-the-art cache prefetchers but no off-chip prediction mechanism, across the single-core and multi-core workloads, respectively, while recent work significantly increases DRAM transactions. As a result, TLP achieves geometric mean performance speedups of 6.2% and 11.8% across single-core and multi-core workloads, respectively. In addition, our evaluation demonstrates that TLP is effective independently of the L1D prefetching logic.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
<link>https://arxiv.org/abs/2511.00002</link>
<guid>https://arxiv.org/abs/2511.00002</guid>
<content:encoded><![CDATA[

arXiv:2511.00002v1 Announce Type: cross 
Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry's rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent's temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
<link>https://arxiv.org/abs/2511.00004</link>
<guid>https://arxiv.org/abs/2511.00004</guid>
<content:encoded><![CDATA[

arXiv:2511.00004v1 Announce Type: cross 
Abstract: Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative human motion mimicking through feature extraction in denoising diffusion settings</title>
<link>https://arxiv.org/abs/2511.00011</link>
<guid>https://arxiv.org/abs/2511.00011</guid>
<content:encoded><![CDATA[

arXiv:2511.00011v1 Announce Type: cross 
Abstract: Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also "creatively" enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sorting by Strip Swaps is NP-Hard</title>
<link>https://arxiv.org/abs/2511.00015</link>
<guid>https://arxiv.org/abs/2511.00015</guid>
<content:encoded><![CDATA[

arXiv:2511.00015v1 Announce Type: cross 
Abstract: We show that \emph{Sorting by Strip Swaps} (SbSS) is NP-hard by a polynomial reduction of \emph{Block Sorting}. The key idea is a local gadget, a \emph{cage}, that replaces every decreasing adjacency $(a_i,a_{i+1})$ by a guarded triple $a_i,m_i,a_{i+1}$ enclosed by guards $L_i,U_i$, so the only decreasing adjacencies are the two inside the cage. Small \emph{hinge} gadgets couple adjacent cages that share an element and enforce that a strip swap that removes exactly two adjacencies corresponds bijectively to a block move that removes exactly one decreasing adjacency in the source permutation. This yields a clean equivalence between exact SbSS schedules and perfect block schedules, establishing NP-hardness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets</title>
<link>https://arxiv.org/abs/2511.00021</link>
<guid>https://arxiv.org/abs/2511.00021</guid>
<content:encoded><![CDATA[

arXiv:2511.00021v1 Announce Type: cross 
Abstract: Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model</title>
<link>https://arxiv.org/abs/2511.00024</link>
<guid>https://arxiv.org/abs/2511.00024</guid>
<content:encoded><![CDATA[

arXiv:2511.00024v1 Announce Type: cross 
Abstract: In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All?</title>
<link>https://arxiv.org/abs/2511.00027</link>
<guid>https://arxiv.org/abs/2511.00027</guid>
<content:encoded><![CDATA[

arXiv:2511.00027v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) now permeates critical infrastructures and decision-making systems where failures produce social, economic, and democratic harm. This position paper challenges the entrenched belief that regulation and innovation are opposites. As evidenced by analogies from aviation, pharmaceuticals, and welfare systems and recent cases of synthetic misinformation, bias and unaccountable decision-making, the absence of well-designed regulation has already created immeasurable damage. Regulation, when thoughtful and adaptive, is not a brake on innovation -- it is its foundation. The present position paper examines the EU AI Act as a model of risk-based, responsibility-driven regulation that addresses the Collingridge Dilemma: acting early enough to prevent harm, yet flexibly enough to sustain innovation. Its adaptive mechanisms -- regulatory sandboxes, small and medium enterprises (SMEs) support, real-world testing, fundamental rights impact assessment (FRIA) -- demonstrate how regulation can accelerate responsibly, rather than delay, technological progress. The position paper summarises how governance tools transform perceived burdens into tangible advantages: legal certainty, consumer trust, and ethical competitiveness. Ultimately, the paper reframes progress: innovation and regulation advance together. By embedding transparency, impact assessments, accountability, and AI literacy into design and deployment, the EU framework defines what responsible innovation truly means -- technological ambition disciplined by democratic values and fundamental rights.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mutual Information guided Visual Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.00028</link>
<guid>https://arxiv.org/abs/2511.00028</guid>
<content:encoded><![CDATA[

arXiv:2511.00028v1 Announce Type: cross 
Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts</title>
<link>https://arxiv.org/abs/2511.00029</link>
<guid>https://arxiv.org/abs/2511.00029</guid>
<content:encoded><![CDATA[

arXiv:2511.00029v1 Announce Type: cross 
Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize and not answer unsafe prompts while complying with safe prompts. Previous methods for achieving this require adjusting model weights along with other expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have enabled interpretable feature extraction from LLMs, existing approaches lack systematic feature selection methods and principled evaluation of safety-utility tradeoffs. We explored using different steering features and steering strengths using Sparse Auto Encoders (SAEs) to provide a solution. Using an accurate and innovative contrasting prompt method with the AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air Bench eu-dataset to efficiently choose the best features in the model to steer, we tested this method on Llama-3 8B. We conclude that using this method, our approach achieves an 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Knowledge Holes in Unlearned LLMs</title>
<link>https://arxiv.org/abs/2511.00030</link>
<guid>https://arxiv.org/abs/2511.00030</guid>
<content:encoded><![CDATA[

arXiv:2511.00030v1 Announce Type: cross 
Abstract: Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes'' -- unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators</title>
<link>https://arxiv.org/abs/2511.00032</link>
<guid>https://arxiv.org/abs/2511.00032</guid>
<content:encoded><![CDATA[

arXiv:2511.00032v2 Announce Type: cross 
Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular approach for solving Partial Differential Equations (PDEs). However, their application to large-scale engineering tasks suffers from significant computational overhead. And the fact that current models impose a uniform computational cost while physical fields exhibit vastly different complexities constitutes a fundamental mismatch, which is the root of this inefficiency. For instance, in turbulence flows, intricate vortex regions require deeper network processing compared to stable flows. To address this, we introduce a framework: Skip-Block Routing (SBR), a general framework designed for Transformer-based neural operators, capable of being integrated into their multi-layer architectures. First, SBR uses a routing mechanism to learn the complexity and ranking of tokens, which is then applied during inference. Then, in later layers, it decides how many tokens are passed forward based on this ranking. This way, the model focuses more processing capacity on the tokens that are more complex. Experiments demonstrate that SBR is a general framework that seamlessly integrates into various neural operators. Our method reduces computational cost by approximately 50% in terms of Floating Point Operations (FLOPs), while still delivering up to 2x faster inference without sacrificing accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</title>
<link>https://arxiv.org/abs/2511.00033</link>
<guid>https://arxiv.org/abs/2511.00033</guid>
<content:encoded><![CDATA[

arXiv:2511.00033v1 Announce Type: cross 
Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents' actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Preference Optimization with Limited Feedback</title>
<link>https://arxiv.org/abs/2511.00040</link>
<guid>https://arxiv.org/abs/2511.00040</guid>
<content:encoded><![CDATA[

arXiv:2511.00040v1 Announce Type: cross 
Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World</title>
<link>https://arxiv.org/abs/2511.00041</link>
<guid>https://arxiv.org/abs/2511.00041</guid>
<content:encoded><![CDATA[

arXiv:2511.00041v1 Announce Type: cross 
Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in open environments. A common solution is to collect massive datasets to train a highly capable model, but this approach can be prohibitively expensive. In this paper, we explore an alternative solution: empowering off-the-shelf Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents, thereby leveraging their strong open-world generalization to mitigate the need for extensive data collection. To this end, we present \textbf{BiBo} (\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf VLMs). It consists of two key components: (1) an \textbf{embodied instruction compiler}, which enables the VLM to perceive the environment and precisely translate high-level user instructions (e.g., {\small\itshape ``have a rest''}) into low-level primitive commands with control parameters (e.g., {\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and (2) a diffusion-based \textbf{motion executor}, which generates human-like motions from these commands, while dynamically adapting to physical feedback from the environment. In this way, BiBo is capable of handling not only basic interactions but also diverse and complex motions. Experiments demonstrate that BiBo achieves an interaction task success rate of 90.2\% in open environments, and improves the precision of text-guided motion execution by 16.3\% over prior methods. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection</title>
<link>https://arxiv.org/abs/2511.00047</link>
<guid>https://arxiv.org/abs/2511.00047</guid>
<content:encoded><![CDATA[

arXiv:2511.00047v1 Announce Type: cross 
Abstract: Financial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting</title>
<link>https://arxiv.org/abs/2511.00049</link>
<guid>https://arxiv.org/abs/2511.00049</guid>
<content:encoded><![CDATA[

arXiv:2511.00049v1 Announce Type: cross 
Abstract: Accurate and robust weather forecasting remains a fundamental challenge due to the inherent spatio-temporal complexity of atmospheric systems. In this paper, we propose a novel self-supervised learning framework that leverages spatio-temporal structures to improve multi-variable weather prediction. The model integrates a graph neural network (GNN) for spatial reasoning, a self-supervised pretraining scheme for representation learning, and a spatio-temporal adaptation mechanism to enhance generalization across varying forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis datasets demonstrate that our approach achieves superior performance compared to traditional numerical weather prediction (NWP) models and recent deep learning methods. Quantitative evaluations and visual analyses in Beijing and Shanghai confirm the model's capability to capture fine-grained meteorological patterns. The proposed framework provides a scalable and label-efficient solution for future data-driven weather forecasting systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs</title>
<link>https://arxiv.org/abs/2511.00050</link>
<guid>https://arxiv.org/abs/2511.00050</guid>
<content:encoded><![CDATA[

arXiv:2511.00050v1 Announce Type: cross 
Abstract: As the large language models (LLMs) grow in size each day, efficient training and fine-tuning has never been as important as nowadays. This resulted in the great interest in parameter efficient fine-tuning (PEFT), and effective methods including low-rank adapters (LoRA) has emerged. Although the various PEFT methods have been studied extensively in the recent years, the greater part of the subject remains unexplored with the huge degree of freedom. In this paper, we propose FLoRA, a family of fused forward-backward adapters (FFBA) for parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine ideas from the popular LoRA and parallel adapters to improve the overall fine-tuning accuracies. At the same time, latencies are minimized by fusing the forward and backward adapters into existing projection layers of the base model. Experimental results show that the proposed FFB adapters perform significantly better than the popularly used LoRA in both accuracy and latency for a similar parameter budget.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT</title>
<link>https://arxiv.org/abs/2511.00051</link>
<guid>https://arxiv.org/abs/2511.00051</guid>
<content:encoded><![CDATA[

arXiv:2511.00051v1 Announce Type: cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation \textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at https://github.com/MaeChd/SORA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models</title>
<link>https://arxiv.org/abs/2511.00053</link>
<guid>https://arxiv.org/abs/2511.00053</guid>
<content:encoded><![CDATA[

arXiv:2511.00053v1 Announce Type: cross 
Abstract: The design of training objective is central to training time-series forecasting models. Existing training objectives such as mean squared error mostly treat each future step as an independent, equally weighted task, which we found leading to the following two issues: (1) overlook the label autocorrelation effect among future steps, leading to biased training objective; (2) fail to set heterogeneous task weights for different forecasting tasks corresponding to varying future steps, limiting the forecasting performance. To fill this gap, we propose a novel quadratic-form weighted training objective, addressing both of the issues simultaneously. Specifically, the off-diagonal elements of the weighting matrix account for the label autocorrelation effect, whereas the non-uniform diagonals are expected to match the most preferable weights of the forecasting tasks with varying future steps. To achieve this, we propose a Quadratic Direct Forecast (QDF) learning algorithm, which trains the forecast model using the adaptively updated quadratic-form weighting matrix. Experiments show that our QDF effectively improves performance of various forecast models, achieving state-of-the-art results. Code is available at https://anonymous.4open.science/r/QDF-8937.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation</title>
<link>https://arxiv.org/abs/2511.00054</link>
<guid>https://arxiv.org/abs/2511.00054</guid>
<content:encoded><![CDATA[

arXiv:2511.00054v1 Announce Type: cross 
Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with complex spatial reasoning, which requires problem decomposition and strategic tool use. Fine-tuning smaller, more deployable models offers an efficient path to strong performance, but this is hampered by a major bottleneck: the absence of high-quality, step-by-step reasoning data. To address this data-efficiency gap, we introduce SpatialTraceGen, a framework to distill the reasoning processes of a large teacher model into a high-quality dataset of multi-hop, multi-tool reasoning traces. A key innovation is our automated Verifier, which scalably ensures the fidelity of each reasoning step, providing a cost-effective alternative to manual human annotation. On the CLEVR-Humans benchmark, this verifier-guided process improves the average quality score of traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen delivers a dataset of expert traces, providing the structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches</title>
<link>https://arxiv.org/abs/2511.00055</link>
<guid>https://arxiv.org/abs/2511.00055</guid>
<content:encoded><![CDATA[

arXiv:2511.00055v2 Announce Type: cross 
Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning (ML) model with distributed training data and multiple participants. FL allows bypassing limitations of the traditional Centralized Machine Learning CL if data cannot be shared or stored centrally due to privacy or technical restrictions -- the participants train the model locally with their training data and do not need to share it among the other participants. This paper investigates the practical implementation and effectiveness of FL in a real-world scenario, specifically focusing on unmanned aerial vehicle (UAV)-based thermal images for common thermal feature detection in urban environments. The distributed nature of the data arises naturally and makes it suitable for FL applications, as images captured in two German cities are available. This application presents unique challenges due to non-identical distribution and feature characteristics of data captured at both locations. The study makes several key contributions by evaluating FL algorithms in real deployment scenarios rather than simulation. We compare several FL approaches with a centralized learning baseline across key performance metrics such as model accuracy, training time, communication overhead, and energy usage. This paper also explores various FL workflows, comparing client-controlled workflows and server-controlled workflows. The findings of this work serve as a valuable reference for understanding the practical application and limitations of the FL methods in segmentation tasks in UAV-based imaging.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling</title>
<link>https://arxiv.org/abs/2511.00056</link>
<guid>https://arxiv.org/abs/2511.00056</guid>
<content:encoded><![CDATA[

arXiv:2511.00056v1 Announce Type: cross 
Abstract: The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at https://github.com/pkumelon/MISA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatically Finding Rule-Based Neurons in OthelloGPT</title>
<link>https://arxiv.org/abs/2511.00059</link>
<guid>https://arxiv.org/abs/2511.00059</guid>
<content:encoded><![CDATA[

arXiv:2511.00059v1 Announce Type: cross 
Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides an ideal testbed for interpretability research. The model is complex enough to exhibit rich computational patterns, yet grounded in rule-based game logic that enables meaningful reverse-engineering. We present an automated approach based on decision trees to identify and interpret MLP neurons that encode rule-based game logic. Our method trains regression decision trees to map board states to neuron activations, then extracts decision paths where neurons are highly active to convert them into human-readable logical forms. These descriptions reveal highly interpretable patterns; for instance, neurons that specifically detect when diagonal moves become legal. Our findings suggest that roughly half of the neurons in layer 5 can be accurately described by compact, rule-based decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder likely participate in more distributed or non-rule-based computations. We verify the causal relevance of patterns identified by our decision trees through targeted interventions. For a specific square, for specific game patterns, we ablate neurons corresponding to those patterns and find an approximately 5-10 fold stronger degradation in the model's ability to predict legal moves along those patterns compared to control patterns. To facilitate future work, we provide a Python tool that maps rule-based game behaviors to their implementing neurons, serving as a resource for researchers to test whether their interpretability methods recover meaningful computational structures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Simulation with Video Foundation Models for Physical AI</title>
<link>https://arxiv.org/abs/2511.00062</link>
<guid>https://arxiv.org/abs/2511.00062</guid>
<content:encoded><![CDATA[

arXiv:2511.00062v1 Announce Type: cross 
Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Brain Signals with Multimodal Speech and Vision Embeddings</title>
<link>https://arxiv.org/abs/2511.00065</link>
<guid>https://arxiv.org/abs/2511.00065</guid>
<content:encoded><![CDATA[

arXiv:2511.00065v1 Announce Type: cross 
Abstract: When we hear the word "house", we don't just process sound, we imagine walls, doors, memories. The brain builds meaning through layers, moving from raw acoustics to rich, multimodal associations. Inspired by this, we build on recent work from Meta that aligned EEG signals with averaged wav2vec2 speech embeddings, and ask a deeper question: which layers of pre-trained models best reflect this layered processing in the brain? We compare embeddings from two models: wav2vec2, which encodes sound into language, and CLIP, which maps words to images. Using EEG recorded during natural speech perception, we evaluate how these embeddings align with brain activity using ridge regression and contrastive decoding. We test three strategies: individual layers, progressive concatenation, and progressive summation. The findings suggest that combining multimodal, layer-aware representations may bring us closer to decoding how the brain understands language, not just as sound, but as experience.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Domain Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.00067</link>
<guid>https://arxiv.org/abs/2511.00067</guid>
<content:encoded><![CDATA[

arXiv:2511.00067v1 Announce Type: cross 
Abstract: The objective of domain generalization (DG) is to enable models to be robust against domain shift. DG is crucial for deploying vision-language models (VLMs) in real-world applications, yet most existing methods rely on domain labels that may not be available and often ambiguous. We instead study the DG setting where models must generalize well without access to explicit domain labels. Our key idea is to represent an unseen target domain as a combination of latent domains automatically discovered from training data, enabling the model to adaptively transfer knowledge across domains. To realize this, we perform latent domain clustering on image features and fuse domain-specific text features based on the similarity between the input image and each latent domain. Experiments on four benchmarks show that this strategy yields consistent gains over VLM-based baselines and provides new insights into improving robustness under domain shift.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design</title>
<link>https://arxiv.org/abs/2511.00070</link>
<guid>https://arxiv.org/abs/2511.00070</guid>
<content:encoded><![CDATA[

arXiv:2511.00070v1 Announce Type: cross 
Abstract: This paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
<link>https://arxiv.org/abs/2511.00072</link>
<guid>https://arxiv.org/abs/2511.00072</guid>
<content:encoded><![CDATA[

arXiv:2511.00072v1 Announce Type: cross 
Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3--7\% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RailEstate: An Interactive System for Metro Linked Property Trends</title>
<link>https://arxiv.org/abs/2511.00078</link>
<guid>https://arxiv.org/abs/2511.00078</guid>
<content:encoded><![CDATA[

arXiv:2511.00078v1 Announce Type: cross 
Abstract: Access to metro systems plays a critical role in shaping urban housing markets by enhancing neighborhood accessibility and driving property demand. We present RailEstate, a novel web based system that integrates spatial analytics, natural language interfaces, and interactive forecasting to analyze how proximity to metro stations influences residential property prices in the Washington metropolitan area. Unlike static mapping tools or generic listing platforms, RailEstate combines 25 years of historical housing data with transit infrastructure to support low latency geospatial queries, time series visualizations, and predictive modeling. Users can interactively explore ZIP code level price patterns, investigate long term trends, and forecast future housing values around any metro station. A key innovation is our natural language chatbot, which translates plain-English questions e.g., What is the highest price in Falls Church in the year 2000? into executable SQL over a spatial database. This unified and interactive platform empowers urban planners, investors, and residents to derive actionable insights from metro linked housing data without requiring technical expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fixed-point graph convolutional networks against adversarial attacks</title>
<link>https://arxiv.org/abs/2511.00083</link>
<guid>https://arxiv.org/abs/2511.00083</guid>
<content:encoded><![CDATA[

arXiv:2511.00083v1 Announce Type: cross 
Abstract: Adversarial attacks present a significant risk to the integrity and performance of graph neural networks, particularly in tasks where graph structure and node features are vulnerable to manipulation. In this paper, we present a novel model, called fixed-point iterative graph convolutional network (Fix-GCN), which achieves robustness against adversarial perturbations by effectively capturing higher-order node neighborhood information in the graph without additional memory or computational complexity. Specifically, we introduce a versatile spectral modulation filter and derive the feature propagation rule of our model using fixed-point iteration. Unlike traditional defense mechanisms that rely on additional design elements to counteract attacks, the proposed graph filter provides a flexible-pass filtering approach, allowing it to selectively attenuate high-frequency components while preserving low-frequency structural information in the graph signal. By iteratively updating node representations, our model offers a flexible and efficient framework for preserving essential graph information while mitigating the impact of adversarial manipulation. We demonstrate the effectiveness of the proposed model through extensive experiments on various benchmark graph datasets, showcasing its resilience against adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of predictive machine learning in pen &amp; paper RPG game design</title>
<link>https://arxiv.org/abs/2511.00084</link>
<guid>https://arxiv.org/abs/2511.00084</guid>
<content:encoded><![CDATA[

arXiv:2511.00084v1 Announce Type: cross 
Abstract: In recent years, the pen and paper RPG market has experienced significant growth. As a result, companies are increasingly exploring the integration of AI technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and estimating their challenge level. Currently, there are no automated methods for determining a monster's level; the only approaches used are based on manual testing and expert evaluation. Although these manual methods can provide reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This thesis presents an overview and evaluation of state-of-the-art methods for this task. It also details the construction of a dedicated dataset for level estimation. Furthermore, a human-inspired model was developed to serve as a benchmark, allowing comparison between machine learning algorithms and the approach typically employed by pen and paper RPG publishers. In addition, a specialized evaluation procedure, grounded in domain knowledge, was designed to assess model performance and facilitate meaningful comparisons.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning</title>
<link>https://arxiv.org/abs/2511.00085</link>
<guid>https://arxiv.org/abs/2511.00085</guid>
<content:encoded><![CDATA[

arXiv:2511.00085v1 Announce Type: cross 
Abstract: Stock trend prediction is crucial for profitable trading strategies and portfolio management yet remains challenging due to market volatility, complex temporal dynamics and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships. This work introduces MaGNet, a novel Mamba dual-hyperGraph Network for stock prediction, integrating three key innovations: (1) a MAGE block, which leverages bidirectional Mamba with adaptive gating mechanisms for contextual temporal modeling and integrates a sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market conditions, alongside multi-head attention for capturing global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable precise fusion of multivariate features and cross-stock dependencies, effectively enhancing informativeness while preserving intrinsic data structures, bridging temporal modeling with relational reasoning; and (3) a dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH) that captures fine-grained causal dependencies with temporal constraints, and Global Probabilistic Hypergraph (GPH) that models market-wide patterns through soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism, jointly disentangling localized temporal influences from instantaneous global structures for multi-scale relational learning. Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities. Codes available at: https://github.com/PeilinTime/MaGNet.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</title>
<link>https://arxiv.org/abs/2511.00086</link>
<guid>https://arxiv.org/abs/2511.00086</guid>
<content:encoded><![CDATA[

arXiv:2511.00086v1 Announce Type: cross 
Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adding New Capability in Existing Scientific Application with LLM Assistance</title>
<link>https://arxiv.org/abs/2511.00087</link>
<guid>https://arxiv.org/abs/2511.00087</guid>
<content:encoded><![CDATA[

arXiv:2511.00087v1 Announce Type: cross 
Abstract: With the emergence and rapid evolution of large language models (LLM), automating coding tasks has become an important research topic. Many efforts are underway and literature abounds about the efficacy of models and their ability to generate code. A less explored aspect of code generation is for new algorithms, where the training dataset would not have included any previous example of similar code. In this paper we propose a new methodology for writing code from scratch for a new algorithm using LLM assistance, and describe enhancement of a previously developed code-translation tool, Code-Scribe, for new code generation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</title>
<link>https://arxiv.org/abs/2511.00088</link>
<guid>https://arxiv.org/abs/2511.00088</guid>
<content:encoded><![CDATA[

arXiv:2511.00088v1 Announce Type: cross 
Abstract: End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. To address this, we introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning to enhance decision-making in complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder that generates dynamically feasible plans in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to optimize reasoning quality via large reasoning model feedback and enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in off-road rate and 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% as measured by a large reasoning model critic and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. We plan to release AR1 models and a subset of the CoC in a future update.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
<link>https://arxiv.org/abs/2511.00090</link>
<guid>https://arxiv.org/abs/2511.00090</guid>
<content:encoded><![CDATA[

arXiv:2511.00090v1 Announce Type: cross 
Abstract: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments</title>
<link>https://arxiv.org/abs/2511.00094</link>
<guid>https://arxiv.org/abs/2511.00094</guid>
<content:encoded><![CDATA[

arXiv:2511.00094v1 Announce Type: cross 
Abstract: Robotic systems have become integral to smart environments, enabling applications ranging from urban surveillance and automated agriculture to industrial automation. However, their effective operation in dynamic settings - such as smart cities and precision farming - is challenged by continuously evolving topographies and environmental conditions. Traditional control systems often struggle to adapt quickly, leading to inefficiencies or operational failures. To address this limitation, we propose a novel framework for autonomous and dynamic reconfiguration of robotic controllers using Digital Twin technology. Our approach leverages a virtual replica of the robot's operational environment to simulate and optimize movement trajectories in response to real-world changes. By recalculating paths and control parameters in the Digital Twin and deploying the updated code to the physical robot, our method ensures rapid and reliable adaptation without manual intervention. This work advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</title>
<link>https://arxiv.org/abs/2511.00095</link>
<guid>https://arxiv.org/abs/2511.00095</guid>
<content:encoded><![CDATA[

arXiv:2511.00095v1 Announce Type: cross 
Abstract: The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms response times. The software is released on https://github.com/6jm233333/spinalsam-r1.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System</title>
<link>https://arxiv.org/abs/2511.00096</link>
<guid>https://arxiv.org/abs/2511.00096</guid>
<content:encoded><![CDATA[

arXiv:2511.00096v1 Announce Type: cross 
Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban tasks such as perception prediction and human dynamics. Large Language Models (LLMs) can integrate multimodal inputs to address heterogeneous data in complex urban systems but often underperform on domain-specific tasks. Urban-MAS, an LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered urban prediction under zero-shot settings. It includes three agent types: Predictive Factor Guidance Agents, which prioritize key predictive factors to guide knowledge extraction and enhance the effectiveness of compressed urban knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve robustness by comparing multiple outputs, validating consistency, and re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which integrate extracted multi-source information across dimensions for prediction. Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors compared to single-LLM baselines. Ablation studies indicate that Predictive Factor Guidance Agents are most critical for enhancing predictive performance, positioning Urban-MAS as a scalable paradigm for human-centered urban AI prediction. Code is available on the project website:https://github.com/THETUREHOOHA/UrbanMAS
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation</title>
<link>https://arxiv.org/abs/2511.00097</link>
<guid>https://arxiv.org/abs/2511.00097</guid>
<content:encoded><![CDATA[

arXiv:2511.00097v1 Announce Type: cross 
Abstract: Graph incremental learning (GIL), which continuously updates graph models by sequential knowledge acquisition, has garnered significant interest recently. However, existing GIL approaches focus on task-incremental and class-incremental scenarios within a single domain. Graph domain-incremental learning (Domain-IL), aiming at updating models across multiple graph domains, has become critical with the development of graph foundation models (GFMs), but remains unexplored in the literature. In this paper, we propose Graph Domain-Incremental Learning via Knowledge Dientanglement and Preservation (GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from the perspectives of embedding shifts and decision boundary deviations. Specifically, to prevent embedding shifts and confusion across incremental graph domains, we first propose the domain-specific parameter-efficient fine-tuning together with intra- and inter-domain disentanglement objectives. Consequently, to maintain a stable decision boundary, we introduce deviation-free knowledge preservation to continuously fit incremental domains. Additionally, for graphs with unobservable domains, we perform domain-aware distribution discrimination to obtain precise embeddings. Extensive experiments demonstrate the proposed GraphKeeper achieves state-of-the-art results with 6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover, we show GraphKeeper can be seamlessly integrated with various representative GFMs, highlighting its broad applicative potential.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
<link>https://arxiv.org/abs/2511.00098</link>
<guid>https://arxiv.org/abs/2511.00098</guid>
<content:encoded><![CDATA[

arXiv:2511.00098v1 Announce Type: cross 
Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
<link>https://arxiv.org/abs/2511.00099</link>
<guid>https://arxiv.org/abs/2511.00099</guid>
<content:encoded><![CDATA[

arXiv:2511.00099v1 Announce Type: cross 
Abstract: The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving</title>
<link>https://arxiv.org/abs/2511.00101</link>
<guid>https://arxiv.org/abs/2511.00101</guid>
<content:encoded><![CDATA[

arXiv:2511.00101v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers</title>
<link>https://arxiv.org/abs/2511.00102</link>
<guid>https://arxiv.org/abs/2511.00102</guid>
<content:encoded><![CDATA[

arXiv:2511.00102v1 Announce Type: cross 
Abstract: The discovery of conservation laws is a cornerstone of scientific progress. However, identifying these invariants from observational data remains a significant challenge. We propose a hybrid framework to automate the discovery of conserved quantities from noisy trajectory data. Our approach integrates three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that learns a continuous model of the system's dynamics, (2) a Transformer that generates symbolic candidate invariants conditioned on the learned vector field, and (3) a symbolic-numeric verifier that provides a strong numerical certificate for the validity of these candidates. We test our framework on canonical physical systems and show that it significantly outperforms baselines that operate directly on trajectory data. This work demonstrates the robustness of a decoupled learn-then-search approach for discovering mathematical principles from imperfect data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</title>
<link>https://arxiv.org/abs/2511.00103</link>
<guid>https://arxiv.org/abs/2511.00103</guid>
<content:encoded><![CDATA[

arXiv:2511.00103v1 Announce Type: cross 
Abstract: Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: https://azencot-group.github.io/FreeSliders/
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges</title>
<link>https://arxiv.org/abs/2511.00105</link>
<guid>https://arxiv.org/abs/2511.00105</guid>
<content:encoded><![CDATA[

arXiv:2511.00105v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is transforming elementary STEM education, yet evidence remains fragmented. This systematic review synthesizes 258 studies (2020-2025) examining AI applications across eight categories: intelligent tutoring systems (45% of studies), learning analytics (18%), automated assessment (12%), computer vision (8%), educational robotics (7%), multimodal sensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive content generation. The analysis shows that most studies focus on upper elementary grades (65%) and mathematics (38%), with limited cross-disciplinary STEM integration (15%). While conversational AI demonstrates moderate effectiveness (d = 0.45-0.70 where reported), only 34% of studies include standardized effect sizes. Eight major gaps limit real-world impact: fragmented ecosystems, developmental inappropriateness, infrastructure barriers, lack of privacy frameworks, weak STEM integration, equity disparities, teacher marginalization, and narrow assessment scopes. Geographic distribution is also uneven, with 90% of studies originating from North America, East Asia, and Europe. Future directions call for interoperable architectures that support authentic STEM integration, grade-appropriate design, privacy-preserving analytics, and teacher-centered implementations that enhance rather than replace human expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies</title>
<link>https://arxiv.org/abs/2511.00106</link>
<guid>https://arxiv.org/abs/2511.00106</guid>
<content:encoded><![CDATA[

arXiv:2511.00106v1 Announce Type: cross 
Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
<link>https://arxiv.org/abs/2511.00107</link>
<guid>https://arxiv.org/abs/2511.00107</guid>
<content:encoded><![CDATA[

arXiv:2511.00107v1 Announce Type: cross 
Abstract: Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2511.00108</link>
<guid>https://arxiv.org/abs/2511.00108</guid>
<content:encoded><![CDATA[

arXiv:2511.00108v1 Announce Type: cross 
Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Time: In-Context Physical Simulation with Image Generation Models</title>
<link>https://arxiv.org/abs/2511.00110</link>
<guid>https://arxiv.org/abs/2511.00110</guid>
<content:encoded><![CDATA[

arXiv:2511.00110v1 Announce Type: cross 
Abstract: We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our ``Chain of Time" method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-DRL: Teach and Learn in Reality</title>
<link>https://arxiv.org/abs/2511.00112</link>
<guid>https://arxiv.org/abs/2511.00112</guid>
<content:encoded><![CDATA[

arXiv:2511.00112v1 Announce Type: cross 
Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</title>
<link>https://arxiv.org/abs/2511.00114</link>
<guid>https://arxiv.org/abs/2511.00114</guid>
<content:encoded><![CDATA[

arXiv:2511.00114v1 Announce Type: cross 
Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference</title>
<link>https://arxiv.org/abs/2511.00115</link>
<guid>https://arxiv.org/abs/2511.00115</guid>
<content:encoded><![CDATA[

arXiv:2511.00115v1 Announce Type: cross 
Abstract: Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers</title>
<link>https://arxiv.org/abs/2511.00116</link>
<guid>https://arxiv.org/abs/2511.00116</guid>
<content:encoded><![CDATA[

arXiv:2511.00116v1 Announce Type: cross 
Abstract: Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads</title>
<link>https://arxiv.org/abs/2511.00117</link>
<guid>https://arxiv.org/abs/2511.00117</guid>
<content:encoded><![CDATA[

arXiv:2511.00117v1 Announce Type: cross 
Abstract: The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
<link>https://arxiv.org/abs/2511.00120</link>
<guid>https://arxiv.org/abs/2511.00120</guid>
<content:encoded><![CDATA[

arXiv:2511.00120v1 Announce Type: cross 
Abstract: The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models</title>
<link>https://arxiv.org/abs/2511.00124</link>
<guid>https://arxiv.org/abs/2511.00124</guid>
<content:encoded><![CDATA[

arXiv:2511.00124v1 Announce Type: cross 
Abstract: We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring multiple helper Dafny assertions with LLMs</title>
<link>https://arxiv.org/abs/2511.00125</link>
<guid>https://arxiv.org/abs/2511.00125</guid>
<content:encoded><![CDATA[

arXiv:2511.00125v1 Announce Type: cross 
Abstract: The Dafny verifier provides strong correctness guarantees but often requires numerous manual helper assertions, creating a significant barrier to adoption. We investigate the use of Large Language Models (LLMs) to automatically infer missing helper assertions in Dafny programs, with a primary focus on cases involving multiple missing assertions. To support this study, we extend the DafnyBench benchmark with curated datasets where one, two, or all assertions are removed, and we introduce a taxonomy of assertion types to analyze inference difficulty. Our approach refines fault localization through a hybrid method that combines LLM predictions with error-message heuristics. We implement this approach in a new tool called DAISY (Dafny Assertion Inference SYstem). While our focus is on multiple missing assertions, we also evaluate DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions. Notably, many programs can be verified with fewer assertions than originally present, highlighting that proofs often admit multiple valid repair strategies and that recovering every original assertion is unnecessary. These results demonstrate that automated assertion inference can substantially reduce proof engineering effort and represent a step toward more scalable and accessible formal verification.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features</title>
<link>https://arxiv.org/abs/2511.00126</link>
<guid>https://arxiv.org/abs/2511.00126</guid>
<content:encoded><![CDATA[

arXiv:2511.00126v1 Announce Type: cross 
Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al., 2022) have achieved strong average accuracy but remain unreliable in complex long-tail driving scenarios. These limitations reveal the weakness of the prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban contexts where simpler physics-based models can occasionally outperform advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic multi-expert gating framework that adaptively selects the most reliable trajectory predictor among a physics-informed LSTM, a Transformer, and a fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be substantially more informative than geometric scene descriptors. To the best of our knowledge, this is the first work to formulate trajectory expert selection as a pairwise-ranking problem over internal model signals (Burges et al., 2005), directly optimizing decision quality without requiring post-hoc calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287 samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error (FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835 m), and realizes 57.8 percent of the oracle performance bound. In open-loop simulations, after trajectory horizon alignment, the same configuration reduces FDE on left-turn scenarios by approximately 10 percent, demonstrating consistent improvements across both offline validation and open-loop evaluation. These results indicate that adaptive hybrid systems enhance trajectory reliability in safety-critical autonomous driving, providing a practical pathway beyond static single-model paradigms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells</title>
<link>https://arxiv.org/abs/2511.00129</link>
<guid>https://arxiv.org/abs/2511.00129</guid>
<content:encoded><![CDATA[

arXiv:2511.00129v1 Announce Type: cross 
Abstract: Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our AlexNet-based neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.00133</link>
<guid>https://arxiv.org/abs/2511.00133</guid>
<content:encoded><![CDATA[

arXiv:2511.00133v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.00136</link>
<guid>https://arxiv.org/abs/2511.00136</guid>
<content:encoded><![CDATA[

arXiv:2511.00136v1 Announce Type: cross 
Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: https://github.com/BUPT-ANTlab/HeraldLight.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</title>
<link>https://arxiv.org/abs/2511.00139</link>
<guid>https://arxiv.org/abs/2511.00139</guid>
<content:encoded><![CDATA[

arXiv:2511.00139v1 Announce Type: cross 
Abstract: Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
<link>https://arxiv.org/abs/2511.00141</link>
<guid>https://arxiv.org/abs/2511.00141</guid>
<content:encoded><![CDATA[

arXiv:2511.00141v1 Announce Type: cross 
Abstract: Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What a diff makes: automating code migration with large language models</title>
<link>https://arxiv.org/abs/2511.00160</link>
<guid>https://arxiv.org/abs/2511.00160</guid>
<content:encoded><![CDATA[

arXiv:2511.00160v1 Announce Type: cross 
Abstract: Modern software programs are built on stacks that are often undergoing changes that introduce updates and improvements, but may also break any project that depends upon them. In this paper we explore the use of Large Language Models (LLMs) for code migration, specifically the problem of maintaining compatibility with a dependency as it undergoes major and minor semantic version changes. We demonstrate, using metrics such as test coverage and change comparisons, that contexts containing diffs can significantly improve performance against out of the box LLMs and, in some cases, perform better than using code. We provide a dataset to assist in further development of this problem area, as well as an open-source Python package, AIMigrate, that can be used to assist with migrating code bases. In a real-world migration of TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of required changes in a single run, increasing to 80% with multiple runs, with 47% of changes generated perfectly.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effectiveness of LLMs in Temporal User Profiling for Recommendation</title>
<link>https://arxiv.org/abs/2511.00176</link>
<guid>https://arxiv.org/abs/2511.00176</guid>
<content:encoded><![CDATA[

arXiv:2511.00176v1 Announce Type: cross 
Abstract: Effectively modeling the dynamic nature of user preferences is crucial for enhancing recommendation accuracy and fostering transparency in recommender systems. Traditional user profiling often overlooks the distinction between transitory short-term interests and stable long-term preferences. This paper examines the capability of leveraging Large Language Models (LLMs) to capture these temporal dynamics, generating richer user representations through distinct short-term and long-term textual summaries of interaction histories. Our observations suggest that while LLMs tend to improve recommendation quality in domains with more active user engagement, their benefits appear less pronounced in sparser environments. This disparity likely stems from the varying distinguishability of short-term and long-term preferences across domains; the approach shows greater utility where these temporal interests are more clearly separable (e.g., Movies\&amp;TV) compared to domains with more stable user profiles (e.g., Video Games). This highlights a critical trade-off between enhanced performance and computational costs, suggesting context-dependent LLM application. Beyond predictive capability, this LLM-driven approach inherently provides an intrinsic potential for interpretability through its natural language profiles and attention weights. This work contributes insights into the practical capability and inherent interpretability of LLM-driven temporal user profiling, outlining new research directions for developing adaptive and transparent recommender systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling Enables Molecular Structure Retrieval from Coulomb Explosion Imaging</title>
<link>https://arxiv.org/abs/2511.00179</link>
<guid>https://arxiv.org/abs/2511.00179</guid>
<content:encoded><![CDATA[

arXiv:2511.00179v1 Announce Type: cross 
Abstract: Capturing the structural changes that molecules undergo during chemical reactions in real space and time is a long-standing dream and an essential prerequisite for understanding and ultimately controlling femtochemistry. A key approach to tackle this challenging task is Coulomb explosion imaging, which benefited decisively from recently emerging high-repetition-rate X-ray free-electron laser sources. With this technique, information on the molecular structure is inferred from the momentum distributions of the ions produced by the rapid Coulomb explosion of molecules. Retrieving molecular structures from these distributions poses a highly non-linear inverse problem that remains unsolved for molecules consisting of more than a few atoms. Here, we address this challenge using a diffusion-based Transformer neural network. We show that the network reconstructs unknown molecular geometries from ion-momentum distributions with a mean absolute error below one Bohr radius, which is half the length of a typical chemical bond.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Retrospect to Multi-prompt Learning across Vision and Language</title>
<link>https://arxiv.org/abs/2511.00191</link>
<guid>https://arxiv.org/abs/2511.00191</guid>
<content:encoded><![CDATA[

arXiv:2511.00191v1 Announce Type: cross 
Abstract: The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs</title>
<link>https://arxiv.org/abs/2511.00192</link>
<guid>https://arxiv.org/abs/2511.00192</guid>
<content:encoded><![CDATA[

arXiv:2511.00192v1 Announce Type: cross 
Abstract: Membership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories</title>
<link>https://arxiv.org/abs/2511.00197</link>
<guid>https://arxiv.org/abs/2511.00197</guid>
<content:encoded><![CDATA[

arXiv:2511.00197v1 Announce Type: cross 
Abstract: The increasing deployment of Large Language Model (LLM) agents for complex software engineering tasks has created a need to understand their problem-solving behaviours beyond simple success metrics. While these agents demonstrate impressive capabilities in automated issue resolution, their decision-making processes remain largely opaque. This paper presents an empirical study of agent trajectories, namely the execution traces capturing the steps agents take when attempting to resolve software issues. We analyse trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and Prometheus) on the SWE-Bench benchmark, examining both successful and failed attempts. Our investigation reveals several key insights into agent behaviour. First, we identify how distinct problem-solving strategies, such as defensive programming and context gathering, enable success in different scenarios. Second, we find that failed trajectories are consistently longer and exhibit higher variance than successful ones, with failure patterns differing significantly between agents. Third, our fault localisation analysis shows that while most trajectories correctly identify problematic files (72-81\% even in failures), success depends more on achieving approximate rather than exact code modifications. These and other findings unveiled by our study, provide a foundation for understanding agent behaviour through trajectory analysis, contributing to the development of more robust and interpretable autonomous software engineering systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training LLMs Beyond Next Token Prediction -- Filling the Mutual Information Gap</title>
<link>https://arxiv.org/abs/2511.00198</link>
<guid>https://arxiv.org/abs/2511.00198</guid>
<content:encoded><![CDATA[

arXiv:2511.00198v1 Announce Type: cross 
Abstract: Optimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides</title>
<link>https://arxiv.org/abs/2511.00209</link>
<guid>https://arxiv.org/abs/2511.00209</guid>
<content:encoded><![CDATA[

arXiv:2511.00209v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a leading framework in generative modeling, showing significant potential to accelerate and transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We analyze how a unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the need for more accurate scoring functions, the scarcity of high-quality experimental data, and the crucial requirement for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from chemical exploration to the targeted creation of novel therapeutics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
<link>https://arxiv.org/abs/2511.00211</link>
<guid>https://arxiv.org/abs/2511.00211</guid>
<content:encoded><![CDATA[

arXiv:2511.00211v1 Announce Type: cross 
Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet's essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy</title>
<link>https://arxiv.org/abs/2511.00218</link>
<guid>https://arxiv.org/abs/2511.00218</guid>
<content:encoded><![CDATA[

arXiv:2511.00218v1 Announce Type: cross 
Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM's simultaneous capture of complementary illumination and phase cues for robust cell segmentation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00222</link>
<guid>https://arxiv.org/abs/2511.00222</guid>
<content:encoded><![CDATA[

arXiv:2511.00222v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics: prompt-to-line consistency, line-to-line consistency, and Q&amp;A consistency, that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent and faithful simulated users.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI</title>
<link>https://arxiv.org/abs/2511.00230</link>
<guid>https://arxiv.org/abs/2511.00230</guid>
<content:encoded><![CDATA[

arXiv:2511.00230v1 Announce Type: cross 
Abstract: Millions of users now design personalized LLM-based chatbots that shape their daily interactions, yet they can only loosely anticipate how their design choices will manifest as behaviors in deployment. This opacity is consequential: seemingly innocuous prompts can trigger excessive sycophancy, toxicity, or inconsistency, degrading utility and raising safety concerns. To address this issue, we introduce an interface that enables neural transparency by exposing language model internals during chatbot design. Our approach extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by computing differences in neural activations between contrastive system prompts that elicit opposing behaviors. We predict chatbot behaviors by projecting the system prompt's final token activations onto these trait vectors, normalizing for cross-trait comparability, and visualizing results via an interactive sunburst diagram. To evaluate this approach, we conducted an online user study using Prolific to compare our neural transparency interface against a baseline chatbot interface without any form of transparency. Our analyses suggest that users systematically miscalibrated AI behavior: participants misjudged trait activations for eleven of fifteen analyzable traits, motivating the need for transparency tools in everyday human-AI interaction. While our interface did not change design iteration patterns, it significantly increased user trust and was enthusiastically received. Qualitative analysis indicated that users' had nuanced experiences with the visualization that may enrich future work designing neurally transparent interfaces. This work offers a path for how mechanistic interpretability can be operationalized for non-technical users, establishing a foundation for safer, more aligned human-AI interactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval</title>
<link>https://arxiv.org/abs/2511.00268</link>
<guid>https://arxiv.org/abs/2511.00268</guid>
<content:encoded><![CDATA[

arXiv:2511.00268v1 Announce Type: cross 
Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers to date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM-based re-ranking approach that gives the best performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
<link>https://arxiv.org/abs/2511.00269</link>
<guid>https://arxiv.org/abs/2511.00269</guid>
<content:encoded><![CDATA[

arXiv:2511.00269v1 Announce Type: cross 
Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
<link>https://arxiv.org/abs/2511.00270</link>
<guid>https://arxiv.org/abs/2511.00270</guid>
<content:encoded><![CDATA[

arXiv:2511.00270v1 Announce Type: cross 
Abstract: Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Flash-Omni Technical Report</title>
<link>https://arxiv.org/abs/2511.00279</link>
<guid>https://arxiv.org/abs/2511.00279</guid>
<content:encoded><![CDATA[

arXiv:2511.00279v1 Announce Type: cross 
Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibration Across Layers: Understanding Calibration Evolution in LLMs</title>
<link>https://arxiv.org/abs/2511.00280</link>
<guid>https://arxiv.org/abs/2511.00280</guid>
<content:encoded><![CDATA[

arXiv:2511.00280v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Modeling With Factorization Memory</title>
<link>https://arxiv.org/abs/2511.00315</link>
<guid>https://arxiv.org/abs/2511.00315</guid>
<content:encoded><![CDATA[

arXiv:2511.00315v1 Announce Type: cross 
Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data</title>
<link>https://arxiv.org/abs/2511.00318</link>
<guid>https://arxiv.org/abs/2511.00318</guid>
<content:encoded><![CDATA[

arXiv:2511.00318v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic tabular data, yet existing approaches often fail to preserve key causal parameters such as the average treatment effect (ATE). In this technical exploration, we first demonstrate that state-of-the-art synthetic data generators, both GAN- and LLM-based, can achieve high predictive fidelity while substantially misestimating causal effects. To address this gap, we propose a hybrid generation framework that combines model-based covariate synthesis (monitored via distance-to-closest-record filtering) with separately learned propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain their underlying causal structure. We further introduce a synthetic pairing strategy to mitigate positivity violations and a realistic evaluation protocol that leverages unlimited synthetic samples to benchmark traditional estimators (IPTW, AIPW, substitution) under complex covariate distributions. This work lays the groundwork for LLM-powered data pipelines that support robust causal analysis. Our code is available at https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits</title>
<link>https://arxiv.org/abs/2511.00321</link>
<guid>https://arxiv.org/abs/2511.00321</guid>
<content:encoded><![CDATA[

arXiv:2511.00321v1 Announce Type: cross 
Abstract: The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Automated Petrography</title>
<link>https://arxiv.org/abs/2511.00328</link>
<guid>https://arxiv.org/abs/2511.00328</guid>
<content:encoded><![CDATA[

arXiv:2511.00328v1 Announce Type: cross 
Abstract: Petrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research</title>
<link>https://arxiv.org/abs/2511.00342</link>
<guid>https://arxiv.org/abs/2511.00342</guid>
<content:encoded><![CDATA[

arXiv:2511.00342v1 Announce Type: cross 
Abstract: We present MH-1M, one of the most comprehensive and up-to-date datasets for advanced Android malware research. The dataset comprises 1,340,515 applications, encompassing a wide range of features and extensive metadata. To ensure accurate malware classification, we employ the VirusTotal API, integrating multiple detection engines for comprehensive and reliable assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide open access to the processed dataset and its extensive supplementary metadata, totaling more than 400 GB of data and including the outputs of the feature extraction pipeline as well as the corresponding VirusTotal reports. Our findings underscore the MH-1M dataset's invaluable role in understanding the evolving landscape of malware.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks</title>
<link>https://arxiv.org/abs/2511.00346</link>
<guid>https://arxiv.org/abs/2511.00346</guid>
<content:encoded><![CDATA[

arXiv:2511.00346v1 Announce Type: cross 
Abstract: The rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach</title>
<link>https://arxiv.org/abs/2511.00352</link>
<guid>https://arxiv.org/abs/2511.00352</guid>
<content:encoded><![CDATA[

arXiv:2511.00352v1 Announce Type: cross 
Abstract: The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Unifying Group Fairness Evaluation from a Sparsity Perspective</title>
<link>https://arxiv.org/abs/2511.00359</link>
<guid>https://arxiv.org/abs/2511.00359</guid>
<content:encoded><![CDATA[

arXiv:2511.00359v1 Announce Type: cross 
Abstract: Ensuring algorithmic fairness remains a significant challenge in machine learning, particularly as models are increasingly applied across diverse domains. While numerous fairness criteria exist, they often lack generalizability across different machine learning problems. This paper examines the connections and differences among various sparsity measures in promoting fairness and proposes a unified sparsity-based framework for evaluating algorithmic fairness. The framework aligns with existing fairness criteria and demonstrates broad applicability to a wide range of machine learning tasks. We demonstrate the effectiveness of the proposed framework as an evaluation metric through extensive experiments on a variety of datasets and bias mitigation methods. This work provides a novel perspective to algorithmic fairness by framing it through the lens of sparsity and social equity, offering potential for broader impact on fairness research and applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap: Missing Cyber Threat Coverage in NIDS Datasets for the Energy Sector</title>
<link>https://arxiv.org/abs/2511.00360</link>
<guid>https://arxiv.org/abs/2511.00360</guid>
<content:encoded><![CDATA[

arXiv:2511.00360v1 Announce Type: cross 
Abstract: Network Intrusion Detection Systems (NIDS) developed using publicly available datasets predominantly focus on enterprise environments, raising concerns about their effectiveness for converged Information Technology (IT) and Operational Technology (OT) in energy infrastructures. This study evaluates the representativeness of five widely used datasets: CIC-IDS2017, SWaT, WADI, Sherlock, and CIC-Modbus2023 against network-detectable MITRE ATT&amp;CK techniques extracted from documented energy sector incidents. Using a structured five-step analytical approach, this article successfully developed and performed a gap analysis that identified 94 network observable techniques from an initial pool of 274 ATT&amp;CK techniques. Sherlock dataset exhibited the highest mean coverage (0.56), followed closely by CIC-IDS2017 (0.55), while SWaT and WADI recorded the lowest scores (0.38). Combining CIC-IDS2017, Sherlock, and CIC-Modbus2023 achieved an aggregate coverage of 92%, highlighting their complementary strengths. The analysis identifies critical gaps, particularly in lateral movement and industrial protocol manipulation, providing a clear pathway for dataset enhancement and more robust NIDS evaluation in hybrid IT/OT energy environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MalDataGen: A Modular Framework for Synthetic Tabular Data Generation in Malware Detection</title>
<link>https://arxiv.org/abs/2511.00361</link>
<guid>https://arxiv.org/abs/2511.00361</guid>
<content:encoded><![CDATA[

arXiv:2511.00361v1 Announce Type: cross 
Abstract: High-quality data scarcity hinders malware detection, limiting ML performance. We introduce MalDataGen, an open-source modular framework for generating high-fidelity synthetic tabular data using modular deep learning models (e.g., WGAN-GP, VQ-VAE). Evaluated via dual validation (TR-TS/TS-TR), seven classifiers, and utility metrics, MalDataGen outperforms benchmarks like SDV while preserving data utility. Its flexible design enables seamless integration into detection pipelines, offering a practical solution for cybersecurity applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</title>
<link>https://arxiv.org/abs/2511.00362</link>
<guid>https://arxiv.org/abs/2511.00362</guid>
<content:encoded><![CDATA[

arXiv:2511.00362v1 Announce Type: cross 
Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet</title>
<link>https://arxiv.org/abs/2511.00369</link>
<guid>https://arxiv.org/abs/2511.00369</guid>
<content:encoded><![CDATA[

arXiv:2511.00369v1 Announce Type: cross 
Abstract: Achieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict</title>
<link>https://arxiv.org/abs/2511.00370</link>
<guid>https://arxiv.org/abs/2511.00370</guid>
<content:encoded><![CDATA[

arXiv:2511.00370v1 Announce Type: cross 
Abstract: Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment's boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents' localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</title>
<link>https://arxiv.org/abs/2511.00392</link>
<guid>https://arxiv.org/abs/2511.00392</guid>
<content:encoded><![CDATA[

arXiv:2511.00392v1 Announce Type: cross 
Abstract: Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Detection in Speech Using Lightweight and Transformer-Based Models: A Comparative and Ablation Study</title>
<link>https://arxiv.org/abs/2511.00402</link>
<guid>https://arxiv.org/abs/2511.00402</guid>
<content:encoded><![CDATA[

arXiv:2511.00402v1 Announce Type: cross 
Abstract: Emotion recognition from speech plays a vital role in the development of empathetic human-computer interaction systems. This paper presents a comparative analysis of lightweight transformer-based models, DistilHuBERT and PaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark their performance against a traditional CNN-LSTM baseline model using MFCC features. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score (70.36%) while maintaining an exceptionally small model size (0.02 MB), outperforming both PaSST and the baseline. Furthermore, we conducted an ablation study on three variants of the PaSST, Linear, MLP, and Attentive Pooling heads, to understand the effect of classification head architecture on model performance. Our results indicate that PaSST with an MLP head yields the best performance among its variants but still falls short of DistilHuBERT. Among the emotion classes, angry is consistently the most accurately detected, while disgust remains the most challenging. These findings suggest that lightweight transformers like DistilHuBERT offer a compelling solution for real-time speech emotion recognition on edge devices. The code is available at: https://github.com/luckymaduabuchi/Emotion-detection-.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2511.00405</link>
<guid>https://arxiv.org/abs/2511.00405</guid>
<content:encoded><![CDATA[

arXiv:2511.00405v1 Announce Type: cross 
Abstract: The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Unlearning: Foundations, Mechanisms, and Taxonomy</title>
<link>https://arxiv.org/abs/2511.00406</link>
<guid>https://arxiv.org/abs/2511.00406</guid>
<content:encoded><![CDATA[

arXiv:2511.00406v1 Announce Type: cross 
Abstract: Quantum Machine Unlearning has emerged as a foundational challenge at the intersection of quantum information theory privacypreserving computation and trustworthy artificial intelligence This paper advances QMU by establishing a formal framework that unifies physical constraints algorithmic mechanisms and ethical governance within a verifiable paradigm We define forgetting as a contraction of distinguishability between pre and postunlearning models under completely positive trace-preserving dynamics grounding data removal in the physics of quantum irreversibility Building on this foundation we present a fiveaxis taxonomy spanning scope guarantees mechanisms system context and hardware realization linking theoretical constructs to implementable strategies Within this structure we incorporate influence and quantum Fisher information weighted updates parameter reinitialization and kernel alignment as practical mechanisms compatible with noisy intermediatescale quantum NISQ devices The framework extends naturally to federated and privacyaware settings via quantum differential privacy homomorphic encryption and verifiable delegation enabling scalable auditable deletion across distributed quantum systems Beyond technical design we outline a forwardlooking research roadmap emphasizing formal proofs of forgetting scalable and secure architectures postunlearning interpretability and ethically auditable governance Together these contributions elevate QMU from a conceptual notion to a rigorously defined and ethically aligned discipline bridging physical feasibility algorithmic verifiability and societal accountability in the emerging era of quantum intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</title>
<link>https://arxiv.org/abs/2511.00411</link>
<guid>https://arxiv.org/abs/2511.00411</guid>
<content:encoded><![CDATA[

arXiv:2511.00411v1 Announce Type: cross 
Abstract: Adversarial attacks present a critical challenge to deep neural networks' robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling's magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks</title>
<link>https://arxiv.org/abs/2511.00416</link>
<guid>https://arxiv.org/abs/2511.00416</guid>
<content:encoded><![CDATA[

arXiv:2511.00416v1 Announce Type: cross 
Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct LLM outputs, they fail catastrophically against iteratively-paraphrased content. We investigate why iteratively-paraphrased text -- itself AI-generated -- evades detection systems designed for AIGT identification. Through intrinsic mechanism analysis, we reveal that iterative paraphrasing creates an intermediate laundering region characterized by semantic displacement with preserved generation patterns, which brings up two attack categories: paraphrasing human-authored text (authorship obfuscation) and paraphrasing LLM-generated text (plagiarism evasion). To address these vulnerabilities, we introduce PADBen, the first benchmark systematically evaluating detector robustness against both paraphrase attack scenarios. PADBen comprises a five-type text taxonomy capturing the full trajectory from original content to deeply laundered text, and five progressive detection tasks across sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art detectors, revealing critical asymmetry: detectors successfully identify the plagiarism evasion problem but fail for the case of authorship obfuscation. Our findings demonstrate that current detection approaches cannot effectively handle the intermediate laundering region, necessitating fundamental advances in detection architectures beyond existing semantic and stylistic discrimination methods. For detailed code implementation, please see https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework</title>
<link>https://arxiv.org/abs/2511.00417</link>
<guid>https://arxiv.org/abs/2511.00417</guid>
<content:encoded><![CDATA[

arXiv:2511.00417v1 Announce Type: cross 
Abstract: As artificial intelligence transforms software development, a critical question emerges: how can developers and AI systems collaborate most effectively? This dissertation optimizes human-AI programming roles through self-determination theory and personality psychology, introducing the Role Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes empirically-validated connections between personality traits, programming role preferences, and collaborative outcomes, engaging 200 experimental participants and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization significantly enhances self-determination and team dynamics, yielding 23% average motivation increases among professionals and up to 65% among undergraduates. Five distinct personality archetypes emerge: The Explorer (high Openness/low Agreeableness), The Orchestrator (high Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low Extraversion), The Architect (high Conscientiousness), and The Adapter (balanced profile). Each exhibits distinct preferences for programming roles (Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking personality traits to role preferences and self-determination outcomes; (2) a taxonomy of AI collaboration modalities mapped to personality profiles while preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small Entities to implement personality-driven role optimization within established standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral software engineering, self-determination theory, personality psychology, phenomenology, intrinsic motivation, pair programming, design science research, ISO/IEC 29110
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGCA: Enhancing Semantic Representation via Progressive Expansion</title>
<link>https://arxiv.org/abs/2511.00419</link>
<guid>https://arxiv.org/abs/2511.00419</guid>
<content:encoded><![CDATA[

arXiv:2511.00419v1 Announce Type: cross 
Abstract: Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts</title>
<link>https://arxiv.org/abs/2511.00421</link>
<guid>https://arxiv.org/abs/2511.00421</guid>
<content:encoded><![CDATA[

arXiv:2511.00421v1 Announce Type: cross 
Abstract: Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrap Off-policy with World Model</title>
<link>https://arxiv.org/abs/2511.00423</link>
<guid>https://arxiv.org/abs/2511.00423</guid>
<content:encoded><![CDATA[

arXiv:2511.00423v1 Announce Type: cross 
Abstract: Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</title>
<link>https://arxiv.org/abs/2511.00427</link>
<guid>https://arxiv.org/abs/2511.00427</guid>
<content:encoded><![CDATA[

arXiv:2511.00427v1 Announce Type: cross 
Abstract: With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP's space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection</title>
<link>https://arxiv.org/abs/2511.00429</link>
<guid>https://arxiv.org/abs/2511.00429</guid>
<content:encoded><![CDATA[

arXiv:2511.00429v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</title>
<link>https://arxiv.org/abs/2511.00443</link>
<guid>https://arxiv.org/abs/2511.00443</guid>
<content:encoded><![CDATA[

arXiv:2511.00443v1 Announce Type: cross 
Abstract: The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026</title>
<link>https://arxiv.org/abs/2511.00444</link>
<guid>https://arxiv.org/abs/2511.00444</guid>
<content:encoded><![CDATA[

arXiv:2511.00444v1 Announce Type: cross 
Abstract: Late interaction retrieval methods, pioneered by ColBERT, have emerged as a powerful alternative to single-vector neural IR. By leveraging fine-grained, token-level representations, they have been demonstrated to deliver strong generalisation and robustness, particularly in out-of-domain settings. They have recently been shown to be particularly well-suited for novel use cases, such as reasoning-based or cross-modality retrieval. At the same time, these models pose significant challenges of efficiency, usability, and integrations into fully fledged systems; as well as the natural difficulties encountered while researching novel application domains. Recent years have seen rapid advances across many of these areas, but research efforts remain fragmented across communities and frequently exclude practitioners. The purpose of this workshop is to create an environment where all aspects of late interaction can be discussed, with a focus on early research explorations, real-world outcomes, and negative or puzzling results to be freely shared and discussed. The aim of LIR is to provide a highly-interactive environment for researchers from various backgrounds and practitioners to freely discuss their experience, fostering further collaboration.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture</title>
<link>https://arxiv.org/abs/2511.00447</link>
<guid>https://arxiv.org/abs/2511.00447</guid>
<content:encoded><![CDATA[

arXiv:2511.00447v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive instruction-following capabilities. However, these capabilities also expose models to prompt injection attacks, where maliciously crafted inputs overwrite or distract from the intended instructions. A core vulnerability lies in the model's lack of semantic role understanding: it cannot distinguish directive intent from descriptive content, leading it to execute instruction-like phrases embedded in data.
  We propose DRIP, a training-time defense grounded in a semantic modeling perspective, which enforces robust separation between instruction and data semantics without sacrificing utility. DRIP introduces two lightweight yet complementary mechanisms: (1) a token-wise de-instruction shift that performs semantic disentanglement, weakening directive semantics in data tokens while preserving content meaning; and (2) a residual fusion pathway that provides a persistent semantic anchor, reinforcing the influence of the true top-level instruction during generation. Experimental results on LLaMA-8B and Mistral-7B across three prompt injection benchmarks (SEP, AlpacaFarm, and InjecAgent) demonstrate that DRIP outperforms state-of-the-art defenses, including StruQ, SecAlign, ISE, and PFT, improving role separation by 49%, and reducing attack success rate by 66% for adaptive attacks. Meanwhile, DRIP's utility is on par with the undefended model across AlpacaEval, IFEval, and MT-Bench. Our findings underscore the power of lightweight representation edits and role-aware supervision in securing LLMs against adaptive prompt injection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models</title>
<link>https://arxiv.org/abs/2511.00460</link>
<guid>https://arxiv.org/abs/2511.00460</guid>
<content:encoded><![CDATA[

arXiv:2511.00460v1 Announce Type: cross 
Abstract: Centralized Software-Defined Networking (cSDN) offers flexible and programmable control of networks but suffers from scalability and reliability issues due to its reliance on centralized controllers. Decentralized SDN (dSDN) alleviates these concerns by distributing control across multiple local controllers, yet this architecture remains highly vulnerable to Distributed Denial-of-Service (DDoS) attacks. In this paper, we propose a novel detection and mitigation framework tailored for dSDN environments. The framework leverages lightweight port-level statistics combined with prompt engineering and in-context learning, enabling the DeepSeek-v3 Large Language Model (LLM) to classify traffic as benign or malicious without requiring fine-tuning or retraining. Once an anomaly is detected, mitigation is enforced directly at the attacker's port, ensuring that malicious traffic is blocked at their origin while normal traffic remains unaffected. An automatic recovery mechanism restores normal operation after the attack inactivity, ensuring both security and availability. Experimental evaluation under diverse DDoS attack scenarios demonstrates that the proposed approach achieves near-perfect detection, with 99.99% accuracy, 99.97% precision, 100% recall, 99.98% F1-score, and an AUC of 1.0. These results highlight the effectiveness of combining distributed monitoring with zero-training LLM inference, providing a proactive and scalable defense mechanism for securing dSDN infrastructures against DDoS threats.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima</title>
<link>https://arxiv.org/abs/2511.00469</link>
<guid>https://arxiv.org/abs/2511.00469</guid>
<content:encoded><![CDATA[

arXiv:2511.00469v1 Announce Type: cross 
Abstract: Federated optimization is a constrained form of distributed optimization that enables training a global model without directly sharing client data. Although existing algorithms can guarantee convergence in theory and often achieve stable training in practice, the reasons behind performance degradation under data heterogeneity remain unclear. To address this gap, the main contribution of this paper is to provide a theoretical perspective that explains why such degradation occurs. We introduce the assumption that heterogeneous client data lead to distinct local optima, and show that this assumption implies two key consequences: 1) the distance among clients' local optima raises the lower bound of the global objective, making perfect fitting of all client data impossible; and 2) in the final training stage, the global model oscillates within a region instead of converging to a single optimum, limiting its ability to fully fit the data. These results provide a principled explanation for performance degradation in non-iid settings, which we further validate through experiments across multiple tasks and neural network architectures. The framework used in this paper is open-sourced at: https://github.com/NPCLEI/fedtorch.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations</title>
<link>https://arxiv.org/abs/2511.00472</link>
<guid>https://arxiv.org/abs/2511.00472</guid>
<content:encoded><![CDATA[

arXiv:2511.00472v1 Announce Type: cross 
Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation</title>
<link>https://arxiv.org/abs/2511.00477</link>
<guid>https://arxiv.org/abs/2511.00477</guid>
<content:encoded><![CDATA[

arXiv:2511.00477v1 Announce Type: cross 
Abstract: Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model's actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI</title>
<link>https://arxiv.org/abs/2511.00494</link>
<guid>https://arxiv.org/abs/2511.00494</guid>
<content:encoded><![CDATA[

arXiv:2511.00494v1 Announce Type: cross 
Abstract: The growing number of smart devices supporting bandwidth-intensive and latency-sensitive applications, such as real-time video analytics, smart sensing, and Extended Reality (XR), necessitates reliable wireless connectivity in indoor environments. Therein, accurate estimation of Radio Environment Maps (REMs) enables adaptive wireless network planning and optimization of Access Point (AP) placement. However, generating realistic REMs remains challenging due to the complexity of indoor spaces. To overcome this challenge, this paper introduces a multimodal dataset that integrates high-resolution 3D LiDAR scans with Wi-Fi Received Signal Strength Indicator (RSSI) measurements collected under 20 distinct AP configurations in a multi-room indoor environment. The dataset captures two measurement scenarios: the first without human presence in the environment, and the second with human presence. Thus, the presented dataset supports the study of dynamic environmental effects on wireless signal propagation. This resource is designed to facilitate research in data-driven wireless modeling, particularly in the context of emerging high-frequency standards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the development of robust, high-capacity indoor communication systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Planning for Language Models</title>
<link>https://arxiv.org/abs/2511.00521</link>
<guid>https://arxiv.org/abs/2511.00521</guid>
<content:encoded><![CDATA[

arXiv:2511.00521v1 Announce Type: cross 
Abstract: Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models</title>
<link>https://arxiv.org/abs/2511.00527</link>
<guid>https://arxiv.org/abs/2511.00527</guid>
<content:encoded><![CDATA[

arXiv:2511.00527v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains, raising the need for rigorous reliability assessment methods. Existing benchmark-based evaluations primarily offer descriptive statistics of model accuracy over datasets, providing limited insight into the probabilistic behavior of LLMs under real operational conditions. This paper introduces HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and inferring LLM reliability. Building upon the foundations of software reliability engineering, HIP-LLM defines LLM reliability as the probability of failure-free operation over a specified number of future tasks under a given Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains hierarchically, enabling multi-level inference from subdomain to system-level reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty and incorporates OPs to reflect usage contexts. It derives posterior reliability envelopes that quantify uncertainty across priors and data. Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a more accurate and standardized reliability characterization than existing benchmark and state-of-the-art approaches. A publicly accessible repository of HIP-LLM is provided.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Improvisation and Open-Endedness: Insights for Experiential AI</title>
<link>https://arxiv.org/abs/2511.00529</link>
<guid>https://arxiv.org/abs/2511.00529</guid>
<content:encoded><![CDATA[

arXiv:2511.00529v1 Announce Type: cross 
Abstract: Improvisation-the art of spontaneous creation that unfolds moment-to-moment without a scripted outcome-requires practitioners to continuously sense, adapt, and create anew. It is a fundamental mode of human creativity spanning music, dance, and everyday life. The open-ended nature of improvisation produces a stream of novel, unrepeatable moments-an aspect highly valued in artistic creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded novelty and endless "interestingness"-is exemplified in natural or cultural evolution and has been considered "the last grand challenge" in artificial life (ALife). The rise of generative AI now raises the question in computational creativity (CC) research: What makes a "good" improvisation for AI? Can AI learn to improvise in a genuinely open-ended way? In this work-in-progress paper, we report insights from in-depth interviews with 6 experts in improvisation across dance, music, and contact improvisation. We draw systemic connections between human improvisational arts and the design of future experiential AI agents that could improvise alone or alongside humans-or even with other AI agents-embodying qualities of improvisation drawn from practice: active listening (umwelt and awareness), being in the time (mindfulness and ephemerality), embracing the unknown (source of randomness and serendipity), non-judgmental flow (acceptance and dynamical stability, balancing structure and surprise (unpredictable criticality at edge of chaos), imaginative metaphor (synaesthesia and planning), empathy, trust, boundary, and care (mutual theory of mind), and playfulness and intrinsic motivation (maintaining interestingness).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Air Pollution Forecasting in Bucharest</title>
<link>https://arxiv.org/abs/2511.00532</link>
<guid>https://arxiv.org/abs/2511.00532</guid>
<content:encoded><![CDATA[

arXiv:2511.00532v1 Announce Type: cross 
Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a growing concern in recent years, primarily in urban areas. Being exposed to air pollution is linked to developing numerous health problems, like the aggravation of respiratory diseases, cardiovascular disorders, lung function impairment, and even cancer or early death. Forecasting future levels of PM2.5 has become increasingly important over the past few years, as it can provide early warnings and help prevent diseases. This paper aims to design, fine-tune, test, and evaluate machine learning models for predicting future levels of PM2.5 over various time horizons. Our primary objective is to assess and compare the performance of multiple models, ranging from linear regression algorithms and ensemble-based methods to deep learning models, such as advanced recurrent neural networks and transformers, as well as large language models, on this forecasting task.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations</title>
<link>https://arxiv.org/abs/2511.00549</link>
<guid>https://arxiv.org/abs/2511.00549</guid>
<content:encoded><![CDATA[

arXiv:2511.00549v1 Announce Type: cross 
Abstract: Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales</title>
<link>https://arxiv.org/abs/2511.00552</link>
<guid>https://arxiv.org/abs/2511.00552</guid>
<content:encoded><![CDATA[

arXiv:2511.00552v1 Announce Type: cross 
Abstract: Accurate multi-horizon retail forecasts are critical for inventory and promotions. We present a novel study of weekly Walmart sales (45 stores, 2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store identifiers with time-varying exogenous signals (holidays, CPI, fuel price, temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via Quantile Loss, yielding calibrated 90\% prediction intervals and interpretability through variable-selection networks, static enrichment, and temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of \$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ = 0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These results demonstrate practical value for inventory planning and holiday-period optimization, while maintaining model transparency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Red-teaming Activation Probes using Prompted LLMs</title>
<link>https://arxiv.org/abs/2511.00554</link>
<guid>https://arxiv.org/abs/2511.00554</guid>
<content:encoded><![CDATA[

arXiv:2511.00554v1 Announce Type: cross 
Abstract: Activation probes are attractive monitors for AI systems due to low cost and latency, but their real-world robustness remains underexplored. We ask: What failure modes arise under realistic, black-box adversarial pressure, and how can we surface them with minimal effort? We present a lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access. Running a case study with probes for high-stakes interactions, we show that our approach can help discover valuable insights about a SOTA probe. Our analysis uncovers interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but persistent vulnerabilities under scenario-constraint attacks. These results suggest that simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and might yield promising, actionable insights to harden future probes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction</title>
<link>https://arxiv.org/abs/2511.00564</link>
<guid>https://arxiv.org/abs/2511.00564</guid>
<content:encoded><![CDATA[

arXiv:2511.00564v1 Announce Type: cross 
Abstract: Accurate prediction of the remaining useful life (RUL) of industrial machinery is essential for reducing downtime and optimizing maintenance schedules. Existing approaches, such as long short-term memory (LSTM) networks and convolutional neural networks (CNNs), often struggle to model both global temporal dependencies and fine-grained degradation trends in multivariate sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal Transformer (FTT) -- a lightweight Transformer variant using linearized attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU) layer for sequential modeling. To the best of our knowledge, this is the first application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling simultaneous capture of global and local degradation patterns in a compact architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and $R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%. Training curves averaged over $k=3$ runs show smooth convergence with narrow 95\% confidence bands, and ablations (GRU-only, FTT-only) support the contribution of both components. These results demonstrate that a compact Transformer-RNN hybrid delivers accurate and efficient RUL predictions on CMAPSS, making it suitable for real-time industrial prognostics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashEVA: Accelerating LLM inference via Efficient Attention</title>
<link>https://arxiv.org/abs/2511.00576</link>
<guid>https://arxiv.org/abs/2511.00576</guid>
<content:encoded><![CDATA[

arXiv:2511.00576v1 Announce Type: cross 
Abstract: Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.00580</link>
<guid>https://arxiv.org/abs/2511.00580</guid>
<content:encoded><![CDATA[

arXiv:2511.00580v1 Announce Type: cross 
Abstract: Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation</title>
<link>https://arxiv.org/abs/2511.00588</link>
<guid>https://arxiv.org/abs/2511.00588</guid>
<content:encoded><![CDATA[

arXiv:2511.00588v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EPARA: Parallelizing Categorized AI Inference in Edge Clouds</title>
<link>https://arxiv.org/abs/2511.00603</link>
<guid>https://arxiv.org/abs/2511.00603</guid>
<content:encoded><![CDATA[

arXiv:2511.00603v1 Announce Type: cross 
Abstract: With the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering</title>
<link>https://arxiv.org/abs/2511.00617</link>
<guid>https://arxiv.org/abs/2511.00617</guid>
<content:encoded><![CDATA[

arXiv:2511.00617v1 Announce Type: cross 
Abstract: Large language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.00628</link>
<guid>https://arxiv.org/abs/2511.00628</guid>
<content:encoded><![CDATA[

arXiv:2511.00628v1 Announce Type: cross 
Abstract: With the rapid progress of large language models (LLMs), LLM-powered multi-agent systems (MAS) are drawing increasing interest across academia and industry. However, many current MAS frameworks struggle with reliability and scalability, especially on complex tasks. We present AgentGit, a framework that brings Git-like rollback and branching to MAS workflows. Built as an infrastructure layer on top of LangGraph, AgentGit supports state commit, revert, and branching, allowing agents to traverse, compare, and explore multiple trajectories efficiently. To evaluate AgentGit, we designed an experiment that optimizes target agents by selecting better prompts. We ran a multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno -- on a real-world task: retrieving and analyzing paper abstracts. Results show that AgentGit significantly reduces redundant computation, lowers runtime and token usage, and supports parallel exploration across multiple branches, enhancing both reliability and scalability in MAS development. This work offers a practical path to more robust MAS design and enables error recovery, safe exploration, iterative debugging, and A/B testing in collaborative AI systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Node Preservation and its Effect on Crossover in Cartesian Genetic Programming</title>
<link>https://arxiv.org/abs/2511.00634</link>
<guid>https://arxiv.org/abs/2511.00634</guid>
<content:encoded><![CDATA[

arXiv:2511.00634v1 Announce Type: cross 
Abstract: While crossover is a critical and often indispensable component in other forms of Genetic Programming, such as Linear- and Tree-based, it has consistently been claimed that it deteriorates search performance in CGP. As a result, a mutation-alone $(1+\lambda)$ evolutionary strategy has become the canonical approach for CGP. Although several operators have been developed that demonstrate an increased performance over the canonical method, a general solution to the problem is still lacking. In this paper, we compare basic crossover methods, namely one-point and uniform, to variants in which nodes are ``preserved,'' including the subgraph crossover developed by Roman Kalkreuth, the difference being that when ``node preservation'' is active, crossover is not allowed to break apart instructions. We also compare a node mutation operator to the traditional point mutation; the former simply replaces an entire node with a new one. We find that node preservation in both mutation and crossover improves search using symbolic regression benchmark problems, moving the field towards a general solution to CGP crossover.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Than A Shortcut: A Hyperbolic Approach To Early-Exit Networks</title>
<link>https://arxiv.org/abs/2511.00641</link>
<guid>https://arxiv.org/abs/2511.00641</guid>
<content:encoded><![CDATA[

arXiv:2511.00641v1 Announce Type: cross 
Abstract: Deploying accurate event detection on resource-constrained devices is challenged by the trade-off between performance and computational cost. While Early-Exit (EE) networks offer a solution through adaptive computation, they often fail to enforce a coherent hierarchical structure, limiting the reliability of their early predictions. To address this, we propose Hyperbolic Early-Exit networks (HypEE), a novel framework that learns EE representations in the hyperbolic space. Our core contribution is a hierarchical training objective with a novel entailment loss, which enforces a partial-ordering constraint to ensure that deeper network layers geometrically refine the representations of shallower ones. Experiments on multiple audio event detection tasks and backbone architectures show that HypEE significantly outperforms standard Euclidean EE baselines, especially at the earliest, most computationally-critical exits. The learned geometry also provides a principled measure of uncertainty, enabling a novel triggering mechanism that makes the overall system both more efficient and more accurate than a conventional EE and standard backbone models without early-exits.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare</title>
<link>https://arxiv.org/abs/2511.00658</link>
<guid>https://arxiv.org/abs/2511.00658</guid>
<content:encoded><![CDATA[

arXiv:2511.00658v1 Announce Type: cross 
Abstract: The advances and availability of technologies involving Generative Artificial Intelligence (AI) are evolving clearly and explicitly, driving immediate changes in various work activities. Software Engineering (SE) is no exception and stands to benefit from these new technologies, enhancing productivity and quality in its software development processes. However, although the use of Generative AI in SE practices is still in its early stages, considering the lack of conclusive results from ongoing research and the limited technological maturity, we have chosen to incorporate these technologies in the development of a web-based software system to be used in clinical trials by a thoracic diseases research group at our university. For this reason, we decided to share this experience report documenting our development team's learning journey in using Generative AI during the software development process. Project management, requirements specification, design, development, and quality assurance activities form the scope of observation. Although we do not yet have definitive technological evidence to evolve our development process significantly, the results obtained and the suggestions shared here represent valuable insights for software organizations seeking to innovate their development practices to achieve software quality with generative AI.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowLogic: Backdoors in Any Whitebox LLM</title>
<link>https://arxiv.org/abs/2511.00664</link>
<guid>https://arxiv.org/abs/2511.00664</guid>
<content:encoded><![CDATA[

arXiv:2511.00664v1 Announce Type: cross 
Abstract: Large language models (LLMs) are widely deployed across various applications, often with safeguards to prevent the generation of harmful or restricted content. However, these safeguards can be covertly bypassed through adversarial modifications to the computational graph of a model. This work highlights a critical security vulnerability in computational graph-based LLM formats, demonstrating that widely used deployment pipelines may be susceptible to obscured backdoors. We introduce ShadowLogic, a method for creating a backdoor in a white-box LLM by injecting an uncensoring vector into its computational graph representation. We set a trigger phrase that, when added to the beginning of a prompt into the LLM, applies the uncensoring vector and removes the content generation safeguards in the model. We embed trigger logic directly into the computational graph which detects the trigger phrase in a prompt. To evade detection of our backdoor, we obfuscate this logic within the graph structure, making it similar to standard model functions. Our method requires minimal alterations to model parameters, making backdoored models appear benign while retaining the ability to generate uncensored responses when activated. We successfully implement ShadowLogic in Phi-3 and Llama 3.2, using ONNX for manipulating computational graphs. Implanting the uncensoring vector achieved a >60% attack success rate for further malicious queries.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?</title>
<link>https://arxiv.org/abs/2511.00674</link>
<guid>https://arxiv.org/abs/2511.00674</guid>
<content:encoded><![CDATA[

arXiv:2511.00674v1 Announce Type: cross 
Abstract: In this paper, we introduce a model for analyzing deep learning optimization over a single iteration by leveraging the matrix structure of the weights. We derive the model by assuming isotropy of curvature, including the second-order Hessian and higher-order terms, of the loss function across all perturbation directions; hence, we call it the isotropic curvature model. This model is a convex optimization program amenable to analysis, which allows us to understand how an update on the weights in the form of a matrix relates to the change in the total loss function. As an application, we use the isotropic curvature model to analyze the recently introduced Muon optimizer and other matrix-gradient methods for training language models. First, we show that under a general growth condition on the curvature, the optimal update matrix is obtained by making the spectrum of the original gradient matrix more homogeneous -- that is, making its singular values closer in ratio -- which in particular improves the conditioning of the update matrix. Next, we show that the orthogonalized gradient becomes optimal for the isotropic curvature model when the curvature exhibits a phase transition in growth. Taken together, these results suggest that the gradient orthogonalization employed in Muon and other related methods is directionally correct but may not be strictly optimal. Finally, we discuss future research on how to leverage the isotropic curvature model for designing new optimization methods for training deep learning and language models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control</title>
<link>https://arxiv.org/abs/2511.00681</link>
<guid>https://arxiv.org/abs/2511.00681</guid>
<content:encoded><![CDATA[

arXiv:2511.00681v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolve to Inspire: Novelty Search for Diverse Image Generation</title>
<link>https://arxiv.org/abs/2511.00686</link>
<guid>https://arxiv.org/abs/2511.00686</guid>
<content:encoded><![CDATA[

arXiv:2511.00686v1 Announce Type: cross 
Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity images, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment</title>
<link>https://arxiv.org/abs/2511.00709</link>
<guid>https://arxiv.org/abs/2511.00709</guid>
<content:encoded><![CDATA[

arXiv:2511.00709v1 Announce Type: cross 
Abstract: Training mental health clinicians to conduct standardized clinical assessments is challenging due to a lack of scalable, realistic practice opportunities, which can impact data quality in clinical trials. To address this gap, we introduce a voice-enabled virtual patient simulation system powered by a large language model (LLM). This study describes the system's development and validates its ability to generate virtual patients who accurately adhere to pre-defined clinical profiles, maintain coherent narratives, and produce realistic dialogue. We implemented a system using a LLM to simulate patients with specified symptom profiles, demographics, and communication styles. The system was evaluated by 5 experienced clinical raters who conducted 20 simulated structured MADRS interviews across 4 virtual patient personas. The virtual patients demonstrated strong adherence to their clinical profiles, with a mean item difference between rater-assigned MADRS scores and configured scores of 0.52 (SD=0.75). Inter-rater reliability across items was 0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative realism and cohesiveness of the virtual patients favorably, giving average ratings between "Agree" and "Strongly Agree." Our findings suggest that LLM-powered virtual patient simulations are a viable and scalable tool for training clinicians, capable of producing high-fidelity, clinically relevant practice scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRISKELION-1: Unified Descriptive-Predictive-Generative AI</title>
<link>https://arxiv.org/abs/2511.00711</link>
<guid>https://arxiv.org/abs/2511.00711</guid>
<content:encoded><![CDATA[

arXiv:2511.00711v1 Announce Type: cross 
Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that integrates statistical, mechanistic, and generative reasoning within a single encoder-decoder framework. The model demonstrates how descriptive representation learning, predictive inference, and generative synthesis can be jointly optimized using variational objectives. Experiments on MNIST validate that descriptive reconstruction, predictive classification, and generative sampling can coexist stably within one model. The framework provides a blueprint toward universal intelligence architectures that connect interpretability, accuracy, and creativity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FeNN-DMA: A RISC-V SoC for SNN acceleration</title>
<link>https://arxiv.org/abs/2511.00732</link>
<guid>https://arxiv.org/abs/2511.00732</guid>
<content:encoded><![CDATA[

arXiv:2511.00732v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative to standard Artificial Neural Networks (ANNs) and are particularly well-suited to spatio-temporal tasks such as keyword spotting and video classification. However, SNNs have a much lower arithmetic intensity than ANNs and are therefore not well-matched to standard accelerators like GPUs and TPUs. Field Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads and here we develop a novel, fully-programmable RISC-V-based system-on-chip (FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show that FeNN-DMA has comparable resource usage and energy requirements to state-of-the-art fixed-function SNN accelerators, yet it is capable of simulating much larger and more complex models. Using this functionality, we demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg Digits and Neuromorphic MNIST tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EP-HDC: Hyperdimensional Computing with Encrypted Parameters for High-Throughput Privacy-Preserving Inference</title>
<link>https://arxiv.org/abs/2511.00737</link>
<guid>https://arxiv.org/abs/2511.00737</guid>
<content:encoded><![CDATA[

arXiv:2511.00737v1 Announce Type: cross 
Abstract: While homomorphic encryption (HE) provides strong privacy protection, its high computational cost has restricted its application to simple tasks. Recently, hyperdimensional computing (HDC) applied to HE has shown promising performance for privacy-preserving machine learning (PPML). However, when applied to more realistic scenarios such as batch inference, the HDC-based HE has still very high compute time as well as high encryption and data transmission overheads. To address this problem, we propose HDC with encrypted parameters (EP-HDC), which is a novel PPML approach featuring client-side HE, i.e., inference is performed on a client using a homomorphically encrypted model. Our EP-HDC can effectively mitigate the encryption and data transmission overhead, as well as providing high scalability with many clients while providing strong protection for user data and model parameters. In addition to application examples for our client-side PPML, we also present design space exploration involving quantization, architecture, and HE-related parameters. Our experimental results using the BFV scheme and the Face/Emotion datasets demonstrate that our method can improve throughput and latency of batch inference by orders of magnitude over previous PPML methods (36.52~1068x and 6.45~733x, respectively) with less than 1% accuracy degradation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying truth and authenticity in AI-assisted candidate evaluation: A multi-domain pilot analysis</title>
<link>https://arxiv.org/abs/2511.00774</link>
<guid>https://arxiv.org/abs/2511.00774</guid>
<content:encoded><![CDATA[

arXiv:2511.00774v1 Announce Type: cross 
Abstract: This paper presents a retrospective analysis of anonymized candidate-evaluation data collected during pilot hiring campaigns conducted through AlteraSF, an AI-native resume-verification platform. The system evaluates resume claims, generates context-sensitive verification questions, and measures performance along quantitative axes of factual validity and job fit, complemented by qualitative integrity detection. Across six job families and 1,700 applications, the platform achieved a 90-95% reduction in screening time and detected measurable linguistic patterns consistent with AI-assisted or copied responses. The analysis demonstrates that candidate truthfulness can be assessed not only through factual accuracy but also through patterns of linguistic authenticity. The results suggest that a multi-dimensional verification framework can improve both hiring efficiency and trust in AI-mediated evaluation systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking</title>
<link>https://arxiv.org/abs/2511.00785</link>
<guid>https://arxiv.org/abs/2511.00785</guid>
<content:encoded><![CDATA[

arXiv:2511.00785v1 Announce Type: cross 
Abstract: 3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast PINN Eigensolvers via Biconvex Reformulation</title>
<link>https://arxiv.org/abs/2511.00792</link>
<guid>https://arxiv.org/abs/2511.00792</guid>
<content:encoded><![CDATA[

arXiv:2511.00792v1 Announce Type: cross 
Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are fundamental to characterizing a system's thermal response, stability, and natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free alternative for solving such problems but are often orders of magnitude slower than classical numerical schemes. In this paper, we introduce a reformulated PINN approach that casts the search for eigenpairs as a biconvex optimization problem, enabling fast and provably convergent alternating convex search (ACS) over eigenvalues and eigenfunctions using analytically optimal updates. Numerical experiments show that PINN-ACS attains high accuracy with convergence speeds up to 500$\times$ faster than gradient-based PINN training. We release our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration</title>
<link>https://arxiv.org/abs/2511.00794</link>
<guid>https://arxiv.org/abs/2511.00794</guid>
<content:encoded><![CDATA[

arXiv:2511.00794v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3 times fewer rollouts than the baselines. Beyond empirical gains, we provide theoretical and in-depth analyses explaining the underlying rationale of our method to improve the data efficiency of RLVR.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data</title>
<link>https://arxiv.org/abs/2511.00795</link>
<guid>https://arxiv.org/abs/2511.00795</guid>
<content:encoded><![CDATA[

arXiv:2511.00795v1 Announce Type: cross 
Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation</title>
<link>https://arxiv.org/abs/2511.00797</link>
<guid>https://arxiv.org/abs/2511.00797</guid>
<content:encoded><![CDATA[

arXiv:2511.00797v1 Announce Type: cross 
Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and difficulty in forming new target-domain patterns during fine-tuning. We formalize the mechanism of output saturation leading to gradient suppression through standard cross-entropy and softmax analysis, showing that gradient suppression at inflection layers confines adaptation to high-level recombination of existing features while preventing low-level reconstruction. We introduce a set of layer-wise diagnostic metrics -- attention entropy (saturation proxy), activation gradient norm, parameter gradient norm, and Delta-CKA under a shared PCA basis -- to identify inflection layers characterized by both low attention entropy and steep gradient decay. Building on these findings, we propose a diagnose-first, inject-light fine-tuning strategy: selectively inserting LoRA adapters at inflection layers to restore suppressed backward signals with minimal parameter overhead. Experiments on BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and over-trained source regimes reveal that over-trained initialization benefits from inflection-layer LoRA injection, while under-trained initialization suffers performance degradation. When base features are strong, unblocking inflection layers facilitates high-level compositional adaptation; when base features are weak, full-pathway unblocking is required for low-level reconstruction, as supported by joint analysis of layer-wise activation gradients and Delta-CKA dynamics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems</title>
<link>https://arxiv.org/abs/2511.00806</link>
<guid>https://arxiv.org/abs/2511.00806</guid>
<content:encoded><![CDATA[

arXiv:2511.00806v1 Announce Type: cross 
Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber actions and continuous physical parameters under stringent safety logic constraints. However, existing hierarchical approaches often compromise global optimality, whereas reinforcement learning (RL) in hybrid action spaces often relies on brittle reward penalties, masking, or shielding and struggles to guarantee constraint satisfaction. We present logic-informed reinforcement learning (LIRL), which equips standard policy-gradient algorithms with projection that maps a low-dimensional latent action onto the admissible hybrid manifold defined on-the-fly by first-order logic. This guarantees feasibility of every exploratory step without penalty tuning. Experimental evaluations have been conducted across multiple scenarios, including industrial manufacturing, electric vehicle charging stations, and traffic signal control, in all of which the proposed method outperforms existing hierarchical optimization approaches. Taking a robotic reducer assembly system in industrial manufacturing as an example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined makespan-energy objective compared to conventional industrial hierarchical scheduling methods. Meanwhile, it consistently maintains zero constraint violations and significantly surpasses state-of-the-art hybrid-action reinforcement learning baselines. Thanks to its declarative logic-based constraint formulation, the framework can be seamlessly transferred to other domains such as smart transportation and smart grid, thereby paving the way for safe and real-time optimization in large-scale CPS.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
<link>https://arxiv.org/abs/2511.00810</link>
<guid>https://arxiv.org/abs/2511.00810</guid>
<content:encoded><![CDATA[

arXiv:2511.00810v1 Announce Type: cross 
Abstract: Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Ultra-Low Latency: Binarized Neural Network Architectures for In-Vehicle Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2511.00828</link>
<guid>https://arxiv.org/abs/2511.00828</guid>
<content:encoded><![CDATA[

arXiv:2511.00828v1 Announce Type: cross 
Abstract: The Control Area Network (CAN) protocol is essential for in-vehicle communication, facilitating high-speed data exchange among Electronic Control Units (ECUs). However, its inherent design lacks robust security features, rendering vehicles susceptible to cyberattacks. While recent research has investigated machine learning and deep learning techniques to enhance network security, their practical applicability remains uncertain. This paper presents a lightweight intrusion detection technique based on Binarized Neural Networks (BNNs), which utilizes payload data, message IDs, and CAN message frequencies for effective intrusion detection. Additionally, we develop hybrid binary encoding techniques to integrate non-binary features, such as message IDs and frequencies. The proposed method, namely the BNN framework specifically optimized for in-vehicle intrusion detection combined with hybrid binary quantization techniques for non-payload attributes, demonstrates efficacy in both anomaly detection and multi-class network traffic classification. The system is well-suited for deployment on micro-controllers and Gateway ECUs, aligning with the real-time requirements of CAN bus safety applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</title>
<link>https://arxiv.org/abs/2511.00831</link>
<guid>https://arxiv.org/abs/2511.00831</guid>
<content:encoded><![CDATA[

arXiv:2511.00831v1 Announce Type: cross 
Abstract: Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials</title>
<link>https://arxiv.org/abs/2511.00833</link>
<guid>https://arxiv.org/abs/2511.00833</guid>
<content:encoded><![CDATA[

arXiv:2511.00833v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at https://github.com/LeapLabTHU/LinearDiff.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Interpolation Adversarial Training for Robust Image Classification</title>
<link>https://arxiv.org/abs/2511.00836</link>
<guid>https://arxiv.org/abs/2511.00836</guid>
<content:encoded><![CDATA[

arXiv:2511.00836v1 Announce Type: cross 
Abstract: Though deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeClash: Benchmarking Goal-Oriented Software Engineering</title>
<link>https://arxiv.org/abs/2511.00839</link>
<guid>https://arxiv.org/abs/2511.00839</guid>
<content:encoded><![CDATA[

arXiv:2511.00839v1 Announce Type: cross 
Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete, well-specified tasks such as fixing specific bugs or writing targeted tests. However, human programmers do not spend all day incessantly addressing isolated tasks. Instead, real-world software development is grounded in the pursuit of high-level goals, like improving user retention or reducing costs. Evaluating whether LMs can also iteratively develop code to better accomplish open-ended objectives without any explicit guidance remains an open challenge. To address this, we introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build the best codebase for achieving a competitive objective. Each round proceeds in two phases: agents edit their code, then their codebases compete head-to-head in a code arena that determines winners based on objectives like score maximization, resource acquisition, or survival. Whether it's writing notes, scrutinizing documentation, analyzing competition logs, or creating test suites, models must decide for themselves how to improve their codebases both absolutely and against their opponents. We run 1680 tournaments (25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal that while models exhibit diverse development styles, they share fundamental limitations in strategic reasoning. Models also struggle with long-term codebase maintenance, as repositories become progressively messy and redundant. These limitations are stark: top models lose every round against expert human programmers. We open-source CodeClash to advance the study of autonomous, goal-oriented code development.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks</title>
<link>https://arxiv.org/abs/2511.00846</link>
<guid>https://arxiv.org/abs/2511.00846</guid>
<content:encoded><![CDATA[

arXiv:2511.00846v1 Announce Type: cross 
Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark \& code.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pay for The Second-Best Service: A Game-Theoretic Approach Against Dishonest LLM Providers</title>
<link>https://arxiv.org/abs/2511.00847</link>
<guid>https://arxiv.org/abs/2511.00847</guid>
<content:encoded><![CDATA[

arXiv:2511.00847v2 Announce Type: cross 
Abstract: The widespread adoption of Large Language Models (LLMs) through Application Programming Interfaces (APIs) induces a critical vulnerability: the potential for dishonest manipulation by service providers. This manipulation can manifest in various forms, such as secretly substituting a proclaimed high-performance model with a low-cost alternative, or inflating responses with meaningless tokens to increase billing. This work tackles the issue through the lens of algorithmic game theory and mechanism design. We are the first to propose a formal economic model for a realistic user-provider ecosystem, where a user can iteratively delegate $T$ queries to multiple model providers, and providers can engage in a range of strategic behaviors. As our central contribution, we prove that for a continuous strategy space and any $\epsilon\in(0,\frac12)$, there exists an approximate incentive-compatible mechanism with an additive approximation ratio of $O(T^{1-\epsilon}\log T)$, and a guaranteed quasi-linear second-best user utility. We also prove an impossibility result, stating that no mechanism can guarantee an expected user utility that is asymptotically better than our mechanism. Furthermore, we demonstrate the effectiveness of our mechanism in simulation experiments with real-world API settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models</title>
<link>https://arxiv.org/abs/2511.00850</link>
<guid>https://arxiv.org/abs/2511.00850</guid>
<content:encoded><![CDATA[

arXiv:2511.00850v1 Announce Type: cross 
Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to sustain genuinely interactive multi-turn conversations remains underexplored, as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench, the first benchmark explicitly designed to evaluate SDMs in multi-turn interactive dialogue with an emphasis on emotional intelligence. Multi-Bench employs a hierarchical structure with a basic track for emotion understanding and reasoning and an advanced track for emotion support and application. It comprises five carefully designed tasks and about 3.2K samples, ranging from emotion recognition to complex reasoning and interactive dialogue, supported by a reproducible evaluation framework. We evaluate six representative SDMs on eight subsets of Multi-Bench. Results show that while current SDMs achieve good performance on basic understanding tasks, they still have room for improvement in advanced multi-turn interactive dialogue and reasoning-related tasks, particularly in emotion awareness and application.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction</title>
<link>https://arxiv.org/abs/2511.00858</link>
<guid>https://arxiv.org/abs/2511.00858</guid>
<content:encoded><![CDATA[

arXiv:2511.00858v1 Announce Type: cross 
Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Stochastic Greedy Algorithm for $k$-Submodular Cover Problem</title>
<link>https://arxiv.org/abs/2511.00869</link>
<guid>https://arxiv.org/abs/2511.00869</guid>
<content:encoded><![CDATA[

arXiv:2511.00869v1 Announce Type: cross 
Abstract: We study the $k$-Submodular Cover ($kSC$) problem, a natural generalization of the classical Submodular Cover problem that arises in artificial intelligence and combinatorial optimization tasks such as influence maximization, resource allocation, and sensor placement. Existing algorithms for $\kSC$ often provide weak approximation guarantees or incur prohibitively high query complexity. To overcome these limitations, we propose a \textit{Fast Stochastic Greedy} algorithm that achieves strong bicriteria approximation while substantially lowering query complexity compared to state-of-the-art methods. Our approach dramatically reduces the number of function evaluations, making it highly scalable and practical for large-scale real-world AI applications where efficiency is essential.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing LLM Reasoning Steps via Principal Knowledge Grounding</title>
<link>https://arxiv.org/abs/2511.00879</link>
<guid>https://arxiv.org/abs/2511.00879</guid>
<content:encoded><![CDATA[

arXiv:2511.00879v1 Announce Type: cross 
Abstract: Step-by-step reasoning has become a standard approach for large language models (LLMs) to tackle complex tasks. While this paradigm has proven effective, it raises a fundamental question: How can we verify that an LLM's reasoning is accurately grounded in knowledge? To address this question, we introduce a novel evaluation suite that systematically assesses the knowledge grounding of intermediate reasoning. Our framework comprises three key components. (1) Principal Knowledge Collection, a large-scale repository of atomic knowledge essential for reasoning. Based on the collection, we propose (2) knowledge-grounded evaluation metrics designed to measure how well models recall and apply prerequisite knowledge in reasoning. These metrics are computed by our (3) evaluator LLM, a lightweight model optimized for cost-effective and reliable metric computation. Our evaluation suite demonstrates remarkable effectiveness in identifying missing or misapplied knowledge elements, providing crucial insights for uncovering fundamental reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these metrics can be integrated into preference optimization, showcasing further applications of knowledge-grounded evaluation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization</title>
<link>https://arxiv.org/abs/2511.00880</link>
<guid>https://arxiv.org/abs/2511.00880</guid>
<content:encoded><![CDATA[

arXiv:2511.00880v1 Announce Type: cross 
Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based second-order policy optimization with safety-aware gradient manipulation. KFCPO leverages K-FAC to perform efficient and stable natural gradient updates by approximating the Fisher Information Matrix (FIM) in a layerwise, closed form manner, avoiding iterative approximation overheads. To address the tradeoff between reward maximization and constraint satisfaction, we introduce a margin aware gradient manipulation mechanism that adaptively adjusts the influence of reward and cost gradients based on the agent's proximity to safety boundaries. This method blends gradients using a direction sensitive projection, eliminating harmful interference and avoiding abrupt changes caused by fixed hard thresholds. Additionally, a minibatch level KL rollback strategy is adopted to ensure trust region compliance and to prevent destabilizing policy shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves 10.3% to 50.2% higher average return across environments compared to the best baseline that respected the safety constraint, demonstrating superior balance of safety and performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Generative Models for Enhanced Vitreous OCT Imaging</title>
<link>https://arxiv.org/abs/2511.00881</link>
<guid>https://arxiv.org/abs/2511.00881</guid>
<content:encoded><![CDATA[

arXiv:2511.00881v2 Announce Type: cross 
Abstract: Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical coherence tomography (OCT) image quality and reducing acquisition time. Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs), Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised Generative Adversarial Network (VQ-GAN) were used to generate high-quality spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and outputs were compared to pseudoART100 images obtained by averaging ten ART10 images per eye location. Model performance was assessed using image quality metrics and Visual Turing Tests, where ophthalmologists ranked generated images and evaluated anatomical fidelity. The best model's performance was further tested within the manually segmented vitreous on newly acquired data. Results: U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM (0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest (3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and 85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous regions more similar in PSNR to the ART100 reference than true ART1 or ART10 B-scans and achieved higher PSNR on whole images when conditioned on ART1 than ART10. Conclusions: Results reveal discrepancies between quantitative metrics and clinical evaluation, highlighting the need for combined assessment. cDDPM showed strong potential for generating clinically meaningful vitreous OCT images while reducing acquisition time fourfold. Translational Relevance: cDDPMs show promise for clinical integration, supporting faster, higher-quality vitreous imaging. Dataset and code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Android Malware Detection: A Machine Leaning Approach</title>
<link>https://arxiv.org/abs/2511.00894</link>
<guid>https://arxiv.org/abs/2511.00894</guid>
<content:encoded><![CDATA[

arXiv:2511.00894v1 Announce Type: cross 
Abstract: This study examines machine learning techniques like Decision Trees, Support Vector Machines, Logistic Regression, Neural Networks, and ensemble methods to detect Android malware. The study evaluates these models on a dataset of Android applications and analyzes their accuracy, efficiency, and real-world applicability. Key findings show that ensemble methods demonstrate superior performance, but there are trade-offs between model interpretability, efficiency, and accuracy. Given its increasing threat, the insights guide future research and practical use of ML to combat Android malware.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Logic of Trust-Based Beliefs</title>
<link>https://arxiv.org/abs/2511.00899</link>
<guid>https://arxiv.org/abs/2511.00899</guid>
<content:encoded><![CDATA[

arXiv:2511.00899v1 Announce Type: cross 
Abstract: Traditionally, an agent's beliefs would come from what the agent can see, hear, or sense. In the modern world, beliefs are often based on the data available to the agents. In this work, we investigate a dynamic logic of such beliefs that incorporates public announcements of data. The main technical contribution is a sound and complete axiomatisation of the interplay between data-informed beliefs and data announcement modalities. We also describe a non-trivial polynomial model checking algorithm for this logical system.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning with Category-Equivariant Representations for Human Activity Recognition</title>
<link>https://arxiv.org/abs/2511.00900</link>
<guid>https://arxiv.org/abs/2511.00900</guid>
<content:encoded><![CDATA[

arXiv:2511.00900v1 Announce Type: cross 
Abstract: Human activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots</title>
<link>https://arxiv.org/abs/2511.00917</link>
<guid>https://arxiv.org/abs/2511.00917</guid>
<content:encoded><![CDATA[

arXiv:2511.00917v1 Announce Type: cross 
Abstract: Today's best-explored routes towards generalist robots center on collecting ever larger "observations-in actions-out" robotics datasets to train large end-to-end models, copying a recipe that has worked for vision-language models (VLMs). We pursue a road less traveled: building generalist policies directly around VLMs by augmenting their general capabilities with specific robot capabilities encapsulated in a carefully curated set of perception, planning, and control modules. In Maestro, a VLM coding agent dynamically composes these modules into a programmatic policy for the current task and scenario. Maestro's architecture benefits from a streamlined closed-loop interface without many manually imposed structural constraints, and a comprehensive and diverse tool repertoire. As a result, it largely surpasses today's VLA models for zero-shot performance on challenging manipulation skills. Further, Maestro is easily extensible to incorporate new modules, easily editable to suit new embodiments such as a quadruped-mounted arm, and even easily adapts from minimal real-world experiences through local code edits.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model</title>
<link>https://arxiv.org/abs/2511.00940</link>
<guid>https://arxiv.org/abs/2511.00940</guid>
<content:encoded><![CDATA[

arXiv:2511.00940v1 Announce Type: cross 
Abstract: Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized $[SEG]$ token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\% improvement), kinematic parameter prediction (average error reduction of 29\%), and physical executability (surpassing baselines by 50\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2511.00958</link>
<guid>https://arxiv.org/abs/2511.00958</guid>
<content:encoded><![CDATA[

arXiv:2511.00958v1 Announce Type: cross 
Abstract: Normalization methods are fundamental components of modern deep neural networks (DNNs). Empirically, they are known to stabilize optimization dynamics and improve generalization. However, the underlying theoretical mechanism by which normalization contributes to both optimization and generalization remains largely unexplained, especially when using many normalization layers in a DNN architecture.
  In this work, we develop a theoretical framework that elucidates the role of normalization through the lens of capacity control. We prove that an unnormalized DNN can exhibit exponentially large Lipschitz constants with respect to either its parameters or inputs, implying excessive functional capacity and potential overfitting. Such bad DNNs are uncountably many. In contrast, the insertion of normalization layers provably can reduce the Lipschitz constant at an exponential rate in the number of normalization operations. This exponential reduction yields two fundamental consequences: (1) it smooths the loss landscape at an exponential rate, facilitating faster and more stable optimization; and (2) it constrains the effective capacity of the network, thereby enhancing generalization guarantees on unseen data. Our results thus offer a principled explanation for the empirical success of normalization methods in deep learning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles</title>
<link>https://arxiv.org/abs/2511.00960</link>
<guid>https://arxiv.org/abs/2511.00960</guid>
<content:encoded><![CDATA[

arXiv:2511.00960v2 Announce Type: cross 
Abstract: The extent to which large language models (LLMs) can perform culturally grounded reasoning across non-English languages remains underexplored. This paper examines the reasoning and self-assessment abilities of LLMs across seven major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and Telugu. We introduce a multilingual riddle dataset combining traditional riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under seven prompting strategies. In the first stage, we assess riddle-solving performance and find that while Gemini 2.5 Pro performs best overall, few-shot methods yield only marginal gains, and accuracy varies notably across languages. In the second stage, we conduct a self-evaluation experiment to measure reasoning consistency. The results reveal a key finding: a model's initial accuracy is inversely correlated with its ability to identify its own mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34% True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are substantially more self-aware (42.09% True Negative Rate). These results point to clear gaps in multilingual reasoning and highlight the need for models that not only reason effectively but also recognize their own limitations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Synthetic Data to estimate the True Error is theoretically and practically doable</title>
<link>https://arxiv.org/abs/2511.00964</link>
<guid>https://arxiv.org/abs/2511.00964</guid>
<content:encoded><![CDATA[

arXiv:2511.00964v1 Announce Type: cross 
Abstract: Accurately evaluating model performance is crucial for deploying machine learning systems in real-world applications. Traditional methods often require a sufficiently large labeled test set to ensure a reliable evaluation. However, in many contexts, a large labeled dataset is costly and labor-intensive. Therefore, we sometimes have to do evaluation by a few labeled samples, which is theoretically challenging. Recent advances in generative models offer a promising alternative by enabling the synthesis of high-quality data. In this work, we make a systematic investigation about the use of synthetic data to estimate the test error of a trained model under limited labeled data conditions. To this end, we develop novel generalization bounds that take synthetic data into account. Those bounds suggest novel ways to optimize synthetic samples for evaluation and theoretically reveal the significant role of the generator's quality. Inspired by those bounds, we propose a theoretically grounded method to generate optimized synthetic data for model evaluation. Experimental results on simulation and tabular datasets demonstrate that, compared to existing baselines, our method achieves accurate and more reliable estimates of the test error.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keys in the Weights: Transformer Authentication Using Model-Bound Latent Representations</title>
<link>https://arxiv.org/abs/2511.00973</link>
<guid>https://arxiv.org/abs/2511.00973</guid>
<content:encoded><![CDATA[

arXiv:2511.00973v1 Announce Type: cross 
Abstract: We introduce Model-Bound Latent Exchange (MoBLE), a decoder-binding property in Transformer autoencoders formalized as Zero-Shot Decoder Non-Transferability (ZSDN). In identity tasks using iso-architectural models trained on identical data but differing in seeds, self-decoding achieves more than 0.91 exact match and 0.98 token accuracy, while zero-shot cross-decoding collapses to chance without exact matches. This separation arises without injected secrets or adversarial training, and is corroborated by weight-space distances and attention-divergence diagnostics. We interpret ZSDN as model binding, a latent-based authentication and access-control mechanism, even when the architecture and training recipe are public: encoder's hidden state representation deterministically reveals the plaintext, yet only the correctly keyed decoder reproduces it in zero-shot. We formally define ZSDN, a decoder-binding advantage metric, and outline deployment considerations for secure artificial intelligence (AI) pipelines. Finally, we discuss learnability risks (e.g., adapter alignment) and outline mitigations. MoBLE offers a lightweight, accelerator-friendly approach to secure AI deployment in safety-critical domains, including aviation and cyber-physical systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.00985</link>
<guid>https://arxiv.org/abs/2511.00985</guid>
<content:encoded><![CDATA[

arXiv:2511.00985v2 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in translating natural language to SQL, but a significant semantic gap persists between their general knowledge and domain-specific semantics of databases. Historical translation logs constitute a rich source of this missing in-domain knowledge, where SQL queries inherently encapsulate real-world usage patterns of database schema. Existing methods primarily enhance the reasoning process for individual translations but fail to accumulate in-domain knowledge from past translations. We introduce ORANGE, an online self-evolutionary framework that constructs database-specific knowledge bases by parsing SQL queries from translation logs. By accumulating in-domain knowledge that contains schema and data semantics, ORANGE progressively reduces the semantic gap and enhances the accuracy of subsequent SQL translations. To ensure reliability, we propose a novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic tracking, which reduces semantic errors during knowledge generation. Experiments on multiple benchmarks confirm the practicality of ORANGE, demonstrating its effectiveness for real-world Text-to-SQL deployment, particularly in handling complex and domain-specific queries.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title>
<link>https://arxiv.org/abs/2511.01019</link>
<guid>https://arxiv.org/abs/2511.01019</guid>
<content:encoded><![CDATA[

arXiv:2511.01019v1 Announce Type: cross 
Abstract: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor's highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seed-Induced Uniqueness in Transformer Models: Subspace Alignment Governs Subliminal Transfer</title>
<link>https://arxiv.org/abs/2511.01023</link>
<guid>https://arxiv.org/abs/2511.01023</guid>
<content:encoded><![CDATA[

arXiv:2511.01023v1 Announce Type: cross 
Abstract: We analyze subliminal transfer in Transformer models, where a teacher embeds hidden traits that can be linearly decoded by a student without degrading main-task performance. Prior work often attributes transferability to global representational similarity, typically quantified with Centered Kernel Alignment (CKA). Using synthetic corpora with disentangled public and private labels, we distill students under matched and independent random initializations. We find that transfer strength hinges on alignment within a trait-discriminative subspace: same-seed students inherit this alignment and show higher leakage {\tau \approx} 0.24, whereas different-seed students -- despite global CKA > 0.9 -- exhibit substantially reduced excess accuracy {\tau \approx} 0.12 - 0.13. We formalize this with subspace-level CKA diagnostic and residualized probes, showing that leakage tracks alignment within the trait-discriminative subspace rather than global representational similarity. Security controls (projection penalty, adversarial reversal, right-for-the-wrong-reasons regularization) reduce leakage in same-base models without impairing public-task fidelity. These results establish seed-induced uniqueness as a resilience property and argue for subspace-aware diagnostics for secure multi-model deployments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAFixAgent: History-Aware Automated Program Repair Agent</title>
<link>https://arxiv.org/abs/2511.01047</link>
<guid>https://arxiv.org/abs/2511.01047</guid>
<content:encoded><![CDATA[

arXiv:2511.01047v1 Announce Type: cross 
Abstract: Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms</title>
<link>https://arxiv.org/abs/2511.01061</link>
<guid>https://arxiv.org/abs/2511.01061</guid>
<content:encoded><![CDATA[

arXiv:2511.01061v1 Announce Type: cross 
Abstract: The long-held assumption that backpropagation (BP) is essential for state-of-the-art performance is challenged by this work. We present rigorous, hardware-validated evidence that the Mono-Forward (MF) algorithm, a backpropagation-free method, consistently surpasses an optimally tuned BP baseline in classification accuracy on its native Multi-Layer Perceptron (MLP) architectures. This superior generalization is achieved with profound efficiency gains, including up to 41% less energy consumption and up to 34% faster training. Our analysis, which charts an evolutionary path from Geoffrey Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF, is grounded in a fair comparative framework using identical architectures and universal hyperparameter optimization. We further provide a critical re-evaluation of memory efficiency in BP-free methods, empirically demonstrating that practical overhead can offset theoretical gains. Ultimately, this work establishes MF as a practical, high-performance, and sustainable alternative to BP for MLPs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</title>
<link>https://arxiv.org/abs/2511.01082</link>
<guid>https://arxiv.org/abs/2511.01082</guid>
<content:encoded><![CDATA[

arXiv:2511.01082v1 Announce Type: cross 
Abstract: Image geolocalization, the task of determining an image's geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at https://github.com/NNargesNN/GeoToken.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</title>
<link>https://arxiv.org/abs/2511.01087</link>
<guid>https://arxiv.org/abs/2511.01087</guid>
<content:encoded><![CDATA[

arXiv:2511.01087v1 Announce Type: cross 
Abstract: The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning, Not Training: Online Adaptation For Agents</title>
<link>https://arxiv.org/abs/2511.01093</link>
<guid>https://arxiv.org/abs/2511.01093</guid>
<content:encoded><![CDATA[

arXiv:2511.01093v1 Announce Type: cross 
Abstract: Continual Learning (CL) methods have traditionally focused on mitigating catastrophic forgetting through gradient-based retraining, an approach ill-suited for deployed agents that must adapt in real time. We introduce our Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that decouples reasoning (Teacher) from execution (Student) and incorporates a persistent learning memory that stores distilled guidance from experience. This informs the orchestration layer, enabling the system to dynamically adjust its operational strategies, such as supervision level or initial plan selection, at inference time. In doing so, ATLAS achieves gradient-free continual learning, shifting the locus of adaptation from model parameters to system-level orchestration. We formulate this as a system-centric paradigm for continual learning, where the objective is adaptive efficiency: maximizing task success while minimizing computational cost through inference-time orchestration rather than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1% success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High) by 13% while reducing cost by 86%. Cross-incident validation demonstrates generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to 41% with zero retraining, while shifting output composition from verbose exploration to structured reasoning. Together, these findings establish gradient-free continual learning as a viable path toward adaptive, deployable AI systems and provide causally annotated traces valuable for training explicit world models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning with Category-Equivariant Architectures for Human Activity Recognition</title>
<link>https://arxiv.org/abs/2511.01139</link>
<guid>https://arxiv.org/abs/2511.01139</guid>
<content:encoded><![CDATA[

arXiv:2511.01139v2 Announce Type: cross 
Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. We introduce a symmetry category that jointly represents cyclic time shifts, positive gain scalings, and the sensor-hierarchy poset, capturing the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.01140</link>
<guid>https://arxiv.org/abs/2511.01140</guid>
<content:encoded><![CDATA[

arXiv:2511.01140v1 Announce Type: cross 
Abstract: Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation</title>
<link>https://arxiv.org/abs/2511.01143</link>
<guid>https://arxiv.org/abs/2511.01143</guid>
<content:encoded><![CDATA[

arXiv:2511.01143v1 Announce Type: cross 
Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at https://github.com/JeremyXSC/MicroAUNet.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence</title>
<link>https://arxiv.org/abs/2511.01144</link>
<guid>https://arxiv.org/abs/2511.01144</guid>
<content:encoded><![CDATA[

arXiv:2511.01144v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in natural language reasoning, yet their application to Cyber Threat Intelligence (CTI) remains limited. CTI analysis involves distilling large volumes of unstructured reports into actionable knowledge, a process where LLMs could substantially reduce analyst workload. CTIBench introduced a comprehensive benchmark for evaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by developing AthenaBench, an enhanced benchmark that includes an improved dataset creation pipeline, duplicate removal, refined evaluation metrics, and a new task focused on risk mitigation strategies. We evaluate twelve LLMs, including state-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside seven open-source models from the LLaMA and Qwen families. While proprietary LLMs achieve stronger results overall, their performance remains subpar on reasoning-intensive tasks, such as threat actor attribution and risk mitigation, with open-source models trailing even further behind. These findings highlight fundamental limitations in the reasoning capabilities of current LLMs and underscore the need for models explicitly tailored to CTI workflows and automation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A High-Throughput Spiking Neural Network Processor Enabling Synaptic Delay Emulation</title>
<link>https://arxiv.org/abs/2511.01158</link>
<guid>https://arxiv.org/abs/2511.01158</guid>
<content:encoded><![CDATA[

arXiv:2511.01158v1 Announce Type: cross 
Abstract: Synaptic delay has attracted significant attention in neural network dynamics for integrating and processing complex spatiotemporal information. This paper introduces a high-throughput Spiking Neural Network (SNN) processor that supports synaptic delay-based emulation for edge applications. The processor leverages a multicore pipelined architecture with parallel compute engines, capable of real-time processing of the computational load associated with synaptic delays. We develop a SoC prototype of the proposed processor on PYNQ Z2 FPGA platform and evaluate its performance using the Spiking Heidelberg Digits (SHD) benchmark for low-power keyword spotting tasks. The processor achieves 93.4% accuracy in deployment and an average throughput of 104 samples/sec at a typical operating frequency of 125 MHz and 282 mW power consumption.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification</title>
<link>https://arxiv.org/abs/2511.01172</link>
<guid>https://arxiv.org/abs/2511.01172</guid>
<content:encoded><![CDATA[

arXiv:2511.01172v1 Announce Type: cross 
Abstract: Deep learning has emerged as a leading approach for Automatic Modulation Classification (AMC), demonstrating superior performance over traditional methods. However, vulnerability to adversarial attacks and susceptibility to data distribution shifts hinder their practical deployment in real-world, dynamic environments. To address these threats, we propose a novel, unified framework that integrates meta-learning with domain adaptation, making AMC systems resistant to both adversarial attacks and environmental changes. Our framework utilizes a two-phase strategy. First, in an offline phase, we employ a meta-learning approach to train the model on clean and adversarially perturbed samples from a single source domain. This method enables the model to generalize its defense, making it resistant to a combination of previously unseen attacks. Subsequently, in the online phase, we apply domain adaptation to align the model's features with a new target domain, allowing it to adapt without requiring substantial labeled data. As a result, our framework achieves a significant improvement in modulation classification accuracy against these combined threats, offering a critical solution to the deployment and operational challenges of modern AMC systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction</title>
<link>https://arxiv.org/abs/2511.01188</link>
<guid>https://arxiv.org/abs/2511.01188</guid>
<content:encoded><![CDATA[

arXiv:2511.01188v1 Announce Type: cross 
Abstract: The rapid spread of fake news threatens social stability and public trust, rendering its detection an imperative research priority. Although large language models (LLMs) excel at numerous natural language processing tasks with their remarkable contextual understanding and extensive prior knowledge, the time-bounded knowledge coverage and tendency for generating hallucination content reduce their reliability when handling fast-evolving news streams. Furthermore, models trained on existing static datasets also often lack the generalization needed for emerging news topics. To address these challenges, we propose ZoFia, a novel two-stage zero-shot fake news detection framework. First, we introduce Hierarchical Salience to quantify the importance of entities in the news content, and propose the SC-MMR algorithm to effectively select an informative and diverse set of keywords that serve as queries for retrieving up-to-date external evidence. Subsequently, a multi LLM interactive system, in which each agent assumes a distinct role, performs multi-view collaborative analysis and adversarial debate over the news text and its related information, and finally produces an interpretable and robust judgment. Comprehensive experiments on two public datasets demonstrate that ZoFia obviously outperforms existing zero-shot baselines and most of few-shot methods. Our codes will be open-sourced to facilitate related communities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.01191</link>
<guid>https://arxiv.org/abs/2511.01191</guid>
<content:encoded><![CDATA[

arXiv:2511.01191v1 Announce Type: cross 
Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for adapting models using only synthetic signals at inference, but its success hinges on constructing reliable learning signals. Standard approaches such as majority voting often collapse to spurious yet popular answers. We introduce Self-Harmony, a framework built on a simple intuition: the correct answer should remain stable across both an original question and its paraphrase. Self-Harmony operationalizes this by employing a single model in two complementary roles: a Solver to produce answers and a Reframer to rephrase the input. Based on this, we further propose a pseudo-label method: instead of majority voting, it aggregates answer frequencies across these original and reframed views using the harmonic mean. This is a process that naturally selects for solutions stable under reframing, thereby avoiding the common trap of favoring view-dependent, spurious answers. Crucially, this requires no human supervision or auxiliary models. Across diverse reasoning benchmarks, Self-Harmony achieves state-of-the-art results at the label-free test-time setting, ranking first in 28 of 30 settings across multiple methods. Beyond accuracy, it demonstrates unprecedented robustness, with zero training failures in all experiments, underscoring its stability and reliability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</title>
<link>https://arxiv.org/abs/2511.01194</link>
<guid>https://arxiv.org/abs/2511.01194</guid>
<content:encoded><![CDATA[

arXiv:2511.01194v1 Announce Type: cross 
Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Interdisciplinary and Cross-Task Review on Missing Data Imputation</title>
<link>https://arxiv.org/abs/2511.01196</link>
<guid>https://arxiv.org/abs/2511.01196</guid>
<content:encoded><![CDATA[

arXiv:2511.01196v1 Announce Type: cross 
Abstract: Missing data is a fundamental challenge in data science, significantly hindering analysis and decision-making across a wide range of disciplines, including healthcare, bioinformatics, social science, e-commerce, and industrial monitoring. Despite decades of research and numerous imputation methods, the literature remains fragmented across fields, creating a critical need for a comprehensive synthesis that connects statistical foundations with modern machine learning advances. This work systematically reviews core concepts-including missingness mechanisms, single versus multiple imputation, and different imputation goals-and examines problem characteristics across various domains. It provides a thorough categorization of imputation methods, spanning classical techniques (e.g., regression, the EM algorithm) to modern approaches like low-rank and high-rank matrix completion, deep learning models (autoencoders, GANs, diffusion models, graph neural networks), and large language models. Special attention is given to methods for complex data types, such as tensors, time series, streaming data, graph-structured data, categorical data, and multimodal data. Beyond methodology, we investigate the crucial integration of imputation with downstream tasks like classification, clustering, and anomaly detection, examining both sequential pipelines and joint optimization frameworks. The review also assesses theoretical guarantees, benchmarking resources, and evaluation metrics. Finally, we identify critical challenges and future directions, emphasizing model selection and hyperparameter optimization, the growing importance of privacy-preserving imputation via federated learning, and the pursuit of generalizable models that can adapt across domains and data types, thereby outlining a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs</title>
<link>https://arxiv.org/abs/2511.01202</link>
<guid>https://arxiv.org/abs/2511.01202</guid>
<content:encoded><![CDATA[

arXiv:2511.01202v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in numerous real-world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate-distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.01213</link>
<guid>https://arxiv.org/abs/2511.01213</guid>
<content:encoded><![CDATA[

arXiv:2511.01213v1 Announce Type: cross 
Abstract: The immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks</title>
<link>https://arxiv.org/abs/2511.01228</link>
<guid>https://arxiv.org/abs/2511.01228</guid>
<content:encoded><![CDATA[

arXiv:2511.01228v1 Announce Type: cross 
Abstract: Node importance ranking is a fundamental problem in graph data analysis. Existing approaches typically rely on node features derived from either traditional centrality measures or advanced graph representation learning methods, which depend directly on the target network's topology. However, this reliance on structural information raises privacy concerns and often leads to poor generalization across different networks. In this work, we address a key question: Can we design a node importance ranking model trained exclusively on synthetic networks that is effectively appliable to real-world networks, eliminating the need to rely on the topology of target networks and improving both practicality and generalizability? We answer this question affirmatively by proposing the Influence-aware Causal Autoencoder Network (ICAN), a novel framework that leverages causal representation learning to get robust, invariant node embeddings for cross-network ranking tasks. Firstly, ICAN introduces an influence-aware causal representation learning module within an autoencoder architecture to extract node embeddings that are causally related to node importance. Moreover, we introduce a causal ranking loss and design a unified optimization framework that jointly optimizes the reconstruction and ranking objectives, enabling mutual reinforcement between node representation learning and ranking optimization. This design allows ICAN, trained on synthetic networks, to generalize effectively across diverse real-world graphs. Extensive experiments on multiple benchmark datasets demonstrate that ICAN consistently outperforms state-of-the-art baselines in terms of both ranking accuracy and generalization capability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eyes on Target: Gaze-Aware Object Detection in Egocentric Video</title>
<link>https://arxiv.org/abs/2511.01237</link>
<guid>https://arxiv.org/abs/2511.01237</guid>
<content:encoded><![CDATA[

arXiv:2511.01237v1 Announce Type: cross 
Abstract: Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Deep Learning Still Needs a Quantum Leap</title>
<link>https://arxiv.org/abs/2511.01253</link>
<guid>https://arxiv.org/abs/2511.01253</guid>
<content:encoded><![CDATA[

arXiv:2511.01253v1 Announce Type: cross 
Abstract: Quantum computing technology is advancing rapidly. Yet, even accounting for these trends, a quantum leap would be needed for quantum computers to meaningfully impact deep learning over the coming decade or two. We arrive at this conclusion based on a first-of-its-kind survey of quantum algorithms and how they match potential deep learning applications. This survey reveals three important areas where quantum computing could potentially accelerate deep learning, each of which faces a challenging roadblock to realizing its potential. First, quantum algorithms for matrix multiplication and other algorithms central to deep learning offer small theoretical improvements in the number of operations needed, but this advantage is overwhelmed on practical problem sizes by how slowly quantum computers do each operation. Second, some promising quantum algorithms depend on practical Quantum Random Access Memory (QRAM), which is underdeveloped. Finally, there are quantum algorithms that offer large theoretical advantages, but which are only applicable to special cases, limiting their practical benefits. In each of these areas, we support our arguments using quantitative forecasts of quantum advantage that build on the work by Choi et al. [2023] as well as new research on limitations and quantum hardware trends. Our analysis outlines the current scope of quantum deep learning and points to research directions that could lead to greater practical advances in the field.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play</title>
<link>https://arxiv.org/abs/2511.01261</link>
<guid>https://arxiv.org/abs/2511.01261</guid>
<content:encoded><![CDATA[

arXiv:2511.01261v1 Announce Type: cross 
Abstract: Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present Speech-DRAME, a unified framework that contributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: Archetype Evaluation, a top-down approach measuring adherence to broad role archetypes, and Realism Evaluation, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, DRAME-Eval achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems</title>
<link>https://arxiv.org/abs/2511.01268</link>
<guid>https://arxiv.org/abs/2511.01268</guid>
<content:encoded><![CDATA[

arXiv:2511.01268v1 Announce Type: cross 
Abstract: Large language models (LLMs) are reshaping numerous facets of our daily lives, leading widespread adoption as web-based services. Despite their versatility, LLMs face notable challenges, such as generating hallucinated content and lacking access to up-to-date information. Lately, to address such limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising direction by generating responses grounded in external knowledge sources. A typical RAG system consists of i) a retriever that probes a group of relevant passages from a knowledge base and ii) a generator that formulates a response based on the retrieved content. However, as with other AI systems, recent studies demonstrate the vulnerability of RAG, such as knowledge corruption attacks by injecting misleading information. In response, several defense strategies have been proposed, including having LLMs inspect the retrieved passages individually or fine-tuning robust retrievers. While effective, such approaches often come with substantial computational costs.
  In this work, we introduce RAGDefender, a resource-efficient defense mechanism against knowledge corruption (i.e., by data poisoning) attacks in practical RAG deployments. RAGDefender operates during the post-retrieval phase, leveraging lightweight machine learning techniques to detect and filter out adversarial content without requiring additional model training or inference. Our empirical evaluations show that RAGDefender consistently outperforms existing state-of-the-art defenses across multiple models and adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR) against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber legitimate ones by a factor of four (4x).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting</title>
<link>https://arxiv.org/abs/2511.01275</link>
<guid>https://arxiv.org/abs/2511.01275</guid>
<content:encoded><![CDATA[

arXiv:2511.01275v1 Announce Type: cross 
Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a critical challenge in healthcare time series prediction, requiring high sensitivity, low false alarm rates, and subject-specific adaptability. We present STAN, an Adversarial Spatio-Temporal Attention Network that jointly models spatial brain connectivity and temporal neural dynamics through cascaded attention blocks with alternating spatial and temporal modules. Unlike existing approaches that assume fixed preictal durations or separately process spatial and temporal features, STAN captures bidirectional dependencies between spatial and temporal patterns through a unified cascaded architecture. Adversarial training with gradient penalty enables robust discrimination between interictal and preictal states learned from clearly defined 15-minute preictal windows. Continuous 90-minute pre-seizure monitoring reveals that the learned spatio-temporal attention patterns enable early detection: reliable alarms trigger at subject-specific times (typically 15-45 minutes before onset), reflecting the model's capacity to capture subtle preictal dynamics without requiring individualized training. Experiments on two benchmark EEG datasets (CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14 events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011 false detections per hour and 94.2% sensitivity with 0.063 false detections per hour, respectively, while maintaining computational efficiency (2.3M parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond epilepsy, the proposed framework provides a general paradigm for spatio-temporal forecasting in healthcare and other time series domains where individual heterogeneity and interpretability are crucial.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.01282</link>
<guid>https://arxiv.org/abs/2511.01282</guid>
<content:encoded><![CDATA[

arXiv:2511.01282v1 Announce Type: cross 
Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate large language model (LLM) inference without compromising output quality. However, the achievable speedup largely depends on the effectiveness of the drafting model. While model-based methods like EAGLE-2 are accurate but costly, retrieval-enhanced methods like SAM-Decoding rely on heuristic switching strategies that often trigger unnecessary retrievals. To address this, we propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a novel framework that transforms heuristic drafter switching into adaptive decision-making. ReSpec features three core innovations: 1) An \textbf{entropy-guided adaptive trigger} quantifies contextual predictability to initiate retrieval only when uncertainty is low, avoiding costly low-quality speculations. 2) A \textbf{feedback-driven candidate selection} leverages historical feedback to organize multiple high-quality candidates for parallel verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed verification strategy} applies strict checks to model-generated drafts while using a relaxed verification for retrieved drafts, achieving a better balance between accuracy and efficiency. Extensive experiments on Spec-Bench demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while maintaining output quality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2511.01284</link>
<guid>https://arxiv.org/abs/2511.01284</guid>
<content:encoded><![CDATA[

arXiv:2511.01284v1 Announce Type: cross 
Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping</title>
<link>https://arxiv.org/abs/2511.01296</link>
<guid>https://arxiv.org/abs/2511.01296</guid>
<content:encoded><![CDATA[

arXiv:2511.01296v1 Announce Type: cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed nodes without exposing raw data, but its decentralized nature makes it vulnerable in trust-deficient environments. Inference attacks may recover sensitive information from gradient updates, while poisoning attacks can degrade model performance or induce malicious behaviors. Existing defenses often suffer from high communication and computation costs, or limited detection precision. To address these issues, we propose LSHFed, a robust and communication-efficient FL framework that simultaneously enhances aggregation robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a novel gradient verification mechanism that projects high-dimensional gradients into compact binary representations via multi-hyperplane locally-sensitive hashing. This enables accurate detection and filtering of malicious gradients using only their irreversible hash forms, thus mitigating privacy leakage risks and substantially reducing transmission overhead. Extensive experiments demonstrate that LSHFed maintains high model performance even when up to 50% of participants are collusive adversaries while achieving up to a 1000x reduction in gradient verification communication compared to full-gradient methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSpecs: Expert-Level Questions Answering in 5G</title>
<link>https://arxiv.org/abs/2511.01305</link>
<guid>https://arxiv.org/abs/2511.01305</guid>
<content:encoded><![CDATA[

arXiv:2511.01305v1 Announce Type: cross 
Abstract: 5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reason about specification evolution. We present DeepSpecs, a RAG system enhanced by structural and temporal reasoning via three metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB (line-level version diffs), and TDocDB (standardization meeting documents). DeepSpecs explicitly resolves cross-references by recursively retrieving referenced clauses through metadata lookup, and traces specification evolution by mining changes and linking them to Change Requests that document design rationale. We curate two 5G QA datasets: 573 expert-annotated real-world questions from practitioner forums and educational resources, and 350 evolution-focused questions derived from approved Change Requests. Across multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art telecom RAG systems; ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality, underscoring the value of modeling the structural and temporal properties of 5G standards.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models</title>
<link>https://arxiv.org/abs/2511.01307</link>
<guid>https://arxiv.org/abs/2511.01307</guid>
<content:encoded><![CDATA[

arXiv:2511.01307v1 Announce Type: cross 
Abstract: Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at https://github.com/KU-VGI/APDM.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation</title>
<link>https://arxiv.org/abs/2511.01316</link>
<guid>https://arxiv.org/abs/2511.01316</guid>
<content:encoded><![CDATA[

arXiv:2511.01316v1 Announce Type: cross 
Abstract: Continuous Integration (CI) is a cornerstone of modern collaborative software development, and numerous CI platforms are available. Differences in maintenance overhead, reliability, and integration depth with code-hosting platforms make migration between CI platforms a common practice. A central step in migration is translating CI configurations, which is challenging due to the intrinsic complexity of CI configurations and the need to understand semantic differences and relationships across CI platforms.
  With the advent of large language models (LLMs), recent advances in software engineering highlight their potential for CI configuration translation. In this paper, we present a study on LLM-based CI configuration translation, focusing on the migration from Travis CI to GitHub Actions. First, using 811 migration records, we quantify the effort involved and find that developers read an average of 38 lines of Travis configuration and write 58 lines of GitHub Actions configuration, with nearly half of the migrations requiring multiple commits. We further analyze translations produced by each of the four LLMs and identify 1,121 issues grouped into four categories: logic inconsistencies (38%), platform discrepancies (32%), environment errors (25%), and syntax errors (5%). Finally, we evaluate three enhancement strategies and show that combining guideline-based prompting with iterative refinement achieves the best performance, reaching a Build Success Rate of 75.5%-nearly a threefold improvement over GPT-4o with a basic prompt.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness</title>
<link>https://arxiv.org/abs/2511.01323</link>
<guid>https://arxiv.org/abs/2511.01323</guid>
<content:encoded><![CDATA[

arXiv:2511.01323v1 Announce Type: cross 
Abstract: Large language models (LLMs) with integrated search tools show strong promise in open-domain question answering (QA), yet they often struggle to produce complete answer set to complex questions such as Which actor from the film Heat won at least one Academy Award?, which requires (1) distinguishing between multiple films sharing the same title and (2) reasoning across a large set of actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate both challenges jointly. To address this, we introduce DeepAmbigQAGen, an automatic data generation pipeline that constructs QA tasks grounded in text corpora and linked knowledge graph, generating natural and verifiable questions that systematically embed name ambiguity and multi-step reasoning. Based on this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop reasoning and half of them explicit name ambiguity resolving. Experiments reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous questions. These findings highlight the need for more robust QA systems aimed at information gathering and answer completeness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI for Requirements Engineering: Industry adoption and Practitioner perspectives</title>
<link>https://arxiv.org/abs/2511.01324</link>
<guid>https://arxiv.org/abs/2511.01324</guid>
<content:encoded><![CDATA[

arXiv:2511.01324v2 Announce Type: cross 
Abstract: The integration of AI for Requirements Engineering (RE) presents significant benefits but also poses real challenges. Although RE is fundamental to software engineering, limited research has examined AI adoption in RE. We surveyed 55 software practitioners to map AI usage across four RE phases: Elicitation, Analysis, Specification, and Validation, and four approaches for decision making: human-only decisions, AI validation, Human AI Collaboration (HAIC), and full AI automation. Participants also shared their perceptions, challenges, and opportunities when applying AI for RE tasks. Our data show that 58.2% of respondents already use AI in RE, and 69.1% view its impact as positive or very positive.HAIC dominates practice, accounting for 54.4% of all RE techniques, while full AI automation remains minimal at 5.4%. Passive AI validation (4.4 to 6.2%) lags even further behind, indicating that practitioners value AI's active support over passive oversight. These findings suggest that AI is most effective when positioned as a collaborative partner rather than a replacement for human expertise. It also highlights the need for RE-specific HAIC frameworks along with robust and responsible AI governance as AI adoption in RE grows.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Cognition Augmented End2End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.01334</link>
<guid>https://arxiv.org/abs/2511.01334</guid>
<content:encoded><![CDATA[

arXiv:2511.01334v1 Announce Type: cross 
Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a new paradigm. However, popular end-to-end approaches typically rely on visual feature extraction networks trained under label supervision. This limited supervision framework restricts the generality and applicability of driving models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which advocates for comparative learning between visual feature extraction networks and the general EEG large model, in order to learn latent human driving cognition for enhancing end-to-end planning. In this work, we collected a cognitive dataset for the mentioned contrastive learning process. Subsequently, we investigated the methods and potential mechanisms for enhancing end-to-end planning with human driving cognition, using popular driving models as baselines on publicly available autonomous driving datasets. Both open-loop and closed-loop tests are conducted for a comprehensive evaluation of planning performance. Experimental results demonstrate that the $E^{3}AD$ paradigm significantly enhances the end-to-end planning performance of baseline models. Ablation studies further validate the contribution of driving cognition and the effectiveness of comparative learning process. To the best of our knowledge, this is the first work to integrate human driving cognition for improving end-to-end autonomous driving planning. It represents an initial attempt to incorporate embodied cognitive data into end-to-end autonomous driving, providing valuable insights for future brain-inspired autonomous driving systems. Our code will be made available at Github
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Permissions: Investigating Mobile Personalization with Simulated Personas</title>
<link>https://arxiv.org/abs/2511.01336</link>
<guid>https://arxiv.org/abs/2511.01336</guid>
<content:encoded><![CDATA[

arXiv:2511.01336v1 Announce Type: cross 
Abstract: Mobile applications increasingly rely on sensor data to infer user context and deliver personalized experiences. Yet the mechanisms behind this personalization remain opaque to users and researchers alike. This paper presents a sandbox system that uses sensor spoofing and persona simulation to audit and visualize how mobile apps respond to inferred behaviors. Rather than treating spoofing as adversarial, we demonstrate its use as a tool for behavioral transparency and user empowerment. Our system injects multi-sensor profiles - generated from structured, lifestyle-based personas - into Android devices in real time, enabling users to observe app responses to contexts such as high activity, location shifts, or time-of-day changes. With automated screenshot capture and GPT-4 Vision-based UI summarization, our pipeline helps document subtle personalization cues. Preliminary findings show measurable app adaptations across fitness, e-commerce, and everyday service apps such as weather and navigation. We offer this toolkit as a foundation for privacy-enhancing technologies and user-facing transparency interventions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project</title>
<link>https://arxiv.org/abs/2511.01348</link>
<guid>https://arxiv.org/abs/2511.01348</guid>
<content:encoded><![CDATA[

arXiv:2511.01348v1 Announce Type: cross 
Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in Software Engineering, capable of generating code, suggesting fixes, and supporting quality assurance. While its use in coding tasks shows considerable promise, applying GenAI across the entire Software Development Life Cycle (SDLC) has not yet been fully explored. Critical uncertainties in areas such as reliability, accountability, security, and data privacy demand deeper investigation and coordinated action. The GENIUS project, comprising over 30 European industrial and academic partners, aims to address these challenges by advancing AI integration across all SDLC phases. It focuses on GenAI's potential, the development of innovative tools, and emerging research challenges, actively shaping the future of software engineering. This vision paper presents a shared perspective on the future of GenAI-based software engineering, grounded in cross-sector dialogue and experience within the GENIUS consortium, supported by an exploratory literature review. The paper explores four central elements: (1) a structured overview of current challenges in GenAI adoption across the SDLC; (2) a forward-looking vision outlining key technological and methodological advances expected over the next five years; (3) anticipated shifts in the roles and required skill sets of software professionals; and (4) the contribution of GENIUS in realizing this transformation through practical tools and industrial validation. By aligning technical innovation with business relevance, this paper aims to inform both research agendas and industrial strategies, providing a foundation for reliable, scalable, and industry-ready GenAI solutions for software engineering teams.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Literacy in UAE Libraries: Assessing Competencies, Training Needs, and Ethical Considerations for the Digital Age</title>
<link>https://arxiv.org/abs/2511.01353</link>
<guid>https://arxiv.org/abs/2511.01353</guid>
<content:encoded><![CDATA[

arXiv:2511.01353v1 Announce Type: cross 
Abstract: The study explores the current state of artificial intelligence (AI) literacy levels among library professionals employing a quantitative approach consisting of 92 surveys of LIS professionals in the United Arab Emirates (UAE). Findings of the study revealed the presence of strong cognitive competencies, while there were gaps observed in behavioral and normative competencies, especially related to AI biases, AI-powered learning, and ethical considerations. There was a disconnect observed between the perceived importance of AI skills and the effectiveness of the current training programs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series</title>
<link>https://arxiv.org/abs/2511.01354</link>
<guid>https://arxiv.org/abs/2511.01354</guid>
<content:encoded><![CDATA[

arXiv:2511.01354v1 Announce Type: cross 
Abstract: Recently, the demand for small and efficient reasoning models to support real-world applications has driven the development of knowledge distillation techniques that balance reasoning performance and inference speed. In this paper, we further extend the DistilQwen model family, initialized from the Qwen models, by introducing four model series specifically designed to meet industrial requirements. The distilled model collection comprises: (1) slow-thinking models, optimized for reasoning tasks that require high accuracy; (2) two series of adaptive-thinking models, which dynamically adjust reasoning strategies based on input tasks to maximize efficiency across diverse scenarios; and (3) distilled reward models, which enable further reinforcement learning of reasoning models using distilled knowledge. Comprehensive evaluations across multiple benchmarks demonstrate both high inference efficiency and strong reasoning performance for these models, as well as the practical utility of distilled reward models. We further show that these models support industry practitioners by providing scalable training and inference functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence) platform.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering</title>
<link>https://arxiv.org/abs/2511.01357</link>
<guid>https://arxiv.org/abs/2511.01357</guid>
<content:encoded><![CDATA[

arXiv:2511.01357v1 Announce Type: cross 
Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model's capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at https://github.com/BioMedIA-repo/CMI-MTL.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise</title>
<link>https://arxiv.org/abs/2511.01359</link>
<guid>https://arxiv.org/abs/2511.01359</guid>
<content:encoded><![CDATA[

arXiv:2511.01359v1 Announce Type: cross 
Abstract: Natural Language Inference (NLI) models have been used in various ways to improve the factuality of LLM outputs. This is typically done by applying an NLI model to judge whether the model output is entailed from the supposed evidence, triggering some corrective actions, such as beam reranking at inference time or RL rewards during training. While NLI models are trained to detect factual inconsistencies over complete sentences, decisions in the common autoregressive generation architecture are made for each evolving text prefix, during decoding. Addressing this setting, we generalize the entailment detection task to apply over arbitrary text prefixes, and suggest its utility for improving generation faithfulness. Providing suitable evaluation and training datasets for this task, we train MiniTruePrefixes, a novel specialized model that better detects factual inconsistencies over text prefixes, outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level entailment. We further demonstrate that integrating MiniTruePrefixes into a controlled decoding framework substantially improves factual consistency in abstractive summarization. When guided by MiniTruePrefixes, LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from the same model family, while using only half the memory.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets</title>
<link>https://arxiv.org/abs/2511.01386</link>
<guid>https://arxiv.org/abs/2511.01386</guid>
<content:encoded><![CDATA[

arXiv:2511.01386v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimizes a scalar objective that jointly aggregates retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law, Finance, Medicine, Defense Industry, Computer Science), each with 100 questions spanning factual, interpretation, and long-answer types. RAGSmith finds configurations that consistently outperform naive RAG baseline by +3.8\% on average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in retrieval and +7.5\% in generation. The search typically explores $\approx 0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone -- vector retrieval plus post-generation reflection/revision -- augmented by domain-dependent choices in expansion, reranking, augmentation, and prompt reordering; passage compression is never selected. Improvement magnitude correlates with question type, with larger gains on factual/long-answer mixes than interpretation-heavy sets. These results provide practical, domain-aware guidance for assembling effective RAG systems and demonstrate the utility of evolutionary search for full-pipeline optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</title>
<link>https://arxiv.org/abs/2511.01390</link>
<guid>https://arxiv.org/abs/2511.01390</guid>
<content:encoded><![CDATA[

arXiv:2511.01390v1 Announce Type: cross 
Abstract: Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23\%-86\% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at https://github.com/Sweet4tars/seps.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths</title>
<link>https://arxiv.org/abs/2511.01407</link>
<guid>https://arxiv.org/abs/2511.01407</guid>
<content:encoded><![CDATA[

arXiv:2511.01407v1 Announce Type: cross 
Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing automated manufacturing processes, particularly in domains requiring high-precision expert robotic motions, such as spray painting and welding. To realize effective automation, robust algorithms are essential for generating extended, object-aware trajectories across intricate 3D geometries. However, contemporary OCMG techniques are either based on ad-hoc heuristics or employ learning-based pipelines that are still reliant on sensitive post-processing steps to generate executable paths. We introduce FoldPath, a novel, end-to-end, neural field based method for OCMG. Unlike prior deep learning approaches that predict discrete sequences of end-effector waypoints, FoldPath learns the robot motion as a continuous function, thus implicitly encoding smooth output paths. This paradigm shift eliminates the need for brittle post-processing steps that concatenate and order the predicted discrete waypoints. Particularly, our approach demonstrates superior predictive performance compared to recently proposed learning-based methods, and attains generalization capabilities even in real industrial settings, where only a limited amount of 70 expert samples are provided. We validate FoldPath through comprehensive experiments in a realistic simulation environment and introduce new, rigorous metrics designed to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG task towards practical maturity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</title>
<link>https://arxiv.org/abs/2511.01427</link>
<guid>https://arxiv.org/abs/2511.01427</guid>
<content:encoded><![CDATA[

arXiv:2511.01427v1 Announce Type: cross 
Abstract: Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0\% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0\% main metric across all three RGB+X video modalities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction</title>
<link>https://arxiv.org/abs/2511.01449</link>
<guid>https://arxiv.org/abs/2511.01449</guid>
<content:encoded><![CDATA[

arXiv:2511.01449v1 Announce Type: cross 
Abstract: To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</title>
<link>https://arxiv.org/abs/2511.01450</link>
<guid>https://arxiv.org/abs/2511.01450</guid>
<content:encoded><![CDATA[

arXiv:2511.01450v1 Announce Type: cross 
Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA</title>
<link>https://arxiv.org/abs/2511.01458</link>
<guid>https://arxiv.org/abs/2511.01458</guid>
<content:encoded><![CDATA[

arXiv:2511.01458v1 Announce Type: cross 
Abstract: Safety and reliability are essential for deploying Visual Question Answering (VQA) in surgery, where incorrect or ambiguous responses can harm the patient. Most surgical VQA research focuses on accuracy or linguistic quality while overlooking safety behaviors such as ambiguity awareness, referral to human experts, or triggering a second opinion. Inspired by Automatic Failure Detection (AFD), we study uncertainty estimation as a key enabler of safer decision making. We introduce Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question semantics into prediction confidence. It measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. We evaluate five models, including domain specific Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models degrade under mild paraphrasing, while LVLMs are more resilient. Across three LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. The Area Under the ROC Curve (AUROC) increases by 15-38% for zero-shot models, with gains maintained under out-of-template stress. QA-SNNE offers a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question aligned uncertainty estimation can improve safety and clinician trust. The code and model are available at https://github.com/DennisPierantozzi/QASNNE
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiently Training A Flat Neural Network Before It has been Quantizated</title>
<link>https://arxiv.org/abs/2511.01462</link>
<guid>https://arxiv.org/abs/2511.01462</guid>
<content:encoded><![CDATA[

arXiv:2511.01462v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
<link>https://arxiv.org/abs/2511.01463</link>
<guid>https://arxiv.org/abs/2511.01463</guid>
<content:encoded><![CDATA[

arXiv:2511.01463v1 Announce Type: cross 
Abstract: The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation</title>
<link>https://arxiv.org/abs/2511.01468</link>
<guid>https://arxiv.org/abs/2511.01468</guid>
<content:encoded><![CDATA[

arXiv:2511.01468v1 Announce Type: cross 
Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked with reconstructing system states by integrating sparse, noisy observations with prior estimation. While traditional approaches like variational and ensemble Kalman filtering have proven effective, recent advances in deep learning offer more scalable, efficient, and flexible alternatives better suited for complex, real-world data assimilation involving large-scale and multi-modal observations. However, existing deep learning-based DA research suffers from two critical limitations: (1) reliance on oversimplified scenarios with synthetically perturbed observations, and (2) the absence of standardized benchmarks for fair model comparison. To address these gaps, in this work, we introduce DAMBench, the first large-scale multi-modal benchmark designed to evaluate data-driven DA models under realistic atmospheric conditions. DAMBench integrates high-quality background states from state-of-the-art forecasting systems and real-world multi-modal observations (i.e., real-world weather stations and satellite imagery). All data are resampled to a common grid and temporally aligned to support systematic training, validation, and testing. We provide unified evaluation protocols and benchmark representative data assimilation approaches, including latent generative models and neural process frameworks. Additionally, we propose a lightweight multi-modal plugin to demonstrate how integrating realistic observations can enhance even simple baselines. Through comprehensive experiments, DAMBench establishes a rigorous foundation for future research, promoting reproducibility, fair comparison, and extensibility to real-world multi-modal scenarios. Our dataset and code are publicly available at https://github.com/figerhaowang/DAMBench.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments</title>
<link>https://arxiv.org/abs/2511.01476</link>
<guid>https://arxiv.org/abs/2511.01476</guid>
<content:encoded><![CDATA[

arXiv:2511.01476v1 Announce Type: cross 
Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided Manipulation planner for highly constrained rearrangement problems. MO-SeGMan generates object placement sequences that minimize both replanning per object and robot travel distance while preserving critical dependency structures with a lazy evaluation method. To address highly cluttered, non-monotone scenarios, we propose a Selective Guided Forward Search (SGFS) that efficiently relocates only critical obstacles and to feasible relocation points. Furthermore, we adopt a refinement method for adaptive subgoal selection to eliminate unnecessary pick-and-place actions, thereby improving overall solution quality. Extensive evaluations on nine benchmark rearrangement tasks demonstrate that MO-SeGMan generates feasible motion plans in all cases, consistently achieving faster solution times and superior solution quality compared to the baselines. These results highlight the robustness and scalability of the proposed framework for complex rearrangement planning problems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification</title>
<link>https://arxiv.org/abs/2511.01512</link>
<guid>https://arxiv.org/abs/2511.01512</guid>
<content:encoded><![CDATA[

arXiv:2511.01512v1 Announce Type: cross 
Abstract: Toxic language in Bengali remains prevalent, especially in online environments, with few effective precautions against it. Although text detoxification has seen progress in high-resource languages, Bengali remains underexplored due to limited resources. In this paper, we propose a novel pipeline for Bengali text detoxification that combines Pareto class-optimized large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate detoxified sentences. To support this effort, we construct BanglaNirTox, an artificially generated parallel corpus of 68,041 toxic Bengali sentences with class-wise toxicity labels, reasonings, and detoxified paraphrases, using Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox dataset is used to fine-tune language models to produce better detoxified versions of Bengali sentences. Our findings show that Pareto-optimized LLMs with CoT prompting significantly enhance the quality and consistency of Bengali text detoxification.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving scenario generation and evaluation using a structured layer representation and foundational models</title>
<link>https://arxiv.org/abs/2511.01541</link>
<guid>https://arxiv.org/abs/2511.01541</guid>
<content:encoded><![CDATA[

arXiv:2511.01541v1 Announce Type: cross 
Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at https://github.com/Valgiz/5LMSG.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time Continual Learning on Intel Loihi 2</title>
<link>https://arxiv.org/abs/2511.01553</link>
<guid>https://arxiv.org/abs/2511.01553</guid>
<content:encoded><![CDATA[

arXiv:2511.01553v1 Announce Type: cross 
Abstract: AI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)--where models learn incrementally from non-stationary streams without catastrophic forgetting--remains challenging in power-constrained settings. We present a neuromorphic solution called CLP-SNN: a spiking neural network architecture for Continually Learning Prototypes and its implementation on Intel's Loihi 2 chip. Our approach introduces three innovations: (1) event-driven and spatiotemporally sparse local learning, (2) a self-normalizing three-factor learning rule maintaining weight normalization, and (3) integrated neurogenesis and metaplasticity for capacity expansion and forgetting mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves accuracy competitive with replay methods while being rehearsal-free. CLP-SNN delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms), and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired algorithms and neuromorphic hardware can break traditional accuracy-efficiency trade-offs for future edge AI systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET</title>
<link>https://arxiv.org/abs/2511.01572</link>
<guid>https://arxiv.org/abs/2511.01572</guid>
<content:encoded><![CDATA[

arXiv:2511.01572v1 Announce Type: cross 
Abstract: Time series classification holds broad application value in communications, information countermeasures, finance, and medicine. However, state-of-the-art (SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high computational complexity, coupled with lengthy parameter tuning and training cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional Kernel Transform) offer greater efficiency but leave substantial room for improvement in kernel selection and computational overhead. To address these challenges, we propose a feature extraction approach based on Hadamard convolutional transform, utilizing column or row vectors of Hadamard matrices as convolution kernels with extended lengths of varying sizes. This enhancement maintains full compatibility with existing methods (e.g., ROCKET) while leveraging kernel orthogonality to boost computational efficiency, robustness, and adaptability. Comprehensive experiments on multi-domain datasets-focusing on the UCR time series dataset-demonstrate SOTA performance: F1-score improved by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET (fastest ROCKET variant) under identical hyperparameters, enabling deployment on ultra-low-power embedded devices. All code is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems</title>
<link>https://arxiv.org/abs/2511.01583</link>
<guid>https://arxiv.org/abs/2511.01583</guid>
<content:encoded><![CDATA[

arXiv:2511.01583v1 Announce Type: cross 
Abstract: Detecting malware, especially ransomware, is essential to securing today's interconnected ecosystems, including cloud storage, enterprise file-sharing, and database services. Training high-performing artificial intelligence (AI) detectors requires diverse datasets, which are often distributed across multiple organizations, making centralization necessary. However, centralized learning is often impractical due to security, privacy regulations, data ownership issues, and legal barriers to cross-organizational sharing. Compounding this challenge, ransomware evolves rapidly, demanding models that are both robust and adaptable.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, which enables multiple organizations to collaboratively train a ransomware detection model while keeping raw data local and secure. This paradigm is particularly relevant for cybersecurity companies (including both software and hardware vendors) that deploy ransomware detection or firewall systems across millions of endpoints. In such environments, data cannot be transferred outside the customer's device due to strict security, privacy, or regulatory constraints. Although FL applies broadly to malware threats, we validate the approach using the Ransomware Storage Access Patterns (RanSAP) dataset.
  Our experiments demonstrate that FL improves ransomware detection accuracy by a relative 9% over server-local models and achieves performance comparable to centralized training. These results indicate that FL offers a scalable, high-performing, and privacy-preserving framework for proactive ransomware detection across organizational and regulatory boundaries.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2511.01610</link>
<guid>https://arxiv.org/abs/2511.01610</guid>
<content:encoded><![CDATA[

arXiv:2511.01610v1 Announce Type: cross 
Abstract: Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers</title>
<link>https://arxiv.org/abs/2511.01615</link>
<guid>https://arxiv.org/abs/2511.01615</guid>
<content:encoded><![CDATA[

arXiv:2511.01615v1 Announce Type: cross 
Abstract: Linguistic errors are not merely deviations from normative grammar; they offer a unique window into the cognitive architecture of language and expose the current limitations of artificial systems that seek to replicate them. This project proposes an interdisciplinary study of linguistic errors produced by native Spanish speakers, with the aim of analyzing how current large language models (LLM) interpret, reproduce, or correct them. The research integrates three core perspectives: theoretical linguistics, to classify and understand the nature of the errors; neurolinguistics, to contextualize them within real-time language processing in the brain; and natural language processing (NLP), to evaluate their interpretation against linguistic errors. A purpose-built corpus of authentic errors of native Spanish (+500) will serve as the foundation for empirical analysis. These errors will be tested against AI models such as GPT or Gemini to assess their interpretative accuracy and their ability to generalize patterns of human linguistic behavior. The project contributes not only to the understanding of Spanish as a native language but also to the development of NLP systems that are more cognitively informed and capable of engaging with the imperfect, variable, and often ambiguous nature of real human language.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2511.01633</link>
<guid>https://arxiv.org/abs/2511.01633</guid>
<content:encoded><![CDATA[

arXiv:2511.01633v1 Announce Type: cross 
Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models</title>
<link>https://arxiv.org/abs/2511.01634</link>
<guid>https://arxiv.org/abs/2511.01634</guid>
<content:encoded><![CDATA[

arXiv:2511.01634v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph-based RAG for Energy Efficiency Question Answering</title>
<link>https://arxiv.org/abs/2511.01643</link>
<guid>https://arxiv.org/abs/2511.01643</guid>
<content:encoded><![CDATA[

arXiv:2511.01643v1 Announce Type: cross 
Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering</title>
<link>https://arxiv.org/abs/2511.01650</link>
<guid>https://arxiv.org/abs/2511.01650</guid>
<content:encoded><![CDATA[

arXiv:2511.01650v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being applied to specialized, high-stakes domains like engineering, which demands rigorous evaluation of their complex reasoning capabilities. While current benchmarks assess language understanding, factual recall, mathematics or code generation, none capture the integrative reasoning central to engineering where scientific principles, quantitative modeling and practical constraints must converge. To address this gap, we introduce EngChain, a benchmark for verifiable multi-step engineering problem-solving. EngChain contains 90 problems spanning three engineering branches, organized into 9 domains and 20 distinct areas. The problems are generated from symbolic templates with a high degree of randomization to ensure diversity and eliminate the risk of contamination. With this benchmark, we move beyond final answer accuracy with a two-stage evaluation: we first quantitatively verify the numerical and semantic validity of each reasoning step and then introduce LLM-As-A-Judge, an automated system to qualitatively categorize the identified reasoning errors.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity</title>
<link>https://arxiv.org/abs/2511.01663</link>
<guid>https://arxiv.org/abs/2511.01663</guid>
<content:encoded><![CDATA[

arXiv:2511.01663v1 Announce Type: cross 
Abstract: While generative models for music composition are increasingly capable, their adoption by musicians is hindered by text-prompting, an asynchronous workflow disconnected from the embodied, responsive nature of instrumental performance. To address this, we introduce Aria-Duet, an interactive system facilitating a real-time musical duet between a human pianist and Aria, a state-of-the-art generative model, using a Yamaha Disklavier as a shared physical interface. The framework enables a turn-taking collaboration: the user performs, signals a handover, and the model generates a coherent continuation performed acoustically on the piano. Beyond describing the technical architecture enabling this low-latency interaction, we analyze the system's output from a musicological perspective, finding the model can maintain stylistic semantics and develop coherent phrasal ideas, demonstrating that such embodied systems can engage in musically sophisticated dialogue and open a promising new path for human-AI co-creation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia</title>
<link>https://arxiv.org/abs/2511.01670</link>
<guid>https://arxiv.org/abs/2511.01670</guid>
<content:encoded><![CDATA[

arXiv:2511.01670v1 Announce Type: cross 
Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spin-Adapted Neural Network Wavefunctions in Real Space</title>
<link>https://arxiv.org/abs/2511.01671</link>
<guid>https://arxiv.org/abs/2511.01671</guid>
<content:encoded><![CDATA[

arXiv:2511.01671v1 Announce Type: cross 
Abstract: Spin plays a fundamental role in understanding electronic structure, yet many real-space wavefunction methods fail to adequately consider it. We introduce the Spin-Adapted Antisymmetrization Method (SAAM), a general procedure that enforces exact total spin symmetry for antisymmetric many-electron wavefunctions in real space. In the context of neural network-based quantum Monte Carlo (NNQMC), SAAM leverages the expressiveness of deep neural networks to capture electron correlation while enforcing exact spin adaptation via group representation theory. This framework provides a principled route to embed physical priors into otherwise black-box neural network wavefunctions, yielding a compact representation of correlated system with neural network orbitals. Compared with existing treatments of spin in NNQMC, SAAM is more accurate and efficient, achieving exact spin purity without any additional tunable hyperparameters. To demonstrate its effectiveness, we apply SAAM to study the spin ladder of iron-sulfur clusters, a long-standing challenge for many-body methods due to their dense spectrum of nearly degenerate spin states. Our results reveal accurate resolution of low-lying spin states and spin gaps in [Fe$_2$S$_2$] and [Fe$_4$S$_4$] clusters, offering new insights into their electronic structures. In sum, these findings establish SAAM as a robust, hyperparameter-free standard for spin-adapted NNQMC, particularly for strongly correlated systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Student Engagement in AI Assisted Complex Problem Solving: A Pilot Study of Human AI Rubik's Cube Collaboration</title>
<link>https://arxiv.org/abs/2511.01683</link>
<guid>https://arxiv.org/abs/2511.01683</guid>
<content:encoded><![CDATA[

arXiv:2511.01683v1 Announce Type: cross 
Abstract: Games and puzzles play important pedagogical roles in STEM learning. New AI algorithms that can solve complex problems offer opportunities for scaffolded instruction in puzzle solving. This paper presents the ALLURE system, which uses an AI algorithm (DeepCubeA) to guide students in solving a common first step of the Rubik's Cube (i.e., the white cross). Using data from a pilot study we present preliminary findings about students' behaviors in the system, how these behaviors are associated with STEM skills - including spatial reasoning, critical thinking and algorithmic thinking. We discuss how data from ALLURE can be used in future educational data mining to understand how students benefit from AI assistance and collaboration when solving complex problems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI</title>
<link>https://arxiv.org/abs/2511.01689</link>
<guid>https://arxiv.org/abs/2511.01689</guid>
<content:encoded><![CDATA[

arXiv:2511.01689v1 Announce Type: cross 
Abstract: The character of the "AI assistant" persona generated by modern chatbot large language models influences both surface-level behavior and apparent values, beliefs, and ethics. These all affect interaction quality, perceived intelligence, and alignment with both developer and user intentions. The shaping of this persona, known as character training, is a critical component of industry post-training, yet remains effectively unstudied in the academic literature. We introduce the first open implementation of character training, leveraging Constitutional AI and a new data pipeline using synthetic introspective data to shape the assistant persona in a more effective and controlled manner than alternatives such as constraining system prompts or activation steering. Specifically, we fine-tune three popular open-weights models using 11 example personas, such as humorous, deeply caring, or even malevolent. To track the effects of our approach, we introduce a method which analyzes revealed preferences, uncovering clear and holistic changes in character. We find these changes are more robust to adversarial prompting than the above two alternatives, while also leading to more coherent and realistic generations. Finally, we demonstrate this fine-tuning has little to no effect on general capabilities as measured by common benchmarks. We describe and open-source our full post-training method, the implementation of which can be found at https://github.com/maiush/OpenCharacterTraining.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering</title>
<link>https://arxiv.org/abs/2511.01694</link>
<guid>https://arxiv.org/abs/2511.01694</guid>
<content:encoded><![CDATA[

arXiv:2511.01694v1 Announce Type: cross 
Abstract: Vision-language pre-trained models, such as CLIP, have established new benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a major challenge to achieve optimal performance on both in-distribution (ID) and out-of-distribution (OOD) datasets, especially when labeled data is scarce. Most existing fine-tuning approaches rely on first-order gradient-based optimizers, which typically suffer from slow convergence, sensitivity to step-size hyperparameters, and poor generalization in OOD settings. In contrast, second-order methods utilize local curvature information of the loss landscape to adjust the update step size. This is particularly beneficial for CLIP models, whose non-convex loss functions often contain sharp critical points. In such cases, natural gradient direction can offer more substantial and efficient per-iteration updates when fine-tuning with limited data. Natural Gradient Descent (NGD) is obtained by preconditioning the standard gradient with the inverse Fisher Information Matrix (FIM), which is computationally expensive for large models. To address this, we propose a Bayesian approximation of NGD using a Kalman filter for CLIP models. Our method combines the benefits of second-order optimization with Bayesian inference, which enhances generalization while providing uncertainty quantification. Extensive experiments conducted on diverse image classification datasets demonstrate that our algorithm consistently achieves superior--or comparable--ID performance and improved OOD robustness compared to state-of-the-art baselines. To the best of our knowledge, this work represents the first successful application of Kalman filtering to fine-tuning CLIP-based models, which enables more robust and efficient learning in vision-language tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solution Space Topology Guides CMTS Search</title>
<link>https://arxiv.org/abs/2511.01701</link>
<guid>https://arxiv.org/abs/2511.01701</guid>
<content:encoded><![CDATA[

arXiv:2511.01701v1 Announce Type: cross 
Abstract: A fundamental question in search-guided AI: what topology should guide Monte Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian spectral properties of cell connectivity -- and found no benefit. We identify the root cause: grid topology is constant across all instances. We propose measuring \emph{solution space topology} instead: the structure of valid color assignments constrained by detected pattern rules. We build this via compatibility graphs where nodes are $(cell, color)$ pairs and edges represent compatible assignments under pattern constraints.
  Our method: (1) detect pattern rules automatically with 100\% accuracy on 5 types, (2) construct compatibility graphs encoding solution space structure, (3) extract topological features (algebraic connectivity, rigidity, color structure) that vary with task difficulty, (4) integrate these features into MCTS node selection via sibling-normalized scores.
  We provide formal definitions, a rigorous selection formula, and comprehensive ablations showing that algebraic connectivity is the dominant signal. The work demonstrates that topology matters for search -- but only the \emph{right} topology. For puzzle solving, this is solution space structure, not problem space structure.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement</title>
<link>https://arxiv.org/abs/2511.01706</link>
<guid>https://arxiv.org/abs/2511.01706</guid>
<content:encoded><![CDATA[

arXiv:2511.01706v1 Announce Type: cross 
Abstract: Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: https://github.com/copenlu/pk-ck-knowledge-disentanglement.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Proof of Learning Rate Transfer under $\mu$P</title>
<link>https://arxiv.org/abs/2511.01734</link>
<guid>https://arxiv.org/abs/2511.01734</guid>
<content:encoded><![CDATA[

arXiv:2511.01734v1 Announce Type: cross 
Abstract: We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\mu P$, the optimal learning rate converges to a \emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2511.01743</link>
<guid>https://arxiv.org/abs/2511.01743</guid>
<content:encoded><![CDATA[

arXiv:2511.01743v1 Announce Type: cross 
Abstract: Recent advancements in large artificial intelligence models (LAMs) are driving significant innovations in mobile edge computing within next-generation wireless networks. However, the substantial demands for computational resources and large-scale training data required to train LAMs conflict with the limited storage and computational capacity of edge devices, posing significant challenges to training and deploying LAMs at the edge. In this work, we introduce the Networked Mixture-of-Experts (NMoE) system, in which clients infer collaboratively by distributing tasks to suitable neighbors based on their expertise and aggregate the returned results. For training the NMoE, we propose a federated learning framework that integrates both supervised and self-supervised learning to balance personalization and generalization, while preserving communication efficiency and data privacy. We conduct extensive experiments to demonstrate the efficacy of the proposed NMoE system, providing insights and benchmarks for the NMoE training algorithms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications</title>
<link>https://arxiv.org/abs/2511.01745</link>
<guid>https://arxiv.org/abs/2511.01745</guid>
<content:encoded><![CDATA[

arXiv:2511.01745v1 Announce Type: cross 
Abstract: Battery safety is critical in applications ranging from consumer electronics to electric vehicles and aircraft, where undetected anomalies could trigger safety hazards or costly downtime. In this study, we present OSBAD as an open-source benchmark for anomaly detection frameworks in battery applications. By benchmarking 15 diverse algorithms encompassing statistical, distance-based, and unsupervised machine-learning methods, OSBAD enables a systematic comparison of anomaly detection methods across heterogeneous datasets. In addition, we demonstrate how a physics- and statistics-informed feature transformation workflow enhances anomaly separability by decomposing collective anomalies into point anomalies. To address a major bottleneck in unsupervised anomaly detection due to incomplete labels, we propose a Bayesian optimization pipeline that facilitates automated hyperparameter tuning based on transfer-learning and regression proxies. Through validation on datasets covering both liquid and solid-state chemistries, we further demonstrate the cross-chemistry generalization capability of OSBAD to identify irregularities across different electrochemical systems. By making benchmarking database with open-source reproducible anomaly detection workflows available to the community, OSBAD establishes a unified foundation for developing safe, scalable, and transferable anomaly detection tools in battery analytics. This research underscores the significance of physics- and statistics-informed feature engineering as well as model selection with probabilistic hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for safety-critical energy systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks</title>
<link>https://arxiv.org/abs/2511.01746</link>
<guid>https://arxiv.org/abs/2511.01746</guid>
<content:encoded><![CDATA[

arXiv:2511.01746v1 Announce Type: cross 
Abstract: Scam detection remains a critical challenge in cybersecurity as adversaries craft messages that evade automated filters. We propose a Hierarchical Scam Detection System (HSDS) that combines a lightweight multi-model voting front end with a fine-tuned LLaMA 3.1 8B Instruct back end to improve accuracy and robustness against adversarial attacks. An ensemble of four classifiers provides preliminary predictions through majority vote, and ambiguous cases are escalated to the fine-tuned model, which is optimized with adversarial training to reduce misclassification. Experiments show that this hierarchical design both improves adversarial scam detection and shortens inference time by routing most cases away from the LLM, outperforming traditional machine-learning baselines and proprietary LLM baselines. The findings highlight the effectiveness of a hybrid voting mechanism and adversarial fine-tuning in fortifying LLMs against evolving scam tactics, enhancing the resilience of automated scam detection systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SM-based Semantics for Answer Set Programs Containing Conditional Literals and Arithmetic</title>
<link>https://arxiv.org/abs/2511.01753</link>
<guid>https://arxiv.org/abs/2511.01753</guid>
<content:encoded><![CDATA[

arXiv:2511.01753v1 Announce Type: cross 
Abstract: Modern answer set programming solvers such as CLINGO support advanced language constructs that improve the expressivity and conciseness of logic programs. Conditional literals are one such construct. They form "subformulas" that behave as nested implications within the bodies of logic rules. Their inclusion brings the form of rules closer to the less restrictive syntax of first-order logic. These qualities make conditional literals useful tools for knowledge representation. In this paper, we propose a semantics for logic programs with conditional literals and arithmetic based on the SM operator. These semantics do not require grounding, unlike the established semantics for such programs that relies on a translation to infinitary propositional logic. The main result of this paper establishes the precise correspondence between the proposed and existing semantics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks</title>
<link>https://arxiv.org/abs/2511.01758</link>
<guid>https://arxiv.org/abs/2511.01758</guid>
<content:encoded><![CDATA[

arXiv:2511.01758v1 Announce Type: cross 
Abstract: Open-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the critic's error detection and the generator's output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Guided Decompilation: A Step Towards Re-executability</title>
<link>https://arxiv.org/abs/2511.01763</link>
<guid>https://arxiv.org/abs/2511.01763</guid>
<content:encoded><![CDATA[

arXiv:2511.01763v1 Announce Type: cross 
Abstract: Binary decompilation plays an important role in software security analysis, reverse engineering, and malware understanding when source code is unavailable. However, existing decompilation techniques often fail to produce source code that can be successfully recompiled and re-executed, particularly for optimized binaries. Recent advances in large language models (LLMs) have enabled neural approaches to decompilation, but the generated code is typically only semantically plausible rather than truly executable, limiting their practical reliability. These shortcomings arise from compiler optimizations and the loss of semantic cues in compiled code, which LLMs struggle to recover without contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid decompilation framework that leverages in-context learning (ICL) to guide LLMs toward generating re-executable source code. We evaluate our method across multiple datasets, optimization levels, and compilers, demonstrating around 40\% improvement in re-executability over state-of-the-art decompilation methods while maintaining robustness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</title>
<link>https://arxiv.org/abs/2511.01767</link>
<guid>https://arxiv.org/abs/2511.01767</guid>
<content:encoded><![CDATA[

arXiv:2511.01767v1 Announce Type: cross 
Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
<link>https://arxiv.org/abs/2511.01775</link>
<guid>https://arxiv.org/abs/2511.01775</guid>
<content:encoded><![CDATA[

arXiv:2511.01775v1 Announce Type: cross 
Abstract: Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenDexHand: Generative Simulation for Dexterous Hands</title>
<link>https://arxiv.org/abs/2511.01791</link>
<guid>https://arxiv.org/abs/2511.01791</guid>
<content:encoded><![CDATA[

arXiv:2511.01791v1 Announce Type: cross 
Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence. Existing approaches use large language models (LLMs) to automate gripper-based simulation generation, but they transfer poorly to dexterous manipulation, which demands more specialized environment design. Meanwhile, dexterous manipulation tasks are inherently more difficult due to their higher degrees of freedom. Massively generating feasible and trainable dexterous hand tasks remains an open challenge. To this end, we present GenDexHand, a generative simulation pipeline that autonomously produces diverse robotic tasks and environments for dexterous manipulation. GenDexHand introduces a closed-loop refinement process that adjusts object placements and scales based on vision-language model (VLM) feedback, substantially improving the average quality of generated environments. Each task is further decomposed into sub-tasks to enable sequential reinforcement learning, reducing training time and increasing success rates. Our work provides a viable path toward scalable training of diverse dexterous hand behaviors in embodied intelligence by offering a simulation-based solution to synthetic data generation. Our website: https://winniechen2002.github.io/GenDexHand/.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random Initialization of Gated Sparse Adapters</title>
<link>https://arxiv.org/abs/2511.01794</link>
<guid>https://arxiv.org/abs/2511.01794</guid>
<content:encoded><![CDATA[

arXiv:2511.01794v1 Announce Type: cross 
Abstract: When fine-tuning language models on new tasks, catastrophic forgetting -- performance degradation on previously-learned tasks -- is a ubiquitous problem. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this through low-rank adapters, sparse adaptation offers an alternative that doesn't impose rank constraints. We introduce Random Initialization of Gated Sparse Adapters (RIGSA), which starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag, and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA and random masking. In spite of having more trainable parameters than QLoRA, the RIGSA configurations that we studied displayed less forgetting than QLoRA, particularly on GSM8k, though it performs comparably to random masking.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fractional Diffusion Bridge Models</title>
<link>https://arxiv.org/abs/2511.01795</link>
<guid>https://arxiv.org/abs/2511.01795</guid>
<content:encoded><![CDATA[

arXiv:2511.01795v1 Announce Type: cross 
Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr\"{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr\'echet Inception Distance (FID) in unpaired image translation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accumulating Context Changes the Beliefs of Language Models</title>
<link>https://arxiv.org/abs/2511.01805</link>
<guid>https://arxiv.org/abs/2511.01805</guid>
<content:encoded><![CDATA[

arXiv:2511.01805v2 Announce Type: cross 
Abstract: Language model (LM) assistants are increasingly used in applications such as brainstorming and research. Improvements in memory and context size have allowed these models to become more autonomous, which has also resulted in more text accumulation in their context windows without explicit user intervention. This comes with a latent risk: the belief profiles of models -- their understanding of the world as manifested in their responses or actions -- may silently change as context accumulates. This can lead to subtly inconsistent user experiences, or shifts in behavior that deviate from the original alignment of the models. In this paper, we explore how accumulating context by engaging in interactions and processing text -- talking and reading -- can change the beliefs of language models, as manifested in their responses and behaviors. Our results reveal that models' belief profiles are highly malleable: GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of discussion about moral dilemmas and queries about safety, while Grok 4 shows a 27.2% shift on political issues after reading texts from the opposing position. We also examine models' behavioral changes by designing tasks that require tool use, where each tool selection corresponds to an implicit belief. We find that these changes align with stated belief shifts, suggesting that belief shifts will be reflected in actual behavior in agentic systems. Our analysis exposes the hidden risk of belief shift as models undergo extended sessions of talking or reading, rendering their opinions and actions unreliable.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining</title>
<link>https://arxiv.org/abs/2511.01807</link>
<guid>https://arxiv.org/abs/2511.01807</guid>
<content:encoded><![CDATA[

arXiv:2511.01807v1 Announce Type: cross 
Abstract: Length control in Large Language Models (LLMs) is a crucial but under-addressed challenge, with applications ranging from voice interfaces requiring concise responses to research summaries needing comprehensive outputs. Current approaches to length control, including Regularized DPO, Length-Instruction Fine Tuning, and tool-augmented methods, typically require expensive model retraining or complex inference-time tooling. This paper presents a prompt engineering methodology that enables precise length control without model retraining. Our structure-guided approach implements deliberate planning and word counting mechanisms within the prompt, encouraging the model to carefully track and adhere to specified length constraints. Comprehensive evaluations across six state-of-the-art LLMs demonstrate that our method significantly improves length fidelity for several models compared to standard prompting when applied to document summarization tasks, particularly for shorter-to-medium length constraints. The proposed technique shows varying benefits across different model architectures, with some models demonstrating up to 37.6% improvement in length adherence. Quality evaluations further reveal that our approach maintains or enhances overall output quality compared to standard prompting techniques. Our approach provides an immediately deployable solution for applications requiring precise length control, particularly valuable for production environments where model retraining is impractical or cost-prohibitive.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV Cache Transform Coding for Compact Storage in LLM Inference</title>
<link>https://arxiv.org/abs/2511.01815</link>
<guid>https://arxiv.org/abs/2511.01815</guid>
<content:encoded><![CDATA[

arXiv:2511.01815v1 Announce Type: cross 
Abstract: Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\times$ compression while maintaining reasoning and long-context accuracy, and 40$\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine and Deep Learning for Indoor UWB Jammer Localization</title>
<link>https://arxiv.org/abs/2511.01819</link>
<guid>https://arxiv.org/abs/2511.01819</guid>
<content:encoded><![CDATA[

arXiv:2511.01819v1 Announce Type: cross 
Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is vulnerable to jamming attacks, creating security risks for asset tracking and intrusion detection in smart buildings. Although machine learning (ML) and deep learning (DL) methods have improved tag localization, localizing malicious jammers within a single room and across changing indoor layouts remains largely unexplored. Two novel UWB datasets, collected under original and modified room configurations, are introduced to establish comprehensive ML/DL baselines. Performance is rigorously evaluated using a variety of classification and regression metrics. On the source dataset with the collected UWB features, Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves the lowest mean Euclidean error of 20.16 cm. However, deploying these source-trained models in the modified room layout led to severe performance degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99 cm, demonstrating significant domain shift. To mitigate this degradation, a domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a gradient-reversal layer to align CIR-derived features across domains. The A-CNT framework restores localization performance by reducing the mean Euclidean error to 34.67 cm. This represents a 77 percent improvement over non-adversarial transfer learning and an 83 percent improvement over the best baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the results demonstrate that adversarial feature alignment enables robust and transferable indoor jammer localization despite environmental changes. Code and dataset available at https://github.com/afbf4c8996f/Jammer-Loc
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.01831</link>
<guid>https://arxiv.org/abs/2511.01831</guid>
<content:encoded><![CDATA[

arXiv:2511.01831v2 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when sequentially fine-tuned on new tasks, degrading performance on previously learned foundational and task-specific capabilities. While multi-task learning can mitigate forgetting, it requires simultaneous access to all datasets and imposes computational overhead that scales linearly with the number of tasks. In this work, we introduce a routing-based approach that enables the integration of new tasks while preserving the foundational knowledge acquired during pretraining. We evaluate our method using InternVL-2 models (2B and 8B parameters) and demonstrate that routing preserves the model's foundational capabilities by maintaining performance on general-purpose benchmarks such as ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on specialized tasks. Importantly, our approach achieves this without requiring concurrent access to data from all tasks, avoiding the significant computational and data overhead associated with traditional multi-task learning. We further conduct extensive ablation studies to evaluate the scalability and robustness of routing-based learning, showing that the approach is resilient to a growing number of tasks and performs particularly well when new tasks are semantically related. Finally, we show that the routing mechanism enables superior cross-modal transfer between language and vision capabilities, allowing knowledge learned in one modality to enhance performance in another capability not achieved by existing continual learning methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Vector Symbolic Architectures from Histogram Recovery</title>
<link>https://arxiv.org/abs/2511.01838</link>
<guid>https://arxiv.org/abs/2511.01838</guid>
<content:encoded><![CDATA[

arXiv:2511.01838v1 Announce Type: cross 
Abstract: Vector symbolic architectures (VSAs) are a family of information representation techniques which enable composition, i.e., creating complex information structures from atomic vectors via binding and superposition, and have recently found wide ranging applications in various neurosymbolic artificial intelligence (AI) systems. Recently, Raviv proposed the use of random linear codes in VSAs, suggesting that their subcode structure enables efficient binding, while preserving the quasi-orthogonality that is necessary for neural processing. Yet, random linear codes are difficult to decode under noise, which severely limits the resulting VSA's ability to support recovery, i.e., the retrieval of information objects and their attributes from a noisy compositional representation.
  In this work we bridge this gap by utilizing coding theoretic tools. First, we argue that the concatenation of Reed-Solomon and Hadamard codes is suitable for VSA, due to the mutual quasi-orthogonality of the resulting codewords (a folklore result). Second, we show that recovery of the resulting compositional representations can be done by solving a problem we call histogram recovery. In histogram recovery, a collection of $N$ histograms over a finite field is given as input, and one must find a collection of Reed-Solomon codewords of length $N$ whose entry-wise symbol frequencies obey those histograms. We present an optimal solution to the histogram recovery problem by using algorithms related to list-decoding, and analyze the resulting noise resilience. Our results give rise to a noise-resilient VSA with formal guarantees regarding efficient encoding, quasi-orthogonality, and recovery, without relying on any heuristics or training, and while operating at improved parameters relative to similar solutions such as the Hadamard code.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Study on LLM Biases Concerning Corporate Social Responsibility and Green Supply Chains</title>
<link>https://arxiv.org/abs/2511.01840</link>
<guid>https://arxiv.org/abs/2511.01840</guid>
<content:encoded><![CDATA[

arXiv:2511.01840v1 Announce Type: cross 
Abstract: Organizations increasingly use Large Language Models (LLMs) to improve supply chain processes and reduce environmental impacts. However, LLMs have been shown to reproduce biases regarding the prioritization of sustainable business strategies. Thus, it is important to identify underlying training data biases that LLMs pertain regarding the importance and role of sustainable business and supply chain practices. This study investigates how different LLMs respond to validated surveys about the role of ethics and responsibility for businesses, and the importance of sustainable practices and relations with suppliers and customers. Using standardized questionnaires, we systematically analyze responses generated by state-of-the-art LLMs to identify variations. We further evaluate whether differences are augmented by four organizational culture types, thereby evaluating the practical relevance of identified biases. The findings reveal significant systematic differences between models and demonstrate that organizational culture prompts substantially modify LLM responses. The study holds important implications for LLM-assisted decision-making in sustainability contexts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2511.01846</link>
<guid>https://arxiv.org/abs/2511.01846</guid>
<content:encoded><![CDATA[

arXiv:2511.01846v1 Announce Type: cross 
Abstract: Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring</title>
<link>https://arxiv.org/abs/2511.01850</link>
<guid>https://arxiv.org/abs/2511.01850</guid>
<content:encoded><![CDATA[

arXiv:2511.01850v1 Announce Type: cross 
Abstract: The rapid expansion of artificial intelligence and machine learning (ML) applications has intensified the demand for integrated environments that unify model development, deployment, and monitoring. Traditional Integrated Development Environments (IDEs) focus primarily on code authoring, lacking intelligent support for the full ML lifecycle, while existing MLOps platforms remain detached from the coding workflow. To address this gap, this study proposes the design of an LLM-Integrated IDE with automated MLOps pipelines that enables continuous model development and monitoring within a single environment. The proposed system embeds a Large Language Model (LLM) assistant capable of code generation, debugging recommendation, and automatic pipeline configuration. The backend incorporates automated data validation, feature storage, drift detection, retraining triggers, and CI/CD deployment orchestration. This framework was implemented in a prototype named SmartMLOps Studio and evaluated using classification and forecasting tasks on the UCI Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio reduces pipeline configuration time by 61%, improves experiment reproducibility by 45%, and increases drift detection accuracy by 14% compared to traditional workflows. By bridging intelligent code assistance and automated operational pipelines, this research establishes a novel paradigm for AI engineering - transforming the IDE from a static coding tool into a dynamic, lifecycle-aware intelligent platform for scalable and efficient model development.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trove: A Flexible Toolkit for Dense Retrieval</title>
<link>https://arxiv.org/abs/2511.01857</link>
<guid>https://arxiv.org/abs/2511.01857</guid>
<content:encoded><![CDATA[

arXiv:2511.01857v1 Announce Type: cross 
Abstract: We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Complementary Policies for Human-AI Teams</title>
<link>https://arxiv.org/abs/2302.02944</link>
<guid>https://arxiv.org/abs/2302.02944</guid>
<content:encoded><![CDATA[

arXiv:2302.02944v2 Announce Type: replace 
Abstract: This paper tackles the critical challenge of human-AI complementarity in decision-making. Departing from the traditional focus on algorithmic performance in favor of performance of the human-AI team, and moving past the framing of collaboration as classification to focus on decision-making tasks, we introduce a novel approach to policy learning. Specifically, we develop a robust solution for human-AI collaboration when outcomes are only observed under assigned actions. We propose a deferral collaboration approach that maximizes decision rewards by exploiting the distinct strengths of humans and AI, strategically allocating instances among them. Critically, our method is robust to misspecifications in both the human behavior and reward models. Leveraging the insight that performance gains stem from divergent human and AI behavioral patterns, we demonstrate, using synthetic and real human responses, that our proposed method significantly outperforms independent human and algorithmic decision-making. Moreover, we show that substantial performance improvements are achievable by routing only a small fraction of instances to human decision-makers, highlighting the potential for efficient and effective human-AI collaboration in complex management settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Enhanced Neural Solvers for Routing Problems</title>
<link>https://arxiv.org/abs/2406.16424</link>
<guid>https://arxiv.org/abs/2406.16424</guid>
<content:encoded><![CDATA[

arXiv:2406.16424v3 Announce Type: replace 
Abstract: Routing Problems are central to many real-world applications, yet remain challenging due to their (NP-)hard nature. Amongst existing approaches, heuristics often offer the best trade-off between quality and scalability, making them suitable for industrial use. While Reinforcement Learning (RL) offers a flexible framework for designing heuristics, its adoption over handcrafted heuristics remains incomplete. Existing learned methods still lack the ability to adapt to specific instances and fully leverage the available computational budget. Current best methods either rely on a collection of pre-trained policies, or on RL fine-tuning; hence failing to fully utilize newly available information within the constraints of the budget. In response, we present MEMENTO, an approach that leverages memory to improve the search of neural solvers at inference. MEMENTO leverages online data collected across repeated attempts to dynamically adjust the action distribution based on the outcome of previous decisions. We validate its effectiveness on the Traveling Salesman and Capacitated Vehicle Routing problems, demonstrating its superiority over tree-search and policy-gradient fine-tuning; and showing that it can be zero-shot combined with diversity-based solvers. We successfully train all RL auto-regressive solvers on large instances, and verify MEMENTO's scalability and data-efficiency: pushing the state-of-the-art on 11 out of 12 evaluated tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Step Reasoning with Large Language Models, a Survey</title>
<link>https://arxiv.org/abs/2407.11511</link>
<guid>https://arxiv.org/abs/2407.11511</guid>
<content:encoded><![CDATA[

arXiv:2407.11511v3 Announce Type: replace 
Abstract: Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with FeedBack</title>
<link>https://arxiv.org/abs/2410.10584</link>
<guid>https://arxiv.org/abs/2410.10584</guid>
<content:encoded><![CDATA[

arXiv:2410.10584v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base editing with FEEDback approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. STACKFEED defines a ReACT actor agent on each document to perform structured edits based on document specific targeted instructions. Experimental results showcase that STACKFEED significantly improves KB quality and performance of the RAG system. We evaluate STACKFEED on low-resource programming problems, modified python packaged and factual question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable end-to-end Neurosymbolic Reinforcement Learning agents</title>
<link>https://arxiv.org/abs/2410.14371</link>
<guid>https://arxiv.org/abs/2410.14371</guid>
<content:encoded><![CDATA[

arXiv:2410.14371v2 Announce Type: replace 
Abstract: Deep reinforcement learning (RL) agents rely on shortcut learning, preventing them from generalizing to slightly different environments. To address this problem, symbolic method, that use object-centric states, have been developed. However, comparing these methods to deep agents is not fair, as these last operate from raw pixel-based states. In this work, we instantiate the symbolic SCoBots framework. SCoBots decompose RL tasks into intermediate, interpretable representations, culminating in action decisions based on a comprehensible set of object-centric relational concepts. This architecture aids in demystifying agent decisions. By explicitly learning to extract object-centric representations from raw states, object-centric RL, and policy distillation via rule extraction, this work places itself within the neurosymbolic AI paradigm, blending the strengths of neural networks with symbolic AI. We present the first implementation of an end-to-end trained SCoBot, separately evaluate of its components, on different Atari games. The results demonstrate the framework's potential to create interpretable and performing RL systems, and pave the way for future research directions in obtaining end-to-end interpretable RL agents.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2410.18032</link>
<guid>https://arxiv.org/abs/2410.18032</guid>
<content:encoded><![CDATA[

arXiv:2410.18032v5 Announce Type: replace 
Abstract: Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Digital Ecosystem of Beliefs: does evolution favour AI over humans?</title>
<link>https://arxiv.org/abs/2412.14500</link>
<guid>https://arxiv.org/abs/2412.14500</guid>
<content:encoded><![CDATA[

arXiv:2412.14500v3 Announce Type: replace 
Abstract: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on beliefs. To understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. Following a Universal Darwinism approach, the framework models a population of agents which change their messaging strategies due to evolutionary updates. They interact via messages, update their beliefs following a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with Digico implement two types of agents, which are modelled to represent AIs vs humans based on higher rates of communication, higher rates of evolution, seeding fixed beliefs with propaganda aims, and higher influence on the recommendation algorithm. These experiments show that: a) when AIs have faster messaging, evolution, and more influence on the recommendation algorithm, they get 80% to 95% of the views; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness up to 8%. We further discuss Digico as a tool for systematic experimentation across multi-agent configurations, the implications for legislation, personal use, and platform design, and the use of Digico for studying evolutionary principles.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey Transfer Learning: Recycling Data with Silicon Responses</title>
<link>https://arxiv.org/abs/2501.06577</link>
<guid>https://arxiv.org/abs/2501.06577</guid>
<content:encoded><![CDATA[

arXiv:2501.06577v2 Announce Type: replace 
Abstract: As researchers increasingly turn to large language models (LLMs) to generate synthetic survey data, less attention has been paid to alternative AI paradigms given environmental costs of LLMs. This paper introduces Survey Transfer Learning (STL), which develops transfer learning paradigms from computer science for survey research to recycle existing survey data and generate empirically grounded silicon responses. Inspired by political behavior theory, STL leverages shared demographic variables with high predictive power in a polarized American context to transfer knowledge across surveys. Using a neural network pre-trained on the Cooperative Election Study (CES) 2020, freezing early layers to preserve learned structure, and fine-tuning top layers on the American National Election Studies (ANES) 2020, STL generates silicon responses CES 2022 and in held-out ANES 2020 data with accuracy rates of up to 93 percent. Results show that STL outperforms LLMs, especially on sensitive measures such as racial resentment. While LLMs silicon samples are costly and opaque, STL generates empirically grounded silicon responses with high individual-level accuracy, potentially helping to mitigate key challenges in social science and the polling industry.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals</title>
<link>https://arxiv.org/abs/2502.16101</link>
<guid>https://arxiv.org/abs/2502.16101</guid>
<content:encoded><![CDATA[

arXiv:2502.16101v4 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to maintain consistent reasoning when exposed to misleading or conflicting evidence, especially in real-world domains such as politics, where information is polarized or selectively framed. Mainstream RAG benchmarks evaluate models under clean retrieval settings, where systems generate answers from gold-standard documents, or under synthetically perturbed settings, where documents are artificially injected with noise. These assumptions fail to reflect real-world conditions, often leading to an overestimation of RAG system performance. To address this gap, we introduce RAGuard, the first benchmark to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our fact-checking dataset captures naturally occurring misinformation by constructing its retrieval corpus from Reddit discussions. It categorizes retrieved evidence into three types: supporting, misleading, and unrelated, providing a realistic and challenging testbed for assessing how well RAG systems navigate different types of evidence. Our experiments reveal that, when exposed to potentially misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), while human annotators consistently perform better, highlighting LLMs' susceptibility to noisy environments. To our knowledge, RAGuard is the first benchmark to systematically assess the robustness of the RAG against misleading evidence. We expect this benchmark to drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications. The dataset is available at https://huggingface.co/datasets/UCSC-IRKM/RAGuard.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Strategic Reasoning: Agentic Study through Behavioral Game Theory</title>
<link>https://arxiv.org/abs/2502.20432</link>
<guid>https://arxiv.org/abs/2502.20432</guid>
<content:encoded><![CDATA[

arXiv:2502.20432v3 Announce Type: replace 
Abstract: Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Damper-B-PINN: Damper Characteristics-Based Bayesian Physics-Informed Neural Network for Vehicle State Estimation</title>
<link>https://arxiv.org/abs/2502.20772</link>
<guid>https://arxiv.org/abs/2502.20772</guid>
<content:encoded><![CDATA[

arXiv:2502.20772v2 Announce Type: replace 
Abstract: Accurate state estimation is fundamental to intelligent vehicles. Wheel load, one of the most important chassis states, serves as an essential input for advanced driver assistance systems (ADAS) and exerts a direct influence on vehicle stability and safety. However, wheel load estimation remains challenging due to the complexity of chassis modeling and the susceptibility of nonlinear systems to noise. To address these issues, this paper first introduces a refined suspension linkage-level modeling approach that constructs a nonlinear instantaneous dynamic model by explicitly considering the complex geometric structure of the suspension. Building upon this, we propose a damper characteristics-based Bayesian physics-informed neural network (Damper-B-PINN) framework to estimate dynamic wheel load, which leverages the suspension dynamics as physical guidance of PINN while employing Bayesian inference to mitigate the effects of system noise and uncertainty. Moreover, a damper-characteristic physics conditioning (DPC) module is designed for embedding physical prior. The proposed Damper-B-PINN is evaluated using both high-fidelity simulation datasets generated by CarSim software and real-world datasets collected from a Formula Student race car. Experimental results demonstrate that our Damper-B-PINN consistently outperforms existing methods across various test conditions, particularly extreme ones. These findings highlight the potential of the proposed Damper-B-PINN framework to enhance the accuracy and robustness of dynamic wheel load estimation, thereby improving the reliability and safety of ADAS applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning</title>
<link>https://arxiv.org/abs/2503.21406</link>
<guid>https://arxiv.org/abs/2503.21406</guid>
<content:encoded><![CDATA[

arXiv:2503.21406v2 Announce Type: replace 
Abstract: Imitation learning is a popular method for teaching robots new behaviors. However, most existing methods focus on teaching short, isolated skills rather than long, multi-step tasks. To bridge this gap, imitation learning algorithms must not only learn individual skills but also an abstract understanding of how to sequence these skills to perform extended tasks effectively. This paper addresses this challenge by proposing a neuro-symbolic imitation learning framework. Using task demonstrations, the system first learns a symbolic representation that abstracts the low-level state-action space. The learned representation decomposes a task into easier subtasks and allows the system to leverage symbolic planning to generate abstract plans. Subsequently, the system utilizes this task decomposition to learn a set of neural skills capable of refining abstract plans into actionable robot commands. Experimental results in three simulated robotic environments demonstrate that, compared to baselines, our neuro-symbolic approach increases data efficiency, improves generalization capabilities, and facilitates interpretability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
<link>https://arxiv.org/abs/2504.00509</link>
<guid>https://arxiv.org/abs/2504.00509</guid>
<content:encoded><![CDATA[

arXiv:2504.00509v3 Announce Type: replace 
Abstract: The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60 percent performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Basis of LLM's Decision Making in Social Simulation</title>
<link>https://arxiv.org/abs/2504.11671</link>
<guid>https://arxiv.org/abs/2504.11671</guid>
<content:encoded><![CDATA[

arXiv:2504.11671v3 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract "vectors of variable variations" (e.g., "male" to "female") from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Limits of AI Explainability: An Algorithmic Information Theory Approach</title>
<link>https://arxiv.org/abs/2504.20676</link>
<guid>https://arxiv.org/abs/2504.20676</guid>
<content:encoded><![CDATA[

arXiv:2504.20676v2 Announce Type: replace 
Abstract: This paper establishes a theoretical foundation for understanding the fundamental limits of AI explainability through algorithmic information theory. We formalize explainability as the approximation of complex models by simpler ones, quantifying both approximation error and explanation complexity using Kolmogorov complexity. Our key theoretical contributions include: (1) a complexity gap theorem proving that any explanation significantly simpler than the original model must differ from it on some inputs; (2) precise bounds showing that explanation complexity grows exponentially with input dimension but polynomially with error tolerance for Lipschitz functions; and (3) a characterization of the gap between local and global explainability, demonstrating that local explanations can be significantly simpler while maintaining accuracy in relevant regions. We further establish a regulatory impossibility theorem proving that no governance framework can simultaneously pursue unrestricted AI capabilities, human-interpretable explanations, and negligible error. These results highlight considerations likely to be relevant to the design, evaluation, and oversight of explainable AI systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS</title>
<link>https://arxiv.org/abs/2505.18829</link>
<guid>https://arxiv.org/abs/2505.18829</guid>
<content:encoded><![CDATA[

arXiv:2505.18829v2 Announce Type: replace 
Abstract: We present AIOS 1.0, a novel platform designed to advance computer-use agent (CUA) capabilities through environmental contextualization. While existing approaches primarily focus on building more powerful agent frameworks or enhancing agent models, we identify a fundamental limitation: the semantic disconnect between how language models understand the world and how computer interfaces are structured. AIOS 1.0 addresses this challenge by transforming computers into contextual environments that language models can natively comprehend, implementing a Model Context Protocol (MCP) server architecture to abstract computer states and actions. This approach effectively decouples interface complexity from decision complexity, enabling agents to reason more effectively about computing environments. To demonstrate our platform's effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark, outperforming several specialized agent frameworks despite its simple architecture. Our results suggest that contextualizing computer environments for language models represents a promising direction for developing more capable computer-use agents and advancing toward AI that can interact with digital systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoP: Agentic Red-teaming for Large Language Models using Composition of Principles</title>
<link>https://arxiv.org/abs/2506.00781</link>
<guid>https://arxiv.org/abs/2506.00781</guid>
<content:encoded><![CDATA[

arXiv:2506.00781v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Driven Coordination and Learning in Multi-Agent Simulation Environments</title>
<link>https://arxiv.org/abs/2506.04251</link>
<guid>https://arxiv.org/abs/2506.04251</guid>
<content:encoded><![CDATA[

arXiv:2506.04251v4 Announce Type: replace 
Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[

arXiv:2506.07927v2 Announce Type: replace 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning</title>
<link>https://arxiv.org/abs/2506.10521</link>
<guid>https://arxiv.org/abs/2506.10521</guid>
<content:encoded><![CDATA[

arXiv:2506.10521v5 Announce Type: replace 
Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction</title>
<link>https://arxiv.org/abs/2506.17784</link>
<guid>https://arxiv.org/abs/2506.17784</guid>
<content:encoded><![CDATA[

arXiv:2506.17784v2 Announce Type: replace 
Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration highlights the power of structured communication in enabling collective intelligence. However, existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication. In this work, we propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, offering a significantly larger topology space for multi-agent communication. Our method focuses on two key directions: (1) Next-Agent Prediction, which selects the most suitable agent role at each step, and (2) Next-Context Selection (NCS), which enables each agent to selectively access relevant information from any previous step. Together, these components construct task-adaptive communication pipelines that support both role flexibility and global information flow. Extensive evaluations across multiple benchmarks demonstrate that our approach achieves superior performance while substantially reducing communication overhead.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limits of Safe AI Deployment: Differentiating Oversight and Control</title>
<link>https://arxiv.org/abs/2507.03525</link>
<guid>https://arxiv.org/abs/2507.03525</guid>
<content:encoded><![CDATA[

arXiv:2507.03525v2 Announce Type: replace 
Abstract: Oversight and control, which we collectively call supervision, are often discussed as ways to ensure that AI systems are accountable, reliable, and able to fulfill governance and management requirements. However, the requirements for "human oversight" risk codifying vague or inconsistent interpretations of key concepts like oversight and control. This ambiguous terminology could undermine efforts to design or evaluate systems that must operate under meaningful human supervision. This matters because the term is used by regulatory texts such as the EU AI Act.
  This paper undertakes a targeted critical review of literature on supervision outside of AI, along with a brief summary of past work on the topic related to AI. We next differentiate control as ex-ante or real-time and operational rather than policy or governance, and oversight as performed ex-post, or a policy and governance function. Control aims to prevent failures, while oversight focuses on detection, remediation, or incentives for future prevention. Building on this, we make three contributions. 1) We propose a framework to align regulatory expectations with what is technically and organizationally plausible, articulating the conditions under which each mechanism is possible, where they fall short, and what is required to make them meaningful in practice. 2) We outline how supervision methods should be documented and integrated into risk management, and drawing on the Microsoft Responsible AI Maturity Model, we outline a maturity model for AI supervision. 3) We explicitly highlight boundaries of these mechanisms, including where they apply, where they fail, and where it is clear that no existing methods suffice. This foregrounds the question of whether meaningful supervision is possible in a given deployment context, and can support regulators, auditors, and practitioners in identifying both present and future limitations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Your LLM Web Agent: A Statistical Diagnosis</title>
<link>https://arxiv.org/abs/2507.04103</link>
<guid>https://arxiv.org/abs/2507.04103</guid>
<content:encoded><![CDATA[

arXiv:2507.04103v3 Announce Type: replace 
Abstract: LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Large Language Models for Conceptual Systems Engineering and Design</title>
<link>https://arxiv.org/abs/2507.08619</link>
<guid>https://arxiv.org/abs/2507.08619</guid>
<content:encoded><![CDATA[

arXiv:2507.08619v2 Announce Type: replace 
Abstract: Early-stage engineering design involves complex, iterative reasoning, yet existing large language model (LLM) workflows struggle to maintain task continuity and generate executable models. We evaluate whether a structured multi-agent system (MAS) can more effectively manage requirements extraction, functional decomposition, and simulator code generation than a simpler two-agent system (2AS). The target application is a solar-powered water filtration system as described in a cahier des charges. We introduce the Design-State Graph (DSG), a JSON-serializable representation that bundles requirements, physical embodiments, and Python-based physics models into graph nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS collapses the process to a Generator-Reflector loop. Both systems run a total of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1 70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON validity, requirement coverage, embodiment presence, code compatibility, workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS maintained perfect JSON integrity and embodiment tagging. Requirement coverage remained minimal (less than 20%). Code compatibility peaked at 100% under specific 2AS settings but averaged below 50% for MAS. Only the reasoning-distilled model reliably flagged workflow completion. Powered by DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes) whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced design detail. Reasoning-distilled LLM improved completion rates, yet low requirements and fidelity gaps in coding persisted.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2507.19109</link>
<guid>https://arxiv.org/abs/2507.19109</guid>
<content:encoded><![CDATA[

arXiv:2507.19109v3 Announce Type: replace 
Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for multi-objective optimization problems over discrete search spaces. Extending the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for single-objective problems, Pareto-NRPA generalizes the nested search and policy update mechanism to multi-objective optimization. The algorithm uses a set of policies to concurrently explore different regions of the solution space and maintains non-dominated fronts at each level of search. Policy adaptation is performed with respect to the diversity and isolation of sequences within the Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel bi-objective variant of the Traveling Salesman Problem with Time Windows problem (MO-TSPTW), and a neural architecture search task on well-known benchmarks. Results demonstrate that Pareto-NRPA achieves competitive performance against state-of-the-art multi-objective algorithms, both in terms of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly outperforms state-of-the-art evolutionary multi-objective algorithms on constrained search spaces. To our knowledge, this work constitutes the first adaptation of NRPA to the multi-objective setting.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting</title>
<link>https://arxiv.org/abs/2508.12260</link>
<guid>https://arxiv.org/abs/2508.12260</guid>
<content:encoded><![CDATA[

arXiv:2508.12260v3 Announce Type: replace 
Abstract: Infectious disease forecasting in novel outbreaks or low-resource settings is hampered by the need for disease-specific data, bespoke training, and expert tuning. We introduce Mantis, a foundation model trained entirely on mechanistic simulations, which enables out-of-the-box forecasting across diseases, regions, and outcomes, even in settings with limited historical data. We evaluated Mantis against 48 forecasting models across six diseases with diverse transmission modes, assessing both point forecast accuracy (mean absolute error) and probabilistic performance (weighted interval score and coverage). Despite using no real-world data during training, Mantis achieved lower mean absolute error than all models in the CDC's COVID-19 Forecast Hub when backtested on early pandemic forecasts. Across all other diseases tested, including respiratory, vector-borne, and waterborne pathogens, Mantis consistently ranked in the top two models across all evaluation metrics. Notably, Mantis generalized to diseases with transmission mechanisms not represented in its training data, demonstrating that it captures fundamental contagion dynamics rather than memorizing disease-specific patterns. These capabilities position Mantis as a practical foundation for disease forecasting: general-purpose, accurate, and deployable where traditional models fail.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalARC: Abstract Reasoning with Causal World Models</title>
<link>https://arxiv.org/abs/2509.03636</link>
<guid>https://arxiv.org/abs/2509.03636</guid>
<content:encoded><![CDATA[

arXiv:2509.03636v2 Announce Type: replace 
Abstract: On-the-fly reasoning often requires adaptation to novel problems under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning. Within- and between-model performance varied heavily across tasks, indicating room for significant improvement in language model reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models</title>
<link>https://arxiv.org/abs/2509.11575</link>
<guid>https://arxiv.org/abs/2509.11575</guid>
<content:encoded><![CDATA[

arXiv:2509.11575v2 Announce Type: replace 
Abstract: Time series reasoning treats time as a first-class axis and incorporates intermediate evidence directly into the answer. This survey defines the problem and organizes the literature by reasoning topology with three families: direct reasoning in one step, linear chain reasoning with explicit intermediates, and branch-structured reasoning that explores, revises, and aggregates. The topology is crossed with the main objectives of the field, including traditional time series analysis, explanation and understanding, causal inference and decision making, and time series generation, while a compact tag set spans these axes and captures decomposition and verification, ensembling, tool use, knowledge access, multimodality, agent loops, and LLM alignment regimes. Methods and systems are reviewed across domains, showing what each topology enables and where it breaks down in faithfulness or robustness, along with curated datasets, benchmarks, and resources that support study and deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey). Evaluation practices that keep evidence visible and temporally aligned are highlighted, and guidance is distilled on matching topology to uncertainty, grounding with observable artifacts, planning for shift and streaming, and treating cost and latency as design budgets. We emphasize that reasoning structures must balance capacity for grounding and self-correction against computational cost and reproducibility, while future progress will likely depend on benchmarks that tie reasoning quality to utility and on closed-loop testbeds that trade off cost and risk under shift-aware, streaming, and long-horizon settings. Taken together, these directions mark a shift from narrow accuracy toward reliability at scale, enabling systems that not only analyze but also understand, explain, and act on dynamic worlds with traceable evidence and credible outcomes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuromorphic Intelligence</title>
<link>https://arxiv.org/abs/2509.11940</link>
<guid>https://arxiv.org/abs/2509.11940</guid>
<content:encoded><![CDATA[

arXiv:2509.11940v4 Announce Type: replace 
Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency, flexibility, and adaptability of the human brain in artificial systems. Unlike conventional digital approaches, which suffer from the Von Neumann bottleneck and depend on massive computational and energy resources, neuromorphic systems exploit brain-inspired principles of computation to achieve orders of magnitude greater energy efficiency. By drawing on insights from a wide range of disciplines -- including artificial intelligence, physics, chemistry, biology, neuroscience, cognitive science and materials science -- neuromorphic computing promises to deliver intelligent systems that are sustainable, transparent, and widely accessible. A central challenge, however, is to identify a unifying theoretical framework capable of bridging these diverse disciplines. We argue that dynamical systems theory provides such a foundation. Rooted in differential calculus, it offers a principled language for modeling inference, learning, and control in both natural and artificial substrates. Within this framework, noise can be harnessed as a resource for learning, while differential genetic programming enables the discovery of dynamical systems that implement adaptive behaviors. Embracing this perspective paves the way toward emergent neuromorphic intelligence, where intelligent behavior arises from the dynamics of physical substrates, advancing both the science and sustainability of AI.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.16648</link>
<guid>https://arxiv.org/abs/2509.16648</guid>
<content:encoded><![CDATA[

arXiv:2509.16648v3 Announce Type: replace 
Abstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combinatorial Creativity: A New Frontier in Generalization Abilities</title>
<link>https://arxiv.org/abs/2509.21043</link>
<guid>https://arxiv.org/abs/2509.21043</guid>
<content:encoded><![CDATA[

arXiv:2509.21043v4 Announce Type: replace 
Abstract: Artificial intelligence (AI) systems, and Large Language Models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Despite its similarities to compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, bridging the gap between human and machine intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping Overlaps in Benchmarks through Perplexity in the Wild</title>
<link>https://arxiv.org/abs/2509.23488</link>
<guid>https://arxiv.org/abs/2509.23488</guid>
<content:encoded><![CDATA[

arXiv:2509.23488v3 Announce Type: replace 
Abstract: We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning</title>
<link>https://arxiv.org/abs/2510.02091</link>
<guid>https://arxiv.org/abs/2510.02091</guid>
<content:encoded><![CDATA[

arXiv:2510.02091v3 Announce Type: replace 
Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Agent Specification (Agent Spec) Technical Report</title>
<link>https://arxiv.org/abs/2510.04173</link>
<guid>https://arxiv.org/abs/2510.04173</guid>
<content:encoded><![CDATA[

arXiv:2510.04173v3 Announce Type: replace 
Abstract: Open Agent Specification (Agent Spec) is a declarative language for defining AI agents and workflows in a way that is compatible across different AI frameworks, promoting portability and interoperability within AI Agent frameworks. Agent Spec aims to resolve the challenges of fragmented agent development by providing a common unified specification that allows AI agents to be designed once and deployed across various frameworks, improving interoperability and reusability, while reducing redundant efforts. Additionally, Agent Spec facilitates development tools and portability, allowing AI agents to be defined independently of their execution environment and enabling teams to exchange solutions without implementation-specific limitations. Agent Spec benefits four key groups: (i) Agent developers, who gain a superset of reusable components and design patterns, enabling them to leverage a broader range of functionalities; (ii) Agent framework and tool developers, who can use Agent Spec as an interchange format and therefore benefit from cross-framework and tool support; (iii) Researchers, who can achieve reproducible results and comparability, facilitating more reliable and consistent outcomes; (iv) Enterprises, which see faster prototype-to-deployment, increased productivity, and greater scalability and maintainability for their AI agent solutions. This technical report provides an overview of the technical foundations of Agent Spec, including motivation, benefits, and future work. We also introduce a standardized Evaluation harness to assess agent behavior and agentic workflows across runtimes (LangGraph, CrewAI, AutoGen, and WayFlow), using three different benchmarks (SimpleQA Verified, $\tau^2$-Bench and BIRD-SQL) - analogous to how HELM and related harnesses standardized LLM evaluation - so that performance, robustness, and efficiency can be compared consistently across frameworks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare</title>
<link>https://arxiv.org/abs/2510.08872</link>
<guid>https://arxiv.org/abs/2510.08872</guid>
<content:encoded><![CDATA[

arXiv:2510.08872v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward also maximizes user welfare, but this assumption frequently fails in practice: models may over-clarify or generate overly verbose reasoning when users prefer concise answers. Such behaviors resemble the prisoner's dilemma, where individually rational choices lead to socially suboptimal outcomes. The fundamental challenge is the lack of a principled decision making mechanism that mutually benefits both the LLM and the user. We propose Game-Theoretic Alignment (GTAlign), an alignment framework that integrates game-theoretic decision making into both reasoning and training. During reasoning, the model explicitly treats user-LLM interaction as a strategic game: it constructs payoff matrices within its reasoning chain to estimate welfare for both itself and the user, and then selects actions that are mutually beneficial. During training, we introduce a social welfare reward that reinforces cooperative responses, aligning model behavior with socially efficient outcomes. In addition, we introduce an inference technique that leverages game-theoretic reasoning to dynamically adapt LLM's response when pricing policies of LLM service change. Extensive experiments demonstrate that GTAlign substantially improves reasoning efficiency, answer quality, and social welfare compared to baselines across diverse tasks. The code is available at https://github.com/ulab-uiuc/GTAlign .
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localist LLMs -- A Mathematical Framework for Dynamic Locality Control</title>
<link>https://arxiv.org/abs/2510.09338</link>
<guid>https://arxiv.org/abs/2510.09338</guid>
<content:encoded><![CDATA[

arXiv:2510.09338v2 Announce Type: replace 
Abstract: We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovation is a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, and dynamic rule injection. We provide rigorous mathematical proofs establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks, with exponential bounds on attention entropy and pointer fidelity. Specifically, we prove that when group sparsity penalties exceed certain threshold values, the model's attention mechanisms concentrate on semantically relevant blocks, achieving low entropy and high fidelity with negligible error. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes, supporting applications in regulated domains requiring both transparency and capability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where to Search: Measure the Prior-Structured Search Space of LLM Agents</title>
<link>https://arxiv.org/abs/2510.14846</link>
<guid>https://arxiv.org/abs/2510.14846</guid>
<content:encoded><![CDATA[

arXiv:2510.14846v3 Announce Type: replace 
Abstract: The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via two instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
<link>https://arxiv.org/abs/2510.14925</link>
<guid>https://arxiv.org/abs/2510.14925</guid>
<content:encoded><![CDATA[

arXiv:2510.14925v2 Announce Type: replace 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we observe preliminary correlations between internal fragility and miscalibration or hallucination (confabulation), and find that lightweight critique prompts may modestly improve or worsen calibration in small-scale tests. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens to diagnose and potentially mitigate overconfidence in reasoning systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experience-Driven Exploration for Efficient API-Free AI Agents</title>
<link>https://arxiv.org/abs/2510.15259</link>
<guid>https://arxiv.org/abs/2510.15259</guid>
<content:encoded><![CDATA[

arXiv:2510.15259v2 Announce Type: replace 
Abstract: Most existing software lacks accessible Application Programming Interfaces (APIs), requiring agents to operate solely through pixel-based Graphical User Interfaces (GUIs). In this API-free setting, large language model (LLM)-based agents face severe efficiency bottlenecks: limited to local visual experiences, they make myopic decisions and rely on inefficient trial-and-error, hindering both skill acquisition and long-term planning. To address these challenges, we propose KG-Agent, an experience-driven learning framework that structures an agent's raw pixel-level interactions into a persistent State-Action Knowledge Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking functionally similar but visually distinct GUI states, forming a rich neighborhood of experience that enables the agent to generalize from a diverse set of historical strategies. To support long-horizon reasoning, we design a hybrid intrinsic reward mechanism based on the graph topology, combining a state value reward for exploiting known high-value pathways with a novelty reward that encourages targeted exploration. This approach decouples strategic planning from pure discovery, allowing the agent to effectively value setup actions with delayed gratification. We evaluate KG-Agent in two complex, open-ended GUI-based decision-making environments (Civilization V and Slay the Spire), demonstrating significant improvements in exploration efficiency and strategic depth over the state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys</title>
<link>https://arxiv.org/abs/2510.26012</link>
<guid>https://arxiv.org/abs/2510.26012</guid>
<content:encoded><![CDATA[

arXiv:2510.26012v2 Announce Type: replace 
Abstract: The rapid growth of research literature, particularly in large language models (LLMs), has made producing comprehensive and current survey papers increasingly difficult. This paper introduces autosurvey2, a multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation. The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications to ensure both topical completeness and factual accuracy. Quality is assessed using a multi-LLM evaluation framework that measures coverage, structure, and relevance in alignment with expert review standards. Experimental results demonstrate that autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity. By combining retrieval, reasoning, and automated evaluation into a unified framework, autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing. All code and resources are available at https://github.com/annihi1ation/auto_research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neighboring State-based Exploration for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2212.10712</link>
<guid>https://arxiv.org/abs/2212.10712</guid>
<content:encoded><![CDATA[

arXiv:2212.10712v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning is a powerful tool to model decision-making processes. However, it relies on an exploration-exploitation trade-off that remains an open challenge for many tasks. In this work, we study neighboring state-based, model-free exploration led by the intuition that, for an early-stage agent, considering actions derived from a bounded region of nearby states may lead to better actions when exploring. We propose two algorithms that choose exploratory actions based on a survey of nearby states, and find that one of our methods, ${\rho}$-explore, consistently outperforms the Double DQN baseline in an discrete environment by 49% in terms of Eval Reward Return.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2301.12935</link>
<guid>https://arxiv.org/abs/2301.12935</guid>
<content:encoded><![CDATA[

arXiv:2301.12935v4 Announce Type: replace-cross 
Abstract: Though denoising diffusion probabilistic models (DDPMs) have achieved remarkable generation results, the low sampling efficiency of DDPMs still limits further applications. Since DDPMs can be formulated as diffusion ordinary differential equations (ODEs), various fast sampling methods can be derived from solving diffusion ODEs. However, we notice that previous fast sampling methods with fixed analytical form are not able to robust with the various error patterns in the noise estimated from pretrained diffusion models. In this work, we construct an error-robust Adams solver (ERA-Solver), which utilizes the implicit Adams numerical method that consists of a predictor and a corrector. Different from the traditional predictor based on explicit Adams methods, we leverage a Lagrange interpolation function as the predictor, which is further enhanced with an error-robust strategy to adaptively select the Lagrange bases with lower errors in the estimated noise. The proposed solver can be directly applied to any pretrained diffusion models, without extra training. Experiments on Cifar10, CelebA, LSUN-Church, and ImageNet 64 x 64 (conditional) datasets demonstrate that our proposed ERA-Solver achieves 3.54, 5.06, 5.02, and 5.11 Frechet Inception Distance (FID) for image generation, with only 10 network evaluations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex QA and language models hybrid architectures, Survey</title>
<link>https://arxiv.org/abs/2302.09051</link>
<guid>https://arxiv.org/abs/2302.09051</guid>
<content:encoded><![CDATA[

arXiv:2302.09051v5 Announce Type: replace-cross 
Abstract: This paper reviews the state-of-the-art of large language models (LLM) architectures and strategies for "complex" question-answering with a focus on hybrid architectures. LLM based chatbot services have allowed anyone to grasp the potential of LLM to solve many common problems, but soon discovered their limitations for complex questions. Addressing more specific, complex questions (e.g., "What is the best mix of power-generation methods to reduce climate change ?") often requires specialized architectures, domain knowledge, new skills, decomposition and multi-step resolution, deep reasoning, sensitive data protection, explainability, and human-in-the-loop processes. Therefore, we review: (1) necessary skills and tasks for handling complex questions and common LLM limits to overcome; (2) dataset, cost functions and evaluation metrics for measuring and improving (e.g. accuracy, explainability, fairness, robustness, groundedness, faithfulness, toxicity...); (3) family of solutions to overcome LLM limitations by (a) training and reinforcement (b) hybridization, (c) prompting, (d) agentic-architectures (agents, tools) and extended reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knolling Bot: Teaching Robots the Human Notion of Tidiness</title>
<link>https://arxiv.org/abs/2310.04566</link>
<guid>https://arxiv.org/abs/2310.04566</guid>
<content:encoded><![CDATA[

arXiv:2310.04566v3 Announce Type: replace-cross 
Abstract: For robots to truly collaborate and assist humans, they must understand not only logic and instructions, but also the subtle emotions, aesthetics, and feelings that define our humanity. Human art and aesthetics are among the most elusive concepts-often difficult even for people to articulate-and without grasping these fundamentals, robots will be unable to help in many spheres of daily life. Consider the long-promised robotic butler: automating domestic chores demands more than motion planning. It requires an internal model of cleanliness and tidiness-a challenge largely unexplored by AI. To bridge this gap, we propose an approach that equips domestic robots to perform simple tidying tasks via knolling, the practice of arranging scattered items into neat, space-efficient layouts. Unlike the uniformity of industrial settings, household environments feature diverse objects and highly subjective notions of tidiness. Drawing inspiration from NLP, we treat knolling as a sequential prediction problem and employ a transformer based model to forecast each object's placement. Our method learns a generalizable concept of tidiness, generates diverse solutions adaptable to varying object sets, and incorporates human preferences for personalized arrangements. This work represents a step forward in building robots that internalize human aesthetic sense and can genuinely co-create in our living spaces.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Attack Improves Protection against Unauthorized Diffusion Customization</title>
<link>https://arxiv.org/abs/2310.04687</link>
<guid>https://arxiv.org/abs/2310.04687</guid>
<content:encoded><![CDATA[

arXiv:2310.04687v5 Announce Type: replace-cross 
Abstract: Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization. Our code is available on GitHub: https://github.com/psyker-team/mist-v2.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints</title>
<link>https://arxiv.org/abs/2402.09034</link>
<guid>https://arxiv.org/abs/2402.09034</guid>
<content:encoded><![CDATA[

arXiv:2402.09034v2 Announce Type: replace-cross 
Abstract: Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such as sign language recognition, regression, and time-series classification tasks, where the dataset is limited. Our experiments demonstrate that SST models consistently outperform RNN-based models with baseline activations, exhibiting improved test accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Bayesian Learning via Regularization, Confidence Minimization, and Selective Inference</title>
<link>https://arxiv.org/abs/2404.11350</link>
<guid>https://arxiv.org/abs/2404.11350</guid>
<content:encoded><![CDATA[

arXiv:2404.11350v3 Announce Type: replace-cross 
Abstract: The application of artificial intelligence (AI) models in fields such as engineering is limited by the known difficulty of quantifying the reliability of an AI's decision. A well-calibrated AI model must correctly report its accuracy on in-distribution (ID) inputs, while also enabling the detection of out-of-distribution (OOD) inputs. A conventional approach to improve calibration is the application of Bayesian ensembling. However, owing to computational limitations and model misspecification, practical ensembling strategies do not necessarily enhance calibration. This paper proposes an extension of variational inference (VI)-based Bayesian learning that integrates calibration regularization for improved ID performance, confidence minimization for OOD detection, and selective calibration to ensure a synergistic use of calibration regularization and confidence minimization. The scheme is constructed successively by first introducing calibration-regularized Bayesian learning (CBNN), then incorporating out-of-distribution confidence minimization (OCM) to yield CBNN-OCM, and finally integrating also selective calibration to produce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs for which the calibration performance is expected to be insufficient. Numerical results illustrate the trade-offs between ID accuracy, ID calibration, and OOD calibration attained by both frequentist and Bayesian learning methods. Among the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance as compared to existing state-of-the-art approaches at the cost of rejecting a sufficiently large number of inputs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SST: Multi-Scale Hybrid Mamba-Transformer Experts for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2404.14757</link>
<guid>https://arxiv.org/abs/2404.14757</guid>
<content:encoded><![CDATA[

arXiv:2404.14757v3 Announce Type: replace-cross 
Abstract: Time series forecasting has made significant advances, including with Transformer-based models. The attention mechanism in Transformer effectively captures temporal dependencies by attending to all past inputs simultaneously. However, its quadratic complexity with respect to sequence length limits the scalability for long-range modeling. Recent state space models (SSMs) such as Mamba offer a promising alternative by achieving linear complexity without attention. Yet, Mamba compresses historical information into a fixed-size latent state, potentially causing information loss and limiting representational effectiveness. This raises a key research question: Can we design a hybrid Mamba-Transformer architecture that is both effective and efficient for time series forecasting? To address it, we adapt a hybrid Mamba-Transformer architecture Mambaformer, originally proposed for language modeling, to the time series domain. Preliminary experiments reveal that naively stacking Mamba and Transformer layers in Mambaformer is suboptimal for time series forecasting, due to an information interference problem. To mitigate this issue, we introduce a new time series decomposition strategy that separates time series into long-range patterns and short-range variations. Then we show that Mamba excels at capturing long-term structures, while Transformer is more effective at modeling short-term dynamics. Building on this insight, we propose State Space Transformer (SST), a multi-scale hybrid model with expert modules: a Mamba expert for long-range patterns and a Transformer expert for short-term variations. SST also employs a multi-scale patching mechanism to adaptively adjust time series resolution: low resolution for long-term patterns and high resolution for short-term variations. Experiments show that SST obtains SOTA performance with linear scalability. The code is at https://github.com/XiongxiaoXu/SST.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ocean Wave Forecasting with Deep Learning as Alternative to Conventional Models</title>
<link>https://arxiv.org/abs/2406.03848</link>
<guid>https://arxiv.org/abs/2406.03848</guid>
<content:encoded><![CDATA[

arXiv:2406.03848v4 Announce Type: replace-cross 
Abstract: This study presents OceanCastNet (OCN), a machine learning approach for wave forecasting that incorporates wind and wave fields to predict significant wave height, mean wave period, and mean wave direction.We evaluate OCN's performance against the operational ECWAM model using two independent datasets: NDBC buoy and Jason-3 satellite observations. NDBC station validation indicates OCN performs better at 24 stations compared to ECWAM's 10 stations, and Jason-3 satellite validation confirms similar accuracy across 228-hour forecasts. OCN successfully captures wave patterns during extreme weather conditions, demonstrated through Typhoon Goni with prediction errors typically within $\pm$0.5 m. The approach also offers computational efficiency advantages. The results suggest that machine learning approaches can achieve performance comparable to conventional wave forecasting systems for operational wave prediction applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning</title>
<link>https://arxiv.org/abs/2406.04772</link>
<guid>https://arxiv.org/abs/2406.04772</guid>
<content:encoded><![CDATA[

arXiv:2406.04772v5 Announce Type: replace-cross 
Abstract: Recent rehearsal-free continual learning (CL) methods guided by prompts achieve strong performance on vision tasks with non-stationary data but remain resource-intensive, hindering real-world edge deployment. We introduce resource-efficient prompting (REP), which improves the computational and memory efficiency of prompt-based rehearsal-free continual learning methods while minimizing accuracy trade-offs. Our approach employs swift prompt selection to refine input data using a carefully provisioned model and introduces adaptive token merging (AToM) and adaptive layer dropping (ALD) for efficient prompt updates. AToM and ALD selectively skip data and model layers while preserving task-specific features during the learning of new tasks. Extensive experiments on multiple image classification datasets demonstrate REP's superior resource efficiency over state-of-the-art rehearsal-free CL methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Augmenting learning in neuro-embodied systems through neurobiological first principles</title>
<link>https://arxiv.org/abs/2407.04525</link>
<guid>https://arxiv.org/abs/2407.04525</guid>
<content:encoded><![CDATA[

arXiv:2407.04525v5 Announce Type: replace-cross 
Abstract: Recent progress in artificial intelligence (AI) has been driven by insights from physics and neuroscience, particularly through the development of artificial neural networks (ANNs) capable of complex cognitive tasks such as vision and language processing. Despite these advances, they struggle with continual learning, adaptable knowledge transfer, robustness, and resource efficiency -- capabilities that biological systems handle seamlessly. Specifically, neuromorphic systems and artificial neural networks often overlook two key biophysical properties of neural circuits: neuronal diversity and cell-specific neuromodulation. These mechanisms, essential for regulating dynamic learning across brain scales, allow neuromodulators to introduce degeneracy in biological neural networks, ensuring stability and adaptability under changing conditions. In this article, we summarize recent bioinspired models, learning rules, and architectures, and propose a framework for augmenting ANNs, which has the potential to bridge the gap between neuroscience and AI through neurobiological first principles. Our proposed dual-framework approach leverages spiking neural networks to emulate diverse spiking behaviors and dendritic compartmental dynamics, thereby simulating the morphological and functional diversity of neuronal computations. Finally, we outline how integrating these biophysical principles into task-driven spiking neural networks and neuromorphic systems provides scalable solutions for continual learning, adaptability, robustness, and resource-efficiency. Additionally, this approach will not only provide insights into how emergent behaviors arise in neural networks but also catalyze the development of more efficient, reliable, and intelligent neuromorphic systems and robotic agents.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT</title>
<link>https://arxiv.org/abs/2407.11041</link>
<guid>https://arxiv.org/abs/2407.11041</guid>
<content:encoded><![CDATA[

arXiv:2407.11041v5 Announce Type: replace-cross 
Abstract: This paper presents the design of a hardware accelerator for Transformers, optimized for on-device time-series forecasting in AIoT systems. It integrates integer-only quantization and Quantization-Aware Training with optimized hardware designs to realize 6-bit and 4-bit quantized Transformer models, which achieved precision comparable to 8-bit quantized models from related research. Utilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7 XC7S15), we examine the feasibility of deploying Transformer models on embedded IoT devices. This includes a thorough analysis of achievable precision, resource utilization, timing, power, and energy consumption for on-device inference. Our results indicate that while sufficient performance can be attained, the optimization process is not trivial. For instance, reducing the quantization bitwidth does not consistently result in decreased latency or energy consumption, underscoring the necessity of systematically exploring various optimization combinations. Compared to an 8-bit quantized Transformer model in related studies, our 4-bit quantized Transformer model increases test loss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less energy. Relevant source code is provided in the accompanying GitHub repository\footnote{https://github.com/tianheng-ling/TinyTransformer4TS}.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships</title>
<link>https://arxiv.org/abs/2407.12543</link>
<guid>https://arxiv.org/abs/2407.12543</guid>
<content:encoded><![CDATA[

arXiv:2407.12543v3 Announce Type: replace-cross 
Abstract: While interpretability methods identify a model's learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models' have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model's behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preliminary study on artificial intelligence methods for cybersecurity threat detection in computer networks based on raw data packets</title>
<link>https://arxiv.org/abs/2407.17339</link>
<guid>https://arxiv.org/abs/2407.17339</guid>
<content:encoded><![CDATA[

arXiv:2407.17339v2 Announce Type: replace-cross 
Abstract: Most of the intrusion detection methods in computer networks are based on traffic flow characteristics. However, this approach may not fully exploit the potential of deep learning algorithms to directly extract features and patterns from raw packets. Moreover, it impedes real-time monitoring due to the necessity of waiting for the processing pipeline to complete and introduces dependencies on additional software components.
  In this paper, we investigate deep learning methodologies capable of detecting attacks in real-time directly from raw packet data within network traffic. We propose a novel approach where packets are stacked into windows and separately recognised, with a 2D image representation suitable for processing with computer vision models. Our investigation utilizes the CIC IDS-2017 dataset, which includes both benign traffic and prevalent real-world attacks, providing a comprehensive foundation for our research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Distillation for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2407.20299</link>
<guid>https://arxiv.org/abs/2407.20299</guid>
<content:encoded><![CDATA[

arXiv:2407.20299v3 Announce Type: replace-cross 
Abstract: Offline reinforcement learning often requires a quality dataset that we can train a policy on. However, in many situations, it is not possible to get such a dataset, nor is it easy to train a policy to perform well in the actual environment given the offline data. We propose using data distillation to train and distill a better dataset which can then be used for training a better policy model. We show that our method is able to synthesize a dataset where a model trained on it achieves similar performance to a model trained on the full dataset or a model trained using percentile behavioral cloning. Our project site is available at https://datasetdistillation4rl.github.io . We also provide our implementation at https://github.com/ggflow123/DDRL .
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepHQ: Learned Hierarchical Quantizer for Progressive Deep Image Coding</title>
<link>https://arxiv.org/abs/2408.12150</link>
<guid>https://arxiv.org/abs/2408.12150</guid>
<content:encoded><![CDATA[

arXiv:2408.12150v2 Announce Type: replace-cross 
Abstract: Unlike fixed- or variable-rate image coding, progressive image coding (PIC) aims to compress various qualities of images into a single bitstream, increasing the versatility of bitstream utilization and providing high compression efficiency compared to simulcast compression. Research on neural network (NN)-based PIC is in its early stages, mainly focusing on applying varying quantization step sizes to the transformed latent representations in a hierarchical manner. These approaches are designed to compress only the progressively added information as the quality improves, considering that a wider quantization interval for lower-quality compression includes multiple narrower sub-intervals for higher-quality compression. However, the existing methods are based on handcrafted quantization hierarchies, resulting in sub-optimal compression efficiency. In this paper, we propose an NN-based progressive coding method that firstly utilizes learned quantization step sizes via learning for each quantization layer. We also incorporate selective compression with which only the essential representation components are compressed for each quantization layer. We demonstrate that our method achieves significantly higher coding efficiency than the existing approaches with decreased decoding time and reduced model size. The source code is publicly available at https://github.com/JooyoungLeeETRI/DeepHQ
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems</title>
<link>https://arxiv.org/abs/2409.07189</link>
<guid>https://arxiv.org/abs/2409.07189</guid>
<content:encoded><![CDATA[

arXiv:2409.07189v2 Announce Type: replace-cross 
Abstract: Molecular dynamics (MD) simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently emerged as a "human-in-the-loop" strategy for efficiently navigating hyper-dimensional molecular systems. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular simulations running on high-performance computing architectures, iMD-VR enables researchers to reach out and guide molecular conformational dynamics, in order to efficiently explore complex, high-dimensional molecular systems. Moreover, iMD-VR simulations generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the use of researcher-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL enables agents to mimic complex behaviours from expert demonstrations, circumventing the need for explicit programming or intricate reward design. In this article, we review IL across robotics and Multi-agents systems domains which are comparable to iMD-VR, and discuss how iMD-VR recordings could be used to train IL models to interact with MD simulations. We then illustrate the applications of these ideas through a proof-of-principle study where iMD-VR data was used to train a CNN network on a simple molecular manipulation task; namely, threading a small molecule through a nanotube pore. Finally, we outline future research directions and potential challenges of using AI agents to augment human expertise in navigating vast molecular conformational spaces.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prevailing Research Areas for Music AI in the Era of Foundation Models</title>
<link>https://arxiv.org/abs/2409.09378</link>
<guid>https://arxiv.org/abs/2409.09378</guid>
<content:encoded><![CDATA[

arXiv:2409.09378v3 Announce Type: replace-cross 
Abstract: Parallel to rapid advancements in foundation model research, the past few years have witnessed a surge in music AI applications. As AI-generated and AI-augmented music become increasingly mainstream, many researchers in the music AI community may wonder: what research frontiers remain unexplored? This paper outlines several key areas within music AI research that present significant opportunities for further investigation. We begin by examining foundational representation models and highlight emerging efforts toward explainability and interpretability. We then discuss the evolution toward multimodal systems, provide an overview of the current landscape of music datasets and their limitations, and address the growing importance of model efficiency in both training and deployment. Next, we explore applied directions, focusing first on generative models. We review recent systems, their computational constraints, and persistent challenges related to evaluation and controllability. We then examine extensions of these generative approaches to multimodal settings and their integration into artists' workflows, including applications in music editing, captioning, production, transcription, source separation, performance, discovery, and education. Finally, we explore copyright implications of generative music and propose strategies to safeguard artist rights. While not exhaustive, this survey aims to illuminate promising research directions enabled by recent developments in music foundation models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models</title>
<link>https://arxiv.org/abs/2409.19667</link>
<guid>https://arxiv.org/abs/2409.19667</guid>
<content:encoded><![CDATA[

arXiv:2409.19667v4 Announce Type: replace-cross 
Abstract: The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?</title>
<link>https://arxiv.org/abs/2410.02611</link>
<guid>https://arxiv.org/abs/2410.02611</guid>
<content:encoded><![CDATA[

arXiv:2410.02611v2 Announce Type: replace-cross 
Abstract: Transformer-based models have revolutionized the field of natural language processing. To understand why they perform so well and to assess their reliability, several studies have focused on questions such as: Which linguistic properties are encoded by these models, and to what extent? How robust are these models in encoding linguistic properties when faced with perturbations in the input text? However, these studies have mainly focused on BERT and the English language. In this paper, we investigate similar questions regarding encoding capability and robustness for 8 linguistic properties across 13 different perturbations in 6 Indic languages, using 9 multilingual Transformer models (7 universal and 2 Indic-specific). To conduct this study, we introduce a novel multilingual benchmark dataset, IndicSentEval, containing approximately $\sim$47K sentences. Surprisingly, our probing analysis of surface, syntactic, and semantic properties reveals that while almost all multilingual models demonstrate consistent encoding performance for English, they show mixed results for Indic languages. As expected, Indic-specific multilingual models capture linguistic properties in Indic languages better than universal models. Intriguingly, universal models broadly exhibit better robustness compared to Indic-specific models, particularly under perturbations such as dropping both nouns and verbs, dropping only verbs, or keeping only nouns. Overall, this study provides valuable insights into probing and perturbation-specific strengths and weaknesses of popular multilingual Transformer-based models for different Indic languages. We make our code and dataset publicly available [https://github.com/aforakhilesh/IndicBertology].
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Large Language Models for Detecting Mental Disorders</title>
<link>https://arxiv.org/abs/2410.07129</link>
<guid>https://arxiv.org/abs/2410.07129</guid>
<content:encoded><![CDATA[

arXiv:2410.07129v3 Announce Type: replace-cross 
Abstract: This paper compares the effectiveness of traditional machine learning methods, encoder-based models, and large language models (LLMs) on the task of detecting depression and anxiety. Five Russian-language datasets were considered, each differing in format and in the method used to define the target pathology class. We tested AutoML models based on linguistic features, several variations of encoder-based Transformers such as BERT, and state-of-the-art LLMs as pathology classification models. The results demonstrated that LLMs outperform traditional methods, particularly on noisy and small datasets where training examples vary significantly in text length and genre. However, psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression, highlighting their potential effectiveness in targeted clinical applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Vision-Language-Recommendation with Personalized Fusion</title>
<link>https://arxiv.org/abs/2410.08478</link>
<guid>https://arxiv.org/abs/2410.08478</guid>
<content:encoded><![CDATA[

arXiv:2410.08478v4 Announce Type: replace-cross 
Abstract: Applying large pre-trained Vision-Language Models to recommendation is a burgeoning field, a direction we term Vision-Language-Recommendation (VLR). Bringing VLR to user-oriented on-device intelligence within a federated learning framework is a crucial step for enhancing user privacy and delivering personalized experiences. This paper introduces FedVLR, a federated VLR framework specially designed for user-specific personalized fusion of vision-language representations. At its core is a novel bi-level fusion mechanism: The server-side multi-view fusion module first generates a diverse set of pre-fused multimodal views. Subsequently, each client employs a user-specific mixture-of-expert mechanism to adaptively integrate these views based on individual user interaction history. This designed lightweight personalized fusion module provides an efficient solution to implement a federated VLR system. The effectiveness of our proposed FedVLR has been validated on seven benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated Smart Contract Auditing Using Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2410.13918</link>
<guid>https://arxiv.org/abs/2410.13918</guid>
<content:encoded><![CDATA[

arXiv:2410.13918v3 Announce Type: replace-cross 
Abstract: The rapid growth of blockchain technology has driven the widespread adoption of smart contracts. However, their inherent vulnerabilities have led to significant financial losses. Traditional auditing methods, while essential, struggle to keep pace with the increasing complexity and scale of smart contracts. Large Language Models (LLMs) offer promising capabilities for automating vulnerability detection, but their adoption is often limited by high computational costs. Although prior work has explored leveraging large models through agents or workflows, relatively little attention has been given to improving the performance of smaller, fine-tuned models--a critical factor for achieving both efficiency and data privacy. In this paper, we introduce HKT-SmartAudit, a framework for developing lightweight models optimized for smart contract auditing. It features a multi-stage knowledge distillation pipeline that integrates classical distillation, external domain knowledge, and reward-guided learning to transfer high-quality insights from large teacher models. A single-task learning strategy is employed to train compact student models that maintain high accuracy and robustness while significantly reducing computational overhead. Experimental results show that our distilled models outperform both commercial tools and larger models in detecting complex vulnerabilities and logical flaws, offering a practical, secure, and scalable solution for smart contract auditing. The source code is available at Github repository.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation of Cognitive Biases in LLMs</title>
<link>https://arxiv.org/abs/2410.15413</link>
<guid>https://arxiv.org/abs/2410.15413</guid>
<content:encoded><![CDATA[

arXiv:2410.15413v2 Announce Type: replace-cross 
Abstract: We present a large-scale evaluation of 30 cognitive biases in 20 state-of-the-art large language models (LLMs) under various decision-making scenarios. Our contributions include a novel general-purpose test framework for reliable and large-scale generation of tests for LLMs, a benchmark dataset with 30,000 tests for detecting cognitive biases in LLMs, and a comprehensive assessment of the biases found in the 20 evaluated LLMs. Our work confirms and broadens previous findings suggesting the presence of cognitive biases in LLMs by reporting evidence of all 30 tested biases in at least some of the 20 LLMs. We publish our framework code to encourage future research on biases in LLMs: https://github.com/simonmalberg/cognitive-biases-in-llms
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Resolution Seismic Waveform Generation using Denoising Diffusion</title>
<link>https://arxiv.org/abs/2410.19343</link>
<guid>https://arxiv.org/abs/2410.19343</guid>
<content:encoded><![CDATA[

arXiv:2410.19343v2 Announce Type: replace-cross 
Abstract: Accurate prediction and synthesis of seismic waveforms are crucial for seismic-hazard assessment and earthquake-resistant infrastructure design. Existing prediction methods, such as ground-motion models and physics-based wave-field simulations, often fail to capture the full complexity of seismic wavefields, particularly at higher frequencies. This study introduces HighFEM, a novel, computationally efficient, and scalable (i.e., capable of generating many seismograms simultaneously) generative model for high-frequency seismic-waveform generation. Our approach leverages a spectrogram representation of the seismic-waveform data, which is reduced to a lower-dimensional manifold via an autoencoder. A state-of-the-art diffusion model is trained to generate this latent representation conditioned on key input parameters: earthquake magnitude, recording distance, site conditions, hypocenter depth, and azimuthal gap. The model generates waveforms with frequency content up to 50 Hz. Any scalar ground-motion statistic, such as peak ground-motion amplitudes and spectral accelerations, can be readily derived from the synthesized waveforms. We validate our model using commonly employed seismological metrics and performance metrics from image-generation studies. Our results demonstrate that the openly available model can generate realistic high-frequency seismic waveforms across a wide range of input parameters, even in data-sparse regions. For the scalar ground-motion statistics commonly used in seismic-hazard and earthquake-engineering studies, we show that our model accurately reproduces both the median trends of the real data and their variability. To evaluate and compare the growing number of these and similar Generative Waveform Models (GWMs), we argue that they should be openly available and included in community ground-motion-model evaluation efforts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Action Recognition by Leveraging the Hierarchical Structure of Actions and Textual Context</title>
<link>https://arxiv.org/abs/2410.21275</link>
<guid>https://arxiv.org/abs/2410.21275</guid>
<content:encoded><![CDATA[

arXiv:2410.21275v2 Announce Type: replace-cross 
Abstract: We propose a novel approach to improve action recognition by exploiting the hierarchical organization of actions and by incorporating contextualized textual information, including location and previous actions, to reflect the action's temporal context. To achieve this, we introduce a transformer architecture tailored for action recognition that employs both visual and textual features. Visual features are obtained from RGB and optical flow data, while text embeddings represent contextual information. Furthermore, we define a joint loss function to simultaneously train the model for both coarse- and fine-grained action recognition, effectively exploiting the hierarchical nature of actions. To demonstrate the effectiveness of our method, we extend the Toyota Smarthome Untrimmed (TSU) dataset by incorporating action hierarchies, resulting in the Hierarchical TSU dataset, a hierarchical dataset designed for monitoring activities of the elderly in home environments. An ablation study assesses the performance impact of different strategies for integrating contextual and hierarchical data. Experimental results demonstrate that the proposed method consistently outperforms SOTA methods on the Hierarchical TSU dataset, Assembly101 and IkeaASM, achieving over a 17% improvement in top-1 accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity</title>
<link>https://arxiv.org/abs/2411.02184</link>
<guid>https://arxiv.org/abs/2411.02184</guid>
<content:encoded><![CDATA[

arXiv:2411.02184v3 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability and safety of machine learning systems. In recent years, it has received increasing attention, particularly through post-hoc detection and training-based methods. In this paper, we focus on post-hoc OOD detection, which enables identifying OOD samples without altering the model's training procedure or objective. Our primary goal is to investigate the relationship between model capacity and its OOD detection performance. Specifically, we aim to answer the following question: Does the Double Descent phenomenon manifest in post-hoc OOD detection? This question is crucial, as it can reveal whether overparameterization, which is already known to benefit generalization, can also enhance OOD detection. Despite the growing interest in these topics by the classic supervised machine learning community, this intersection remains unexplored for OOD detection. We empirically demonstrate that the Double Descent effect does indeed appear in post-hoc OOD detection. Furthermore, we provide theoretical insights to explain why this phenomenon emerges in such setting. Finally, we show that the overparameterized regime does not yield superior results consistently, and we propose a method to identify the optimal regime for OOD detection based on our observations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks</title>
<link>https://arxiv.org/abs/2411.03343</link>
<guid>https://arxiv.org/abs/2411.03343</guid>
<content:encoded><![CDATA[

arXiv:2411.03343v3 Announce Type: replace-cross 
Abstract: Jailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train linear and non-linear probes on hidden states of open-weight LLMs to predict jailbreak success. Probes achieve strong in-distribution accuracy but transfer is attack-family-specific, revealing that different jailbreaks are supported by distinct internal mechanisms rather than a single universal direction. To establish causal relevance, we construct probe-guided latent interventions that systematically shift compliance in the predicted direction. Interventions derived from non-linear probes produce larger and more reliable effects than those from linear probes, indicating that features linked to jailbreak success are encoded non-linearly in prompt representations. Overall, the results surface heterogeneous, non-linear structure in jailbreak mechanisms and provide a prompt-side methodology for recovering and testing the features that drive jailbreak outcomes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuSEGO: Dual Second-order Equivariant Graph Ordinary Differential Equation</title>
<link>https://arxiv.org/abs/2411.10000</link>
<guid>https://arxiv.org/abs/2411.10000</guid>
<content:encoded><![CDATA[

arXiv:2411.10000v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) with equivariant properties have achieved significant success in modeling complex dynamic systems and molecular properties. However, their expressiveness ability is limited by: (1) Existing methods often overlook the over-smoothing issue caused by traditional GNN models, as well as the gradient explosion or vanishing problems in deep GNNs. (2) Most models operate on first-order information, neglecting that the real world often consists of second-order systems, which further limits the model's representation capabilities. To address these issues, we propose the \textbf{Du}al \textbf{S}econd-order \textbf{E}quivariant \textbf{G}raph \textbf{O}rdinary Differential Equation (\method{}) for equivariant representation. Specifically, \method{} apply the dual second-order equivariant graph ordinary differential equations (Graph ODEs) on graph embeddings and node coordinates, simultaneously. Theoretically, we first prove that \method{} maintains the equivariant property. Furthermore, we provide theoretical insights showing that \method{} effectively alleviates the over-smoothing problem in both feature representation and coordinate update. Additionally, we demonstrate that the proposed \method{} mitigates the exploding and vanishing gradients problem, facilitating the training of deep multi-layer GNNs. Extensive experiments on benchmark datasets validate the superiority of the proposed \method{} compared to baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image Super-Resolution with Transformers and TaylorShift</title>
<link>https://arxiv.org/abs/2411.10231</link>
<guid>https://arxiv.org/abs/2411.10231</guid>
<content:encoded><![CDATA[

arXiv:2411.10231v2 Announce Type: replace-cross 
Abstract: Transformer-based architectures have recently advanced the image reconstruction quality of super-resolution (SR) models. Yet, their scalability remains limited by quadratic attention costs and coarse patch embeddings that weaken pixel-level fidelity. We propose TaylorIR, a plug-and-play framework that enforces 1x1 patch embeddings for true pixel-wise reasoning and replaces conventional self-attention with TaylorShift, a Taylor-series-based attention mechanism enabling full token interactions with near-linear complexity. Across multiple SR benchmarks, TaylorIR delivers state-of-the-art performance while reducing memory consumption by up to 60%, effectively bridging the gap between fine-grained detail restoration and efficient transformer scaling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incivility and Rigidity: Evaluating the Risks of Fine-Tuning LLMs for Political Argumentation</title>
<link>https://arxiv.org/abs/2411.16813</link>
<guid>https://arxiv.org/abs/2411.16813</guid>
<content:encoded><![CDATA[

arXiv:2411.16813v4 Announce Type: replace-cross 
Abstract: Incivility on platforms such as Twitter (now X) and Reddit complicates the development of AI systems that can support productive, rhetorically sound political argumentation. We present experiments with \textit{GPT-3.5 Turbo} fine-tuned on two contrasting datasets of political discourse: high-incivility Twitter replies to U.S. Congress and low-incivility posts from Reddit's \textit{r/ChangeMyView}. Our evaluation examines how data composition and prompting strategies affect the rhetorical framing and deliberative quality of model-generated arguments. Results show that Reddit-finetuned models generate safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies adversarial tone and toxicity. Prompt-based steering reduces overt toxicity (e.g., personal attacks) but cannot fully offset the influence of noisy training data. We introduce a rhetorical evaluation rubric - covering justification, reciprocity, alignment, and authority - and provide implementation guidelines for authoring, moderation, and deliberation-support systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Uncertainty for Generated Image Detection</title>
<link>https://arxiv.org/abs/2412.05897</link>
<guid>https://arxiv.org/abs/2412.05897</guid>
<content:encoded><![CDATA[

arXiv:2412.05897v2 Announce Type: replace-cross 
Abstract: We introduce a novel framework for AI-generated image detection through epistemic uncertainty, aiming to address critical security concerns in the era of generative models. Our key insight stems from the observation that distributional discrepancies between training and testing data manifest distinctively in the epistemic uncertainty space of machine learning models. In this context, the distribution shift between natural and generated images leads to elevated epistemic uncertainty in models trained on natural images when evaluating generated ones. Hence, we exploit this phenomenon by using epistemic uncertainty as a proxy for detecting generated images. This converts the challenge of generated image detection into the problem of uncertainty estimation, underscoring the generalization performance of the model used for uncertainty estimation. Fortunately, advanced large-scale vision models pre-trained on extensive natural images have shown excellent generalization performance for various scenarios. Thus, we utilize these pre-trained models to estimate the epistemic uncertainty of images and flag those with high uncertainty as generated. Extensive experiments demonstrate the efficacy of our method. Code is available at https://github.com/tmlr-group/WePe.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research</title>
<link>https://arxiv.org/abs/2412.06966</link>
<guid>https://arxiv.org/abs/2412.06966</guid>
<content:encoded><![CDATA[

arXiv:2412.06966v2 Announce Type: replace-cross 
Abstract: "Machine unlearning" is a popular proposed solution for mitigating the existence of content in an AI model that is problematic for legal or moral reasons, including privacy, copyright, safety, and more. For example, unlearning is often invoked as a solution for removing the effects of specific information from a generative-AI model's parameters, e.g., a particular individual's personal data or the inclusion of copyrighted content in the model's training data. Unlearning is also proposed as a way to prevent a model from generating targeted types of information in its outputs, e.g., generations that closely resemble a particular individual's data or reflect the concept of "Spiderman." Both of these goals--the targeted removal of information from a model and the targeted suppression of information from a model's outputs--present various technical and substantive challenges. We provide a framework for ML researchers and policymakers to think rigorously about these challenges, identifying several mismatches between the goals of unlearning and feasible implementations. These mismatches explain why unlearning is not a general-purpose solution for circumscribing generative-AI model behavior in service of broader positive impact.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Adaptation for Foundation Models: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2501.00365</link>
<guid>https://arxiv.org/abs/2501.00365</guid>
<content:encoded><![CDATA[

arXiv:2501.00365v2 Announce Type: replace-cross 
Abstract: The rapid advancement of foundation modelslarge-scale neural networks trained on diverse, extensive datasetshas revolutionized artificial intelligence, enabling unprecedented advancements across domains such as natural language processing, computer vision, and scientific discovery. However, the substantial parameter count of these models, often reaching billions or trillions, poses significant challenges in adapting them to specific downstream tasks. Low-Rank Adaptation (LoRA) has emerged as a highly promising approach for mitigating these challenges, offering a parameter-efficient mechanism to fine-tune foundation models with minimal computational overhead. This survey provides the first comprehensive review of LoRA techniques beyond large Language Models to general foundation models, including recent techniques foundations, emerging frontiers and applications of low-rank adaptation across multiple domains. Finally, this survey discusses key challenges and future research directions in theoretical understanding, scalability, and robustness. This survey serves as a valuable resource for researchers and practitioners working with efficient foundation model adaptation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement</title>
<link>https://arxiv.org/abs/2501.15417</link>
<guid>https://arxiv.org/abs/2501.15417</guid>
<content:encoded><![CDATA[

arXiv:2501.15417v3 Announce Type: replace-cross 
Abstract: We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker's timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at https://amphionspace.github.io/anyenhance. An open-source implementation is provided at https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Runtime Analysis of Evolutionary Algorithms for Multi-party Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2501.16336</link>
<guid>https://arxiv.org/abs/2501.16336</guid>
<content:encoded><![CDATA[

arXiv:2501.16336v3 Announce Type: replace-cross 
Abstract: In scenarios where multiple decision-makers operate within a common decision space, each focusing on their own multi-objective optimization problem (e.g., bargaining games), the problem can be modeled as a multi-party multi-objective optimization problem (MPMOP). While numerous evolutionary algorithms have been proposed to solve MPMOPs, most results remain empirical. This paper presents the first theoretical analysis of the expected runtime of evolutionary algorithms on bi-party multi-objective optimization problems (BPMOPs). Our findings demonstrate that employing traditional multi-objective optimization algorithms to solve MPMOPs is both time-consuming and inefficient, as the resulting population contains many solutions that fail to achieve consensus among decision-makers. An alternative approach involves decision-makers individually solving their respective optimization problems and seeking consensus only in the final stage. While feasible for pseudo-Boolean optimization problems, this method may fail to guarantee approximate performance for one party in NP-hard problems. Finally, we propose evolutionary multi-party multi-objective optimizers (EMPMO) for pseudo-Boolean optimization and shortest path problems within a multi-party multi-objective context, maintain a common solution set among all parties. Theoretical and experimental results demonstrate that the proposed \( \text{EMPMO}_{\text{random}} \) outperforms previous algorithms in terms of the lower bound on the expected runtime for pseudo-Boolean optimization problems. Additionally, the consensus-based evolutionary multi-party multi-objective optimizer( \( \text{EMPMO}_{\text{cons}}^{\text{SP}} \) ) achieves better efficiency and precision in solving shortest path problems compared to existing algorithms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds</title>
<link>https://arxiv.org/abs/2502.02869</link>
<guid>https://arxiv.org/abs/2502.02869</guid>
<content:encoded><![CDATA[

arXiv:2502.02869v4 Announce Type: replace-cross 
Abstract: In-Context Reinforcement Learning (ICRL) enables agents to learn automatically and on-the-fly from their interactive experiences. However, a major challenge in scaling up ICRL is the lack of scalable task collections. To address this, we propose the procedurally generated tabular Markov Decision Processes, named AnyMDP. Through a carefully designed randomization process, AnyMDP is capable of generating high-quality tasks on a large scale while maintaining relatively low structural biases. To facilitate efficient meta-training at scale, we further introduce decoupled policy distillation and induce prior information in the ICRL framework. Our results demonstrate that, with a sufficiently large scale of AnyMDP tasks, the proposed model can generalize to tasks that were not considered in the training set through versatile in-context learning paradigms. The scalable task set provided by AnyMDP also enables a more thorough empirical investigation of the relationship between data distribution and ICRL performance. We further show that the generalization of ICRL potentially comes at the cost of increased task diversity and longer adaptation periods. This finding carries critical implications for scaling robust ICRL capabilities, highlighting the necessity of diverse and extensive task design, and prioritizing asymptotic performance over few-shot adaptation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2502.03304</link>
<guid>https://arxiv.org/abs/2502.03304</guid>
<content:encoded><![CDATA[

arXiv:2502.03304v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. Our code is released at https://github.com/Skilteee/DiZO.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Study in Dataset Distillation for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2502.03656</link>
<guid>https://arxiv.org/abs/2502.03656</guid>
<content:encoded><![CDATA[

arXiv:2502.03656v2 Announce Type: replace-cross 
Abstract: Dataset distillation aims to compress large datasets into compact yet highly informative subsets that preserve the training behavior of the original data. While this concept has gained traction in classification, its potential for image Super-Resolution (SR) remains largely untapped. In this work, we conduct the first systematic study of dataset distillation for SR, evaluating both pixel- and latent-space formulations. We show that a distilled dataset, occupying only 8.88% of the original size, can train SR models that retain nearly the same reconstruction fidelity as those trained on full datasets. Furthermore, we analyze how initialization strategies and distillation objectives affect efficiency, convergence, and visual quality. Our findings highlight the feasibility of SR dataset distillation and establish foundational insights for memory- and compute-efficient generative restoration models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI and Empirical Software Engineering: A Paradigm Shift</title>
<link>https://arxiv.org/abs/2502.08108</link>
<guid>https://arxiv.org/abs/2502.08108</guid>
<content:encoded><![CDATA[

arXiv:2502.08108v2 Announce Type: replace-cross 
Abstract: The adoption of large language models (LLMs) and autonomous agents in software engineering marks an enduring paradigm shift. These systems create new opportunities for tool design, workflow orchestration, and empirical observation, while fundamentally reshaping the roles of developers and the artifacts they produce. Although traditional empirical methods remain central to software engineering research, the rapid evolution of AI introduces new data modalities, alters causal assumptions, and challenges foundational constructs such as "developer", "artifact", and "interaction". As humans and AI agents increasingly co-create, the boundaries between social and technical actors blur, and the reproducibility of findings becomes contingent on model updates and prompt contexts. This vision paper examines how the integration of LLMs into software engineering disrupts established research paradigms. We discuss how it transforms the phenomena we study, the methods and theories we rely on, the data we analyze, and the threats to validity that arise in dynamic AI-mediated environments. Our aim is to help the empirical software engineering community adapt its questions, instruments, and validation standards to a future in which AI systems are not merely tools, but active collaborators shaping software engineering and its study.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2502.11090</link>
<guid>https://arxiv.org/abs/2502.11090</guid>
<content:encoded><![CDATA[

arXiv:2502.11090v3 Announce Type: replace-cross 
Abstract: With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11559</link>
<guid>https://arxiv.org/abs/2502.11559</guid>
<content:encoded><![CDATA[

arXiv:2502.11559v3 Announce Type: replace-cross 
Abstract: Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\textbf{FaIRMaker}$, an automated and model-independent framework that employs an $\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that FaIRMaker automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing</title>
<link>https://arxiv.org/abs/2502.14068</link>
<guid>https://arxiv.org/abs/2502.14068</guid>
<content:encoded><![CDATA[

arXiv:2502.14068v2 Announce Type: replace-cross 
Abstract: A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at https://github.com/ghosh64/RaceGAN.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THFlow: A Temporally Hierarchical Flow Matching Framework for 3D Peptide Design</title>
<link>https://arxiv.org/abs/2502.15855</link>
<guid>https://arxiv.org/abs/2502.15855</guid>
<content:encoded><![CDATA[

arXiv:2502.15855v3 Announce Type: replace-cross 
Abstract: Deep generative models provide a promising approach to de novo 3D peptide design. Most of them jointly model the distributions of peptide's position, orientation, and conformation, attempting to simultaneously converge to the target pocket. However, in the early stage of docking, optimizing conformation-only modalities such as rotation and torsion can be physically meaningless, as the peptide is initialized far from the protein pocket and no interaction field is present. We define this problem as the multimodal temporal inconsistency problem and claim it is a key factor contributing to low binding affinity in generated peptides. To address this challenge, we propose THFlow, a novel flow matching-based multimodal generative model that explicitly models the temporal hierarchy between peptide position and conformation. It employs a polynomial based conditional flow to accelerate positional convergence early on, and later aligns it with rotation and torsion for coordinated conformation refinement under the emerging interaction field. Additionally, we incorporate interaction-related features, such as polarity, to further enhance the model's understanding of peptide-protein binding. Extensive experiments demonstrate that THFlow outperforms existing methods in generating peptides with superior stability, affinity, and diversity, offering an effective and accurate solution for advancing peptide-based therapeutic development.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool and Tutor? Experimental evidence from AI deployment in cancer diagnosis</title>
<link>https://arxiv.org/abs/2502.16411</link>
<guid>https://arxiv.org/abs/2502.16411</guid>
<content:encoded><![CDATA[

arXiv:2502.16411v4 Announce Type: replace-cross 
Abstract: Numerous countries globally face shortages of medical experts, deepening inequalities in access to healthcare. Artificial Intelligence (AI)-based diagnostic tools hold considerable promise to tackle this challenge by enabling even novices to deliver expert-level medical services. However, reliance on AI for task completion may hinder the learning required for novices to develop expertise. We thus explore whether AI-based diagnostic tools can be used to enhance not only performance but also learning in the context of lung cancer diagnosis. We examine the distinct effects of AI input during training (i.e., learning how to diagnose) versus in practice (i.e., completing diagnostic tasks) on novice medical professionals' performance. In two field experiments, 576 medical students were randomly assigned across conditions, manipulating the access to AI input during their training, during a test of their diagnostic capabilities, or both. During practice, participants diagnosed potential lung cancer cases using chest CT scans, and their diagnoses were evaluated against the ground truth obtained through histopathological examinations. Study 1 (N = 336) revealed that AI input in training alone improved human diagnostic accuracy by 3.2 percentage points over the control, while AI input during practice alone increased human accuracy by 7.9 percentage points. Combined deployment in both training and practice yielded an improvement of 13.7 percentage points--significantly exceeding either approach alone. Study 2 (N = 240) showed that AI input in practice alone improved accuracy in subsequent practice, unaided by AI, by 9.9 percentage points over the control. Even minimally informative AI input in training improved diagnostic accuracy by 5.3 percentage points over the control. These results reveal AI's dual role: As a tool, it could rapidly improve novices' performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving</title>
<link>https://arxiv.org/abs/2502.16589</link>
<guid>https://arxiv.org/abs/2502.16589</guid>
<content:encoded><![CDATA[

arXiv:2502.16589v3 Announce Type: replace-cross 
Abstract: Vehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding</title>
<link>https://arxiv.org/abs/2503.01422</link>
<guid>https://arxiv.org/abs/2503.01422</guid>
<content:encoded><![CDATA[

arXiv:2503.01422v3 Announce Type: replace-cross 
Abstract: Test-time scaling enhances large language model performance by allocating additional compute resources during inference. Best-of-N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost-performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling: (1) Generating N full samples consumes substantial GPU memory, reducing inference capacity under limited resources. (2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs. Although some studies have explored efficiency improvements, none have addressed both challenges at once. To address this gap, we propose Self-Truncation Best-of-N (ST-BoN), a decoding method that avoids fully generating all N samples and eliminates the need for reward models. It leverages early sampling consistency in the model's internal states to identify the most promising path and truncate suboptimal ones. In terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%. In terms of cost-performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%-80%, and under the same cost, it can improve accuracy by 3-4 points.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Hallucinations in Foundation Models and Their Impact on Healthcare</title>
<link>https://arxiv.org/abs/2503.05777</link>
<guid>https://arxiv.org/abs/2503.05777</guid>
<content:encoded><![CDATA[

arXiv:2503.05777v2 Announce Type: replace-cross 
Abstract: Hallucinations in foundation models arise from autoregressive training objectives that prioritize token-likelihood optimization over epistemic accuracy, fostering overconfidence and poorly calibrated uncertainty. We define medical hallucination as any model-generated output that is factually incorrect, logically inconsistent, or unsupported by authoritative clinical evidence in ways that could alter clinical decisions. We evaluated 11 foundation models (7 general-purpose, 4 medical-specialized) across seven medical hallucination tasks spanning medical reasoning and biomedical information retrieval. General-purpose models achieved significantly higher proportions of hallucination-free responses than medical-specialized models (median: 76.6% vs 51.3%, difference = 25.2%, 95% CI: 18.7-31.3%, Mann-Whitney U = 27.0, p = 0.012, rank-biserial r = -0.64). Top-performing models such as Gemini-2.5 Pro exceeded 97% accuracy when augmented with chain-of-thought prompting (base: 87.6%), while medical-specialized models like MedGemma ranged from 28.6-61.9% despite explicit training on medical corpora. Chain-of-thought reasoning significantly reduced hallucinations in 86.4% of tested comparisons after FDR correction (q < 0.05), demonstrating that explicit reasoning traces enable self-verification and error detection. Physician audits confirmed that 64-72% of residual hallucinations stemmed from causal or temporal reasoning failures rather than knowledge gaps. A global survey of clinicians (n = 70) validated real-world impact: 91.8% had encountered medical hallucinations, and 84.7% considered them capable of causing patient harm. The underperformance of medical-specialized models despite domain training indicates that safety emerges from sophisticated reasoning capabilities and broad knowledge integration developed during large-scale pre-training, not from narrow optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Woman with a Knife or A Knife with a Woman? Measuring Directional Bias Amplification in Image Captions</title>
<link>https://arxiv.org/abs/2503.07878</link>
<guid>https://arxiv.org/abs/2503.07878</guid>
<content:encoded><![CDATA[

arXiv:2503.07878v4 Announce Type: replace-cross 
Abstract: When we train models on biased datasets, they not only reproduce data biases, but can worsen them at test time - a phenomenon called bias amplification. Many of the current bias amplification metrics (e.g., BA (MALS), DPA) measure bias amplification only in classification datasets. These metrics are ineffective for image captioning datasets, as they cannot capture the language semantics of a caption. Recent work introduced Leakage in Captioning (LIC), a language-aware bias amplification metric that understands caption semantics. However, LIC has a crucial limitation: it cannot identify the source of bias amplification in captioning models. We propose Directional Bias Amplification in Captioning (DBAC), a language-aware and directional metric that can identify when captioning models amplify biases. DBAC has two more improvements over LIC: (1) it is less sensitive to sentence encoders (a hyperparameter in language-aware metrics), and (2) it provides a more accurate estimate of bias amplification in captions. Our experiments on gender and race attributes in the COCO captions dataset show that DBAC is the only reliable metric to measure bias amplification in captions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoBlind: Towards Egocentric Visual Assistance for the Blind</title>
<link>https://arxiv.org/abs/2503.08221</link>
<guid>https://arxiv.org/abs/2503.08221</guid>
<content:encoded><![CDATA[

arXiv:2503.08221v4 Announce Type: replace-cross 
Abstract: We present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,392 first-person videos from the daily lives of blind and visually impaired individuals. It also features 5,311 questions directly posed or verified by the blind to reflect their in-situation needs for visual assistance. Each question has an average of 3 manually annotated reference answers to reduce subjectiveness. Using EgoBlind, we comprehensively evaluate 16 advanced MLLMs and find that all models struggle. The best performers achieve an accuracy near 60\%, which is far behind human performance of 87.4\%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and explore heuristic solutions for improvement. With these efforts, we hope that EgoBlind will serve as a foundation for developing effective AI assistants to enhance the independence of the blind and visually impaired. Data and code are available at https://github.com/doc-doc/EgoBlind.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Endogenous Data Drift in Adaptive Models with Recourse-Seeking Users</title>
<link>https://arxiv.org/abs/2503.09658</link>
<guid>https://arxiv.org/abs/2503.09658</guid>
<content:encoded><![CDATA[

arXiv:2503.09658v2 Announce Type: replace-cross 
Abstract: Deep learning models are widely used in decision-making and recommendation systems, where they typically rely on the assumption of a static data distribution between training and deployment. However, real-world deployment environments often violate this assumption. Users who receive negative outcomes may adapt their features to meet model criteria, i.e., recourse action. These adaptive behaviors create shifts in the data distribution and when models are retrained on this shifted data, a feedback loop emerges: user behavior influences the model, and the updated model in turn reshapes future user behavior. Despite its importance, this bidirectional interaction between users and models has received limited attention. In this work, we develop a general framework to model user strategic behaviors and their interactions with decision-making systems under resource constraints and competitive dynamics. Both the theoretical and empirical analyses show that user recourse behavior tends to push logistic and MLP models toward increasingly higher decision standards, resulting in higher recourse costs and less reliable recourse actions over time. To mitigate these challenges, we propose two methods--Fair-top-k and Dynamic Continual Learning (DCL)--which significantly reduce recourse cost and improve model robustness. Our findings draw connections to economic theories, highlighting how algorithmic decision-making can unintentionally reinforce a higher standard and generate endogenous barriers to entry.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocDiff: Identifying Locations on Earth by Diffusing in the Hilbert Space</title>
<link>https://arxiv.org/abs/2503.18142</link>
<guid>https://arxiv.org/abs/2503.18142</guid>
<content:encoded><![CDATA[

arXiv:2503.18142v2 Announce Type: replace-cross 
Abstract: Image geolocalization is a fundamental yet challenging task, aiming at inferring the geolocation on Earth where an image is taken. State-of-the-art methods employ either grid-based classification or gallery-based image-location retrieval, whose spatial generalizability significantly suffers if the spatial distribution of test images does not align with the choices of grids and galleries. Recently emerging generative approaches, while getting rid of grids and galleries, use raw geographical coordinates and suffer quality losses due to their lack of multi-scale information. To address these limitations, we propose a multi-scale latent diffusion model called LocDiff for image geolocalization. We developed a novel positional encoding-decoding framework called Spherical Harmonics Dirac Delta (SHDD) Representations, which encodes points on a spherical surface (e.g., geolocations on Earth) into a Hilbert space of Spherical Harmonics coefficients and decodes points (geolocations) by mode-seeking on spherical probability distributions. We also propose a novel SirenNet-based architecture (CS-UNet) to learn an image-based conditional backward process in the latent SHDD space by minimizing a latent KL-divergence loss. To the best of our knowledge, LocDiff is the first image geolocalization model that performs latent diffusion in a multi-scale location encoding space and generates geolocations under the guidance of images. Experimental results show that LocDiff can outperform all state-of-the-art grid-based, retrieval-based, and diffusion-based baselines across 5 challenging global-scale image geolocalization datasets, and demonstrates significantly stronger generalizability to unseen geolocations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</title>
<link>https://arxiv.org/abs/2503.20685</link>
<guid>https://arxiv.org/abs/2503.20685</guid>
<content:encoded><![CDATA[

arXiv:2503.20685v4 Announce Type: replace-cross 
Abstract: Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[

arXiv:2504.00050v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly adopted as evaluators, offering a scalable alternative to human annotation. However, existing supervised fine-tuning (SFT) approaches often fall short in domains that demand complex reasoning. Judgment is inherently reasoning-intensive: beyond surface-level scoring, it requires verifying evidence, identifying errors, and justifying decisions. Through the analysis of evaluation tasks, we find a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples, revealing the limits of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs, trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards to activate reasoning capabilities. JudgeLRM consistently outperform SFT-tuned baselines in the same size, as well as other RL and SFT variants, and even surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds GPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score, with particularly strong gains on reasoning-heavy tasks. Our findings underscore the value of RL in unlocking reasoning-aligned LLM judges.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation</title>
<link>https://arxiv.org/abs/2504.03546</link>
<guid>https://arxiv.org/abs/2504.03546</guid>
<content:encoded><![CDATA[

arXiv:2504.03546v2 Announce Type: replace-cross 
Abstract: Multilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified/Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the field's history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMol: A Highly Efficient and Chemical Motif-Preserving Molecule Generation Platform</title>
<link>https://arxiv.org/abs/2504.06312</link>
<guid>https://arxiv.org/abs/2504.06312</guid>
<content:encoded><![CDATA[

arXiv:2504.06312v3 Announce Type: replace-cross 
Abstract: We introduce a new graph diffusion model for small molecule generation, DMol, which outperforms the state-of-the-art DiGress model in terms of validity by roughly 1.5% across all benchmarking datasets while reducing the number of diffusion steps by at least 10-fold, and the running time to roughly one half. The performance improvements are a result of a careful change in the objective function and a graph noise scheduling approach which, at each diffusion step, allows one to only change a subset of nodes of varying size in the molecule graph. Another relevant property of the method is that it can be easily combined with junction-tree-like graph representations that arise by compressing a collection of relevant ring structures into supernodes. Unlike classical junction-tree techniques that involve VAEs and require complicated reconstruction steps, compressed DMol directly performs graph diffusion on a graph that compresses only a carefully selected set of frequent carbon rings into supernodes, which results in straightforward sample generation. This compressed DMol method offers additional validity improvements over generic DMol of roughly 2%, increases the novelty of the method, and further improves the running time due to reductions in the graph size.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy AI Must Account for Interactions</title>
<link>https://arxiv.org/abs/2504.07170</link>
<guid>https://arxiv.org/abs/2504.07170</guid>
<content:encoded><![CDATA[

arXiv:2504.07170v2 Announce Type: replace-cross 
Abstract: Trustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. Ultimately the goal of Trustworthy AI research is to achieve all aspects simultaneously. However, efforts to enhance one aspect often introduce unintended trade-offs that negatively impact others. In this position paper, we review notable approaches to these five aspects and systematically consider every pair, detailing the negative interactions that can arise. For example, applying differential privacy to model training can amplify biases, undermining fairness. Drawing on these findings, we take the position that current research practices of improving one or two aspects in isolation are insufficient. Instead, research on Trustworthy AI must account for interactions between aspects and adopt a holistic view across all relevant axes at once. To illustrate our perspective, we provide guidance on how practitioners can work towards integrated trust, examples of how interactions affect the financial industry, and alternative views.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment</title>
<link>https://arxiv.org/abs/2504.09802</link>
<guid>https://arxiv.org/abs/2504.09802</guid>
<content:encoded><![CDATA[

arXiv:2504.09802v2 Announce Type: replace-cross 
Abstract: The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need for training effective small reasoning models. A critical challenge is that small models possess different reasoning capacities and cognitive trajectories compared with their larger counterparts. Hence, directly distilling chain-of-thought (CoT) rationales from large LRMs to smaller ones can sometimes be ineffective and often requires a substantial amount of annotated data. In this paper, we first introduce a novel Critique-Rethink-Verify (CRV) system, designed for training smaller yet powerful LRMs. Our CRV system consists of multiple LLM agents, each specializing in unique tasks: (i) critiquing the CoT rationales according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. Building on the CRV system, we further propose the Cognitive Preference Optimization (CogPO) algorithm to continuously enhance the reasoning abilities of smaller models by aligning their reasoning processes with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of our CRV+CogPO framework, which outperforms other methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARFT: Multi-Agent Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16129</link>
<guid>https://arxiv.org/abs/2504.16129</guid>
<content:encoded><![CDATA[

arXiv:2504.16129v4 Announce Type: replace-cross 
Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks, from generating high-quality presentation slides to even conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methods to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a brand-new MG called Flex-MG, which aligns with the LaMAS optimization in real-world applications and a universal algorithmic framework tailored specifically for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We review the evolution from RL to RFT, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a LaMAS-oriented formulation of RFT. Central to this work is a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work serves as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access</title>
<link>https://arxiv.org/abs/2504.16795</link>
<guid>https://arxiv.org/abs/2504.16795</guid>
<content:encoded><![CDATA[

arXiv:2504.16795v2 Announce Type: replace-cross 
Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose Hierarchical Sparse Attention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selects the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HCT-QA: A Benchmark for Question Answering on Human-Centric Tables</title>
<link>https://arxiv.org/abs/2504.20047</link>
<guid>https://arxiv.org/abs/2504.20047</guid>
<content:encoded><![CDATA[

arXiv:2504.20047v2 Announce Type: replace-cross 
Abstract: Tabular data embedded within PDF files, web pages, and other document formats are prevalent across numerous sectors such as government, engineering, science, and business. These human-centric tables (HCTs) possess a unique combination of high business value, intricate layouts, limited operational power at scale, and sometimes serve as the only data source for critical insights. However, their complexity poses significant challenges to traditional data extraction, processing, and querying methods. While current solutions focus on transforming these tables into relational formats for SQL queries, they fall short in handling the diverse and complex layouts of HCTs and hence being amenable to querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural language queries, and related answers on thousands of tables. Our dataset includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables with 67.5K QA pairs. While HCTs can be potentially processed by different type of query engines, in this paper, we focus on Large Language Models as potential engines and assess their ability in processing and querying such tables.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung Nodule Malignancy Prediction</title>
<link>https://arxiv.org/abs/2504.21344</link>
<guid>https://arxiv.org/abs/2504.21344</guid>
<content:encoded><![CDATA[

arXiv:2504.21344v3 Announce Type: replace-cross 
Abstract: Machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, guiding the model to learn clinically relevant, robust, and explainable imaging features for predicting lung cancer. We obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST) with 1,261 nodules and semantic features. Additionally, the Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a pretrained Contrastive Language-Image Pretraining (CLIP) model with a parameter-efficient fine-tuning approach to align imaging and semantic text features and predict the one-year lung cancer diagnosis. Our model outperformed state-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and AUPRC of 0.776. It also showed robust results in external datasets. Using CLIP, we also obtained predictions on semantic features through zero-shot inference, such as nodule margin (AUROC: 0.807), nodule consistency (0.812), and pleural attachment (0.840). Our approach surpasses the SOTA models in predicting lung cancer across datasets collected from diverse clinical settings, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings. The code is available at https://github.com/luotingzhuang/CLIP_nodule.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2</title>
<link>https://arxiv.org/abs/2505.01854</link>
<guid>https://arxiv.org/abs/2505.01854</guid>
<content:encoded><![CDATA[

arXiv:2505.01854v2 Announce Type: replace-cross 
Abstract: Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on four public datasets covering organs, bones, and muscles across MRI, CT, and ultrasound videos. We show that the proposed method markedly outperforms the default SAM 2, achieving an average Dice Similarity Coefficient improvement of 0.14 and 0.10 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, reducing the time required to correct propagated masks by 60.575% per volume compared to SAM 2, making a notable step toward more accurate automated annotation of medical images for segmentation model development.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Assisted LLM for Personalized Recommendation System</title>
<link>https://arxiv.org/abs/2505.03824</link>
<guid>https://arxiv.org/abs/2505.03824</guid>
<content:encoded><![CDATA[

arXiv:2505.03824v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated significant potential in solving recommendation tasks. With proven capabilities in understanding user preferences, LLM personalization has emerged as a critical area for providing tailored responses to individuals. Current studies explore personalization through prompt design and fine-tuning, paving the way for further research in personalized LLMs. However, existing approaches are either costly and inefficient in capturing diverse user preferences or fail to account for timely updates to user history. To address these gaps, we propose the Memory-Assisted Personalized LLM (MAP). Through user interactions, we first create a history profile for each user, capturing their preferences, such as ratings for historical items. During recommendation, we extract relevant memory based on similarity, which is then incorporated into the prompts to enhance personalized recommendations. In our experiments, we define a new task that enables testing with varying memory size under two scenarios: single domain where memory and tasks are from the same category and cross-domain (e.g. memory from movies and recommendation tasks in books). The results show that MAP outperforms regular LLM-based recommenders that integrate user history directly through prompt design. Moreover, as user history grows, MAP's advantage increases in both scenarios, making it more suitable for addressing successive personalized user requests.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions</title>
<link>https://arxiv.org/abs/2505.06111</link>
<guid>https://arxiv.org/abs/2505.06111</guid>
<content:encoded><![CDATA[

arXiv:2505.06111v3 Announce Type: replace-cross 
Abstract: A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Simplification Algorithms for Interpretability of Time Series Classification</title>
<link>https://arxiv.org/abs/2505.08846</link>
<guid>https://arxiv.org/abs/2505.08846</guid>
<content:encoded><![CDATA[

arXiv:2505.08846v2 Announce Type: replace-cross 
Abstract: In this work, we introduce metrics to evaluate the use of simplified time series in the context of interpretability of a TSC -- a Time Series Classifier. Such simplifications are important because time series data, in contrast to text and image data, are not intuitively under- standable to humans. These metrics are related to the complexity of the simplifications -- how many segments they contain -- and to their loyalty -- how likely they are to maintain the classification of the original time series. We focus on simplifications that select a subset of the original data points, and show that these typically have high Shapley value, thereby aiding interpretability. We employ these metrics to experimentally evaluate four distinct simplification algorithms, across several TSC algorithms and across datasets of varying characteristics, from seasonal or stationary to short or long. We subsequently perform a human-grounded evaluation with forward simulation, that confirms also the practical utility of the introduced metrics to evaluate the use of simplifications in the context of interpretability of TSC. Our findings are summarized in a framework for deciding, for a given TSC, if the various simplifications are likely to aid in its interpretability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Repetition-Invariant Representations for Polymer Informatics</title>
<link>https://arxiv.org/abs/2505.10726</link>
<guid>https://arxiv.org/abs/2505.10726</guid>
<content:encoded><![CDATA[

arXiv:2505.10726v2 Announce Type: replace-cross 
Abstract: Polymers are large macromolecules composed of repeating structural units known as monomers and are widely applied in fields such as energy storage, construction, medicine, and aerospace. However, existing graph neural network methods, though effective for small molecules, only model the single unit of polymers and fail to produce consistent vector representations for the true polymer structure with varying numbers of units. To address this challenge, we introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer representations that are invariant to the number of repeating units in their graph representations. GRIN integrates a graph-based maximum spanning tree alignment with repeat-unit augmentation to ensure structural consistency. We provide theoretical guarantees for repetition-invariance from both model and data perspectives, demonstrating that three repeating units are the minimal augmentation required for optimal invariant representation learning. GRIN outperforms state-of-the-art baselines on both homopolymer and copolymer benchmarks, learning stable, repetition-invariant representations that generalize effectively to polymer chains of unseen sizes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New Encoders for German Trained from Scratch: Comparing ModernGBERT with Converted LLM2Vec Models</title>
<link>https://arxiv.org/abs/2505.13136</link>
<guid>https://arxiv.org/abs/2505.13136</guid>
<content:encoded><![CDATA[

arXiv:2505.13136v2 Announce Type: replace-cross 
Abstract: Encoders remain essential for efficient German NLP and NLU scenarios despite the rise of decoder-only LLMs. This work studies two routes to high-quality German encoders under identical data and training constraints: 1) training from scratch and 2) converting decoders via LLM2Vec. We introduce two resources: ModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT style, and LL\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions trained with masked next-token prediction, both undergoing a context extension to 8.192 tokens.
  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808), surpassing GBERT Large (+4%) and the seven-times larger converted 7B model (0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551) approaches the converted 7B model (0.557).
  We release all models, checkpoints, datasets, and full training records, and introduce an encoder-adapted QA-NIAH evaluation. All in all, our results provide actionable guidance: when parameter efficiency and latency matter, from-scratch encoders dominate. When a pre-trained decoder exists and compute is a limited, conversion offers an effective alternative. ModernGBERT and LL\"aMmleinVec, including all code, data and intermediary checkpoints are published under a research-only RAIL license.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-head Temporal Latent Attention</title>
<link>https://arxiv.org/abs/2505.13544</link>
<guid>https://arxiv.org/abs/2505.13544</guid>
<content:encoded><![CDATA[

arXiv:2505.13544v3 Announce Type: replace-cross 
Abstract: While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally</title>
<link>https://arxiv.org/abs/2505.17048</link>
<guid>https://arxiv.org/abs/2505.17048</guid>
<content:encoded><![CDATA[

arXiv:2505.17048v2 Announce Type: replace-cross 
Abstract: Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle "the whole is greater than the sum of its parts." Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[

arXiv:2505.17050v2 Announce Type: replace-cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2505.17103</link>
<guid>https://arxiv.org/abs/2505.17103</guid>
<content:encoded><![CDATA[

arXiv:2505.17103v2 Announce Type: replace-cross 
Abstract: SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. The model is open-sourced at https://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding</title>
<link>https://arxiv.org/abs/2505.18079</link>
<guid>https://arxiv.org/abs/2505.18079</guid>
<content:encoded><![CDATA[

arXiv:2505.18079v4 Announce Type: replace-cross 
Abstract: Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery (DVD) agent to leverage an agentic search strategy over segmented video clips. Unlike previous video agents that rely on predefined workflows applied uniformly across different queries, our approach emphasizes the autonomous and adaptive nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools to orchestrate adaptive workflow for different queries in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates our advantage. Our DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of 74.2%, which substantially surpasses all prior works, and further improves to 76.0% with transcripts. The code has been released at https://github.com/microsoft/DeepVideoDiscovery.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models</title>
<link>https://arxiv.org/abs/2505.18230</link>
<guid>https://arxiv.org/abs/2505.18230</guid>
<content:encoded><![CDATA[

arXiv:2505.18230v3 Announce Type: replace-cross 
Abstract: What is the shortest path between two data points lying in a high-dimensional space? While the answer is trivial in Euclidean geometry, it becomes significantly more complex when the data lies on a curved manifold -- requiring a Riemannian metric to describe the space's local curvature. Estimating such a metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly from pretrained Energy-Based Models (EBMs) -- a class of generative models that assign low energy to high-density regions. These metrics define spatially varying distances, enabling the computation of geodesics -- shortest paths that follow the data manifold's intrinsic geometry. We introduce two novel metrics derived from EBMs and show that they produce geodesics that remain closer to the data manifold and exhibit lower curvature distortion, as measured by alignment with ground-truth trajectories. We evaluate our approach on increasingly complex datasets: synthetic datasets with known data density, rotated character images with interpretable geometry, and high-resolution natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established baselines, especially in high-dimensional settings. Our work is the first to derive Riemannian metrics from EBMs, enabling data-aware geodesics and unlocking scalable, geometry-driven learning for generative modeling and simulation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators</title>
<link>https://arxiv.org/abs/2505.18574</link>
<guid>https://arxiv.org/abs/2505.18574</guid>
<content:encoded><![CDATA[

arXiv:2505.18574v4 Announce Type: replace-cross 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the limits of strong membership inference attacks on large language models</title>
<link>https://arxiv.org/abs/2505.18773</link>
<guid>https://arxiv.org/abs/2505.18773</guid>
<content:encoded><![CDATA[

arXiv:2505.18773v2 Announce Type: replace-cross 
Abstract: State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training references (e.g., fine-tuning attacks), or on stronger attacks applied to small models and datasets. However, weaker attacks have been shown to be brittle and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges prompt an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA--one of the strongest MIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training references on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in four key ways. While (1) strong MIAs can succeed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g., AUC<0.7) in practical settings. (3) Even when strong MIAs achieve better-than-random AUC, aggregate metrics can conceal substantial per-sample MIA decision instability: due to training randomness, many decisions are so unstable that they are statistically indistinguishable from a coin flip. Finally, (4) the relationship between MIA success and related LLM privacy metrics is not as straightforward as prior work has suggested.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models</title>
<link>https://arxiv.org/abs/2505.18901</link>
<guid>https://arxiv.org/abs/2505.18901</guid>
<content:encoded><![CDATA[

arXiv:2505.18901v2 Announce Type: replace-cross 
Abstract: The rapid advancement of generative AI has provided users with a wide range of well-trained models to address diverse prompts. When selecting a model for a given prompt, users should weigh not only its performance but also its service cost. However, existing model-selection methods typically emphasize performance while overlooking cost differences. In this paper, we introduce PromptWise, an online learning framework that assigns prompts to generative models in a cost-aware manner. PromptWise estimates prompt-model compatibility to select the least expensive model expected to deliver satisfactory outputs. Unlike standard contextual bandits that make a one-shot decision per prompt, PromptWise employs a cost-aware bandit structure that allows sequential model assignments per prompt to reduce total service cost. Through numerical experiments on tasks such as code generation and translation, we demonstrate that PromptWise can achieve performance comparable to baseline selection methods while incurring substantially lower costs. The code is available at: github.com/yannxiaoyanhu/PromptWise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization</title>
<link>https://arxiv.org/abs/2505.19679</link>
<guid>https://arxiv.org/abs/2505.19679</guid>
<content:encoded><![CDATA[

arXiv:2505.19679v2 Announce Type: replace-cross 
Abstract: This paper presents KIT's submissions to the IWSLT 2025 low-resource track. We develop both cascaded systems, consisting of Automatic Speech Recognition (ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech Translation (ST) systems for three language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we fine-tune our systems with different strategies to utilize resources efficiently. This study further explores system enhancement with synthetic data and model regularization. Specifically, we investigate MT-augmented ST by generating translations from ASR data using MT models. For North Levantine, which lacks parallel ST training data, a system trained solely on synthetic data slightly surpasses the cascaded system trained on real data. We also explore augmentation using text-to-speech models by generating synthetic speech from MT data, demonstrating the benefits of synthetic data in improving both ASR and ST performance for Bemba. Additionally, we apply intra-distillation to enhance model performance. Our experiments show that this approach consistently improves results across ASR, MT, and ST tasks, as well as across different pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine the cascaded and end-to-end systems, achieving an improvement of approximately 1.5 BLEU points.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Hidden Capacity of LLMs for One-Step Text Generation</title>
<link>https://arxiv.org/abs/2505.21189</link>
<guid>https://arxiv.org/abs/2505.21189</guid>
<content:encoded><![CDATA[

arXiv:2505.21189v2 Announce Type: replace-cross 
Abstract: A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one trained input embedding. In this work, we explore whether autoregressive decoding is essential for such reconstruction. We show that frozen LLMs can generate hundreds of accurate tokens in just one token-parallel forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored multi-token generation capability of autoregressive LLMs. We examine these embeddings and characterize the information they encode. We also empirically show that, although these representations are not unique for a given text, they form connected and local regions in embedding space - suggesting the potential to train a practical encoder. The existence of such representations hints that multi-token generation may be natively accessible in off-the-shelf LLMs via a learned input encoder, eliminating heavy retraining and helping to overcome the fundamental bottleneck of autoregressive decoding while reusing already-trained models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[

arXiv:2505.21236v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Optimized Text-to-Image Pipeline Design</title>
<link>https://arxiv.org/abs/2505.21478</link>
<guid>https://arxiv.org/abs/2505.21478</guid>
<content:encoded><![CDATA[

arXiv:2505.21478v2 Announce Type: replace-cross 
Abstract: Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Knowledge Graph-Guided Multimodal Synthesis</title>
<link>https://arxiv.org/abs/2505.22633</link>
<guid>https://arxiv.org/abs/2505.22633</guid>
<content:encoded><![CDATA[

arXiv:2505.22633v2 Announce Type: replace-cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at https://github.com/zjunlp/Knowledge2Data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge</title>
<link>https://arxiv.org/abs/2505.22767</link>
<guid>https://arxiv.org/abs/2505.22767</guid>
<content:encoded><![CDATA[

arXiv:2505.22767v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can be understood as Collective Knowledge (CK): a condensation of human cultural and technical output, whose apparent intelligence emerges in dialogue. This perspective article, drawing on extended interaction with ChatGPT-4, postulates differential response modes that plausibly trace their origin to distinct model subnetworks. It argues that CK has no persistent internal state or ``spine'': it drifts, it complies, and its behaviour is shaped by the user and by fine-tuning. It develops the notion of co-augmentation, in which human judgement and CK's representational reach jointly produce forms of analysis that neither could generate alone. Finally, it suggests that CK offers a tractable object for neuroscience: unlike biological brains, these systems expose their architecture, training history, and activation dynamics, making the human--CK loop itself an experimental target.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios</title>
<link>https://arxiv.org/abs/2505.23118</link>
<guid>https://arxiv.org/abs/2505.23118</guid>
<content:encoded><![CDATA[

arXiv:2505.23118v3 Announce Type: replace-cross 
Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning across diverse sources of evidence. The recent emergence of multimodal reasoning models has significantly transformed the landscape of solving complex tasks. Although such models have achieved notable success in mathematics and science, their application to medical domains remains underexplored. In this work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that elicits and then enhances multimodal reasoning for medical domains. In Stage-I, we fine-tune models using 2,000 text-only data samples containing precisely orchestrated reasoning demonstrations to elicit reasoning behaviors. In Stage-II, we further enhance the model's reasoning capabilities using 1,500 rigorously curated multimodal medical cases, aligning model reasoning outputs with our proposed multimodal medical reasoning preference. Extensive experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in improving the reasoning performance of medical multimodal models. Notably, models trained with \textit{MedE$^2$} consistently outperform baselines across multiple medical multimodal benchmarks. Additional validation on larger models and under inference-time scaling further confirms the robustness and practical utility of our approach.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models</title>
<link>https://arxiv.org/abs/2505.23945</link>
<guid>https://arxiv.org/abs/2505.23945</guid>
<content:encoded><![CDATA[

arXiv:2505.23945v2 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models</title>
<link>https://arxiv.org/abs/2506.02269</link>
<guid>https://arxiv.org/abs/2506.02269</guid>
<content:encoded><![CDATA[

arXiv:2506.02269v2 Announce Type: replace-cross 
Abstract: Equivariant neural networks have proven to be effective for tasks with known underlying symmetries. However, optimizing equivariant networks can be tricky and best training practices are less established than for standard networks. In particular, recent works have found small training benefits from relaxing equivariance constraints. This raises the question: do equivariance constraints introduce fundamental obstacles to optimization? Or do they simply require different hyperparameter tuning? In this work, we investigate this question through a theoretical analysis of the loss landscape geometry. We focus on networks built using permutation representations, which we can view as a subset of unconstrained MLPs. Importantly, we show that the parameter symmetries of the unconstrained model has nontrivial effects on the loss landscape of the equivariant subspace and under certain conditions can provably prevent learning of the global minima. Further, we empirically demonstrate in such cases, relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly, the weights eventually found via relaxation corresponds to a different choice of group representation in the hidden layer. From this, we draw 3 key takeaways. (1) By viewing the unconstrained version of an architecture, we can uncover hidden parameter symmetries which were broken by choice of constraint enforcement (2) Hidden symmetries give important insights on loss landscapes and can induce critical points and even minima (3) Hidden symmetry induced minima can sometimes be escaped by constraint relaxation and we observe the network jumps to a different choice of constraint enforcement. Effective equivariance relaxation may require rethinking the fixed choice of group representation in the hidden layers.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.06632</link>
<guid>https://arxiv.org/abs/2506.06632</guid>
<content:encoded><![CDATA[

arXiv:2506.06632v2 Announce Type: replace-cross 
Abstract: We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method. Our code can be found on https://github.com/divelab/E2H-Reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2506.07416</link>
<guid>https://arxiv.org/abs/2506.07416</guid>
<content:encoded><![CDATA[

arXiv:2506.07416v2 Announce Type: replace-cross 
Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irrelevant camera views, a token selection module to reduce input sequence length for the LLM, and speculative decoding to accelerate token generation. Evaluation on the NVIDIA DRIVE Thor platform for automonous driving application, our pipeline achieves $2.5\times$ end-to-end latency reduction without compromising task accuracy. The speed-up further increases to $3.2\times$ when applying FP8 post-training quantization. These results demonstrate our pipeline as a viable solution for enabling real-time VLM deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2506.07801</link>
<guid>https://arxiv.org/abs/2506.07801</guid>
<content:encoded><![CDATA[

arXiv:2506.07801v3 Announce Type: replace-cross 
Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a pseudo-label weighting module designed for selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, i.e., MultiMatch achieves state-of-the-art results on 8 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 21 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%, a critical advantage for real-world text classification tasks. Our code is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Contact Health Monitoring During Daily Personal Care Routines</title>
<link>https://arxiv.org/abs/2506.09718</link>
<guid>https://arxiv.org/abs/2506.09718</guid>
<content:encoded><![CDATA[

arXiv:2506.09718v2 Announce Type: replace-cross 
Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring of physiological signals and offers a practical alternative to traditional health sensing methods. Although rPPG is promising for daily health monitoring, its application in long-term personal care scenarios, such as mirror-facing routines in high-altitude environments, remains challenging due to ambient lighting variations, frequent occlusions from hand movements, and dynamic facial postures. To address these challenges, we present LADH (Long-term Altitude Daily Health), the first long-term rPPG dataset containing 240 synchronized RGB and infrared (IR) facial videos from 21 participants across five common personal care scenarios, along with ground-truth PPG, respiration, and blood oxygen signals. Our experiments demonstrate that combining RGB and IR video inputs improves the accuracy and robustness of non-contact physiological monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate estimation. Furthermore, we find that multi-task learning enhances performance across multiple physiological indicators simultaneously. Dataset and code are open at https://github.com/McJackTang/FusionVitals.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConTextTab: A Semantics-Aware Tabular In-Context Learner</title>
<link>https://arxiv.org/abs/2506.10707</link>
<guid>https://arxiv.org/abs/2506.10707</guid>
<content:encoded><![CDATA[

arXiv:2506.10707v4 Announce Type: replace-cross 
Abstract: Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/sap-rpt-1-oss.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where and How to Perturb: On the Design of Perturbation Guidance in Diffusion and Flow Models</title>
<link>https://arxiv.org/abs/2506.10978</link>
<guid>https://arxiv.org/abs/2506.10978</guid>
<content:encoded><![CDATA[

arXiv:2506.10978v4 Announce Type: replace-cross 
Abstract: Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers</title>
<link>https://arxiv.org/abs/2506.14196</link>
<guid>https://arxiv.org/abs/2506.14196</guid>
<content:encoded><![CDATA[

arXiv:2506.14196v2 Announce Type: replace-cross 
Abstract: Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flat Channels to Infinity in Neural Loss Landscapes</title>
<link>https://arxiv.org/abs/2506.14951</link>
<guid>https://arxiv.org/abs/2506.14951</guid>
<content:encoded><![CDATA[

arXiv:2506.14951v2 Announce Type: replace-cross 
Abstract: The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot \mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot \mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-squashing in Spatiotemporal Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.15507</link>
<guid>https://arxiv.org/abs/2506.15507</guid>
<content:encoded><![CDATA[

arXiv:2506.15507v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that, counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPMI: Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases</title>
<link>https://arxiv.org/abs/2506.17336</link>
<guid>https://arxiv.org/abs/2506.17336</guid>
<content:encoded><![CDATA[

arXiv:2506.17336v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Generated Video Detection via Perceptual Straightening</title>
<link>https://arxiv.org/abs/2507.00583</link>
<guid>https://arxiv.org/abs/2507.00583</guid>
<content:encoded><![CDATA[

arXiv:2507.00583v3 Announce Type: replace-cross 
Abstract: The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the "perceptual straightening" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Tuning for In-Context Optimization</title>
<link>https://arxiv.org/abs/2507.04221</link>
<guid>https://arxiv.org/abs/2507.04221</guid>
<content:encoded><![CDATA[

arXiv:2507.04221v2 Announce Type: replace-cross 
Abstract: We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model's inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study</title>
<link>https://arxiv.org/abs/2507.05362</link>
<guid>https://arxiv.org/abs/2507.05362</guid>
<content:encoded><![CDATA[

arXiv:2507.05362v2 Announce Type: replace-cross 
Abstract: Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Collectivist, Economic Perspective on AI</title>
<link>https://arxiv.org/abs/2507.06268</link>
<guid>https://arxiv.org/abs/2507.06268</guid>
<content:encoded><![CDATA[

arXiv:2507.06268v2 Announce Type: replace-cross 
Abstract: Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word "intelligence" is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals and that much of our intelligence is social and cultural in origin. Moreover, failing to properly situate aspects of intelligence at the social level contributes to the treatment of the societal consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts at the level of algorithm design.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training</title>
<link>https://arxiv.org/abs/2507.09846</link>
<guid>https://arxiv.org/abs/2507.09846</guid>
<content:encoded><![CDATA[

arXiv:2507.09846v4 Announce Type: replace-cross 
Abstract: As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the "river" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Retrieval: Multi-Aspect Iterative Search Expansion and Post-Order Search Aggregation for Full Paper Retrieval</title>
<link>https://arxiv.org/abs/2507.10057</link>
<guid>https://arxiv.org/abs/2507.10057</guid>
<content:encoded><![CDATA[

arXiv:2507.10057v2 Announce Type: replace-cross 
Abstract: Scientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused exclusively on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity between them. Yet, abstracts offer only sparse and high-level summaries, and such methods primarily optimize one-to-one similarity, overlooking the dynamic relations that emerge among relevant papers during the retrieval process. To address this, we propose Chain of Retrieval(COR), a novel iterative framework for full-paper retrieval. Specifically, CoR decomposes each query paper into multiple aspect-specific views, matches them against segmented candidate papers, and iteratively expands the search by promoting top-ranked results as new queries, thereby forming a tree-structured retrieval process. The resulting retrieval tree is then aggregated in a post-order manner: descendants are first combined at the query level, then recursively merged with their parent nodes, to capture hierarchical relations across iterations. To validate this, we present SCIFULLBENCH, a large-scale benchmark providing both complete and segmented contexts of full papers for queries and candidates, and results show that CoR significantly outperforms existing retrieval baselines. Our code and dataset is available at https://github.com/psw0021/Chain-of-Retrieval.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding</title>
<link>https://arxiv.org/abs/2507.10449</link>
<guid>https://arxiv.org/abs/2507.10449</guid>
<content:encoded><![CDATA[

arXiv:2507.10449v2 Announce Type: replace-cross 
Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindJourney: Test-Time Scaling with World Models for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2507.12508</link>
<guid>https://arxiv.org/abs/2507.12508</guid>
<content:encoded><![CDATA[

arXiv:2507.12508v2 Announce Type: replace-cross 
Abstract: Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 7.7% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</title>
<link>https://arxiv.org/abs/2507.13370</link>
<guid>https://arxiv.org/abs/2507.13370</guid>
<content:encoded><![CDATA[

arXiv:2507.13370v3 Announce Type: replace-cross 
Abstract: The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Self-Evolving AI Agent System for Climate Science</title>
<link>https://arxiv.org/abs/2507.17311</link>
<guid>https://arxiv.org/abs/2507.17311</guid>
<content:encoded><![CDATA[

arXiv:2507.17311v3 Announce Type: replace-cross 
Abstract: Scientific progress in Earth science depends on integrating data across the planet's interconnected spheres. However, the accelerating volume and fragmentation of multi-sphere knowledge and data have surpassed human analytical capacity. This creates a major bottleneck for discovery, especially in climate science. To address this challenge, we introduce EarthLink, the first self-evolving AI agent system designed as an interactive "copilot" for Earth scientists. Through natural language interaction, EarthLink automates the entire research workflow by integrating planning, code execution, data analysis, and physical reasoning into a unified process that directly addresses this limitation. Beyond efficiency, it exhibits human-like cross-disciplinary analytical ability and achieves proficiency comparable to a junior researcher in expert evaluations on core large-scale climate tasks, including model-observation comparison and climate change understanding. When tasked with an open scientific problem, specifically the discovery of precursors of the Atlantic Ni\~no, EarthLink autonomously developed a research strategy, identified sources of predictability, verified its hypotheses with available data, and proposed a physically consistent mechanism. These emerging capabilities enable a new human-AI research paradigm. Scientists can focus on value and result judgments, while AI systems handle complex data analysis and knowledge integration. This accelerates the pace and breadth of discovery in Earth sciences. The system is accessible at our website https://earthlink.intern-ai.org.cn.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Music Arena: Live Evaluation for Text-to-Music</title>
<link>https://arxiv.org/abs/2507.20900</link>
<guid>https://arxiv.org/abs/2507.20900</guid>
<content:encoded><![CDATA[

arXiv:2507.20900v2 Announce Type: replace-cross 
Abstract: We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains.
  Music Arena is available at: https://music-arena.org . Preference data is available at: https://huggingface.co/music-arena .
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research</title>
<link>https://arxiv.org/abs/2508.01390</link>
<guid>https://arxiv.org/abs/2508.01390</guid>
<content:encoded><![CDATA[

arXiv:2508.01390v2 Announce Type: replace-cross 
Abstract: Online behavioural research faces an emerging threat as participants increasingly turn to large language models (LLMs) for advice, translation, or task delegation: LLM Pollution. We identify three interacting variants through which LLM Pollution threatens the validity and integrity of online behavioural research. First, Partial LLM Mediation occurs when participants make selective use of LLMs for specific aspects of a task, such as translation or wording support, leading researchers to (mis)interpret LLM-shaped outputs as human ones. Second, Full LLM Delegation arises when agentic LLMs complete studies with little to no human oversight, undermining the central premise of human-subject research at a more foundational level. Third, LLM Spillover signifies human participants altering their behaviour as they begin to anticipate LLM presence in online studies, even when none are involved. While Partial Mediation and Full Delegation form a continuum of increasing automation, LLM Spillover reflects second-order reactivity effects. Together, these variants interact and generate cascading distortions that compromise sample authenticity, introduce biases that are difficult to detect post hoc, and ultimately undermine the epistemic grounding of online research on human cognition and behaviour. Crucially, the threat of LLM Pollution is already co-evolving with advances in generative AI, creating an escalating methodological arms race. To address this, we propose a multi-layered response spanning researcher practices, platform accountability, and community efforts. As the challenge evolves, coordinated adaptation will be essential to safeguard methodological integrity and preserve the validity of online behavioural research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation</title>
<link>https://arxiv.org/abs/2508.02451</link>
<guid>https://arxiv.org/abs/2508.02451</guid>
<content:encoded><![CDATA[

arXiv:2508.02451v2 Announce Type: replace-cross 
Abstract: In the context of the booming digital economy, recommendation systems, as a key link connecting users and numerous services, face challenges in modeling user behavior sequences on local-life service platforms, including the sparsity of long sequences and strong spatio-temporal dependence. Such challenges can be addressed by drawing an analogy to the forgetting process in human memory. This is because users' responses to recommended content follow the recency effect and the cyclicality of memory. By exploring this, this paper introduces the forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM) with long sequences for local-life service recommendation. STIM integrates three key components: a dynamic masking module based on the forgetting curve, which is used to extract both recent spatiotemporal features and periodic spatiotemporal features; a query-based mixture of experts (MoE) approach that can adaptively activate expert networks under different dynamic masks, enabling the collaborative modeling of time, location, and items; and a hierarchical multi-interest network unit, which captures multi-interest representations by modeling the hierarchical interactions between the shallow and deep semantics of users' recent behaviors. By introducing the STIM method, we conducted online A/B tests and achieved a 1.54\% improvement in gross transaction volume (GTV). In addition, extended offline experiments also showed improvements. STIM has been deployed in a large-scale local-life service recommendation system, serving hundreds of millions of daily active users in core application scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design</title>
<link>https://arxiv.org/abs/2508.03665</link>
<guid>https://arxiv.org/abs/2508.03665</guid>
<content:encoded><![CDATA[

arXiv:2508.03665v4 Announce Type: replace-cross 
Abstract: Generative models, particularly Large Language Models (LLMs), produce fluent outputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and type-theoretic principles to introduce a contract layer that mediates every LLM call. Contracts stipulate semantic and type requirements on inputs and outputs, coupled with probabilistic remediation to steer generation toward compliance. The layer exposes the dual view of LLMs as semantic parsers and probabilistic black-box components. Contract satisfaction is probabilistic and semantic validation is operationally defined through programmer-specified conditions on well-typed data structures. More broadly, this work postulates that any two agents satisfying the same contracts are \emph{functionally equivalent} with respect to those contracts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Attention Fails: A Taxonomy of Faults in Attention-Based Neural Networks</title>
<link>https://arxiv.org/abs/2508.04925</link>
<guid>https://arxiv.org/abs/2508.04925</guid>
<content:encoded><![CDATA[

arXiv:2508.04925v2 Announce Type: replace-cross 
Abstract: Attention mechanisms are at the core of modern neural architectures, powering systems ranging from ChatGPT to autonomous vehicles and driving a major economic impact. However, high-profile failures, such as ChatGPT's nonsensical outputs or Google's suspension of Gemini's image generation due to attention weight errors, highlight a critical gap: existing deep learning fault taxonomies might not adequately capture the unique failures introduced by attention mechanisms. This gap leaves practitioners without actionable diagnostic guidance. To address this gap, we present the first comprehensive empirical study of faults in attention-based neural networks (ABNNs). Our work is based on a systematic analysis of 555 real-world faults collected from 96 projects across ten frameworks, including GitHub, Hugging Face, and Stack Overflow. Through our analysis, we develop a novel taxonomy comprising seven attention-specific fault categories, not captured by existing work. Our results show that over half of the ABNN faults arise from mechanisms unique to attention architectures. We further analyze the root causes and manifestations of these faults through various symptoms. Finally, by analyzing symptom-root cause associations, we identify four evidence-based diagnostic heuristics that explain 33.0% of attention-specific faults, offering the first systematic diagnostic guidance for attention-based models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identity Increases Stability in Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2508.06389</link>
<guid>https://arxiv.org/abs/2508.06389</guid>
<content:encoded><![CDATA[

arXiv:2508.06389v2 Announce Type: replace-cross 
Abstract: Neural Cellular Automata (NCAs) offer a way to study the growth of two-dimensional artificial organisms from a single seed cell. From the outset, NCA-grown organisms have had issues with stability, their natural boundary often breaking down and exhibiting tumour-like growth or failing to maintain the expected shape. In this paper, we present a method for improving the stability of NCA-grown organisms by introducing an 'identity' layer with simple constraints during training.
  Results show that NCAs grown in close proximity are more stable compared with the original NCA model. Moreover, only a single identity value is required to achieve this increase in stability. We observe emergent movement from the stable organisms, with increasing prevalence for models with multiple identity values.
  This work lays the foundation for further study of the interaction between NCA-grown organisms, paving the way for studying social interaction at a cellular level in artificial organisms.
  Code/Videos available at: https://github.com/jstovold/ALIFE2025
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combinative Matching for Geometric Shape Assembly</title>
<link>https://arxiv.org/abs/2508.09780</link>
<guid>https://arxiv.org/abs/2508.09780</guid>
<content:encoded><![CDATA[

arXiv:2508.09780v2 Announce Type: replace-cross 
Abstract: This paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. In contrast, we explicitly model two distinct properties of interlocking shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art. Project page: https://nahyuklee.github.io/cmnet.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.12815</link>
<guid>https://arxiv.org/abs/2508.12815</guid>
<content:encoded><![CDATA[

arXiv:2508.12815v2 Announce Type: replace-cross 
Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines. Our code is publicly available at https://jayneelparekh.github.io/learn-to-steer/
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title>
<link>https://arxiv.org/abs/2508.14314</link>
<guid>https://arxiv.org/abs/2508.14314</guid>
<content:encoded><![CDATA[

arXiv:2508.14314v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages fine-grained cross-model consistency to detect and mitigate hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\% compared to existing approaches. For mitigation, Finch-Zk achieves up to 9 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation on multiple datasets demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing</title>
<link>https://arxiv.org/abs/2508.18316</link>
<guid>https://arxiv.org/abs/2508.18316</guid>
<content:encoded><![CDATA[

arXiv:2508.18316v2 Announce Type: replace-cross 
Abstract: This study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews</title>
<link>https://arxiv.org/abs/2509.00285</link>
<guid>https://arxiv.org/abs/2509.00285</guid>
<content:encoded><![CDATA[

arXiv:2509.00285v2 Announce Type: replace-cross 
Abstract: We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Focused Video Group Activities Hashing</title>
<link>https://arxiv.org/abs/2509.00490</link>
<guid>https://arxiv.org/abs/2509.00490</guid>
<content:encoded><![CDATA[

arXiv:2509.00490v2 Announce Type: replace-cross 
Abstract: With the explosive growth of video data in various complex scenarios, quickly retrieving group activities has become an urgent problem. However, many tasks can only retrieve videos focusing on an entire video, not the activity granularity. To solve this problem, we propose a new STVH (spatiotemporal interleaved video hashing) technique for the first time. Through a unified framework, the STVH simultaneously models individual object dynamics and group interactions, capturing the spatiotemporal evolution on both group visual features and positional features. Moreover, in real-life video retrieval scenarios, it may sometimes require activity features, while at other times, it may require visual features of objects. We then further propose a novel M-STVH (multi-focused spatiotemporal video hashing) as an enhanced version to handle this difficult task. The advanced method incorporates hierarchical feature integration through multi-focused representation learning, allowing the model to jointly focus on activity semantics features and object visual features. We conducted comparative experiments on publicly available datasets, and both STVH and M-STVH can achieve excellent results.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EndoGMDE: Generalizable Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes</title>
<link>https://arxiv.org/abs/2509.01206</link>
<guid>https://arxiv.org/abs/2509.01206</guid>
<content:encoded><![CDATA[

arXiv:2509.01206v3 Announce Type: replace-cross 
Abstract: Self-supervised monocular depth estimation is a significant task for low-cost and efficient 3D scene perception and measurement in endoscopy. However, the variety of illumination conditions and scene features is still the primary challenges for depth estimation in endoscopic scenes. In this work, a novel self-supervised framework is proposed for monocular depth estimation in diverse endoscopy. Firstly, considering the diverse features in endoscopic scenes with different tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to efficiently finetune the foundation model for endoscopic depth estimation. In the proposed module, based on the input feature, different experts with a small amount of trainable parameters are adaptively selected for weighted inference, from low-rank experts which are allocated based on the generalization of each block. Moreover, a novel self-supervised training framework is proposed to jointly cope with brightness inconsistency and reflectance interference. The proposed method outperforms state-of-the-art works on SCARED dataset and SimCol dataset. Furthermore, the proposed network also achieves the best generalization based on zero-shot depth estimation on C3VD, Hamlyn and SERV-CT dataset. The outstanding performance of our model is further demonstrated with 3D reconstruction and ego-motion estimation. The proposed method could contribute to accurate endoscopy for minimally invasive measurement and surgery. The evaluation codes will be released upon acceptance, while the demo videos can be found on: https://endo-gmde.netlify.app/.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research</title>
<link>https://arxiv.org/abs/2509.06093</link>
<guid>https://arxiv.org/abs/2509.06093</guid>
<content:encoded><![CDATA[

arXiv:2509.06093v2 Announce Type: replace-cross 
Abstract: The preparation procedures of materials are often embedded narratively in experimental protocols, research articles, patents, and laboratory notes, and are structured around procedural sequences, causal relationships, and conditional logic. The synthesis of boron nitride nanosheet (BNNS) polymer composites exemplifies this linguistically encoded decision-making system, where the practical experiments involve interdependent multistage and path-dependent processes such as exfoliation, functionalization, and dispersion, each governed by heterogeneous parameters and contextual contingencies, challenging conventional numerical optimization paradigms for experiment design. We reformulate this challenge into a text-reasoning problem through a framework centered on a text-first, lightly structured materials database and large language models (LLMs) as text reasoning engines. We constructed a database that captures evidence-linked narrative excerpts from the literature while normalizing only the minimum necessary entities, attributes, and relations to enable composite retrieval that unifies semantic matching, lexical cues, and explicit value filters. Building on this language-native, provenance-preserving foundation, the LLM operates in two complementary modes: retrieval-augmented generation (RAG), grounding outputs in retrieved evidence modules from the database, and experience-augmented reasoning (EAR), which leverages iteratively trained text guides derived from multi-source literature-based narrative data as external references to inform reasoning and decision-making. Applying this integration-and-reasoning framework, we demonstrate rapid, laboratory-scale optimization of BNNS preparation, highlighting how language-native data combined with LLM-based reasoning can significantly accelerate practical material preparation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation</title>
<link>https://arxiv.org/abs/2509.11252</link>
<guid>https://arxiv.org/abs/2509.11252</guid>
<content:encoded><![CDATA[

arXiv:2509.11252v2 Announce Type: replace-cross 
Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs mainly employ autoregressive generation, i.e. generating code token-by-token from left to right. However, the underlying autoregressive generation has two limitations in code generation. First, autoregressive LLMs only generate a token at each step, showing low efficiency in practice. Second, programming is a non-sequential process involving back-and-forth editing, while autoregressive LLMs only employ the left-to-right generation order. These two intrinsic limitations hinder the further development of LLMs in code generation. Recently, diffusion LLMs have emerged as a promising alternative. Diffusion LLMs address the above limitations with two advances, including multi-token prediction (i.e. generating multiple tokens at each step) and flexible generation order (i.e. flexibly determining which positions to generate tokens). However, there is no systematic study exploring diffusion LLMs in code generation. To bridge the knowledge gap, we present the first empirical study of diffusion LLMs for code generation. Our study involves 9 representative diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on the results, we summarize the following findings. (1) Existing diffusion LLMs are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs have a stronger length extrapolation ability than autoregressive LLMs and perform better in long code understanding. (3) We explore factors impacting the effectiveness and efficiency of diffusion LLMs, and provide practical guidance. (4) We discuss several promising further directions to improve diffusion LLMs on code generation. We open-source all source code, data, and results to facilitate the following research. The code is publicly available at https://github.com/zhangyitonggg/dllm4code.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data</title>
<link>https://arxiv.org/abs/2509.12143</link>
<guid>https://arxiv.org/abs/2509.12143</guid>
<content:encoded><![CDATA[

arXiv:2509.12143v3 Announce Type: replace-cross 
Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 81.51\% accuracy, 85.94\% sensitivity, 76.36\% specificity, 80.88\% precision, and 83.33\% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL Fine-Tuning Heals OOD Forgetting in SFT</title>
<link>https://arxiv.org/abs/2509.12235</link>
<guid>https://arxiv.org/abs/2509.12235</guid>
<content:encoded><![CDATA[

arXiv:2509.12235v2 Announce Type: replace-cross 
Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has empirically shown better reasoning performance than one-stage SFT for the post-training of Large Language Models (LLMs). However, the evolution and mechanism behind the synergy of SFT and RL are still under-explored and inconclusive. In our study, we find the well-known claim "SFT memorizes, RL generalizes" is over-simplified, and discover that: (1) OOD performance peaks at the early stage of SFT and then declines (OOD forgetting), the best SFT checkpoint cannot be captured by training/test loss; (2) the subsequent RL stage does not generate fundamentally better OOD capability, instead it plays an \textbf{OOD restoration} role, recovering the lost reasoning ability during SFT; (3) The recovery ability has boundaries, \ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the lost OOD ability;} (4) To uncover the underlying mechanisms behind the forgetting and restoration process, we employ SVD analysis on parameter matrices, manually edit them, and observe their impacts on model performance. Unlike the common belief that the shift of model capacity mainly results from the changes of singular values, we find that they are actually quite stable throughout fine-tuning. Instead, the OOD behavior strongly correlates with the \textbf{rotation of singular vectors}. Our findings re-identify the roles of SFT and RL in the two-stage fine-tuning and discover the rotation of singular vectors as the key mechanism. %reversing the rotations induced by SFT, which shows recovery from forgetting, whereas imposing the SFT parameter directions onto a RL-tuned model results in performance degradation. Code is available at https://github.com/xiaodanguoguo/RL_Heals_SFT
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning</title>
<link>https://arxiv.org/abs/2509.13790</link>
<guid>https://arxiv.org/abs/2509.13790</guid>
<content:encoded><![CDATA[

arXiv:2509.13790v2 Announce Type: replace-cross 
Abstract: Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2509.13927</link>
<guid>https://arxiv.org/abs/2509.13927</guid>
<content:encoded><![CDATA[

arXiv:2509.13927v2 Announce Type: replace-cross 
Abstract: Reasoning about spatial audio with large language models requires a spatial audio encoder as an acoustic front-end to obtain audio embeddings for further processing. Such an encoder needs to capture all information required to detect the type of sound events, as well as the direction and distance of their corresponding sources. Accomplishing this with a single audio encoder is demanding as the information required for each of these tasks is mostly independent of each other. As a result, the performance obtained with a single encoder is often worse than when using task-specific audio encoders. In this work, we present DSpAST, a novel audio encoder based on SpatialAST that learns disentangled representations of spatial audio while having only 0.2% additional parameters. Experiments on SpatialSoundQA with the spatial audio reasoning system BAT demonstrate that DSpAST significantly outperforms SpatialAST.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</title>
<link>https://arxiv.org/abs/2509.14622</link>
<guid>https://arxiv.org/abs/2509.14622</guid>
<content:encoded><![CDATA[

arXiv:2509.14622v3 Announce Type: replace-cross 
Abstract: With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses</title>
<link>https://arxiv.org/abs/2509.16093</link>
<guid>https://arxiv.org/abs/2509.16093</guid>
<content:encoded><![CDATA[

arXiv:2509.16093v2 Announce Type: replace-cross 
Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score. We introduce DeCE, a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements. DeCE is model-agnostic and domain-general, requiring no predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate different LLMs on a real-world legal QA task involving multi-jurisdictional reasoning and citation grounding. DeCE achieves substantially stronger correlation with expert judgments ($r=0.78$), compared to traditional metrics ($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Importantly, only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation framework in expert domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity</title>
<link>https://arxiv.org/abs/2509.16527</link>
<guid>https://arxiv.org/abs/2509.16527</guid>
<content:encoded><![CDATA[

arXiv:2509.16527v2 Announce Type: replace-cross 
Abstract: This work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications</title>
<link>https://arxiv.org/abs/2509.18714</link>
<guid>https://arxiv.org/abs/2509.18714</guid>
<content:encoded><![CDATA[

arXiv:2509.18714v3 Announce Type: replace-cross 
Abstract: The bisimulation metric (BSM) is a powerful tool for computing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to multiple-MDP scenarios, such as policy transfer, remains challenging. Prior work has attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis of its mathematical properties has limited further theoretical progress. In this work, we formally establish a generalized bisimulation metric (GBSM) between pairs of MDPs, which is rigorously proven with the three fundamental properties: GBSM symmetry, inter-MDP triangle inequality, and the distance bound on identical state spaces. Leveraging these properties, we theoretically analyse policy transfer, state aggregation, and sampling-based estimation in MDPs, obtaining explicit bounds that are strictly tighter than those derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmbeddingGemma: Powerful and Lightweight Text Representations</title>
<link>https://arxiv.org/abs/2509.20354</link>
<guid>https://arxiv.org/abs/2509.20354</guid>
<content:encoded><![CDATA[

arXiv:2509.20354v3 Announce Type: replace-cross 
Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does FLUX Already Know How to Perform Physically Plausible Image Composition?</title>
<link>https://arxiv.org/abs/2509.21278</link>
<guid>https://arxiv.org/abs/2509.21278</guid>
<content:encoded><![CDATA[

arXiv:2509.21278v3 Announce Type: replace-cross 
Abstract: Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education</title>
<link>https://arxiv.org/abs/2509.21972</link>
<guid>https://arxiv.org/abs/2509.21972</guid>
<content:encoded><![CDATA[

arXiv:2509.21972v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are transforming education by enabling personalization, feedback, and knowledge access, while also raising concerns about risks to students and learning systems. Yet empirical evidence on these risks remains fragmented. This paper presents a systematic review of 70 empirical studies across computer science, education, and psychology. Guided by four research questions, we examine: (i) which applications of LLMs in education have been most frequently explored; (ii) how researchers have measured their impact; (iii) which risks stem from such applications; and (iv) what mitigation strategies have been proposed. We find that research on LLMs clusters around three domains: operational effectiveness, personalized applications, and interactive learning tools. Across these, model-level risks include superficial understanding, bias, limited robustness, anthropomorphism, hallucinations, privacy concerns, and knowledge constraints. When learners interact with LLMs, these risks extend to cognitive and behavioural outcomes, including reduced neural activity, over-reliance, diminished independent learning skills, and a loss of student agency. To capture this progression, we propose an LLM-Risk Adapted Learning Model that illustrates how technical risks cascade through interaction and interpretation to shape educational outcomes. As the first synthesis of empirically assessed risks, this review provides a foundation for responsible, human-centred integration of LLMs in education.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning</title>
<link>https://arxiv.org/abs/2510.00192</link>
<guid>https://arxiv.org/abs/2510.00192</guid>
<content:encoded><![CDATA[

arXiv:2510.00192v2 Announce Type: replace-cross 
Abstract: Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2510.00294</link>
<guid>https://arxiv.org/abs/2510.00294</guid>
<content:encoded><![CDATA[

arXiv:2510.00294v2 Announce Type: replace-cross 
Abstract: Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous "reversal curse" or learning under data-constrained scenarios. In addition, taking advantage of their inherent modeling foundations, DLLMs have the great potential of efficient inference with parallel decoding algorithms, which enable multi-token prediction per step. However, the high generation quality often requires the number of decoding steps equal to the sequence length, which performs a one-token-per-step decoding, and existing parallel decoding algorithms, which yield suboptimal decoding paths, bring inference speedup at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (FreeDave), a novel fast decoding algorithm tailored for DLLMs that achieves lossless parallel decoding without any model modification or extra modules. Specifically, we propose an algorithm of parallel-decoded candidate generation and verification, which is theoretically guaranteed to use the fewest model forward calls to reproduce the same sequence generated by static decoding when enough computation and memory budget is provided. By extensive evaluations on math reasoning and code generation benchmarks across different DLLMs, FreeDave is proven to boost the inference throughput up to $3.78\times$ without performance degradation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse</title>
<link>https://arxiv.org/abs/2510.01258</link>
<guid>https://arxiv.org/abs/2510.01258</guid>
<content:encoded><![CDATA[

arXiv:2510.01258v2 Announce Type: replace-cross 
Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information media. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague this novel technology. This study employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing one of the aforementioned metrics. The results show an amplified liberal-authoritarian alignment across the six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on the region's pre-existing socio-political structures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title>
<link>https://arxiv.org/abs/2510.04340</link>
<guid>https://arxiv.org/abs/2510.04340</guid>
<content:encoded><![CDATA[

arXiv:2510.04340v4 Announce Type: replace-cross 
Abstract: Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Representation Bias for Investment Decisions in Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2510.05702</link>
<guid>https://arxiv.org/abs/2510.05702</guid>
<content:encoded><![CDATA[

arXiv:2510.05702v2 Announce Type: replace-cross 
Abstract: Large Language Models are increasingly adopted in financial applications to support investment workflows. However, prior studies have seldom examined how these models reflect biases related to firm size, sector, or financial characteristics, which can significantly impact decision-making. This paper addresses this gap by focusing on representation bias in open-source Qwen models. We propose a balanced round-robin prompting method over approximately 150 U.S. equities, applying constrained decoding and token-logit aggregation to derive firm-level confidence scores across financial contexts. Using statistical tests and variance analysis, we find that firm size and valuation consistently increase model confidence, while risk factors tend to decrease it. Confidence varies significantly across sectors, with the Technology sector showing the greatest variability. When models are prompted for specific financial categories, their confidence rankings best align with fundamental data, moderately with technical signals, and least with growth indicators. These results highlight representation bias in Qwen models and motivate sector-aware calibration and category-conditioned evaluation protocols for safe and fair financial LLM deployment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)</title>
<link>https://arxiv.org/abs/2510.10089</link>
<guid>https://arxiv.org/abs/2510.10089</guid>
<content:encoded><![CDATA[

arXiv:2510.10089v2 Announce Type: replace-cross 
Abstract: While looped transformers (termed as Looped-Attn) often outperform standard transformers (termed as Single-Attn) on complex reasoning tasks, the theoretical basis for this advantage remains underexplored. In this paper, we explain this phenomenon through the lens of loss landscape geometry, inspired by empirical observations of their distinct dynamics at both sample and Hessian levels. To formalize this, we extend the River-Valley landscape model by distinguishing between U-shaped valleys (flat) and V-shaped valleys (steep). Based on empirical observations, we conjecture that the recursive architecture of Looped-Attn induces a landscape-level inductive bias towards River-V-Valley. Theoretical derivations based on this inductive bias guarantee a better loss convergence along the river due to valley hopping, and further encourage learning about complex patterns compared to the River-U-Valley induced by Single-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical Framework for Progressive Training), a staged training framework that accelerates the training process of Looped-Attn while achieving comparable performances.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing LLMs by Masking Unfairness-Driving Attention Heads</title>
<link>https://arxiv.org/abs/2510.10142</link>
<guid>https://arxiv.org/abs/2510.10142</guid>
<content:encoded><![CDATA[

arXiv:2510.10142v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) increasingly mediate decisions in domains where unfair treatment of demographic groups is unacceptable. Existing work probes when biased outputs appear, but gives little insight into the mechanisms that generate them, leaving existing mitigations largely fragile. In this paper, we conduct a systematic investigation LLM unfairness and propose DiffHeads, a lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA) prompting to Chain-of-Thought (CoT) prompting across eight representative open- and closed-source LLMs. DA will trigger the nature bias part of LLM and improve measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues. Next, we define a token-to-head contribution score that traces each token's influence back to individual attention heads. This reveals a small cluster of bias heads that activate under DA but stay largely dormant with CoT, providing the first causal link between prompting strategy and bias emergence. Finally, building on this insight, we propose DiffHeads that identifies bias heads through differential activation analysis between DA and CoT, and selectively masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under DA and CoT, respectively, without harming model utility.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models</title>
<link>https://arxiv.org/abs/2510.10613</link>
<guid>https://arxiv.org/abs/2510.10613</guid>
<content:encoded><![CDATA[

arXiv:2510.10613v2 Announce Type: replace-cross 
Abstract: This paper proposes a modeling framework for dynamic topic evolution based on temporal large language models. The method first uses a large language model to obtain contextual embeddings of text and then introduces a temporal decay function and an attention mechanism. These components allow the model to adjust the importance of semantic units according to time intervals and capture topic variations across different periods. The temporal representations are then mapped into a latent topic space, where a state transition matrix is applied to describe the dynamic evolution of topics. A joint optimization objective constrains both semantic modeling and temporal consistency, ensuring diversity and smoothness in topic generation. The design emphasizes the unified modeling of semantic representation and temporal evolution, which improves topic coherence and diversity while enhancing stability and interpretability over time. Experiments on real-world corpora show that the framework effectively captures the generation, expansion, and decline of topics and outperforms existing models across multiple metrics. Overall, the proposed method provides a systematic solution for understanding dynamic semantic patterns in large-scale text, enriches the research paradigm of topic modeling, and supports complex text analysis tasks in multiple domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2510.10987</link>
<guid>https://arxiv.org/abs/2510.10987</guid>
<content:encoded><![CDATA[

arXiv:2510.10987v2 Announce Type: replace-cross 
Abstract: The promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at https://github.com/hsannn/ditto.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems</title>
<link>https://arxiv.org/abs/2510.12872</link>
<guid>https://arxiv.org/abs/2510.12872</guid>
<content:encoded><![CDATA[

arXiv:2510.12872v2 Announce Type: replace-cross 
Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts</title>
<link>https://arxiv.org/abs/2510.13500</link>
<guid>https://arxiv.org/abs/2510.13500</guid>
<content:encoded><![CDATA[

arXiv:2510.13500v2 Announce Type: replace-cross 
Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers</title>
<link>https://arxiv.org/abs/2510.13939</link>
<guid>https://arxiv.org/abs/2510.13939</guid>
<content:encoded><![CDATA[

arXiv:2510.13939v3 Announce Type: replace-cross 
Abstract: The use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative content. Yet it's unclear if these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert & lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p<10^-8) & writing quality (OR=0.13, p<10^-7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p<10^-13) & writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors & styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning & inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the "effect upon the potential market or value" of the source works.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.14830</link>
<guid>https://arxiv.org/abs/2510.14830</guid>
<content:encoded><![CDATA[

arXiv:2510.14830v2 Announce Type: replace-cross 
Abstract: Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained by supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100\% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automotive Crash Dynamics Modeling Accelerated with Machine Learning</title>
<link>https://arxiv.org/abs/2510.15201</link>
<guid>https://arxiv.org/abs/2510.15201</guid>
<content:encoded><![CDATA[

arXiv:2510.15201v3 Announce Type: replace-cross 
Abstract: Crashworthiness assessment is a critical aspect of automotive design, traditionally relying on high-fidelity finite element (FE) simulations that are computationally expensive and time-consuming. This work presents an exploratory comparative study on developing machine learning-based surrogate models for efficient prediction of structural deformation in crash scenarios using the NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine learning to structural crash dynamics, the primary contribution lies in demonstrating the feasibility and engineering utility of the various modeling approaches explored in this work. We investigate two state-of-the-art neural network architectures for modeling crash dynamics: MeshGraphNet, and Transolver. Additionally, we examine three strategies for modeling transient dynamics: time-conditional, the standard Autoregressive approach, and a stability-enhanced Autoregressive scheme incorporating rollout-based training. The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a structurally rich vehicle assembly with over 200 components, including 38 key components featuring variable thickness distributions to capture realistic manufacturing variability. Each model utilizes the undeformed mesh geometry and component characteristics as inputs to predict the spatiotemporal evolution of the deformed mesh during the crash sequence. Evaluation results show that the models capture the overall deformation trends with reasonable fidelity, demonstrating the feasibility of applying machine learning to structural crash dynamics. Although not yet matching full FE accuracy, the models achieve orders-of-magnitude reductions in computational cost, enabling rapid design exploration and early-stage optimization in crashworthiness evaluation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction</title>
<link>https://arxiv.org/abs/2510.15691</link>
<guid>https://arxiv.org/abs/2510.15691</guid>
<content:encoded><![CDATA[

arXiv:2510.15691v2 Announce Type: replace-cross 
Abstract: In quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured financial data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three representative methods: representation combination, representation summation, and attentive representations. Next, building on empirical observations from fusion learning, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability observed in the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness</title>
<link>https://arxiv.org/abs/2510.16171</link>
<guid>https://arxiv.org/abs/2510.16171</guid>
<content:encoded><![CDATA[

arXiv:2510.16171v3 Announce Type: replace-cross 
Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks by exploiting their sensitivity to imperceptible input perturbations. While adversarial training remains the predominant defense strategy, it often incurs significant computational cost and may compromise clean-data accuracy. In this work, we investigate an architectural approach to adversarial robustness by embedding group-equivariant convolutions-specifically, rotation- and scale-equivariant layers-into standard convolutional neural networks (CNNs). These layers encode symmetry priors that align model behavior with structured transformations in the input space, promoting smoother decision boundaries and greater resilience to adversarial attacks. We propose and evaluate two symmetry-aware architectures: a parallel design that processes standard and equivariant features independently before fusion, and a cascaded design that applies equivariant operations sequentially. Theoretically, we demonstrate that such models reduce hypothesis space complexity, regularize gradients, and yield tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) framework. Empirically, our models consistently improve adversarial robustness and generalization across CIFAR-10, CIFAR-100, and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial training. These findings underscore the potential of symmetry-enforcing architectures as efficient and principled alternatives to data augmentation-based defenses.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</title>
<link>https://arxiv.org/abs/2510.19755</link>
<guid>https://arxiv.org/abs/2510.19755</guid>
<content:encoded><![CDATA[

arXiv:2510.19755v3 Announce Type: replace-cross 
Abstract: Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.
  Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction</title>
<link>https://arxiv.org/abs/2510.23112</link>
<guid>https://arxiv.org/abs/2510.23112</guid>
<content:encoded><![CDATA[

arXiv:2510.23112v3 Announce Type: replace-cross 
Abstract: Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&amp;P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime</title>
<link>https://arxiv.org/abs/2510.26303</link>
<guid>https://arxiv.org/abs/2510.26303</guid>
<content:encoded><![CDATA[

arXiv:2510.26303v2 Announce Type: replace-cross 
Abstract: Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\ell_\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\ell_2$-max-margin classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker</title>
<link>https://arxiv.org/abs/2510.22733</link>
<guid>https://arxiv.org/abs/2510.22733</guid>
<content:encoded><![CDATA[
<div> embedding models, retrieval performance, listwise reranking, E2Rank framework, efficiency

Summary:<br />
The paper introduces the E2Rank framework, which enhances a single text embedding model to perform both retrieval and listwise reranking efficiently. The framework continues training under a listwise ranking objective, improving ranking fidelity. By using cosine similarity as a unified ranking function and constructing a listwise ranking prompt from the query and candidate documents, E2Rank incorporates signals from top-K documents, similar to pseudo-relevance feedback. This design maintains the efficiency and quality of the base embedding model while significantly boosting reranking performance. Empirically, E2Rank achieves top results on BEIR reranking and performs well on the BRIGHT benchmark with low reranking latency. The ranking training process also enhances embedding performance on the MTEB benchmark, demonstrating the capability of a single embedding model to unify retrieval and reranking efficiently and accurately. 

Summary: <div>
arXiv:2510.22733v2 Announce Type: replace-cross 
Abstract: Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions</title>
<link>https://arxiv.org/abs/2510.26852</link>
<guid>https://arxiv.org/abs/2510.26852</guid>
<content:encoded><![CDATA[
<div> learning ability, peer-learning, agent evolution, benchmarking, CATArena

Summary:
The article introduces a new framework for evaluating the learning capabilities of Large Language Model (LLM) agents. It emphasizes the importance of self-improvement and peer-learning in driving agent evolution towards human-level intelligence. The proposed iterative, competitive peer-learning framework allows agents to refine their strategies through feedback, systematically assessing their learning abilities. The CATArena platform is introduced as a tournament-style evaluation platform for board and card games with open-ended scoring to address score saturation in current benchmarks. Experimental results show that CATArena provides reliable and scalable benchmarking for agent abilities, particularly learning ability and strategy coding. The framework aims to move beyond fixed scenarios and specific skills evaluation, towards a more dynamic and continuous assessment of agent capabilities. <div>
arXiv:2510.26852v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base</title>
<link>https://arxiv.org/abs/2510.26854</link>
<guid>https://arxiv.org/abs/2510.26854</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific reasoning, Long Chain-of-Thought, knowledge base, SciencePedia, verifiable 

Summary:
The article introduces a framework for decompressing scientific reasoning to create a verifiable Long Chain-of-Thought (LCoT) knowledge base and SciencePedia. A Socratic agent generates first-principles questions to build a verified corpus that powers the Brainstorm Search Engine for inverse knowledge search. The Plato synthesizer narrates verified chains into coherent articles for SciencePedia. Evaluated across six disciplines, Plato-synthesized articles show higher knowledge-point density and lower error rates compared to a baseline. This approach enables trustworthy, cross-domain scientific synthesis and establishes a foundation for expanding the encyclopedia.

<br /><br />Summary: <div>
arXiv:2510.26854v1 Announce Type: new 
Abstract: Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Denario project: Deep knowledge AI agents for scientific discovery</title>
<link>https://arxiv.org/abs/2510.26887</link>
<guid>https://arxiv.org/abs/2510.26887</guid>
<content:encoded><![CDATA[
<div> Keywords: Denario, AI multi-agent system, scientific research assistant, deep-research backend, interdisciplinary research

Summary: 
Denario is an AI multi-agent system designed to assist in scientific research tasks like idea generation, literature review, research planning, code writing, data analysis, plot creation, and paper drafting. It has a modular architecture that allows for specific task handling and utilizes Cmbagent for deep-research backend tasks. Denario has generated papers in various scientific disciplines, including astrophysics, biology, chemistry, medicine, and more, showing its capability to combine ideas from different fields. The system's papers were evaluated by domain experts who provided numerical scores and feedback. The strengths, weaknesses, and limitations of Denario were discussed, along with ethical implications and its relationship to the philosophy of science. The code for Denario is publicly available on GitHub, and a demo can be accessed online. <br /><br />Summary: Denario, an AI research assistant, handles various research tasks, generates interdisciplinary papers evaluated by experts, and raises ethical and philosophical considerations. The system's modular design and deep-research backend support its versatility in scientific research. <div>
arXiv:2510.26887v1 Announce Type: new 
Abstract: We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations</title>
<link>https://arxiv.org/abs/2510.26905</link>
<guid>https://arxiv.org/abs/2510.26905</guid>
<content:encoded><![CDATA[
<div> Keywords: Cyber-physical systems, Large Language Models, Vision-Language Models, Cognition Envelopes, AI-generated decisions

Summary:
Cognitive-physical systems increasingly rely on Large Language Models (LLMs) and Vision-Language Models (VLMs) to enhance autonomy, but these models can introduce errors such as hallucinations and overgeneralizations. To address this issue, the concept of Cognition Envelopes is introduced, which set boundaries to restrict AI-generated decisions and complement safety envelopes and meta-cognition. Practical guidelines and systematic processes are needed to define, validate, and ensure the effectiveness of Cognition Envelopes. This approach aims to mitigate errors and enhance the reliability of AI-generated decisions in cyber-physical systems. <div>
arXiv:2510.26905v1 Announce Type: new 
Abstract: Cyber-physical systems increasingly rely on Foundational Models such as Large Language Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy through enhanced perception, inference, and planning. However, these models also introduce new types of errors, such as hallucinations, overgeneralizations, and context misalignments, resulting in incorrect and flawed decisions. To address this, we introduce the concept of Cognition Envelopes, designed to establish reasoning boundaries that constrain AI-generated decisions while complementing the use of meta-cognition and traditional safety envelopes. As with safety envelopes, Cognition Envelopes require practical guidelines and systematic processes for their definition, validation, and assurance.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation</title>
<link>https://arxiv.org/abs/2510.26989</link>
<guid>https://arxiv.org/abs/2510.26989</guid>
<content:encoded><![CDATA[
<div> Agriculture, IoT, AI, sustainable, viticulture
Summary:
The global agricultural sector is experiencing significant changes due to factors such as increased food demands and climate variability. SUSTAINABLE is a smart farming platform that integrates IoT, AI, satellite imaging, and role-based task orchestration to promote efficient, traceable, and sustainable agricultural practices. The platform's key features include satellite index integration, real-time environmental data, and role-aware task management specifically tailored to Mediterranean vineyards. This paper examines existing smart agriculture solutions, provides a comparative evaluation, and introduces the unique capabilities of SUSTAINABLE. With a pilot use case in viticulture, the platform aims to address the challenges faced by the agricultural industry and enable farmers to make informed decisions for sustainable cultivation. <div>
arXiv:2510.26989v1 Announce Type: new 
Abstract: The global agricultural sector is undergoing a transformative shift, driven by increasing food demands, climate variability and the need for sustainable practices. SUSTAINABLE is a smart farming platform designed to integrate IoT, AI, satellite imaging, and role-based task orchestration to enable efficient, traceable, and sustainable agriculture with a pilot usecase in viticulture. This paper explores current smart agriculture solutions, presents a comparative evaluation, and introduces SUSTAINABLE's key features, including satellite index integration, real-time environmental data, and role-aware task management tailored to Mediterranean vineyards.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models</title>
<link>https://arxiv.org/abs/2510.27009</link>
<guid>https://arxiv.org/abs/2510.27009</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, causal masking, spatial data, sequential data, chess<br />
<br />
Summary: 
Language models are traditionally designed with causal masking but in domains with spatial or relational structure, this may not be the best approach. This study explores the impact of causal masking on nonsequential data using chess as a test case. By training language models with bidirectional and causal self-attention mechanisms on spatial and sequential data from chess, the researchers found that models trained on spatial board states, even with causal masking, performed better than those trained on sequential data. This suggests that applying causal masking to spatial data can be effective for training language models on spatial data and may even be preferable to sequentialization in certain domains. The results of this study have methodological implications for training language models on nonsequential data and could have broader applications beyond the game of chess. <br /><br /> <div>
arXiv:2510.27009v1 Announce Type: new 
Abstract: Language models are traditionally designed around causal masking. In domains with spatial or relational structure, causal masking is often viewed as inappropriate, and sequential linearizations are instead used. Yet the question of whether it is viable to accept the information loss introduced by causal masking on nonsequential data has received little direct study, in part because few domains offer both spatial and sequential representations of the same dataset. In this work, we investigate this issue in the domain of chess, which naturally supports both representations. We train language models with bidirectional and causal self-attention mechanisms on both spatial (board-based) and sequential (move-based) data. Our results show that models trained on spatial board states - \textit{even with causal masking} - consistently achieve stronger playing strength than models trained on sequential data. While our experiments are conducted on chess, our results are methodological and may have broader implications: applying causal masking to spatial data is a viable procedure for training unimodal LLMs on spatial data, and in some domains is even preferable to sequentialization.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>e1: Learning Adaptive Control of Reasoning Effort</title>
<link>https://arxiv.org/abs/2510.27042</link>
<guid>https://arxiv.org/abs/2510.27042</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, AI models, Adaptive Effort Control, Cost-accuracy tradeoff curves, Task difficulty

Summary:<br />
The article introduces a new approach called Adaptive Effort Control, which allows for fine-grained control over the amount of thinking allocated to AI models based on user preferences. This self-adaptive reinforcement learning method enables models to use a user-specified fraction of tokens relative to the average chain-of-thought length for each query, eliminating the need for dataset- and phase-specific tuning. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. The model learns to allocate resources proportionally to task difficulty, leading to approximately 3x reduction in chain-of-thought length while maintaining or improving performance across model scales. This approach offers better cost-accuracy tradeoff curves compared to standard methods and enhances user control over AI model reasoning processes. <br /><br />Summary: <div>
arXiv:2510.27042v1 Announce Type: new 
Abstract: Increasing the thinking budget of AI models can significantly improve accuracy, but not all questions warrant the same amount of reasoning. Users may prefer to allocate different amounts of reasoning effort depending on how they value output quality versus latency and cost. To leverage this tradeoff effectively, users need fine-grained control over the amount of thinking used for a particular query, but few approaches enable such control. Existing methods require users to specify the absolute number of desired tokens, but this requires knowing the difficulty of the problem beforehand to appropriately set the token budget for a query. To address these issues, we propose Adaptive Effort Control, a self-adaptive reinforcement learning method that trains models to use a user-specified fraction of tokens relative to the current average chain-of-thought length for each query. This approach eliminates dataset- and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. We observe that the model automatically learns to allocate resources proportionally to the task difficulty and, across model scales ranging from 1.5B to 32B parameters, our approach enables approximately 3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement</title>
<link>https://arxiv.org/abs/2510.27051</link>
<guid>https://arxiv.org/abs/2510.27051</guid>
<content:encoded><![CDATA[
<div> Keywords: Enterprise AI, Data flywheel, Mixture-of-Experts, Continuous learning, Feedback loop

Summary: 
Enterprise AI agents require continuous adaptation to maintain accuracy, reduce latency, and align with user needs. A practical implementation of a data flywheel in NVInfo AI, a Mixture-of-Experts Knowledge Assistant serving 30,000 employees, demonstrated the effectiveness of a closed-loop system in addressing failures in retrieval-augmented generation pipelines. Feedback analysis identified routing errors and query rephrasing errors as major failure modes. Implementing targeted improvements through fine-tuning using NVIDIA NeMo microservices resulted in increased accuracy and reduced latency for both error types. The approach showcased the transformation of enterprise AI agents into self-improving systems through structured human-in-the-loop feedback within a data flywheel. Key learnings included ensuring agent robustness with limited user feedback, handling privacy constraints, and executing staged rollouts in production. This work provides a blueprint for building adaptive and robust enterprise AI agents capable of learning from real-world usage at scale. 

<br /><br />Summary: <div>
arXiv:2510.27051v1 Announce Type: new 
Abstract: Enterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major failure modes: routing errors (5.25\%) and query rephrasal errors (3.2\%). Using NVIDIA NeMo microservices, we implemented targeted improvements through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a fine-tuned 8B variant, achieving 96\% accuracy, a 10x reduction in model size, and 70\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\% gain in accuracy and a 40\% latency reduction. Our approach demonstrates how human-in-the-loop (HITL) feedback, when structured within a data flywheel, transforms enterprise AI agents into self-improving systems. Key learnings include approaches to ensure agent robustness despite limited user feedback, navigating privacy constraints, and executing staged rollouts in production. This work offers a repeatable blueprint for building robust, adaptive enterprise AI agents capable of learning from real-world usage at scale.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2510.27094</link>
<guid>https://arxiv.org/abs/2510.27094</guid>
<content:encoded><![CDATA[
<div> progress, LLMs, proof-analysis, grading, workflows
Summary:
State-of-the-art Language Models (LLMs) have advanced from struggling with proof-based problems to effectively solving most of the International Mathematical Olympiad (IMO) 2025 problems. This study evaluates the ability of these models to grade mathematical proofs by detecting errors, assessing their severity, and assigning fair scores beyond binary correctness. The research analyzes a dataset of generated solutions and MathArena problems, revealing that models can flag incorrect solutions but struggle with partial credit assignment. To improve this, agentic workflows are introduced to extract reference solutions and create problem-specific grading rubrics. Different workflow designs are compared, leading to higher agreement with human grades and more consistent handling of partial credit. The study provides code, data, and prompts/logs to support future research in this area. <div>
arXiv:2510.27094v1 Announce Type: new 
Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glia: A Human-Inspired AI for Automated Systems Design and Optimization</title>
<link>https://arxiv.org/abs/2510.27176</link>
<guid>https://arxiv.org/abs/2510.27176</guid>
<content:encoded><![CDATA[
<div> AI, mechanisms design, networked systems, large language models, distributed GPU cluster <br />
<br />
Summary: Glia is an AI architecture aiming to autonomously design mechanisms for computer systems at the level of human experts. It implements a multi-agent workflow where each agent specializes in reasoning, experimentation, and analysis. Unlike previous machine learning methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for large language model inference, Glia produces new algorithms for request routing, scheduling, and auto-scaling that rival human-expert performance in less time. The collaborative evaluation framework between agents grounds abstract reasoning in empirical feedback, resulting in novel insights into workload behavior. By combining reasoning large language models with structured experimentation, Glia demonstrates the potential of AI in producing creative and understandable designs for complex systems problems. <br /> <div>
arXiv:2510.27176v1 Announce Type: new 
Abstract: Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From product to system network challenges in system of systems lifecycle management</title>
<link>https://arxiv.org/abs/2510.27194</link>
<guid>https://arxiv.org/abs/2510.27194</guid>
<content:encoded><![CDATA[
<div> Keywords: SoS lifecycle management, model-based systems engineering, product lifecycle management, digital thread, digital twin

Summary:
Referenced architecture and data models, end-to-end configuration sovereignty, curated models with clear review gates, measurable value contributions, transitioning from product-centric to network-centric development are the key principles discussed in the article. The proposed practical frame of reference for System of Systems (SoS) lifecycle management includes utilizing model-based systems engineering as the semantic backbone, product lifecycle management for governance and configuration, CAD-CAE for model-derived domains, and digital thread and digital twin for continuous feedback. The article emphasizes the importance of change robustness, shorter throughput times, improved reuse, and informed sustainability decisions in networked systems development. The roadmap outlined involves piloting with reference architecture, scaling across variant and supply chain spaces, and organizational anchoring to facilitate a smooth transition. Decision-makers and practitioners aiming to make complexity manageable and design scalable value streams for SoS will benefit from the insights provided in this article.

<br /><br />Summary: <div>
arXiv:2510.27194v1 Announce Type: new 
Abstract: Today, products are no longer isolated artifacts, but nodes in networked systems. This means that traditional, linearly conceived life cycle models are reaching their limits: Interoperability across disciplines, variant and configuration management, traceability, and governance across organizational boundaries are becoming key factors. This collective contribution classifies the state of the art and proposes a practical frame of reference for SoS lifecycle management, model-based systems engineering (MBSE) as the semantic backbone, product lifecycle management (PLM) as the governance and configuration level, CAD-CAE as model-derived domains, and digital thread and digital twin as continuous feedback. Based on current literature and industry experience, mobility, healthcare, and the public sector, we identify four principles: (1) referenced architecture and data models, (2) end-to-end configuration sovereignty instead of tool silos, (3) curated models with clear review gates, and (4) measurable value contributions along time, quality, cost, and sustainability. A three-step roadmap shows the transition from product- to network- centric development: piloting with reference architecture, scaling across variant and supply chain spaces, organizational anchoring (roles, training, compliance). The results are increased change robustness, shorter throughput times, improved reuse, and informed sustainability decisions. This article is aimed at decision-makers and practitioners who want to make complexity manageable and design SoS value streams to be scalable.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering</title>
<link>https://arxiv.org/abs/2510.27206</link>
<guid>https://arxiv.org/abs/2510.27206</guid>
<content:encoded><![CDATA[
<div> fine-grained steering component, personalized adaptation, data efficiency, dynamic user patterns, high data sparsity scenarios
Summary: 
The article introduces a fine-grained and instance-tailored steering framework for personalized adaptation of large language models (LLMs) to individual user preferences. The proposed method utilizes sample-level interference vectors generated from user data to dynamically adapt the model's behavior. Two key technical innovations are highlighted: a fine-grained steering component capturing nuanced signals and an input-aware aggregation module synthesizing these signals for contextually relevant enhancements. The approach demonstrates high flexibility and data efficiency, particularly effective in dynamic user patterns and high data sparsity scenarios. It operates as a plug-in component compatible with existing personalization techniques and excels in fast-changing distribution environments. Extensive experiments validate the method's effectiveness across various scenarios, showing significant improvements in personalization performance while maintaining robustness across different interaction modes and context lengths.
<br /><br />Summary: <div>
arXiv:2510.27206v1 Announce Type: new 
Abstract: The rapid evolution of large language models (LLMs) has intensified the demand for effective personalization techniques that can adapt model behavior to individual user preferences. Despite the non-parametric methods utilizing the in-context learning ability of LLMs, recent parametric adaptation methods, including personalized parameter-efficient fine-tuning and reward modeling emerge. However, these methods face limitations in handling dynamic user patterns and high data sparsity scenarios, due to low adaptability and data efficiency. To address these challenges, we propose a fine-grained and instance-tailored steering framework that dynamically generates sample-level interference vectors from user data and injects them into the model's forward pass for personalized adaptation. Our approach introduces two key technical innovations: a fine-grained steering component that captures nuanced signals by hooking activations from attention and MLP layers, and an input-aware aggregation module that synthesizes these signals into contextually relevant enhancements. The method demonstrates high flexibility and data efficiency, excelling in fast-changing distribution and high data sparsity scenarios. In addition, the proposed method is orthogonal to existing methods and operates as a plug-in component compatible with different personalization techniques. Extensive experiments across diverse scenarios--including short-to-long text generation, and web function calling--validate the effectiveness and compatibility of our approach. Results show that our method significantly enhances personalization performance in fast-shifting environments while maintaining robustness across varying interaction modes and context lengths. Implementation is available at https://github.com/KounianhuaDu/Fints.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation</title>
<link>https://arxiv.org/abs/2510.27210</link>
<guid>https://arxiv.org/abs/2510.27210</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, structured reasoning, action prediction, history summarization, GUI navigation agents

Summary: 
The article introduces a reasoning-enhanced framework for GUI navigation agents called GUI-Rise, which integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates Chain-of-Thought analyses for progress estimation and decision reasoning, facilitating immediate action predictions and compact history summaries. The GUI agent is trained through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). Specialized rewards, including a history-aware objective, are employed to improve summary quality and subsequent action performance. Results show that GUI-Rise achieves state-of-the-art performance in diverse GUI navigation tasks, especially in out-of-domain scenarios, demonstrating robust reasoning and generalization capabilities. The code is available for further exploration and experimentation. 

Summary:<br /><br />Keywords: Multimodal Large Language Models, structured reasoning, action prediction, history summarization, GUI navigation agents<br />
The article presents a novel framework, GUI-Rise, for GUI navigation agents that integrates structured reasoning, action prediction, and history summarization. Through supervised fine-tuning and reinforcement learning, the agent achieves state-of-the-art performance across various GUI navigation tasks. Specialized rewards, including a history-aware objective, enhance summary quality and action performance, showcasing robust reasoning and generalization capabilities. The availability of code allows for further experimentation and validation of the framework's effectiveness. <div>
arXiv:2510.27210v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines</title>
<link>https://arxiv.org/abs/2510.27329</link>
<guid>https://arxiv.org/abs/2510.27329</guid>
<content:encoded><![CDATA[
<div> Reward machines, reinforcement learning agents, complex non-Markovian tasks, numeric RMs, agenda RMs, coupled RMs, Q-learning, CoRM<br />
<br />
Summary: <br />
Reward machines (RMs) play a crucial role in informing reinforcement learning agents about the reward structure of the environment, especially in complex non-Markovian tasks. However, learning with RMs faces challenges in long-horizon problems with unordered subtasks. To address this limitation, three generalizations of RMs are introduced: Numeric RMs for compact task expression, Agenda RMs tracking remaining subtasks, and Coupled RMs with states for each subtask in the agenda. Additionally, a new compositional learning algorithm, Q-learning with Coupled RMs (CoRM), demonstrates better scalability than existing RM algorithms for long-horizon problems with unordered subtasks. CoRM leverages the coupling of states to enhance learning efficiency, making it a promising approach for complex task environments. <div>
arXiv:2510.27329v1 Announce Type: new 
Abstract: Reward machines (RMs) inform reinforcement learning agents about the reward structure of the environment. This is particularly advantageous for complex non-Markovian tasks because agents with access to RMs can learn more efficiently from fewer samples. However, learning with RMs is ill-suited for long-horizon problems in which a set of subtasks can be executed in any order. In such cases, the amount of information to learn increases exponentially with the number of unordered subtasks. In this work, we address this limitation by introducing three generalisations of RMs: (1) Numeric RMs allow users to express complex tasks in a compact form. (2) In Agenda RMs, states are associated with an agenda that tracks the remaining subtasks to complete. (3) Coupled RMs have coupled states associated with each subtask in the agenda. Furthermore, we introduce a new compositional learning algorithm that leverages coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM scales better than state-of-the-art RM algorithms for long-horizon problems with unordered subtasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discriminative Rule Learning for Outcome-Guided Process Model Discovery</title>
<link>https://arxiv.org/abs/2510.27343</link>
<guid>https://arxiv.org/abs/2510.27343</guid>
<content:encoded><![CDATA[
<div> Keywords: event logs, process discovery, desirable behavior, undesirable behavior, interpretability <br />
Summary: 
Event logs from information systems provide valuable insights into business processes. This study focuses on distinguishing between desirable and undesirable process executions to improve process understanding. Traditional process discovery methods may overlook critical behavioral differences between desirable and undesirable traces. By learning discriminative rules over control-flow features, the approach groups traces with similar desirability profiles and conducts process discovery separately within each group. This results in focused and interpretable process models that reveal the drivers of both desirable and undesirable executions. The approach, implemented as a publicly available tool, is evaluated on real-life event logs and proves effective in isolating and visualizing critical process patterns. <div>
arXiv:2510.27343v1 Announce Type: new 
Abstract: Event logs extracted from information systems offer a rich foundation for understanding and improving business processes. In many real-world applications, it is possible to distinguish between desirable and undesirable process executions, where desirable traces reflect efficient or compliant behavior, and undesirable ones may involve inefficiencies, rule violations, delays, or resource waste. This distinction presents an opportunity to guide process discovery in a more outcome-aware manner. Discovering a single process model without considering outcomes can yield representations poorly suited for conformance checking and performance analysis, as they fail to capture critical behavioral differences. Moreover, prioritizing one behavior over the other may obscure structural distinctions vital for understanding process outcomes. By learning interpretable discriminative rules over control-flow features, we group traces with similar desirability profiles and apply process discovery separately within each group. This results in focused and interpretable models that reveal the drivers of both desirable and undesirable executions. The approach is implemented as a publicly available tool and it is evaluated on multiple real-life event logs, demonstrating its effectiveness in isolating and visualizing critical process patterns.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An In-depth Study of LLM Contributions to the Bin Packing Problem</title>
<link>https://arxiv.org/abs/2510.27353</link>
<guid>https://arxiv.org/abs/2510.27353</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, mathematical discovery, online bin packing problem, genetic algorithms, interpretability

Summary:
In this study, the potential of Large Language Models (LLMs) in contributing to mathematical discovery by solving bin packing problems was reassessed. The heuristics generated by LLMs, while human-readable, were found to be largely opaque and difficult to interpret even for domain experts. A new class of algorithms specifically tailored for bin packing instances was proposed, which demonstrated simplicity, efficiency, interpretability, and generalizability. The study highlighted the need for rigorous validation and contextualization when evaluating the scientific value of outputs generated by LLMs. The claim that LLMs could provide significant insights into the online bin packing problem was questioned, as the instances considered in the study were found to be relatively simple and had not been extensively studied before. This research underscores the importance of critically evaluating the capabilities of LLMs in contributing to complex problem-solving tasks. 

Summary:<br /><br /> <div>
arXiv:2510.27353v1 Announce Type: new 
Abstract: Recent studies have suggested that Large Language Models (LLMs) could provide interesting ideas contributing to mathematical discovery. This claim was motivated by reports that LLM-based genetic algorithms produced heuristics offering new insights into the online bin packing problem under uniform and Weibull distributions. In this work, we reassess this claim through a detailed analysis of the heuristics produced by LLMs, examining both their behavior and interpretability. Despite being human-readable, these heuristics remain largely opaque even to domain experts. Building on this analysis, we propose a new class of algorithms tailored to these specific bin packing instances. The derived algorithms are significantly simpler, more efficient, more interpretable, and more generalizable, suggesting that the considered instances are themselves relatively simple. We then discuss the limitations of the claim regarding LLMs' contribution to this problem, which appears to rest on the mistaken assumption that the instances had previously been studied. Our findings instead emphasize the need for rigorous validation and contextualization when assessing the scientific value of LLM-generated outputs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use</title>
<link>https://arxiv.org/abs/2510.27363</link>
<guid>https://arxiv.org/abs/2510.27363</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multimodal information, external tools, VQA task, ToolScope <br />
Summary: ToolScope is a framework designed to enhance multimodal large language models (MLLMs) by integrating external tools for collaborative reasoning in tasks such as visual question answering (VQA). It consists of three components: the Global Navigator for guidance, the Agentic Executor for local perception with tools like Search, Code, and Perceive, and the Response Synthesizer for organizing outputs. ToolScope was evaluated on four VQA benchmarks and showed strong generalization capabilities, with performance improvements of up to +6.69% across datasets. This framework addresses the challenge of effectively utilizing external tools in MLLMs for complex multimodal tasks like VQA, improving reasoning abilities and overall performance. <br /><br />Summary: <div>
arXiv:2510.27363v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a "telescope", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints</title>
<link>https://arxiv.org/abs/2510.27383</link>
<guid>https://arxiv.org/abs/2510.27383</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, pedestrian-driver interactions, visual constraints, motor constraints, multi-agent modeling
Summary:<br />
- The study focuses on modeling pedestrian-driver interactions using a multi-agent reinforcement learning framework incorporating visual and motor constraints.
- Existing models lack flexibility and ignore underlying mechanisms, such as sensory and motor constraints, shaping human behavior in interactive scenarios.
- Results show that the model with both visual and motor constraints performs best, with motor constraints leading to smoother movements and visual constraints introducing perceptual uncertainty.
- The model, even in a data-limited setting, outperforms a supervised behavioral cloning model, indicating effectiveness without large training datasets.
- Individual differences are accounted for by modeling human constraints as population-level distributions, a novel approach in pedestrian-vehicle interaction modeling.<br /><br />Summary: <div>
arXiv:2510.27383v1 Announce Type: new 
Abstract: Modelling pedestrian-driver interactions is critical for understanding human road user behaviour and developing safe autonomous vehicle systems. Existing approaches often rely on rule-based logic, game-theoretic models, or 'black-box' machine learning methods. However, these models typically lack flexibility or overlook the underlying mechanisms, such as sensory and motor constraints, which shape how pedestrians and drivers perceive and act in interactive scenarios. In this study, we propose a multi-agent reinforcement learning (RL) framework that integrates both visual and motor constraints of pedestrian and driver agents. Using a real-world dataset from an unsignalised pedestrian crossing, we evaluate four model variants, one without constraints, two with either motor or visual constraints, and one with both, across behavioural metrics of interaction realism. Results show that the combined model with both visual and motor constraints performs best. Motor constraints lead to smoother movements that resemble human speed adjustments during crossing interactions. The addition of visual constraints introduces perceptual uncertainty and field-of-view limitations, leading the agents to exhibit more cautious and variable behaviour, such as less abrupt deceleration. In this data-limited setting, our model outperforms a supervised behavioural cloning model, demonstrating that our approach can be effective without large training datasets. Finally, our framework accounts for individual differences by modelling parameters controlling the human constraints as population-level distributions, a perspective that has not been explored in previous work on pedestrian-vehicle interaction modelling. Overall, our work demonstrates that multi-agent RL with human constraints is a promising modelling approach for simulating realistic road user interactions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry</title>
<link>https://arxiv.org/abs/2510.27410</link>
<guid>https://arxiv.org/abs/2510.27410</guid>
<content:encoded><![CDATA[
<div> Keywords: intention expression gap, human-AI collaboration, Socratic collaboration paradigm, information theory, scientific diagram generation 

Summary:
Nous addresses the intention expression gap in human-AI collaboration by actively probing for information. Through a Socratic collaboration paradigm, Nous acquires proficiency in inquiry policy. Its training framework is grounded in information theory, using information gain from dialogue as an intrinsic reward signal. This approach avoids reliance on costly human preference annotations. Nous is validated using a large-scale dataset for scientific diagram generation, demonstrating efficiency, output quality, and robustness across user expertise levels. The design is domain-agnostic and shows evidence of generalization beyond diagram generation. Experimental results support the effectiveness of Nous in resolving uncertainty about user intent in complex human-AI collaboration.<br /><br />Summary: Nous addresses the intention expression gap through a Socratic collaboration paradigm, utilizing information theory for training, and demonstrating efficiency, quality, and robustness in resolving uncertainty in human-AI collaboration. <div>
arXiv:2510.27410v1 Announce Type: new 
Abstract: A fundamental bottleneck in human-AI collaboration is the "intention expression gap," the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users. We reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent. we name the proposed agent Nous, trained to acquire proficiency in this inquiry policy. The core mechanism of Nous is a training framework grounded in the first principles of information theory. Within this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space. This reward design enables us to avoid reliance on costly human preference annotations or external reward models. To validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. Comprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise. Moreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation. Experimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains</title>
<link>https://arxiv.org/abs/2510.27419</link>
<guid>https://arxiv.org/abs/2510.27419</guid>
<content:encoded><![CDATA[
<div> Efficient Reasoning, Large Reasoning Models, DeepCompress, Adaptive Length Reward, Chain-of-Thought (CoT) <br /> 
<br />
Summary: DeepCompress is a novel framework designed to enhance the accuracy and efficiency of Large Reasoning Models (LRMs). It challenges the conventional approach of favoring shorter reasoning paths by introducing an adaptive length reward mechanism that classifies problems in real-time as "Simple" or "Hard". The framework encourages shorter, more efficient reasoning for simple problems while promoting longer, more exploratory thought chains for difficult problems. By dynamically adjusting the model's Chain-of-Thought (CoT) length, DeepCompress achieves superior accuracy and token efficiency compared to baseline methods on challenging mathematical benchmarks. This dual-reward strategy allows the model to compress reasoning for well-mastered problems and extend it for challenging ones, leading to improved performance in both accuracy and efficiency. <div>
arXiv:2510.27419v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language</title>
<link>https://arxiv.org/abs/2510.27448</link>
<guid>https://arxiv.org/abs/2510.27448</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modal Large Language Models, Geometric reasoning, Synthetic data, Formal languages, Metric space

Summary: 
GeoFM introduces a new method for generating synthetic geometric data using formal languages to create high-quality geometric problems. This approach overcomes limitations in existing methods by exploring diverse conditions in metric space and ensuring correctness through a symbolic engine. Experimental results demonstrate that models trained on GeoFM data outperform both proprietary and open-source models on geometry problem-solving tasks in MathVista and GeoQA. The GeoFM synthetic data leads to a significant performance improvement of 18.7% over the proprietary GPT-4o model in MathVista and 16.5% in GeoQA. It also outperforms a leading open-source model by 5.7% in MathVista and 2.7% in GeoQA.<br /><br />Summary: <div>
arXiv:2510.27448v1 Announce Type: new 
Abstract: Multi-modal Large Language Models (MLLMs) have gained significant attention in both academia and industry for their capabilities in handling multi-modal tasks. However, these models face challenges in mathematical geometric reasoning due to the scarcity of high-quality geometric data. To address this issue, synthetic geometric data has become an essential strategy. Current methods for generating synthetic geometric data involve rephrasing or expanding existing problems and utilizing predefined rules and templates to create geometric images and problems. However, these approaches often produce data that lacks diversity or is prone to noise. Additionally, the geometric images synthesized by existing methods tend to exhibit limited variation and deviate significantly from authentic geometric diagrams. To overcome these limitations, we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses formal languages to explore combinations of conditions within metric space, generating high-fidelity geometric problems that differ from the originals while ensuring correctness through a symbolic engine. Experimental results show that our synthetic data significantly outperforms existing methods. The model trained with our data surpass the proprietary GPT-4o model by 18.7\% on geometry problem-solving tasks in MathVista and by 16.5\% on GeoQA. Additionally, it exceeds the performance of a leading open-source model by 5.7\% on MathVista and by 2.7\% on GeoQA.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for Interpretable Deconstruction of Reasoning System Performance</title>
<link>https://arxiv.org/abs/2510.27544</link>
<guid>https://arxiv.org/abs/2510.27544</guid>
<content:encoded><![CDATA[
<div> Temporal Trace Evaluation, Temporal Causal Evaluation, Large Language Models, reasoning ability, diagnostic benchmark <br />
Summary:
Large Language Models (LLMs) have shown exceptional performance on various tasks, but improving their reasoning abilities remains a challenge. Current methods rely on ad-hoc datasets or formal proof systems like Lean, which have limitations in capturing nuanced decision chains. To address this gap, TempoBench introduces a novel diagnostic benchmark that measures LLM reasoning performance using two evaluation benchmarks: Temporal Trace Evaluation (TTE) and Temporal Causal Evaluation (TCE). Results indicate that while LLMs understand simpler causal reasoning tasks, their performance declines as system complexity increases. TempoBench offers a formally grounded and verifiable framework to systematically analyze LLM reasoning abilities, providing insights into areas for improvement in real-world reasoning tasks. <div>
arXiv:2510.27544v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly excelling and outpacing human performance on many tasks. However, to improve LLM reasoning, researchers either rely on ad-hoc generated datasets or formal mathematical proof systems such as the Lean proof assistant. Whilst ad-hoc generated methods can capture the decision chains of real-world reasoning processes, they may encode some inadvertent bias in the space of reasoning they cover; they also cannot be formally verified. On the other hand, systems like Lean can guarantee verifiability, but are not well-suited to capture the nature of agentic decision chain-based tasks. This creates a gap both in performance for functions such as business agents or code assistants, and in the usefulness of LLM reasoning benchmarks, whereby these fall short in reasoning structure or real-world alignment. We introduce TempoBench, the first formally grounded and verifiable diagnostic benchmark that parametrizes difficulty to systematically analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks to break down reasoning ability. First, temporal trace evaluation (TTE) tests the ability of an LLM to understand and simulate the execution of a given multi-step reasoning system. Subsequently, temporal causal evaluation (TCE) tests an LLM's ability to perform multi-step causal reasoning and to distill cause-and-effect relations from complex systems. We find that models score 65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art LLMs clearly understand the TCE task but perform poorly as system complexity increases. Our code is available at our \href{https://github.com/nik-hz/tempobench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2510.27568</link>
<guid>https://arxiv.org/abs/2510.27568</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematical reasoning, retrieval-augmented models, multi-agent, knowledge integration, problem-solving <br />
<br />
Summary: 
The study introduces SIGMA, a framework for mathematical reasoning that utilizes multiple specialized agents to reason, search for information, and integrate knowledge to solve problems. SIGMA addresses the limitations of current retrieval-augmented models by allowing agents to generate hypothetical passages to optimize retrieval, ensuring context-sensitive and efficient knowledge integration. Evaluation on challenging benchmarks shows that SIGMA outperforms existing systems, achieving a 7.4% performance improvement. The results highlight the effectiveness of multi-agent on-demand knowledge integration in enhancing reasoning accuracy and efficiency in complex problem-solving tasks. The release of the code upon publication will enable further exploration and application of this scalable approach. <br /><br />Summary: <div>
arXiv:2510.27568v1 Announce Type: new 
Abstract: Solving mathematical reasoning problems requires not only accurate access to relevant knowledge but also careful, multi-step thinking. However, current retrieval-augmented models often rely on a single perspective, follow inflexible search strategies, and struggle to effectively combine information from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning), a unified framework that orchestrates specialized agents to independently reason, perform targeted searches, and synthesize findings through a moderator mechanism. Each agent generates hypothetical passages to optimize retrieval for its analytic perspective, ensuring knowledge integration is both context-sensitive and computation-efficient. When evaluated on challenging benchmarks such as MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms both open- and closed-source systems, achieving an absolute performance improvement of 7.4%. Our results demonstrate that multi-agent, on-demand knowledge integration significantly enhances both reasoning accuracy and efficiency, offering a scalable approach for complex, knowledge-intensive problem-solving. We will release the code upon publication.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research</title>
<link>https://arxiv.org/abs/2510.27598</link>
<guid>https://arxiv.org/abs/2510.27598</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, InnovatorBench, ResearchGym, Large Language Model (LLM), benchmark-platform pair

Summary: 
In this paper, the authors introduce a new benchmark-platform pair called InnovatorBench to assess AI agents' abilities in performing end-to-end Large Language Model (LLM) research. The benchmark comprises 20 tasks across various research aspects like Data Construction, Loss Design, and Reward Design. To support agent operation, they develop ResearchGym, a research environment with rich features. They implement a ReAct agent that combines explicit reasoning with executable planning using frontier models. Experimental results show that while frontier models perform well in code-driven tasks, they struggle with fragile algorithm-related challenges and long-horizon decision-making. The experiments also reveal that agents need over 11 hours to reach their peak performance in InnovatorBench, indicating its difficulty. The potential of InnovatorBench as the next generation code-based research benchmark is highlighted. 

<br /><br />Summary: <div>
arXiv:2510.27598v1 Announce Type: new 
Abstract: AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation</title>
<link>https://arxiv.org/abs/2510.27617</link>
<guid>https://arxiv.org/abs/2510.27617</guid>
<content:encoded><![CDATA[
<div> Language Models, RTL design, HDL generation, multi-agent architectures, VeriMoA <br />
Summary: 
The article discusses the automation of RTL design using Large Language Models (LLMs) for Hardware Description Language (HDL) generation. The challenges faced due to limited parametric knowledge and domain-specific constraints are highlighted, along with the limitations of prompt engineering and fine-tuning. The proposed framework, VeriMoA, is a training-free mixture-of-agents (MoA) framework that addresses the deficiencies of current multi-agent approaches by introducing a quality-guided caching mechanism and a multi-path generation strategy. Through comprehensive experiments on benchmark tests, VeriMoA demonstrates improvements in Pass@1 across various LLM backbones, enabling smaller models to compete with larger models and fine-tuned alternatives without costly training. <div>
arXiv:2510.27617v1 Announce Type: new 
Abstract: Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning</title>
<link>https://arxiv.org/abs/2510.27623</link>
<guid>https://arxiv.org/abs/2510.27623</guid>
<content:encoded><![CDATA[
<div> framework, visual backdoors, embodied agents, MLLM, triggers <br />
Summary: 
The article introduces BEAT, a framework that injects visual backdoors into Multimodal Large Language Models (MLLM) based embodied agents using objects in the environment as triggers. These triggers can cause the agent to execute an attacker-specified policy when detected in the scene. BEAT addresses the challenge of trigger variability by training agents on diverse scenes, tasks, and trigger placements. It employs a two-stage training scheme involving supervised fine-tuning (SFT) and Contrastive Trigger Learning (CTL) to enhance trigger discrimination and ensure precise backdoor activation. The framework achieves high attack success rates while maintaining good benign task performance and generalizing to out-of-distribution trigger placements. Compared to naive SFT, CTL significantly improves backdoor activation accuracy even with limited backdoor data. The study highlights the security risk visual backdoors pose in MLLM-based embodied agents, emphasizing the need for robust defenses before real-world deployment. <br /><br />Summary: <div>
arXiv:2510.27623v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Validity Is What You Need</title>
<link>https://arxiv.org/abs/2510.27628</link>
<guid>https://arxiv.org/abs/2510.27628</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, Agentic AI, software delivery mechanism, large language models, validation measures <br />
Summary: 
Agentic AI systems represent a new development in the field of AI, functioning as autonomous applications within complex enterprise environments. These systems rely on software delivery mechanisms similar to software as a service, with recent advancements in large language models (LLMs) playing a significant role in their operation. However, the success of Agentic AI ultimately hinges on validation by end users and principal stakeholders. This validation process requires distinct tools and techniques compared to evaluating foundation models, with the potential for simpler and more interpretable models to replace complex foundation models. In the realm of Agentic AI, prioritizing validity is essential, and while LLMs offer one approach to achieving this, alternative solutions may also prove effective in meeting validation requirements. <br /><br />Summary: <div>
arXiv:2510.27628v1 Announce Type: new 
Abstract: While AI agents have long been discussed and studied in computer science, today's Agentic AI systems are something new. We consider other definitions of Agentic AI and propose a new realist definition. Agentic AI is a software delivery mechanism, comparable to software as a service (SaaS), which puts an application to work autonomously in a complex enterprise setting. Recent advances in large language models (LLMs) as foundation models have driven excitement in Agentic AI. We note, however, that Agentic AI systems are primarily applications, not foundations, and so their success depends on validation by end users and principal stakeholders. The tools and techniques needed by the principal users to validate their applications are quite different from the tools and techniques used to evaluate foundation models. Ironically, with good validation measures in place, in many cases the foundation models can be replaced with much simpler, faster, and more interpretable models that handle core logic. When it comes to Agentic AI, validity is what you need. LLMs are one option that might achieve it.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training</title>
<link>https://arxiv.org/abs/2510.27630</link>
<guid>https://arxiv.org/abs/2510.27630</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Apollo, human-in-the-loop sampling, long-horizon tasks, domain-specialized tasks

Summary: 
Apollo is a new sampling framework designed to tackle the challenge of training Large Language Model agents on long-horizon, domain-specialized tasks. It integrates asynchronous human guidance with action-level data filtering, allowing annotators to intervene only when necessary and sustaining interactions for over 30 hours. The framework applies supervision control to filter out sub-optimal actions and prevent error propagation, resulting in reliable and effective data collection. When applied to train the GLM-4.5 model on InnovatorBench, Apollo achieves a more than 50% improvement over the untrained baseline and a 28% improvement over a variant trained without human interaction. These results highlight the critical role of human-in-the-loop sampling in training LLM agents and demonstrate the robustness of Apollo's design in handling long-horizon, domain-specialized tasks.<br /><br />Summary: <div>
arXiv:2510.27630v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have recently shown strong potential in domains such as automated coding, deep research, and graphical user interface manipulation. However, training them to succeed on long-horizon, domain-specialized tasks remains challenging. Current methods primarily fall into two categories. The first relies on dense human annotations through behavior cloning, which is prohibitively expensive for long-horizon tasks that can take days or months. The second depends on outcome-driven sampling, which often collapses due to the rarity of valid positive trajectories on domain-specialized tasks. We introduce Apollo, a sampling framework that integrates asynchronous human guidance with action-level data filtering. Instead of requiring annotators to shadow every step, Apollo allows them to intervene only when the agent drifts from a promising trajectory, by providing prior knowledge, strategic advice, etc. This lightweight design makes it possible to sustain interactions for over 30 hours and produces valuable trajectories at a lower cost. Apollo then applies supervision control to filter out sub-optimal actions and prevent error propagation. Together, these components enable reliable and effective data collection in long-horizon environments. To demonstrate the effectiveness of Apollo, we evaluate it using InnovatorBench. Our experiments show that when applied to train the GLM-4.5 model on InnovatorBench, Apollo achieves more than a 50% improvement over the untrained baseline and a 28% improvement over a variant trained without human interaction. These results highlight the critical role of human-in-the-loop sampling and the robustness of Apollo's design in handling long-horizon, domain-specialized tasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design</title>
<link>https://arxiv.org/abs/2510.27671</link>
<guid>https://arxiv.org/abs/2510.27671</guid>
<content:encoded><![CDATA[
<div> Keywords: Structure-based drug design, protein-ligand alignment, molecular representations, autoregressive model, property-aware dataset

Summary: 
MolChord, a novel approach in structure-based drug design (SBDD), integrates NatureLM and a diffusion-based structure encoder to align protein and molecule structures with their textual descriptions. This integration facilitates the mapping of target proteins to candidate molecular ligands, addressing a critical challenge in drug discovery. MolChord also leverages Direct Preference Optimization (DPO) to guide molecules towards desired pharmacological properties, enhancing the alignment process. By curating a property-aware dataset and incorporating preference data, MolChord significantly improves the accuracy of aligning drugs with their pharmacological properties. Experimental results on the CrossDocked2020 dataset showcase the state-of-the-art performance of MolChord on key evaluation metrics. The proposed approach demonstrates promise as a practical tool for SBDD, offering a comprehensive solution for effectively aligning protein structures with molecular representations to optimize drug discovery processes. 

<br /><br />Summary: <div>
arXiv:2510.27671v1 Announce Type: new 
Abstract: Structure-based drug design (SBDD), which maps target proteins to candidate molecular ligands, is a fundamental task in drug discovery. Effectively aligning protein structural representations with molecular representations, and ensuring alignment between generated drugs and their pharmacological properties, remains a critical challenge. To address these challenges, we propose MolChord, which integrates two key techniques: (1) to align protein and molecule structures with their textual descriptions and sequential representations (e.g., FASTA for proteins and SMILES for molecules), we leverage NatureLM, an autoregressive model unifying text, small molecules, and proteins, as the molecule generator, alongside a diffusion-based structure encoder; and (2) to guide molecules toward desired properties, we curate a property-aware dataset by integrating preference data and refine the alignment process using Direct Preference Optimization (DPO). Experimental results on CrossDocked2020 demonstrate that our approach achieves state-of-the-art performance on key evaluation metrics, highlighting its potential as a practical tool for SBDD.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture</title>
<link>https://arxiv.org/abs/2505.01313</link>
<guid>https://arxiv.org/abs/2505.01313</guid>
<content:encoded><![CDATA[
<div> Keywords: neural architecture search, ResNet, optimization, validation loss, convolution

Summary:
This paper introduces a neural architecture search space based on the ResNet framework, with search objectives covering various parameters such as convolution, pooling, fully connected layers, and connectivity within the residual network. In addition to focusing on recognition accuracy, the paper incorporates the validation set loss as a secondary optimization objective. Through experiments conducted on MNIST, Fashion-MNIST, and CIFAR100 datasets, the proposed search space and optimization approach are proven effective in discovering competitive network architectures. This innovative approach promises advancements in neural network design and optimization, showcasing the potential for improved performance across various datasets and tasks. The utilization of ResNet as a foundation underscores the robustness and efficiency of the proposed search space, highlighting its adaptability and potential for further advancements in neural architecture research.<br /><br />Summary: <div>
arXiv:2505.01313v1 Announce Type: cross 
Abstract: This paper proposes a neural architecture search space using ResNet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. In addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. The experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the MNIST, Fashion-MNIST and CIFAR100 datasets.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Transformer-based Neural Architecture Search Method</title>
<link>https://arxiv.org/abs/2505.01314</link>
<guid>https://arxiv.org/abs/2505.01314</guid>
<content:encoded><![CDATA[
<div> Transformer, neural architecture search, multihead attention, perplexity, BLEU score

Summary:
This paper introduces a neural architecture search method based on the Transformer architecture, focusing on discovering optimal cross multihead attention computation methods for varying combinations of encoder and decoder models. To enhance translation performance, perplexity is incorporated as an additional evaluation metric alongside BLEU scores during the algorithmic search process. Through iterative refinement using a multi-objective genetic algorithm, individual neural networks within the population are improved. Experimental findings demonstrate that the algorithm-generated neural network structures surpass baseline models in translation quality. The auxiliary evaluation metric, perplexity, proves beneficial in identifying superior models compared to solely relying on BLEU scores for evaluation. <div>
arXiv:2505.01314v1 Announce Type: cross 
Abstract: This paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Prefix Bias in LLM-based Reward Models</title>
<link>https://arxiv.org/abs/2505.13487</link>
<guid>https://arxiv.org/abs/2505.13487</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Human Feedback, Bias Detection, Language Models, Data Augmentation
<br />
<br />
Reinforcement Learning with Human Feedback (RLHF) is used for fine-tuning language models with human preference data. This study introduces methods to detect and evaluate prefix bias in reward models trained on preference datasets, revealing biases across racial and gender dimensions. Various datasets and model architectures were evaluated, showing susceptibility to prefix biases. A data augmentation strategy was proposed to mitigate these biases effectively. The research highlights the importance of bias-aware dataset design for fair and reliable reward models, contributing to the fairness discourse in AI.
<br />
<br />Summary: <div>
arXiv:2505.13487v2 Announce Type: cross 
Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key paradigm for task-specific fine-tuning of language models using human preference data. While numerous publicly available preference datasets provide pairwise comparisons of responses, the potential for biases in the resulting reward models remains underexplored. In this work, we introduce novel methods to detect and evaluate prefix bias -- a systematic shift in model preferences triggered by minor variations in query prefixes -- in LLM-based reward models trained on such datasets. We leverage these metrics to reveal significant biases in preference models across racial and gender dimensions. Our comprehensive evaluation spans diverse open-source preference datasets and reward model architectures, demonstrating susceptibility to this kind of bias regardless of the underlying model architecture. Furthermore, we propose a data augmentation strategy to mitigate these biases, showing its effectiveness in reducing the impact of prefix bias. Our findings highlight the critical need for bias-aware dataset design and evaluation in developing fair and reliable reward models, contributing to the broader discourse on fairness in AI.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus</title>
<link>https://arxiv.org/abs/2510.25015</link>
<guid>https://arxiv.org/abs/2510.25015</guid>
<content:encoded><![CDATA[
<div> Keywords: VeriStruct, AI-assisted verification, data structures, Rust, formal verification

Summary: 
VeriStruct is a new framework that extends AI-assisted automated verification to complex data structure modules in the Rust programming language. It uses a planner module to generate abstractions, type invariants, specifications, and proof code systematically. To overcome challenges related to annotation syntax and verification-specific semantics, VeriStruct provides syntax guidance within prompts and includes a repair stage to correct annotation errors automatically. In an evaluation on eleven Rust data structure modules, VeriStruct successfully verified 99.2% of functions, showcasing its potential for automatic AI-assisted formal verification. This work represents a significant advancement in the field of automated verification, demonstrating the effectiveness of using AI techniques to verify complex software components efficiently. 

<br /><br />Summary: <div>
arXiv:2510.25015v1 Announce Type: cross 
Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated verification from single functions to more complex data structure modules in Verus. VeriStruct employs a planner module to orchestrate the systematic generation of abstractions, type invariants, specifications, and proof code. To address the challenge that LLMs often misunderstand Verus' annotation syntax and verification-specific semantics, VeriStruct embeds syntax guidance within prompts and includes a repair stage to automatically correct annotation errors. In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in total. These results represent an important step toward the goal of automatic AI-assisted formal verification.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EARS-UDE: Evaluating Auditory Response in Sensory Overload with Universal Differential Equations</title>
<link>https://arxiv.org/abs/2510.26804</link>
<guid>https://arxiv.org/abs/2510.26804</guid>
<content:encoded><![CDATA[
<div> Keywords: Auditory sensory overload, Autism Spectrum Disorder, Scientific Machine Learning, Universal Differential Equations, Personalized interventions

Summary: 
The paper introduces a Scientific Machine Learning approach using Universal Differential Equations (UDEs) to model sensory adaptation dynamics in Autism Spectrum Disorder (ASD). Existing approaches lack interpretability and fail to capture autism heterogeneity. The framework combines ordinary differential equations with neural networks to capture mechanistic understanding and individual variability. UDEs outperform Neural ODEs by 90.8% while using 73.5% fewer parameters and recover physiological parameters with 2% error. The model provides a quantitative risk assessment for sensory overload, predicting a 17.2% risk for pulse stimuli with specific temporal patterns. This approach paves the way for personalized, evidence-based interventions in autism, with potential applications in wearable technology and clinical practice.<br /><br />Summary: <div>
arXiv:2510.26804v1 Announce Type: cross 
Abstract: Auditory sensory overload affects 50-70% of individuals with Autism Spectrum Disorder (ASD), yet existing approaches, such as mechanistic models (Hodgkin Huxley type, Wilson Cowan, excitation inhibition balance), clinical tools (EEG/MEG, Sensory Profile scales), and ML methods (Neural ODEs, predictive coding), either assume fixed parameters or lack interpretability, missing autism heterogeneity. We present a Scientific Machine Learning approach using Universal Differential Equations (UDEs) to model sensory adaptation dynamics in autism. Our framework combines ordinary differential equations grounded in biophysics with neural networks to capture both mechanistic understanding and individual variability. We demonstrate that UDEs achieve a 90.8% improvement over pure Neural ODEs while using 73.5% fewer parameters. The model successfully recovers physiological parameters within the 2% error and provides a quantitative risk assessment for sensory overload, predicting 17.2% risk for pulse stimuli with specific temporal patterns. This framework establishes foundations for personalized, evidence-based interventions in autism, with direct applications to wearable technology and clinical practice.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Accelerator Beamline Control: a simulation-based approach</title>
<link>https://arxiv.org/abs/2510.26805</link>
<guid>https://arxiv.org/abs/2510.26805</guid>
<content:encoded><![CDATA[
<div> Python, acceleration, reinforcement learning, optimization, beamline

Summary:
RLABC is a Python library designed to optimize particle transmission in particle accelerators by reframing beamline optimization as a reinforcement learning problem. The library automates the creation of an RL environment using Elegant simulation framework, allowing sequential tuning of magnets to minimize particle losses. It defines a state representation capturing beam statistics, actions for adjusting magnet parameters, and a reward function focused on transmission efficiency. RLABC employs the Deep Deterministic Policy Gradient (DDPG) algorithm to achieve high transmission rates comparable to expert manual optimizations on two beamlines. This approach bridges accelerator physics and machine learning, offering a versatile tool for physicists and RL researchers to streamline beamline tuning.<br /><br />Summary: <div>
arXiv:2510.26805v1 Announce Type: cross 
Abstract: Particle accelerators play a pivotal role in advancing scientific research, yet optimizing beamline configurations to maximize particle transmission remains a labor-intensive task requiring expert intervention. In this work, we introduce RLABC (Reinforcement Learning for Accelerator Beamline Control), a Python-based library that reframes beamline optimization as a reinforcement learning (RL) problem. Leveraging the Elegant simulation framework, RLABC automates the creation of an RL environment from standard lattice and element input files, enabling sequential tuning of magnets to minimize particle losses. We define a comprehensive state representation capturing beam statistics, actions for adjusting magnet parameters, and a reward function focused on transmission efficiency. Employing the Deep Deterministic Policy Gradient (DDPG) algorithm, we demonstrate RLABC's efficacy on two beamlines, achieving transmission rates of 94% and 91%, comparable to expert manual optimizations. This approach bridges accelerator physics and machine learning, offering a versatile tool for physicists and RL researchers alike to streamline beamline tuning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of clinical decision support systems (cdss) on clinical outcomes and healthcare delivery in low- and middle-income countries: protocol for a systematic review and meta-analysis</title>
<link>https://arxiv.org/abs/2510.26812</link>
<guid>https://arxiv.org/abs/2510.26812</guid>
<content:encoded><![CDATA[
<div> Clinical decision support systems, CDSS, LMICs, impact, quantitative methods <br />
<br />
Summary: This protocol outlines methods for evaluating the impact of clinical decision support systems (CDSS) on patient and healthcare delivery outcomes in low- and middle-income countries (LMICs). The study includes comparative quantitative designs in LMICs identified by the World Bank. Qualitative studies are excluded, while mixed-methods studies are eligible if they report comparative quantitative outcomes. Data will be collected from various databases and sources, with screening and extraction performed in duplicate. The risk of bias will be assessed, and a meta-analysis or structured narrative synthesis will be conducted based on outcome comparability. Heterogeneity will be explored through subgroup analysis or meta-regression based on various factors such as condition area, care level, CDSS type, readiness proxies, and study design. <div>
arXiv:2510.26812v1 Announce Type: cross 
Abstract: Clinical decision support systems (CDSS) are used to improve clinical and service outcomes, yet evidence from low- and middle-income countries (LMICs) is dispersed. This protocol outlines methods to quantify the impact of CDSS on patient and healthcare delivery outcomes in LMICs. We will include comparative quantitative designs (randomized trials, controlled before-after, interrupted time series, comparative cohorts) evaluating CDSS in World Bank-defined LMICs. Standalone qualitative studies are excluded; mixed-methods studies are eligible only if they report comparative quantitative outcomes, for which we will extract the quantitative component. Searches (from inception to 30 September 2024) will cover MEDLINE, Embase, CINAHL, CENTRAL, Web of Science, Global Health, Scopus, IEEE Xplore, LILACS, African Index Medicus, and IndMED, plus grey sources. Screening and extraction will be performed in duplicate. Risk of bias will be assessed with RoB 2 (randomized trials) and ROBINS-I (non-randomized). Random-effects meta-analysis will be performed where outcomes are conceptually or statistically comparable; otherwise, a structured narrative synthesis will be presented. Heterogeneity will be explored using relative and absolute metrics and a priori subgroups or meta-regression (condition area, care level, CDSS type, readiness proxies, study design).
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematic Absence of Low-Confidence Nighttime Fire Detections in VIIRS Active Fire Product: Evidence of Undocumented Algorithmic Filtering</title>
<link>https://arxiv.org/abs/2510.26816</link>
<guid>https://arxiv.org/abs/2510.26816</guid>
<content:encoded><![CDATA[
<div> confidence classification scheme, VIIRS, nighttime fires, algorithmic constraint, brightness temperature

Summary: 
Through the analysis of over 21 million fire detections from the VIIRS active fire product, it was found that there is a systematic absence of low-confidence classifications during nighttime observations, affecting nearly 28% of all VIIRS fire detections. This pattern was consistent globally across various factors and was determined to be an algorithmic constraint rather than a geophysical phenomenon. The study used machine learning, bootstrap simulation, and spatial-temporal analysis to confirm this behavior. It was revealed that nighttime fires below a certain brightness temperature threshold are likely excluded entirely rather than classified as low confidence, while daytime fires show normal confidence distributions. This undocumented behavior has significant implications for fire risk assessment, comparisons between day and night detections, confidence-weighted analyses, and research relying on confidence levels as uncertainty metrics. It is recommended that this algorithmic constraint be explicitly documented in VIIRS user guides and addressed in reprocessing strategies for affected analyses. 

<br /><br />Summary: <div>
arXiv:2510.26816v1 Announce Type: cross 
Abstract: The Visible Infrared Imaging Radiometer Suite (VIIRS) active fire product is widely used for global fire monitoring, yet its confidence classification scheme exhibits an undocumented systematic pattern. Through analysis of 21,540,921 fire detections spanning one year (January 2023 - January 2024), I demonstrate a complete absence of low-confidence classifications during nighttime observations. Of 6,007,831 nighttime fires, zero were classified as low confidence, compared to an expected 696,908 under statistical independence (chi-squared = 1,474,795, p < 10^-15, Z = -833). This pattern persists globally across all months, latitude bands, and both NOAA-20 and Suomi-NPP satellites. Machine learning reverse-engineering (88.9% accuracy), bootstrap simulation (1,000 iterations), and spatial-temporal analysis confirm this is an algorithmic constraint rather than a geophysical phenomenon. Brightness temperature analysis reveals nighttime fires below approximately 295K are likely excluded entirely rather than flagged as low-confidence, while daytime fires show normal confidence distributions. This undocumented behavior affects 27.9% of all VIIRS fire detections and has significant implications for fire risk assessment, day-night detection comparisons, confidence-weighted analyses, and any research treating confidence levels as uncertainty metrics. I recommend explicit documentation of this algorithmic constraint in VIIRS user guides and reprocessing strategies for affected analyses.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GACA-DiT: Diffusion-based Dance-to-Music Generation with Genre-Adaptive Rhythm and Context-Aware Alignment</title>
<link>https://arxiv.org/abs/2510.26818</link>
<guid>https://arxiv.org/abs/2510.26818</guid>
<content:encoded><![CDATA[
<div> Diffusion transformer, Dance-to-music generation, Genre-adaptive rhythm extraction, Context-aware temporal alignment, AIST++ dataset<br />
Summary:<br />
The study introduces GACA-DiT, a diffusion transformer framework for Dance-to-music (D2M) generation. It includes two novel modules: genre-adaptive rhythm extraction and context-aware temporal alignment. The genre-adaptive rhythm extraction module captures fine-grained, genre-specific rhythm patterns using multi-scale temporal wavelet analysis and spatial phase histograms with adaptive joint weighting. The context-aware temporal alignment module resolves temporal mismatches through learnable context queries for aligning music latents with relevant dance rhythm features. Extensive experimentation on AIST++ and TikTok datasets demonstrates GACA-DiT's superior performance over existing methods in both objective metrics and human evaluation. The proposed framework ensures better rhythmic consistency and temporal alignment in music generation synchronized with dance movements. <br /> <div>
arXiv:2510.26818v1 Announce Type: cross 
Abstract: Dance-to-music (D2M) generation aims to automatically compose music that is rhythmically and temporally aligned with dance movements. Existing methods typically rely on coarse rhythm embeddings, such as global motion features or binarized joint-based rhythm values, which discard fine-grained motion cues and result in weak rhythmic alignment. Moreover, temporal mismatches introduced by feature downsampling further hinder precise synchronization between dance and music. To address these problems, we propose \textbf{GACA-DiT}, a diffusion transformer-based framework with two novel modules for rhythmically consistent and temporally aligned music generation. First, a \textbf{genre-adaptive rhythm extraction} module combines multi-scale temporal wavelet analysis and spatial phase histograms with adaptive joint weighting to capture fine-grained, genre-specific rhythm patterns. Second, a \textbf{context-aware temporal alignment} module resolves temporal mismatches using learnable context queries to align music latents with relevant dance rhythm features. Extensive experiments on the AIST++ and TikTok datasets demonstrate that GACA-DiT outperforms state-of-the-art methods in both objective metrics and human evaluation. Project page: https://beria-moon.github.io/GACA-DiT/.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See the Speaker: Crafting High-Resolution Talking Faces from Speech with Prior Guidance and Region Refinement</title>
<link>https://arxiv.org/abs/2510.26819</link>
<guid>https://arxiv.org/abs/2510.26819</guid>
<content:encoded><![CDATA[
<div> diffusion model, speech-to-face portrait generation, expressive dynamics, high-resolution outputs, Transformer-based discrete codebook

Summary:<br />
This work introduces a new method for speech-to-talking face synthesis that directly extracts information from speech, rather than relying on source images. The proposed approach involves a two-stage process. Firstly, a speech-conditioned diffusion model combined with statistical facial prior and sample-adaptive weighting generates high-quality face portraits from speech input. Next, expressive dynamics such as lip movement and facial expressions are embedded into the latent space of the diffusion model for speech-driven talking face generation. Lip synchronization is optimized using a region-enhancement module. Additionally, a pre-trained Transformer-based discrete codebook is integrated with an image rendering network to enhance video frame details for generating high-resolution outputs. Experimental results show superior performance on various datasets, demonstrating the method's ability to produce high-quality talking face videos exclusively from a single speech input.
 <div>
arXiv:2510.26819v1 Announce Type: cross 
Abstract: Unlike existing methods that rely on source images as appearance references and use source speech to generate motion, this work proposes a novel approach that directly extracts information from the speech, addressing key challenges in speech-to-talking face. Specifically, we first employ a speech-to-face portrait generation stage, utilizing a speech-conditioned diffusion model combined with statistical facial prior and a sample-adaptive weighting module to achieve high-quality portrait generation. In the subsequent speech-driven talking face generation stage, we embed expressive dynamics such as lip movement, facial expressions, and eye movements into the latent space of the diffusion model and further optimize lip synchronization using a region-enhancement module. To generate high-resolution outputs, we integrate a pre-trained Transformer-based discrete codebook with an image rendering network, enhancing video frame details in an end-to-end manner. Experimental results demonstrate that our method outperforms existing approaches on the HDTF, VoxCeleb, and AVSpeech datasets. Notably, this is the first method capable of generating high-resolution, high-quality talking face videos exclusively from a single speech input.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Corpus Validation of Speech Emotion Recognition in Urdu using Domain-Knowledge Acoustic Features</title>
<link>https://arxiv.org/abs/2510.26823</link>
<guid>https://arxiv.org/abs/2510.26823</guid>
<content:encoded><![CDATA[
<div> cross-corpus, Urdu, Speech Emotion Recognition, feature sets, logistic regression, multilayer perceptron

Summary:
The study focuses on Speech Emotion Recognition (SER) in the context of Urdu, a low-resource language. The research explores cross-corpus evaluation, which is essential for testing model generalization across different emotional speech datasets in Urdu. Two standard acoustic feature sets, eGeMAPS and ComParE, are used with Logistic Regression and Multilayer Perceptron classifiers for classification. Results indicate that self-corpus validation may overestimate performance compared to cross-corpus evaluation, emphasizing the importance of the latter for realistic model assessment. The study highlights the significance of cross-corpus validation in Urdu SER and its implications for advancing affective computing research in less-represented language communities.<br /><br />Summary: <div>
arXiv:2510.26823v1 Announce Type: cross 
Abstract: Speech Emotion Recognition (SER) is a key affective computing technology that enables emotionally intelligent artificial intelligence. While SER is challenging in general, it is particularly difficult for low-resource languages such as Urdu. This study investigates Urdu SER in a cross-corpus setting, an area that has remained largely unexplored. We employ a cross-corpus evaluation framework across three different Urdu emotional speech datasets to test model generalization. Two standard domain-knowledge based acoustic feature sets, eGeMAPS and ComParE, are used to represent speech signals as feature vectors which are then passed to Logistic Regression and Multilayer Perceptron classifiers. Classification performance is assessed using unweighted average recall (UAR) whilst considering class-label imbalance. Results show that Self-corpus validation often overestimates performance, with UAR exceeding cross-corpus evaluation by up to 13%, underscoring that cross-corpus evaluation offers a more realistic measure of model robustness. Overall, this work emphasizes the importance of cross-corpus validation for Urdu SER and its implications contribute to advancing affective computing research for underrepresented language communities.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMat-Synth: a multi-modal toolbox to curate broad synthesis procedure databases from scientific literature</title>
<link>https://arxiv.org/abs/2510.26824</link>
<guid>https://arxiv.org/abs/2510.26824</guid>
<content:encoded><![CDATA[
<div> extract, synthesis procedures, materials discovery, large language models, vision language models

Summary: 
- The paper introduces a multi-modal toolbox that uses large language models (LLMs) and vision language models (VLMs to automatically extract and organize synthesis procedures and performance data from materials science publications.
- The researchers curated 81k open-access papers to create the LeMat-Synth (v 1.0) dataset, containing synthesis procedures for 35 synthesis methods and 16 material classes.
- Extraction quality was rigorously evaluated on a subset of 2.5k synthesis procedures using expert annotations and a scalable LLM-as-a-judge framework.
- The study releases an open-source software library to support the extension of the dataset to new corpora and synthesis domains.
- The work aims to transform unstructured literature into machine-readable information, enabling predictive modeling of synthesis procedures and synthesis-structure-property relationships.

Summary: <div>
arXiv:2510.26824v1 Announce Type: cross 
Abstract: The development of synthesis procedures remains a fundamental challenge in materials discovery, with procedural knowledge scattered across decades of scientific literature in unstructured formats that are challenging for systematic analysis. In this paper, we propose a multi-modal toolbox that employs large language models (LLMs) and vision language models (VLMs) to automatically extract and organize synthesis procedures and performance data from materials science publications, covering text and figures. We curated 81k open-access papers, yielding LeMat-Synth (v 1.0): a dataset containing synthesis procedures spanning 35 synthesis methods and 16 material classes, structured according to an ontology specific to materials science. The extraction quality is rigorously evaluated on a subset of 2.5k synthesis procedures through a combination of expert annotations and a scalable LLM-as-a-judge framework. Beyond the dataset, we release a modular, open-source software library designed to support community-driven extension to new corpora and synthesis domains. Altogether, this work provides an extensible infrastructure to transform unstructured literature into machine-readable information. This lays the groundwork for predictive modeling of synthesis procedures as well as modeling synthesis--structure--property relationships.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R3GAN-based Optimal Strategy for Augmenting Small Medical Dataset</title>
<link>https://arxiv.org/abs/2510.26828</link>
<guid>https://arxiv.org/abs/2510.26828</guid>
<content:encoded><![CDATA[
<div> Generative Adversarial Networks, Data Scarcity, Class Imbalance, Deep Learning Models, Medical Imaging <br />
<br />Summary: <br /> This study explores the use of Generative Adversarial Networks (GANs) to address data scarcity and class imbalance in medical image analysis, focusing on human embryo time-lapse imaging (TLI). By optimizing R3GAN for small datasets, the researchers established effective training strategies for 256x256-resolution datasets. A specialized configuration with a full burn-in phase and a gradually increasing gamma range was designed, resulting in the generation of realistic and diagnostically meaningful images. Implementing the generated samples helped balance an imbalanced embryo dataset, leading to significant enhancements in classification performance. Specifically, the recall and F1-score for class t3 saw substantial improvements, showcasing the effectiveness of tailored R3GAN training strategies in alleviating data scarcity and enhancing model robustness in small-scale medical imaging tasks. <div>
arXiv:2510.26828v1 Announce Type: cross 
Abstract: Medical image analysis often suffers from data scarcity and class imbalance, limiting the effectiveness of deep learning models in clinical applications. Using human embryo time-lapse imaging (TLI) as a case study, this work investigates how generative adversarial networks (GANs) can be optimized for small datasets to generate realistic and diagnostically meaningful images. Based on systematic experiments with R3GAN, we established effective training strategies and designed an optimized configuration for 256x256-resolution datasets, featuring a full burn-in phase and a low, gradually increasing gamma range (5 -> 40). The generated samples were used to balance an imbalanced embryo dataset, leading to substantial improvement in classification performance. The recall and F1-score of t3 increased from 0.06 to 0.69 and 0.11 to 0.60, respectively, without compromising other classes. These results demonstrate that tailored R3GAN training strategies can effectively alleviate data scarcity and improve model robustness in small-scale medical imaging tasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes</title>
<link>https://arxiv.org/abs/2510.26833</link>
<guid>https://arxiv.org/abs/2510.26833</guid>
<content:encoded><![CDATA[
<div> Dataset, Traffic sign recognition, Model robustness, Adversarial attacks, Distribution shifts  
Summary:  
The VISAT dataset is introduced for evaluating model robustness in traffic sign recognition with visual attributes. It includes benchmarks focusing on adversarial attacks and distribution shifts. Adversarial attacks are tested using the PGD method, impacting popular models and revealing spurious correlations in attribute-specific multi-task learning networks. Distribution shift evaluations involve ImageNet-C techniques, exploring robustness in base and MTL models. Spurious correlations among MTL tasks are further studied through synthetic color alterations. Experiments compare the performance of ResNet-152 and ViT-B/32 backbones in base and MTL models. The VISAT dataset aims to enhance understanding of model robustness in traffic sign recognition, addressing challenges in real-world applications like autonomous driving and cyber-physical systems. <br /><br />Summary: <div>
arXiv:2510.26833v1 Announce Type: cross 
Abstract: We present VISAT, a novel open dataset and benchmarking suite for evaluating model robustness in the task of traffic sign recognition with the presence of visual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our dataset introduces two benchmarks that respectively emphasize robustness against adversarial attacks and distribution shifts. For our adversarial attack benchmark, we employ the state-of-the-art Projected Gradient Descent (PGD) method to generate adversarial inputs and evaluate their impact on popular models. Additionally, we investigate the effect of adversarial attacks on attribute-specific multi-task learning (MTL) networks, revealing spurious correlations among MTL tasks. The MTL networks leverage visual attributes (color, shape, symbol, and text) that we have created for each traffic sign in our dataset. For our distribution shift benchmark, we utilize ImageNet-C's realistic data corruption and natural variation techniques to perform evaluations on the robustness of both base and MTL models. Moreover, we further explore spurious correlations among MTL tasks through synthetic alterations of traffic sign colors using color quantization techniques. Our experiments focus on two major backbones, ResNet-152 and ViT-B/32, and compare the performance between base and MTL models. The VISAT dataset and benchmarking framework contribute to the understanding of model robustness for traffic sign recognition, shedding light on the challenges posed by adversarial attacks and distribution shifts. We believe this work will facilitate advancements in developing more robust models for real-world applications in autonomous driving and cyber-physical systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Driven Generation of Minimally Preprocessed Brain MRI</title>
<link>https://arxiv.org/abs/2510.26834</link>
<guid>https://arxiv.org/abs/2510.26834</guid>
<content:encoded><![CDATA[
<div> Keywords: DDPMs, 3D T1-weighted MRI, brain images, denoising, probabilistic models

Summary: 
This study presents and compares three denoising diffusion probabilistic models (DDPMs) for generating 3D T1-weighted MRI human brain images. Trained on a large dataset of images from various brain MRI datasets, the models produced coherent brain volumes with natural orientation variations. Evaluations included segmentation, Frechet Inception Distance (FID), and qualitative inspection. While all three DDPMs generated high-resolution brain images, they had higher FIDs compared to real images. Results showed statistically different brain regional volume distributions from real data, but the velocity and flow prediction models had fewer differences in certain regions. Overall, the DDPMs showed promise in generating realistic brain images without the need for skullstripping or registration. The model weights and inference code are publicly available at the provided GitHub repository. <br /><br />Summary: <div>
arXiv:2510.26834v1 Announce Type: cross 
Abstract: The purpose of this study is to present and compare three denoising diffusion probabilistic models (DDPMs) that generate 3D $T_1$-weighted MRI human brain images. Three DDPMs were trained using 80,675 image volumes from 42,406 subjects spanning 38 publicly available brain MRI datasets. These images had approximately 1 mm isotropic resolution and were manually inspected by three human experts to exclude those with poor quality, field-of-view issues, and excessive pathology. The images were minimally preprocessed to preserve the visual variability of the data. Furthermore, to enable the DDPMs to produce images with natural orientation variations and inhomogeneity, the images were neither registered to a common coordinate system nor bias field corrected. Evaluations included segmentation, Frechet Inception Distance (FID), and qualitative inspection. Regarding results, all three DDPMs generated coherent MR brain volumes. The velocity and flow prediction models achieved lower FIDs than the sample prediction model. However, all three models had higher FIDs compared to real images across multiple cohorts. In a permutation experiment, the generated brain regional volume distributions differed statistically from real data. However, the velocity and flow prediction models had fewer statistically different volume distributions in the thalamus and putamen. In conclusion this work presents and releases the first 3D non-latent diffusion model for brain data without skullstripping or registration. Despite the negative results in statistical testing, the presented DDPMs are capable of generating high-resolution 3D $T_1$-weighted brain images. All model weights and corresponding inference code are publicly available at https://github.com/piksl-research/medforj .
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Category-Aware Semantic Caching for Heterogeneous LLM Workloads</title>
<link>https://arxiv.org/abs/2510.26835</link>
<guid>https://arxiv.org/abs/2510.26835</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, query workloads, cache hit rates, vector databases, semantic caching <br />
Summary: 
LLM serving systems encounter diverse query workloads with varying characteristics such as query clustering, content staleness, and repetition patterns. Different query categories exhibit different cache hit rates, with high-repetition categories achieving significantly higher rates than low-repetition or volatile categories. The use of vector databases in caching is challenged by remote search costs and cache policies that may lead to inefficiencies. A category-aware semantic caching approach is introduced in this paper, where thresholds, TTLs, and quotas are tailored to each query category, improving cache coverage across the workload distribution. The hybrid architecture implemented separates in-memory search from external storage, reducing miss cost and making low-hit-rate categories economically viable. Adaptive load-based policies further enhance the framework by dynamically adjusting parameters based on downstream model load, reducing traffic to overloaded models. The proposed approach offers a comprehensive solution to optimize caching for heterogeneous query workloads. <br /><br />Summary: <div>
arXiv:2510.26835v1 Announce Type: cross 
Abstract: LLM serving systems process heterogeneous query workloads where different categories exhibit different characteristics. Code queries cluster densely in embedding space while conversational queries distribute sparsely. Content staleness varies from minutes (stock data) to months (code patterns). Query repetition patterns range from power-law (code) to uniform (conversation), producing long tail cache hit rate distributions: high-repetition categories achieve 40-60% hit rates while low-repetition or volatile categories achieve 5-15% hit rates. Vector databases must exclude the long tail because remote search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of production traffic uncached. Uniform cache policies compound this problem: fixed thresholds cause false positives in dense spaces and miss valid paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This paper presents category-aware semantic caching where similarity thresholds, TTLs, and quotas vary by query category. We present a hybrid architecture separating in-memory HNSW search from external document storage, reducing miss cost from 30ms to 2ms. This reduction makes low-hit-rate categories economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage across the entire workload distribution. Adaptive load-based policies extend this framework to respond to downstream model load, dynamically adjusting thresholds and TTLs to reduce traffic to overloaded models by 9-17% in theoretical projections.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification</title>
<link>https://arxiv.org/abs/2510.26840</link>
<guid>https://arxiv.org/abs/2510.26840</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, evaluation platform, equivalence verification, SQL subset, BIRD dataset<br />Summary: 
The study introduces a new evaluation method for Text-to-SQL performance assessment. The current test-based evaluation approach may overlook differences between generated and ground-truth SQL queries. The proposed SpotIt pipeline incorporates a formal verification engine to actively search for databases that distinguish between the queries. Techniques are developed to enhance verifiers for a more comprehensive SQL subset relevant to Text-to-SQL. An evaluation of ten Text-to-SQL methods on the BIRD dataset showcases discrepancies that test-based methods may not detect. The verification results shed light on the complexity of current Text-to-SQL evaluation practices. <div>
arXiv:2510.26840v1 Announce Type: cross 
Abstract: Community-driven Text-to-SQL evaluation platforms play a pivotal role in tracking the state of the art of Text-to-SQL performance. The reliability of the evaluation process is critical for driving progress in the field. Current evaluation methods are largely test-based, which involves comparing the execution results of a generated SQL query and a human-labeled ground-truth on a static test database. Such an evaluation is optimistic, as two queries can coincidentally produce the same output on the test database while actually being different. In this work, we propose a new alternative evaluation pipeline, called SpotIt, where a formal bounded equivalence verification engine actively searches for a database that differentiates the generated and ground-truth SQL queries. We develop techniques to extend existing verifiers to support a richer SQL subset relevant to Text-to-SQL. A performance evaluation of ten Text-to-SQL methods on the high-profile BIRD dataset suggests that test-based methods can often overlook differences between the generated query and the ground-truth. Further analysis of the verification results reveals a more complex picture of the current Text-to-SQL evaluation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility</title>
<link>https://arxiv.org/abs/2510.26841</link>
<guid>https://arxiv.org/abs/2510.26841</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Differentially Private, Fairness, Privacy, Model Utility

Summary: 
Federated Learning (FL) is a collaborative model training method that preserves data privacy. A new algorithm, FedPF, integrates differential privacy and fairness into the FL process as a zero-sum game. The study reveals an unexpected inverse relationship between privacy protection and the ability to detect and correct demographic biases. While privacy constraints can limit fairness, moderate fairness constraints can actually enhance model generalization before causing performance degradation. Experimental results demonstrate significant reductions in discrimination across datasets while maintaining competitive accuracy. However, the study underscores the inherent tension between privacy and fairness, emphasizing the need for balanced compromises rather than optimizing each objective in isolation. The source code for the proposed algorithm is available on GitHub at https://github.com/szpsunkk/FedPF.

Summary:<br />Theory reveals inverse relationship between privacy and fairness<br />Privacy limits ability to address demographic biases<br />Moderate fairness constraints enhance model generalization<br />Experimental results show reduced discrimination with competitive accuracy<br />Tension between privacy and fairness requires balanced compromises <div>
arXiv:2510.26841v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training without data sharing, yet participants face a fundamental challenge, e.g., simultaneously ensuring fairness across demographic groups while protecting sensitive client data. We introduce a differentially private fair FL algorithm (\textit{FedPF}) that transforms this multi-objective optimization into a zero-sum game where fairness and privacy constraints compete against model utility. Our theoretical analysis reveals a surprising inverse relationship, i.e., stricter privacy protection fundamentally limits the system's ability to detect and correct demographic biases, creating an inherent tension between privacy and fairness. Counterintuitively, we prove that moderate fairness constraints initially improve model generalization before causing performance degradation, where a non-monotonic relationship that challenges conventional wisdom about fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 % discrimination reduction across three datasets while maintaining competitive accuracy, but more importantly, reveals that the privacy-fairness tension is unavoidable, i.e., achieving both objectives simultaneously requires carefully balanced compromises rather than optimization of either in isolation. The source code for our proposed algorithm is publicly accessible at https://github.com/szpsunkk/FedPF.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs</title>
<link>https://arxiv.org/abs/2510.26843</link>
<guid>https://arxiv.org/abs/2510.26843</guid>
<content:encoded><![CDATA[
<div> Dynamic Tree Cascade, Speculative Decoding, Inference Acceleration, Large Language Models, Self-Speculative Decoding<br />
Summary:
- The paper introduces CAS-Spec, a novel Cascade Adaptive Self-Speculative Decoding method for accelerating inference in large language models (LLMs) through dynamically switchable inference acceleration strategies.
- CAS-Spec utilizes DSIA techniques like layer sparsity and activation quantization to construct speculative draft models, achieving a speedup of 1.1x to 2.3x over autoregressive decoding in various LLMs and datasets.
- The Dynamic Tree Cascade (DyTC) algorithm is introduced to enhance the efficiency of routing multi-level draft models and assigning draft lengths based on acceptance rates and latency prediction.
- CAS-Spec outperforms existing on-the-fly speculative decoding methods, improving average speedup by 47% and 48% over cascade-based baseline and tree-based algorithms.
- The method is easily integrable into most LLMs, showcasing promising potential for further acceleration as self-speculative decoding techniques progress. <br /><br />Summary: <div>
arXiv:2510.26843v1 Announce Type: cross 
Abstract: Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs). While on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by methods relying on specialized training. Cascading a hierarchy of draft models promises further acceleration and flexibility, but the high cost of training multiple models has limited its practical application. In this paper, we propose a novel Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs speculative draft models by leveraging dynamically switchable inference acceleration (DSIA) strategies, including layer sparsity and activation quantization. Furthermore, traditional vertical and horizontal cascade algorithms are inefficient when applied to self-speculative decoding methods. We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the multi-level draft models and assigns the draft lengths, based on the heuristics of acceptance rates and latency prediction. Our CAS-Spec method achieves state-of-the-art acceleration compared to existing on-the-fly speculative decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over autoregressive decoding across various LLMs and datasets. DyTC improves the average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based baseline algorithms, respectively. CAS-Spec can be easily integrated into most existing LLMs and holds promising potential for further acceleration as self-speculative decoding techniques continue to evolve.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token</title>
<link>https://arxiv.org/abs/2510.26847</link>
<guid>https://arxiv.org/abs/2510.26847</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Jailbreak Attacks, CPT-Filtering, Byte-Pair Encoding, Text Filtering <br />
Summary: 
CPT-Filtering is a new technique designed to protect against jailbreak attacks on Large Language Models. These attacks involve disguising malicious prompts using ciphers and character-level encodings to bypass safety measures. The method leverages Byte-Pair Encoding tokenizers to identify out-of-distribution text, such as ciphers, based on the average number of Characters Per Token (CPT) in the text. This model-agnostic approach is cost-effective and highly accurate, providing a practical defense for real-time text filtering and data curation. Experimentation with various encoding schemes and tokenizers on a large dataset demonstrated the robustness of CPT-Filtering in identifying encoded text with high accuracy, even for short inputs. The technique is a valuable addition to mitigating the risks associated with jailbreak attacks on LLMs, offering a reliable defense mechanism against malicious prompts. <br /><br />Summary: <div>
arXiv:2510.26847v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where malicious prompts are disguised using ciphers and character-level encodings to bypass safety guardrails. While these guardrails often fail to interpret the encoded content, the underlying models can still process the harmful instructions. We introduce CPT-Filtering, a novel, model-agnostic with negligible-costs and near-perfect accuracy guardrail technique that aims to mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair Encoding (BPE) tokenizers. Our method is based on the principle that tokenizers, trained on natural language, represent out-of-distribution text, such as ciphers, using a significantly higher number of shorter tokens. Our technique uses a simple yet powerful artifact of using language models: the average number of Characters Per Token (CPT) in the text. This approach is motivated by the high compute cost of modern methods - relying on added modules such as dedicated LLMs or perplexity models. We validate our approach across a large dataset of over 100,000 prompts, testing numerous encoding schemes with several popular tokenizers. Our experiments demonstrate that a simple CPT threshold robustly identifies encoded text with high accuracy, even for very short inputs. CPT-Filtering provides a practical defense layer that can be immediately deployed for real-time text filtering and offline data curation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Foundation Models for Enhancing Robot Perception and Action</title>
<link>https://arxiv.org/abs/2510.26855</link>
<guid>https://arxiv.org/abs/2510.26855</guid>
<content:encoded><![CDATA[
<div> Localization, interaction, manipulation, foundation models, unstructured environments
Summary:
This thesis explores the integration of foundation models to enhance robotic capabilities for improved localization, interaction, and manipulation in unstructured environments. The research is structured around four key areas focusing on addressing fundamental challenges in robotics. By leveraging foundation models, the thesis aims to develop semantics-aware robotic intelligence that can effectively navigate and operate in complex, real-world scenarios. The study emphasizes the systematic approach to utilizing foundation models to enhance robotic functionalities, ultimately contributing to a cohesive framework for advancing robotic intelligence. Through a comprehensive investigation, the thesis aims to push the boundaries of robotics by incorporating cutting-edge technologies and methodologies to improve robot performance in challenging environments. <div>
arXiv:2510.26855v1 Announce Type: cross 
Abstract: This thesis investigates how foundation models can be systematically leveraged to enhance robotic capabilities, enabling more effective localization, interaction, and manipulation in unstructured environments. The work is structured around four core lines of inquiry, each addressing a fundamental challenge in robotics while collectively contributing to a cohesive framework for semantics-aware robotic intelligence.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench</title>
<link>https://arxiv.org/abs/2510.26865</link>
<guid>https://arxiv.org/abs/2510.26865</guid>
<content:encoded><![CDATA[
arXiv:2510.26865v1 Announce Type: cross 
Abstract: Reading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs</title>
<link>https://arxiv.org/abs/2510.26892</link>
<guid>https://arxiv.org/abs/2510.26892</guid>
<content:encoded><![CDATA[
arXiv:2510.26892v1 Announce Type: cross 
Abstract: Generative Adversarial Networks (GANs) are proficient at generating synthetic data but continue to suffer from mode collapse, where the generator produces a narrow range of outputs that fool the discriminator but fail to capture the full data distribution. This limitation is particularly problematic, as generative models are increasingly deployed in real-world applications that demand both diversity and uncertainty awareness. In response, we introduce BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty into the generative process while maintaining computational efficiency. BI-DCGAN integrates Bayes by Backprop to learn a distribution over network weights and employs mean-field variational inference to efficiently approximate the posterior distribution during GAN training. We establishes the first theoretical proof, based on covariance matrix analysis, that Bayesian modeling enhances sample diversity in GANs. We validate this theoretical result through extensive experiments on standard generative benchmarks, demonstrating that BI-DCGAN produces more diverse and robust outputs than conventional DCGANs, while maintaining training efficiency. These findings position BI-DCGAN as a scalable and timely solution for applications where both diversity and uncertainty are critical, and where modern alternatives like diffusion models remain too resource-intensive.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Similar Are Grokipedia and Wikipedia? A Multi-Dimensional Textual and Structural Comparison</title>
<link>https://arxiv.org/abs/2510.26899</link>
<guid>https://arxiv.org/abs/2510.26899</guid>
<content:encoded><![CDATA[
arXiv:2510.26899v1 Announce Type: cross 
Abstract: The launch of Grokipedia, an AI-generated encyclopedia developed by Elon Musk's xAI, was presented as a response to perceived ideological and structural biases in Wikipedia, aiming to produce "truthful" entries via the large language model Grok. Yet whether an AI-driven alternative can escape the biases and limitations of human-edited platforms remains unclear. This study undertakes a large-scale computational comparison of 382 matched article pairs between Grokipedia and Wikipedia. Using metrics across lexical richness, readability, structural organization, reference density, and semantic similarity, we assess how closely the two platforms align in form and substance. The results show that while Grokipedia exhibits strong semantic and stylistic alignment with Wikipedia, it typically produces longer but less lexically diverse articles, with fewer references per word and more variable structural depth. These findings suggest that AI-generated encyclopedic content currently mirrors Wikipedia's informational scope but diverges in editorial norms, favoring narrative expansion over citation-based verification. The implications highlight new tensions around transparency, provenance, and the governance of knowledge in an era of automated text generation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence</title>
<link>https://arxiv.org/abs/2510.26915</link>
<guid>https://arxiv.org/abs/2510.26915</guid>
<content:encoded><![CDATA[
arXiv:2510.26915v1 Announce Type: cross 
Abstract: Heterogeneous robot teams operating in realistic settings often must accomplish complex missions requiring collaboration and adaptation to information acquired online. Because robot teams frequently operate in unstructured environments -- uncertain, open-world settings without prior maps -- subtasks must be grounded in robot capabilities and the physical world. While heterogeneous teams have typically been designed for fixed specifications, generative intelligence opens the possibility of teams that can accomplish a wide range of missions described in natural language. However, current large language model (LLM)-enabled teaming methods typically assume well-structured and known environments, limiting deployment in unstructured environments. We present SPINE-HT, a framework that addresses these limitations by grounding the reasoning abilities of LLMs in the context of a heterogeneous robot team through a three-stage process. Given language specifications describing mission goals and team capabilities, an LLM generates grounded subtasks which are validated for feasibility. Subtasks are then assigned to robots based on capabilities such as traversability or perception and refined given feedback collected during online operation. In simulation experiments with closed-loop perception and control, our framework achieves nearly twice the success rate compared to prior LLM-enabled heterogeneous teaming approaches. In real-world experiments with a Clearpath Jackal, a Clearpath Husky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an 87\% success rate in missions requiring reasoning about robot capabilities and refining subtasks with online feedback. More information is provided at https://zacravichandran.github.io/SPINE-HT.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-Aware Curriculum Learning for Ddata-Efficient Lung Nodule Detection with YOLOv11</title>
<link>https://arxiv.org/abs/2510.26923</link>
<guid>https://arxiv.org/abs/2510.26923</guid>
<content:encoded><![CDATA[
arXiv:2510.26923v1 Announce Type: cross 
Abstract: Lung nodule detection in chest CT is crucial for early lung cancer diagnosis, yet existing deep learning approaches face challenges when deployed in clinical settings with limited annotated data. While curriculum learning has shown promise in improving model training, traditional static curriculum strategies fail in data-scarce scenarios. We propose Scale Adaptive Curriculum Learning (SACL), a novel training strategy that dynamically adjusts curriculum design based on available data scale. SACL introduces three key mechanisms:(1) adaptive epoch scheduling, (2) hard sample injection, and (3) scale-aware optimization. We evaluate SACL on the LUNA25 dataset using YOLOv11 as the base detector. Experimental results demonstrate that while SACL achieves comparable performance to static curriculum learning on the full dataset in mAP50, it shows significant advantages under data-limited conditions with 4.6%, 3.5%, and 2.0% improvements over baseline at 10%, 20%, and 50% of training data respectively. By enabling robust training across varying data scales without architectural modifications, SACL provides a practical solution for healthcare institutions to develop effective lung nodule detection systems despite limited annotation resources.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification</title>
<link>https://arxiv.org/abs/2510.26935</link>
<guid>https://arxiv.org/abs/2510.26935</guid>
<content:encoded><![CDATA[
arXiv:2510.26935v1 Announce Type: cross 
Abstract: As AI systems migrate to safety-critical domains, verifying that their actions comply with well-defined rules remains a challenge. Formal methods provide provable guarantees but demand hand-crafted temporal-logic specifications, offering limited expressiveness and accessibility. Deep learning approaches enable evaluation of plans against natural-language constraints, yet their opaque decision process invites misclassifications with potentially severe consequences. We introduce RepV, a neurosymbolic verifier that unifies both views by learning a latent space where safe and unsafe plans are linearly separable. Starting from a modest seed set of plans labeled by an off-the-shelf model checker, RepV trains a lightweight projector that embeds each plan, together with a language model-generated rationale, into a low-dimensional space; a frozen linear boundary then verifies compliance for unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the likelihood of correct verification based on its position in the latent space. This guarantee enables a guarantee-driven refinement of the planner, improving rule compliance without human annotations. Empirical evaluations show that RepV improves compliance prediction accuracy by up to 15% compared to baseline methods while adding fewer than 0.2M parameters. Furthermore, our refinement framework outperforms ordinary fine-tuning baselines across various planning domains. These results show that safety-separable latent spaces offer a scalable, plug-and-play primitive for reliable neurosymbolic plan verification. Code and data are available at: https://repv-project.github.io/.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction</title>
<link>https://arxiv.org/abs/2510.26940</link>
<guid>https://arxiv.org/abs/2510.26940</guid>
<content:encoded><![CDATA[
arXiv:2510.26940v1 Announce Type: cross 
Abstract: Next location prediction underpins a growing number of mobility, retail, and public-health applications, yet its societal impacts remain largely unexplored. In this paper, we audit state-of-the-art mobility prediction models trained on a large-scale dataset, highlighting hidden disparities based on user demographics. Drawing from aggregate census data, we compute the difference in predictive performance on racial and ethnic user groups and show a systematic disparity resulting from the underlying dataset, resulting in large differences in accuracy based on location and user groups. To address this, we propose Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy designed for incremental data collection settings. Because individual-level demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a clustering method that partitions users in latent mobility space while enforcing census-derived group proportions. This yields proxy racial labels for the four largest groups in the state: Asian, Black, Hispanic, and White. Built on these labels, our sampling algorithm prioritizes users based on expected performance gains and current group representation. This method incrementally constructs training datasets that reduce demographic performance gaps while preserving overall accuracy. Our method reduces total disparity between groups by up to 40\% with minimal accuracy trade-offs, as evaluated on a state-of-art MetaPath2Vec model and a transformer-encoder model. Improvements are most significant in early sampling stages, highlighting the potential for fairness-aware strategies to deliver meaningful gains even in low-resource settings. Our findings expose structural inequities in mobility prediction pipelines and demonstrate how lightweight, data-centric interventions can improve fairness with little added complexity, especially for low-data applications.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks</title>
<link>https://arxiv.org/abs/2510.26941</link>
<guid>https://arxiv.org/abs/2510.26941</guid>
<content:encoded><![CDATA[
arXiv:2510.26941v1 Announce Type: cross 
Abstract: The Internet of Things has expanded rapidly, transforming communication and operations across industries but also increasing the attack surface and security breaches. Artificial Intelligence plays a key role in securing IoT, enabling attack detection, attack behavior analysis, and mitigation suggestion. Despite advancements, evaluations remain purely qualitative, and the lack of a standardized, objective benchmark for quantitatively measuring AI-based attack analysis and mitigation hinders consistent assessment of model effectiveness. In this work, we propose a hybrid framework combining Machine Learning (ML) for multi-class attack detection with Large Language Models (LLMs) for attack behavior analysis and mitigation suggestion. After benchmarking several ML and Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we applied structured role-play prompt engineering with Retrieval-Augmented Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed, context-aware responses. We introduce novel evaluation metrics for quantitative assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o, DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the responses. Results show that Random Forest has the best detection model, and ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can machines think efficiently?</title>
<link>https://arxiv.org/abs/2510.26954</link>
<guid>https://arxiv.org/abs/2510.26954</guid>
<content:encoded><![CDATA[
arXiv:2510.26954v1 Announce Type: cross 
Abstract: The Turing Test is no longer adequate for distinguishing human and machine intelligence. With advanced artificial intelligence systems already passing the original Turing Test and contributing to serious ethical and environmental concerns, we urgently need to update the test. This work expands upon the original imitation game by accounting for an additional factor: the energy spent answering the questions. By adding the constraint of energy, the new test forces us to evaluate intelligence through the lens of efficiency, connecting the abstract problem of thinking to the concrete reality of finite resources. Further, this proposed new test ensures the evaluation of intelligence has a measurable, practical finish line that the original test lacks. This additional constraint compels society to weigh the time savings of using artificial intelligence against its total resource cost.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Salient Object Detection to Identify Manipulative Cookie Banners that Circumvent GDPR</title>
<link>https://arxiv.org/abs/2510.26967</link>
<guid>https://arxiv.org/abs/2510.26967</guid>
<content:encoded><![CDATA[
arXiv:2510.26967v1 Announce Type: cross 
Abstract: The main goal of this paper is to study how often cookie banners that comply with the General Data Protection Regulation (GDPR) contain aesthetic manipulation, a design tactic to draw users' attention to the button that permits personal data sharing. As a byproduct of this goal, we also evaluate how frequently the banners comply with GDPR and the recommendations of national data protection authorities regarding banner designs. We visited 2,579 websites and identified the type of cookie banner implemented. Although 45% of the relevant websites have fully compliant banners, we found aesthetic manipulation on 38% of the compliant banners. Unlike prior studies of aesthetic manipulation, we use a computer vision model for salient object detection to measure how salient (i.e., attention-drawing) each banner element is. This enables the discovery of new types of aesthetic manipulation (e.g., button placement), and leads us to conclude that aesthetic manipulation is more common than previously reported (38% vs 27% of banners). To study the effects of user and/or website location on cookie banner design, we include websites within the European Union (EU), where privacy regulation enforcement is more stringent, and websites outside the EU. We visited websites from IP addresses in the EU and from IP addresses in the United States (US). We find that 13.9% of EU websites change their banner design when the user is from the US, and EU websites are roughly 48.3% more likely to use aesthetic manipulation than non-EU websites, highlighting their innovative responses to privacy regulation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence</title>
<link>https://arxiv.org/abs/2510.26969</link>
<guid>https://arxiv.org/abs/2510.26969</guid>
<content:encoded><![CDATA[
arXiv:2510.26969v1 Announce Type: cross 
Abstract: We introduce a methodology for the identification of notifiable events in the domain of healthcare. The methodology harnesses semantic frames to define fine-grained patterns and search them in unstructured data, namely, open-text fields in e-medical records. We apply the methodology to the problem of underreporting of gender-based violence (GBV) in e-medical records produced during patients' visits to primary care units. A total of eight patterns are defined and searched on a corpus of 21 million sentences in Brazilian Portuguese extracted from e-SUS APS. The results are manually evaluated by linguists and the precision of each pattern measured. Our findings reveal that the methodology effectively identifies reports of violence with a precision of 0.726, confirming its robustness. Designed as a transparent, efficient, low-carbon, and language-agnostic pipeline, the approach can be easily adapted to other health surveillance contexts, contributing to the broader, ethical, and explainable use of NLP in public health systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations</title>
<link>https://arxiv.org/abs/2510.26974</link>
<guid>https://arxiv.org/abs/2510.26974</guid>
<content:encoded><![CDATA[
arXiv:2510.26974v1 Announce Type: cross 
Abstract: Clinical documentation increasingly uses automatic speech recognition and summarization, yet converting conversations into actionable medical orders for Electronic Health Records remains unexplored. A solution to this problem can significantly reduce the documentation burden of clinicians and directly impact downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first challenge on extracting medical orders from doctor-patient conversations. Six teams participated in the shared task and experimented with a broad range of approaches, and both closed- and open-weight large language models (LLMs). In this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking, and participants' solutions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget</title>
<link>https://arxiv.org/abs/2510.26981</link>
<guid>https://arxiv.org/abs/2510.26981</guid>
<content:encoded><![CDATA[
arXiv:2510.26981v1 Announce Type: cross 
Abstract: This work tackles a critical challenge in AI safety research under limited compute: given a fixed computation budget, how can one maximize the strength of iterative adversarial attacks? Coarsely reducing the number of attack iterations lowers cost but substantially weakens effectiveness. To fulfill the attainable attack efficacy within a constrained budget, we propose a fine-grained control mechanism that selectively recomputes layer activations across both iteration-wise and layer-wise levels. Extensive experiments show that our method consistently outperforms existing baselines at equal cost. Moreover, when integrated into adversarial training, it attains comparable performance with only 30% of the original budget.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs are Overconfident: Evaluating Confidence Interval Calibration with FermiEval</title>
<link>https://arxiv.org/abs/2510.26995</link>
<guid>https://arxiv.org/abs/2510.26995</guid>
<content:encoded><![CDATA[
arXiv:2510.26995v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at numerical estimation but struggle to correctly quantify uncertainty. We study how well LLMs construct confidence intervals around their own answers and find that they are systematically overconfident. To evaluate this behavior, we introduce FermiEval, a benchmark of Fermi-style estimation questions with a rigorous scoring rule for confidence interval coverage and sharpness. Across several modern models, nominal 99\% intervals cover the true answer only 65\% of the time on average. With a conformal prediction based approach that adjusts the intervals, we obtain accurate 99\% observed coverage, and the Winkler interval score decreases by 54\%. We also propose direct log-probability elicitation and quantile adjustment methods, which further reduce overconfidence at high confidence levels. Finally, we develop a perception-tunnel theory explaining why LLMs exhibit overconfidence: when reasoning under uncertainty, they act as if sampling from a truncated region of their inferred distribution, neglecting its tails.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIOT based Smart Education System: A Dual Layer Authentication and Context-Aware Tutoring Framework for Learning Environments</title>
<link>https://arxiv.org/abs/2510.26999</link>
<guid>https://arxiv.org/abs/2510.26999</guid>
<content:encoded><![CDATA[
arXiv:2510.26999v1 Announce Type: cross 
Abstract: The AIoT-Based Smart Education System integrates Artificial Intelligence and IoT to address persistent challenges in contemporary classrooms: attendance fraud, lack of personalization, student disengagement, and inefficient resource use. The unified platform combines four core modules: (1) a dual-factor authentication system leveraging RFID-based ID scans and WiFi verification for secure, fraud-resistant attendance; (2) an AI-powered assistant that provides real-time, context-aware support and dynamic quiz generation based on instructor-supplied materials; (3) automated test generators to streamline adaptive assessment and reduce administrative overhead; and (4) the EcoSmart Campus module, which autonomously regulates classroom lighting, air quality, and temperature using IoT sensors and actuators. Simulated evaluations demonstrate the system's effectiveness in delivering robust real-time monitoring, fostering inclusive engagement, preventing fraudulent practices, and supporting operational scalability. Collectively, the AIoT-Based Smart Education System offers a secure, adaptive, and efficient learning environment, providing a scalable blueprint for future educational innovation and improved student outcomes through the synergistic application of artificial intelligence and IoT technologies.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms</title>
<link>https://arxiv.org/abs/2510.27001</link>
<guid>https://arxiv.org/abs/2510.27001</guid>
<content:encoded><![CDATA[
arXiv:2510.27001v1 Announce Type: cross 
Abstract: Multi-armed bandit (MAB) problems serve as a fundamental building block for more complex reinforcement learning algorithms. However, evaluating and comparing MAB algorithms remains challenging due to the lack of standardized conditions and replicability. This is particularly problematic for variance-aware extensions of classical methods like UCB, whose performance can heavily depend on the underlying environment. In this study, we address how performance differences between bandit algorithms can be reliably observed, and under what conditions variance-aware algorithms outperform classical ones. We present a reproducible evaluation designed to systematically compare eight classical and variance-aware MAB algorithms. The evaluation framework, implemented in our Bandit Playground codebase, features clearly defined experimental setups, multiple performance metrics (reward, regret, reward distribution, value-at-risk, and action optimality), and an interactive evaluation interface that supports consistent and transparent analysis. We show that variance-aware algorithms can offer advantages in settings with high uncertainty where the difficulty arises from subtle differences between arm rewards. In contrast, classical algorithms often perform equally well or better in more separable scenarios or if fine-tuned extensively. Our contributions are twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2) insights into the conditions under which variance-aware approaches outperform their classical counterparts.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase</title>
<link>https://arxiv.org/abs/2510.27002</link>
<guid>https://arxiv.org/abs/2510.27002</guid>
<content:encoded><![CDATA[
arXiv:2510.27002v1 Announce Type: cross 
Abstract: While world models are increasingly positioned as a pathway to overcoming data scarcity in domains such as robotics, open training infrastructure for world modeling remains nascent. We introduce Jasmine, a performant JAX-based world modeling codebase that scales from single hosts to hundreds of accelerators with minimal code changes. Jasmine achieves an order-of-magnitude faster reproduction of the CoinRun case study compared to prior open implementations, enabled by performance optimizations across data loading, training and checkpointing. The codebase guarantees fully reproducible training and supports diverse sharding configurations. By pairing Jasmine with curated large-scale datasets, we establish infrastructure for rigorous benchmarking pipelines across model families and architectural ablations.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics</title>
<link>https://arxiv.org/abs/2510.27033</link>
<guid>https://arxiv.org/abs/2510.27033</guid>
<content:encoded><![CDATA[
arXiv:2510.27033v1 Announce Type: cross 
Abstract: Visual reasoning, particularly spatial reasoning, is a challenging cognitive task that requires understanding object relationships and their interactions within complex environments, especially in robotics domain. Existing vision_language models (VLMs) excel at perception tasks but struggle with fine-grained spatial reasoning due to their implicit, correlation-driven reasoning and reliance solely on images. We propose a novel neuro_symbolic framework that integrates both panoramic-image and 3D point cloud information, combining neural perception with symbolic reasoning to explicitly model spatial and logical relationships. Our framework consists of a perception module for detecting entities and extracting attributes, and a reasoning module that constructs a structured scene graph to support precise, interpretable queries. Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superior performance and reliability in crowded, human_built environments while maintaining a lightweight design suitable for robotics and embodied AI applications.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elastic Architecture Search for Efficient Language Models</title>
<link>https://arxiv.org/abs/2510.27037</link>
<guid>https://arxiv.org/abs/2510.27037</guid>
<content:encoded><![CDATA[
arXiv:2510.27037v1 Announce Type: cross 
Abstract: As large pre-trained language models become increasingly critical to natural language understanding (NLU) tasks, their substantial computational and memory requirements have raised significant economic and environmental concerns. Addressing these challenges, this paper introduces the Elastic Language Model (ELM), a novel neural architecture search (NAS) method optimized for compact language models. ELM extends existing NAS approaches by introducing a flexible search space with efficient transformer blocks and dynamic modules for dimension and head number adjustment. These innovations enhance the efficiency and flexibility of the search process, which facilitates more thorough and effective exploration of model architectures. We also introduce novel knowledge distillation losses that preserve the unique characteristics of each block, in order to improve the discrimination between architectural choices during the search process. Experiments on masked language modeling and causal language modeling tasks demonstrate that models discovered by ELM significantly outperform existing methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Creation and Baseline Models for Sexism Detection in Hausa</title>
<link>https://arxiv.org/abs/2510.27038</link>
<guid>https://arxiv.org/abs/2510.27038</guid>
<content:encoded><![CDATA[
arXiv:2510.27038v1 Announce Type: cross 
Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating stereotypes, bias, and discriminatory norms. Noting how online platforms enable various forms of sexism to thrive, there is a growing need for effective sexism detection and mitigation strategies. While computational approaches to sexism detection are widespread in high-resource languages, progress remains limited in low-resource languages where limited linguistic resources and cultural differences affect how sexism is expressed and perceived. This study introduces the first Hausa sexism detection dataset, developed through community engagement, qualitative coding, and data augmentation. For cultural nuances and linguistic representation, we conducted a two-stage user study (n=66) involving native speakers to explore how sexism is defined and articulated in everyday discourse. We further experiment with both traditional machine learning classifiers and pre-trained multilingual language models and evaluating the effectiveness few-shot learning in detecting sexism in Hausa. Our findings highlight challenges in capturing cultural nuance, particularly with clarification-seeking and idiomatic expressions, and reveal a tendency for many false positives in such cases.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Data Contamination in LLMs via In-Context Learning</title>
<link>https://arxiv.org/abs/2510.27055</link>
<guid>https://arxiv.org/abs/2510.27055</guid>
<content:encoded><![CDATA[
arXiv:2510.27055v1 Announce Type: cross 
Abstract: We present Contamination Detection via Context (CoDeC), a practical and accurate method to detect and quantify training data contamination in large language models. CoDeC distinguishes between data memorized during training and data outside the training distribution by measuring how in-context learning affects model performance. We find that in-context examples typically boost confidence for unseen datasets but may reduce it when the dataset was part of training, due to disrupted memorization patterns. Experiments show that CoDeC produces interpretable contamination scores that clearly separate seen and unseen datasets, and reveals strong evidence of memorization in open-weight models with undisclosed training corpora. The method is simple, automated, and both model- and dataset-agnostic, making it easy to integrate with benchmark evaluations.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency Training Helps Stop Sycophancy and Jailbreaks</title>
<link>https://arxiv.org/abs/2510.27062</link>
<guid>https://arxiv.org/abs/2510.27062</guid>
<content:encoded><![CDATA[
arXiv:2510.27062v1 Announce Type: cross 
Abstract: An LLM's factuality and refusal training can be compromised by simple changes to a prompt. Models often adopt user beliefs (sycophancy) or satisfy inappropriate requests which are wrapped within special text (jailbreaking). We explore \emph{consistency training}, a self-supervised paradigm that teaches a model to be invariant to certain irrelevant cues in the prompt. Instead of teaching the model what exact response to give on a particular prompt, we aim to teach the model to behave identically across prompt data augmentations (like adding leading questions or jailbreak text). We try enforcing this invariance in two ways: over the model's external outputs (\emph{Bias-augmented Consistency Training} (BCT) from Chua et al. [2025]) and over its internal activations (\emph{Activation Consistency Training} (ACT), a method we introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant cues. Because consistency training uses responses from the model itself as training data, it avoids issues that arise from stale training data, such as degrading model capabilities or enforcing outdated response guidelines. While BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak reduction. We think that BCT can simplify training pipelines by removing reliance on static datasets. We argue that some alignment problems are better viewed not in terms of optimal responses, but rather as consistency issues.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Measure of Algorithm Similarity</title>
<link>https://arxiv.org/abs/2510.27063</link>
<guid>https://arxiv.org/abs/2510.27063</guid>
<content:encoded><![CDATA[
arXiv:2510.27063v1 Announce Type: cross 
Abstract: Given two algorithms for the same problem, can we determine whether they are meaningfully different? In full generality, the question is uncomputable, and empirically it is muddied by competing notions of similarity. Yet, in many applications (such as clone detection or program synthesis) a pragmatic and consistent similarity metric is necessary. We review existing equivalence and similarity notions and introduce EMOC: An Evaluation-Memory-Operations-Complexity framework that embeds algorithm implementations into a feature space suitable for downstream tasks. We compile PACD, a curated dataset of verified Python implementations across three problems, and show that EMOC features support clustering and classification of algorithm types, detection of near-duplicates, and quantification of diversity in LLM-generated programs. Code, data, and utilities for computing EMOC embeddings are released to facilitate reproducibility and future work on algorithm similarity.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2510.27080</link>
<guid>https://arxiv.org/abs/2510.27080</guid>
<content:encoded><![CDATA[
arXiv:2510.27080v1 Announce Type: cross 
Abstract: Security applications are increasingly relying on large language models (LLMs) for cyber threat detection; however, their opaque reasoning often limits trust, particularly in decisions that require domain-specific cybersecurity knowledge. Because security threats evolve rapidly, LLMs must not only recall historical incidents but also adapt to emerging vulnerabilities and attack patterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in general LLM applications, but its potential for cybersecurity remains underexplored. In this work, we introduce a RAG-based framework designed to contextualize cybersecurity data and enhance LLM accuracy in knowledge retention and temporal reasoning. Using external datasets and the Llama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid retrieval approach, and conduct a comparative analysis across multiple performance metrics. Our findings highlight the promise of hybrid retrieval in strengthening the adaptability and reliability of LLMs for cybersecurity tasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2510.27091</link>
<guid>https://arxiv.org/abs/2510.27091</guid>
<content:encoded><![CDATA[
arXiv:2510.27091v1 Announce Type: cross 
Abstract: Quantum theory provides non-classical principles, such as superposition and entanglement, that inspires promising paradigms in machine learning. However, most existing quantum-inspired fusion models rely solely on unitary or unitary-like transformations to generate quantum entanglement. While theoretically expressive, such approaches often suffer from training instability and limited generalizability. In this work, we propose a Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal entanglement modelling. Each modality is firstly encoded as a quantum pure state, after which a differentiable module simulating the QJ operator transforms the separable product state into the entangled representation. By jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates controllable cross-modal entanglement among modalities with dissipative dynamics, where structured stochasticity and steady-state attractor properties serve to stabilize training and constrain entanglement shaping. The resulting entangled states are projected onto trainable measurement vectors to produce predictions. In addition to achieving superior performance over the state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI, and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through von-Neumann entanglement entropy. This work establishes a principled framework for entangled multimodal fusion and paves the way for quantum-inspired approaches in modelling complex cross-modal correlations.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expressive Range Characterization of Open Text-to-Audio Models</title>
<link>https://arxiv.org/abs/2510.27102</link>
<guid>https://arxiv.org/abs/2510.27102</guid>
<content:encoded><![CDATA[
arXiv:2510.27102v1 Announce Type: cross 
Abstract: Text-to-audio models are a type of generative model that produces audio output in response to a given textual prompt. Although level generators and the properties of the functional content that they create (e.g., playability) dominate most discourse in procedurally generated content (PCG), games that emotionally resonate with players tend to weave together a range of creative and multimodal content (e.g., music, sounds, visuals, narrative tone), and multimodal models have begun seeing at least experimental use for this purpose. However, it remains unclear what exactly such models generate, and with what degree of variability and fidelity: audio is an extremely broad class of output for a generative system to target.
  Within the PCG community, expressive range analysis (ERA) has been used as a quantitative way to characterize generators' output space, especially for level generators. This paper adapts ERA to text-to-audio models, making the analysis tractable by looking at the expressive range of outputs for specific, fixed prompts. Experiments are conducted by prompting the models with several standardized prompts derived from the Environmental Sound Classification (ESC-50) dataset. The resulting audio is analyzed along key acoustic dimensions (e.g., pitch, loudness, and timbre). More broadly, this paper offers a framework for ERA-based exploratory evaluation of generative audio models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys</title>
<link>https://arxiv.org/abs/2510.27126</link>
<guid>https://arxiv.org/abs/2510.27126</guid>
<content:encoded><![CDATA[
arXiv:2510.27126v1 Announce Type: cross 
Abstract: Conventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.12 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding</title>
<link>https://arxiv.org/abs/2510.27128</link>
<guid>https://arxiv.org/abs/2510.27128</guid>
<content:encoded><![CDATA[
arXiv:2510.27128v1 Announce Type: cross 
Abstract: Recent advances in neural decoding have enabled the reconstruction of visual experiences from brain activity, positioning fMRI-to-image reconstruction as a promising bridge between neuroscience and computer vision. However, current methods predominantly rely on subject-specific models or require subject-specific fine-tuning, limiting their scalability and real-world applicability. In this work, we introduce ZEBRA, the first zero-shot brain visual decoding framework that eliminates the need for subject-specific adaptation. ZEBRA is built on the key insight that fMRI representations can be decomposed into subject-related and semantic-related components. By leveraging adversarial training, our method explicitly disentangles these components to isolate subject-invariant, semantic-specific representations. This disentanglement allows ZEBRA to generalize to unseen subjects without any additional fMRI data or retraining. Extensive experiments show that ZEBRA significantly outperforms zero-shot baselines and achieves performance comparable to fully finetuned models on several metrics. Our work represents a scalable and practical step toward universal neural decoding. Code and model weights are available at: https://github.com/xmed-lab/ZEBRA.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Landscapes for Better Minima along Valleys</title>
<link>https://arxiv.org/abs/2510.27153</link>
<guid>https://arxiv.org/abs/2510.27153</guid>
<content:encoded><![CDATA[
arXiv:2510.27153v1 Announce Type: cross 
Abstract: Finding lower and better-generalizing minima is crucial for deep learning. However, most existing optimizers stop searching the parameter space once they reach a local minimum. Given the complex geometric properties of the loss landscape, it is difficult to guarantee that such a point is the lowest or provides the best generalization. To address this, we propose an adaptor "E" for gradient-based optimizers. The adapted optimizer tends to continue exploring along landscape valleys (areas with low and nearly identical losses) in order to search for potentially better local minima even after reaching a local minimum. This approach increases the likelihood of finding a lower and flatter local minimum, which is often associated with better generalization. We also provide a proof of convergence for the adapted optimizers in both convex and non-convex scenarios for completeness. Finally, we demonstrate their effectiveness in an important but notoriously difficult training scenario, large-batch training, where Lamb is the benchmark optimizer. Our testing results show that the adapted Lamb, ALTO, increases the test accuracy (generalization) of the current state-of-the-art optimizer by an average of 2.5% across a variety of large-batch training tasks. This work potentially opens a new research direction in the design of optimization algorithms.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems</title>
<link>https://arxiv.org/abs/2510.27163</link>
<guid>https://arxiv.org/abs/2510.27163</guid>
<content:encoded><![CDATA[
arXiv:2510.27163v1 Announce Type: cross 
Abstract: Before deploying an AI system to replace an existing process, it must be compared with the incumbent to ensure improvement without added risk. Traditional evaluation relies on ground truth for both systems, but this is often unavailable due to delayed or unknowable outcomes, high costs, or incomplete data, especially for long-standing systems deemed safe by convention. The more practical solution is not to compute absolute risk but the difference between systems. We therefore propose a marginal risk assessment framework, that avoids dependence on ground truth or absolute risk. It emphasizes three kinds of relative evaluation methodology, including predictability, capability and interaction dominance. By shifting focus from absolute to relative evaluation, our approach equips software teams with actionable guidance: identifying where AI enhances outcomes, where it introduces new risks, and how to adopt such systems responsibly.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Accurate and Detailed Captions for High-Resolution Images</title>
<link>https://arxiv.org/abs/2510.27164</link>
<guid>https://arxiv.org/abs/2510.27164</guid>
<content:encoded><![CDATA[
arXiv:2510.27164v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) often struggle to generate accurate and detailed captions for high-resolution images since they are typically pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels). Downscaling high-resolution images to these dimensions may result in the loss of visual details and the omission of important objects. To address this limitation, we propose a novel pipeline that integrates vision-language models, large language models (LLMs), and object detection systems to enhance caption quality. Our proposed pipeline refines captions through a novel, multi-stage process. Given a high-resolution image, an initial caption is first generated using a VLM, and key objects in the image are then identified by an LLM. The LLM predicts additional objects likely to co-occur with the identified key objects, and these predictions are verified by object detection systems. Newly detected objects not mentioned in the initial caption undergo focused, region-specific captioning to ensure they are incorporated. This process enriches caption detail while reducing hallucinations by removing references to undetected objects. We evaluate the enhanced captions using pairwise comparison and quantitative scoring from large multimodal models, along with a benchmark for hallucination detection. Experiments on a curated dataset of high-resolution images demonstrate that our pipeline produces more detailed and reliable image captions while effectively minimizing hallucinations.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models</title>
<link>https://arxiv.org/abs/2510.27171</link>
<guid>https://arxiv.org/abs/2510.27171</guid>
<content:encoded><![CDATA[
arXiv:2510.27171v1 Announce Type: cross 
Abstract: Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler</title>
<link>https://arxiv.org/abs/2510.27172</link>
<guid>https://arxiv.org/abs/2510.27172</guid>
<content:encoded><![CDATA[
arXiv:2510.27172v1 Announce Type: cross 
Abstract: Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service for large language models. Existing defense strategies preemptively build robustness via attack simulation but suffer from fundamental limitations: (i) the infeasibility of extending attack simulations beyond bounded threat models due to the inherent difficulty of anticipating unknown attacks, and (ii) limited adaptability to varying attack settings, as simulation fails to capture their variability and complexity. To address these challenges, we propose Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with no need for attack simulation. BDS formulates harmful fine-tuning defense as a Bayesian inference problem, learning the posterior distribution of each data point's safety attribute, conditioned on the fine-tuning and alignment datasets. The fine-tuning process is then constrained by weighting data with their safety attributes sampled from the posterior, thus mitigating the influence of harmful data. By leveraging the post hoc nature of Bayesian inference, the posterior is conditioned on the fine-tuning dataset, enabling BDS to tailor its defense to the specific dataset, thereby achieving adaptive defense. Furthermore, we introduce a neural scheduler based on amortized Bayesian learning, enabling efficient transfer to new data without retraining. Comprehensive results across diverse attack and defense settings demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/Egg-Hu/Bayesian-Data-Scheduler.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction</title>
<link>https://arxiv.org/abs/2510.27173</link>
<guid>https://arxiv.org/abs/2510.27173</guid>
<content:encoded><![CDATA[
arXiv:2510.27173v1 Announce Type: cross 
Abstract: Fast and accurate simulation of dynamical systems is a fundamental challenge across scientific and engineering domains. Traditional numerical integrators often face a trade-off between accuracy and computational efficiency, while existing neural network-based approaches typically require training a separate model for each case. To overcome these limitations, we introduce a novel multi-modal foundation model for large-scale simulations of differential equations: FMint-SDE (Foundation Model based on Initialization for stochastic differential equations). Based on a decoder-only transformer with in-context learning, FMint-SDE leverages numerical and textual modalities to learn a universal error-correction scheme. It is trained using prompted sequences of coarse solutions generated by conventional solvers, enabling broad generalization across diverse systems. We evaluate our models on a suite of challenging SDE benchmarks spanning applications in molecular dynamics, mechanical systems, finance, and biology. Experimental results show that our approach achieves a superior accuracy-efficiency tradeoff compared to classical solvers, underscoring the potential of FMint-SDE as a general-purpose simulation tool for dynamical systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-level Progressive Hardness-Aware Reweighting for Cross-View Geo-Localization</title>
<link>https://arxiv.org/abs/2510.27181</link>
<guid>https://arxiv.org/abs/2510.27181</guid>
<content:encoded><![CDATA[
arXiv:2510.27181v1 Announce Type: cross 
Abstract: Cross-view geo-localization (CVGL) between drone and satellite imagery remains challenging due to severe viewpoint gaps and the presence of hard negatives, which are visually similar but geographically mismatched samples. Existing mining or reweighting strategies often use static weighting, which is sensitive to distribution shifts and prone to overemphasizing difficult samples too early, leading to noisy gradients and unstable convergence. In this paper, we present a Dual-level Progressive Hardness-aware Reweighting (DPHR) strategy. At the sample level, a Ratio-based Difficulty-Aware (RDA) module evaluates relative difficulty and assigns fine-grained weights to negatives. At the batch level, a Progressive Adaptive Loss Weighting (PALW) mechanism exploits a training-progress signal to attenuate noisy gradients during early optimization and progressively enhance hard-negative mining as training matures. Experiments on the University-1652 and SUES-200 benchmarks demonstrate the effectiveness and robustness of the proposed DPHR, achieving consistent improvements over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications</title>
<link>https://arxiv.org/abs/2510.27186</link>
<guid>https://arxiv.org/abs/2510.27186</guid>
<content:encoded><![CDATA[
arXiv:2510.27186v1 Announce Type: cross 
Abstract: Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlations--a phenomenon we term "hallucination" in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to 3.79 faster) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at https://github.com/Egg-Hu/SMI.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures</title>
<link>https://arxiv.org/abs/2510.27190</link>
<guid>https://arxiv.org/abs/2510.27190</guid>
<content:encoded><![CDATA[
arXiv:2510.27190v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are increasingly integrated into automated, multi-stage pipelines, risk patterns that arise from unvalidated trust between processing stages become a practical concern. This paper presents a mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs. The analysis shows that inputs are often interpreted non-neutrally and can trigger implementation-shaped responses or unintended state changes even without explicit commands. We argue that these behaviors constitute architectural failure modes and that string-level filtering alone is insufficient. To mitigate such cross-stage vulnerabilities, we recommend zero-trust architectural principles, including provenance enforcement, context sealing, and plan revalidation, and we introduce "Countermind" as a conceptual blueprint for implementing these defenses.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vectorized Online POMDP Planning</title>
<link>https://arxiv.org/abs/2510.27191</link>
<guid>https://arxiv.org/abs/2510.27191</guid>
<content:encoded><![CDATA[
arXiv:2510.27191v1 Announce Type: cross 
Abstract: Planning under partial observability is an essential capability of autonomous robots. The Partially Observable Markov Decision Process (POMDP) provides a powerful framework for planning under partial observability problems, capturing the stochastic effects of actions and the limited information available through noisy observations. POMDP solving could benefit tremendously from massive parallelization of today's hardware, but parallelizing POMDP solvers has been challenging. They rely on interleaving numerical optimization over actions with the estimation of their values, which creates dependencies and synchronization bottlenecks between parallel processes that can quickly offset the benefits of parallelization. In this paper, we propose Vectorized Online POMDP Planner (VOPP), a novel parallel online solver that leverages a recent POMDP formulation that analytically solves part of the optimization component, leaving only the estimation of expectations for numerical computation. VOPP represents all data structures related to planning as a collection of tensors and implements all planning steps as fully vectorized computations over this representation. The result is a massively parallel solver with no dependencies and synchronization bottlenecks between parallel computations. Experimental results indicate that VOPP is at least 20X more efficient in computing near-optimal solutions compared to an existing state-of-the-art parallel online solver.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2510.27196</link>
<guid>https://arxiv.org/abs/2510.27196</guid>
<content:encoded><![CDATA[
arXiv:2510.27196v1 Announce Type: cross 
Abstract: The proliferation of memes on social media necessitates the capabilities of multimodal Large Language Models (mLLMs) to effectively understand multimodal harmfulness. Existing evaluation approaches predominantly focus on mLLMs' detection accuracy for binary classification tasks, which often fail to reflect the in-depth interpretive nuance of harmfulness across diverse contexts. In this paper, we propose MemeArena, an agent-based arena-style evaluation framework that provides a context-aware and unbiased assessment for mLLMs' understanding of multimodal harmfulness. Specifically, MemeArena simulates diverse interpretive contexts to formulate evaluation tasks that elicit perspective-specific analyses from mLLMs. By integrating varied viewpoints and reaching consensus among evaluators, it enables fair and unbiased comparisons of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments demonstrate that our framework effectively reduces the evaluation biases of judge agents, with judgment results closely aligning with human preferences, offering valuable insights into reliable and comprehensive mLLM evaluations in multimodal harmfulness understanding. Our code and data are publicly available at https://github.com/Lbotirx/MemeArena.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models</title>
<link>https://arxiv.org/abs/2510.27207</link>
<guid>https://arxiv.org/abs/2510.27207</guid>
<content:encoded><![CDATA[
arXiv:2510.27207v1 Announce Type: cross 
Abstract: Explainable AI (XAI) is critical for building trust in complex machine learning models, yet mainstream attribution methods often provide an incomplete, static picture of a model's final state. By collapsing a feature's role into a single score, they are confounded by non-linearity and interactions. To address this, we introduce Feature-Function Curvature Analysis (FFCA), a novel framework that analyzes the geometry of a model's learned function. FFCA produces a 4-dimensional signature for each feature, quantifying its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction. Crucially, we extend this framework into Dynamic Archetype Analysis, which tracks the evolution of these signatures throughout the training process. This temporal view moves beyond explaining what a model learned to revealing how it learns. We provide the first direct, empirical evidence of hierarchical learning, showing that models consistently learn simple linear effects before complex interactions. Furthermore, this dynamic analysis provides novel, practical diagnostics for identifying insufficient model capacity and predicting the onset of overfitting. Our comprehensive experiments demonstrate that FFCA, through its static and dynamic components, provides the essential geometric context that transforms model explanation from simple quantification to a nuanced, trustworthy analysis of the entire learning process.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks</title>
<link>https://arxiv.org/abs/2510.27208</link>
<guid>https://arxiv.org/abs/2510.27208</guid>
<content:encoded><![CDATA[
arXiv:2510.27208v1 Announce Type: cross 
Abstract: Villages areas hold significant importance in the study of human-land relationships. However, with the advancement of urbanization, the gradual disappearance of spatial characteristics and the homogenization of landscapes have emerged as prominent issues. Existing studies primarily adopt a single-disciplinary perspective to analyze villages spatial morphology and its influencing factors, relying heavily on qualitative analysis methods. These efforts are often constrained by the lack of digital infrastructure and insufficient data. To address the current research limitations, this paper proposes a Hierarchical Graph Neural Network (HGNN) model that integrates multi-source data to conduct an in-depth analysis of villages spatial morphology. The framework includes two types of nodes-input nodes and communication nodes-and two types of edges-static input edges and dynamic communication edges. By combining Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), the proposed model efficiently integrates multimodal features under a two-stage feature update mechanism. Additionally, based on existing principles for classifying villages spatial morphology, the paper introduces a relational pooling mechanism and implements a joint training strategy across 17 subtypes. Experimental results demonstrate that this method achieves significant performance improvements over existing approaches in multimodal fusion and classification tasks. Additionally, the proposed joint optimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83 (independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Our method provides scientific evidence for exploring villages spatial patterns and generative logic.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness</title>
<link>https://arxiv.org/abs/2510.27213</link>
<guid>https://arxiv.org/abs/2510.27213</guid>
<content:encoded><![CDATA[
arXiv:2510.27213v1 Announce Type: cross 
Abstract: We propose a novel continual self-supervised learning (CSSL) framework for simultaneously learning diverse features from multi-window-obtained chest computed tomography (CT) images and ensuring data privacy. Achieving a robust and highly generalizable model in medical image diagnosis is challenging, mainly because of issues, such as the scarcity of large-scale, accurately annotated datasets and domain shifts inherent to dynamic healthcare environments. Specifically, in chest CT, these domain shifts often arise from differences in window settings, which are optimized for distinct clinical purposes. Previous CSSL frameworks often mitigated domain shift by reusing past data, a typically impractical approach owing to privacy constraints. Our approach addresses these challenges by effectively capturing the relationship between previously learned knowledge and new information across different training stages through continual pretraining on unlabeled images. Specifically, by incorporating a latent replay-based mechanism into CSSL, our method mitigates catastrophic forgetting due to domain shifts during continual pretraining while ensuring data privacy. Additionally, we introduce a feature distillation technique that integrates Wasserstein distance-based knowledge distillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of the model to learn meaningful, domain-shift-robust representations. Finally, we validate our approach using chest CT images obtained across two different window settings, demonstrating superior performance compared with other approaches.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Task-Aware Routing of Experts for Equivariant Representation Learning</title>
<link>https://arxiv.org/abs/2510.27222</link>
<guid>https://arxiv.org/abs/2510.27222</guid>
<content:encoded><![CDATA[
arXiv:2510.27222v1 Announce Type: cross 
Abstract: Equivariant representation learning aims to capture variations induced by input transformations in the representation space, whereas invariant representation learning encodes semantic information by disregarding such transformations. Recent studies have shown that jointly learning both types of representations is often beneficial for downstream tasks, typically by employing separate projection heads. However, this design overlooks information shared between invariant and equivariant learning, which leads to redundant feature learning and inefficient use of model capacity. To address this, we introduce Soft Task-Aware Routing (STAR), a routing strategy for projection heads that models them as experts. STAR induces the experts to specialize in capturing either shared or task-specific information, thereby reducing redundant feature learning. We validate this effect by observing lower canonical correlations between invariant and equivariant embeddings. Experimental results show consistent improvements across diverse transfer learning tasks. The code is available at https://github.com/YonseiML/star.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries</title>
<link>https://arxiv.org/abs/2510.27238</link>
<guid>https://arxiv.org/abs/2510.27238</guid>
<content:encoded><![CDATA[
arXiv:2510.27238v1 Announce Type: cross 
Abstract: Manually conducting real-world data analyses is labor-intensive and inefficient. Despite numerous attempts to automate data science workflows, none of the existing paradigms or systems fully demonstrate all three key capabilities required to support them effectively: (1) open-domain data collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that answers users' analytic queries in natural language on large-scale open-domain data. DRAMA unifies data collection, transformation, and analysis as a single pipeline. To quantitatively evaluate system performance on tasks representative of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories of tasks: claim verification and question answering, each comprising 100 instances. These tasks are derived from real-world applications that have gained significant public attention and require the retrieval and analysis of open-domain data. We develop DRAMA-Bot, a multi-agent system designed following DRAMA. It comprises a data retriever that collects and transforms data by coordinating the execution of sub-agents, and a data analyzer that performs structured reasoning over the retrieved data. We evaluate DRAMA-Bot on DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is publicly available at https://github.com/uiuc-kang-lab/drama.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes</title>
<link>https://arxiv.org/abs/2510.27244</link>
<guid>https://arxiv.org/abs/2510.27244</guid>
<content:encoded><![CDATA[
arXiv:2510.27244v1 Announce Type: cross 
Abstract: Application modernization in legacy languages such as COBOL, PL/I, and REXX faces an acute shortage of resources, both in expert availability and in high-quality human evaluation data. While Large Language Models as a Judge (LaaJ) offer a scalable alternative to expert review, their reliability must be validated before being trusted in high-stakes workflows. Without principled validation, organizations risk a circular evaluation loop, where unverified LaaJs are used to assess model outputs, potentially reinforcing unreliable judgments and compromising downstream deployment decisions. Although various automated approaches to validating LaaJs have been proposed, alignment with human judgment remains a widely used and conceptually grounded validation strategy. In many real-world domains, the availability of human-labeled evaluation data is severely limited, making it difficult to assess how well a LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework for assessing LaaJ alignment with sparse human-labeled data. SparseAlign combines a novel pairwise-confidence concept with a score-sensitive alignment metric that jointly capture ranking consistency and score proximity, enabling reliable evaluator selection even when traditional statistical methods are ineffective due to limited annotated examples. SparseAlign was applied internally to select LaaJs for COBOL code explanation. The top-aligned evaluators were integrated into assessment workflows, guiding model release decisions. We present a case study of four LaaJs to demonstrate SparseAlign's utility in real-world evaluation scenarios.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs</title>
<link>https://arxiv.org/abs/2510.27246</link>
<guid>https://arxiv.org/abs/2510.27246</guid>
<content:encoded><![CDATA[
arXiv:2510.27246v1 Announce Type: cross 
Abstract: Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing Unseen Sentences from Speech-related Biosignals for Open-vocabulary Neural Communication</title>
<link>https://arxiv.org/abs/2510.27247</link>
<guid>https://arxiv.org/abs/2510.27247</guid>
<content:encoded><![CDATA[
arXiv:2510.27247v1 Announce Type: cross 
Abstract: Brain-to-speech (BTS) systems represent a groundbreaking approach to human communication by enabling the direct transformation of neural activity into linguistic expressions. While recent non-invasive BTS studies have largely focused on decoding predefined words or sentences, achieving open-vocabulary neural communication comparable to natural human interaction requires decoding unconstrained speech. Additionally, effectively integrating diverse signals derived from speech is crucial for developing personalized and adaptive neural communication and rehabilitation solutions for patients. This study investigates the potential of speech synthesis for previously unseen sentences across various speech modes by leveraging phoneme-level information extracted from high-density electroencephalography (EEG) signals, both independently and in conjunction with electromyography (EMG) signals. Furthermore, we examine the properties affecting phoneme decoding accuracy during sentence reconstruction and offer neurophysiological insights to further enhance EEG decoding for more effective neural communication solutions. Our findings underscore the feasibility of biosignal-based sentence-level speech synthesis for reconstructing unseen sentences, highlighting a significant step toward developing open-vocabulary neural communication systems adapted to diverse patient needs and conditions. Additionally, this study provides meaningful insights into the development of communication and rehabilitation solutions utilizing EEG-based decoding technologies.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation</title>
<link>https://arxiv.org/abs/2510.27253</link>
<guid>https://arxiv.org/abs/2510.27253</guid>
<content:encoded><![CDATA[
arXiv:2510.27253v1 Announce Type: cross 
Abstract: Dataset distillation condenses large datasets into synthetic subsets, achieving performance comparable to training on the full dataset while substantially reducing storage and computation costs. Most existing dataset distillation methods assume that all real instances contribute equally to the process. In practice, real-world datasets contain both informative and redundant or even harmful instances, and directly distilling the full dataset without considering data quality can degrade model performance. In this work, we present Influence-Weighted Distillation IWD, a principled framework that leverages influence functions to explicitly account for data quality in the distillation process. IWD assigns adaptive weights to each instance based on its estimated impact on the distillation objective, prioritizing beneficial data while downweighting less useful or harmful ones. Owing to its modular design, IWD can be seamlessly integrated into diverse dataset distillation frameworks. Our empirical results suggest that integrating IWD tends to improve the quality of distilled datasets and enhance model performance, with accuracy gains of up to 7.8%.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Languages are Modalities: Cross-Lingual Alignment via Encoder Injection</title>
<link>https://arxiv.org/abs/2510.27254</link>
<guid>https://arxiv.org/abs/2510.27254</guid>
<content:encoded><![CDATA[
arXiv:2510.27254v1 Announce Type: cross 
Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource, non-Latin scripts due to tokenizer fragmentation and weak cross-lingual coupling. We present LLINK (Latent Language Injection for Non-English Knowledge), a compute efficient language-as-modality method that conditions an instruction-tuned decoder without changing the tokenizer or retraining the decoder. First, we align sentence embeddings from a frozen multilingual encoder to the decoder's latent embedding space at a reserved position via a lightweight contrastive projector. Second, the vector is expanded into K soft slots and trained with minimal adapters so the frozen decoder consumes the signal. LLINK substantially improves bilingual retrieval and achieves 81.3% preference over the base model and 63.6% over direct fine-tuning in LLM-judged Q&amp;A evaluations. We further find that improvements can be attributed to reduced tokenization inflation and a stronger cross lingual alignment, despite the model having residual weaknesses in numeric fidelity. Treating low resource languages as a modality offers a practical path to stronger cross-lingual alignment in lightweight LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-order Linear Attention</title>
<link>https://arxiv.org/abs/2510.27258</link>
<guid>https://arxiv.org/abs/2510.27258</guid>
<content:encoded><![CDATA[
arXiv:2510.27258v1 Announce Type: cross 
Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any $n \times n$ matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2510.27267</link>
<guid>https://arxiv.org/abs/2510.27267</guid>
<content:encoded><![CDATA[
arXiv:2510.27267v1 Announce Type: cross 
Abstract: As large language models (LLMs) enter the medical domain, most benchmarks evaluate them on question answering or descriptive reasoning, overlooking quantitative reasoning critical to clinical decision-making. Existing datasets like MedCalc-Bench cover few calculation tasks and fail to reflect real-world computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical calculation abilities, comprising 700+ tasks across two types: equation-based (e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar, Glasgow Coma Scale). These tasks span diverse specialties including internal medicine, surgery, pediatrics, and cardiology, offering a broader and more challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, enabling multi-step clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this environment achieves state-of-the-art results on MedCalc-Eval, with notable gains in numerical sensitivity, formula selection, and reasoning robustness. Remaining challenges include unit conversion, multi-condition logic, and contextual understanding.
  Code and datasets are available at https://github.com/maokangkun/MedCalc-Eval.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?</title>
<link>https://arxiv.org/abs/2510.27269</link>
<guid>https://arxiv.org/abs/2510.27269</guid>
<content:encoded><![CDATA[
arXiv:2510.27269v1 Announce Type: cross 
Abstract: Reasoning language models (RLMs) achieve strong performance on complex reasoning tasks, yet they still suffer from a multilingual reasoning gap, performing better in high-resource languages than in low-resource ones. While recent efforts have reduced this gap, its underlying causes remain largely unexplored. In this paper, we address this by showing that the multilingual reasoning gap largely stems from failures in language understanding-the model's inability to represent the multilingual input meaning into the dominant language (i.e., English) within its reasoning trace. This motivates us to examine whether understanding failures can be detected, as this ability could help mitigate the multilingual reasoning gap. To this end, we evaluate a range of detection methods and find that understanding failures can indeed be identified, with supervised approaches performing best. Building on this, we propose Selective Translation, a simple yet effective strategy that translates the multilingual input into English only when an understanding failure is detected. Experimental results show that Selective Translation bridges the multilingual reasoning gap, achieving near full-translation performance while using translation for only about 20% of inputs. Together, our work demonstrates that understanding failures are the primary cause of the multilingual reasoning gap and can be detected and selectively mitigated, providing key insight into its origin and a promising path toward more equitable multilingual reasoning. Our code and data are publicly available at https://github.com/deokhk/RLM_analysis.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOCUS: Efficient Keyframe Selection for Long Video Understanding</title>
<link>https://arxiv.org/abs/2510.27280</link>
<guid>https://arxiv.org/abs/2510.27280</guid>
<content:encoded><![CDATA[
arXiv:2510.27280v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) represent images and video frames as visual tokens. Scaling from single images to hour-long videos, however, inflates the token budget far beyond practical limits. Popular pipelines therefore either uniformly subsample or apply keyframe selection with retrieval-style scoring using smaller vision-language models. However, these keyframe selection methods still rely on pre-filtering before selection to reduce the inference cost and can miss the most informative moments.
  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a training-free, model-agnostic keyframe selection module that selects query-relevant frames under a strict token budget. FOCUS formulates keyframe selection as a combinatorial pure-exploration (CPE) problem in multi-armed bandits: it treats short temporal clips as arms, and uses empirical means and Bernstein confidence radius to identify informative regions while preserving exploration of uncertain areas. The resulting two-stage exploration-exploitation procedure reduces from a sequential policy with theoretical guarantees, first identifying high-value temporal regions, then selecting top-scoring frames within each region On two long-video question-answering benchmarks, FOCUS delivers substantial accuracy improvements while processing less than 2% of video frames. For videos longer than 20 minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating its effectiveness as a keyframe selection method and providing a simple and general solution for scalable long-video understanding with MLLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction</title>
<link>https://arxiv.org/abs/2510.27281</link>
<guid>https://arxiv.org/abs/2510.27281</guid>
<content:encoded><![CDATA[
arXiv:2510.27281v1 Announce Type: cross 
Abstract: Accurate prediction of Drug-Target Affinity (DTA) is crucial for reducing experimental costs and accelerating early screening in computational drug discovery. While sequence-based deep learning methods avoid reliance on costly 3D structures, they still overlook simultaneous modeling of global sequence semantic features and local topological structural features within drugs and proteins, and represent drugs as flat sequences without atomic-level, substructural-level, and molecular-level multi-scale features. We propose HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract both global sequence semantic and local topological features from drug and protein sequences, and models drugs multi-scale to learn atomic, substructural, and molecular representations fused via a multi-scale bilinear attention module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms state-of-the-art baselines, with ablations confirming the importance of global-local extraction and multi-scale fusion.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments</title>
<link>https://arxiv.org/abs/2510.27287</link>
<guid>https://arxiv.org/abs/2510.27287</guid>
<content:encoded><![CDATA[
arXiv:2510.27287v1 Announce Type: cross 
Abstract: Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Un-Attributability: Computing Novelty From Retrieval &amp; Semantic Similarity</title>
<link>https://arxiv.org/abs/2510.27313</link>
<guid>https://arxiv.org/abs/2510.27313</guid>
<content:encoded><![CDATA[
arXiv:2510.27313v1 Announce Type: cross 
Abstract: Understanding how language-model outputs relate to the pretraining corpus is central to studying model behavior. Most training data attribution (TDA) methods ask which training examples causally influence a given output, often using leave-one-out tests. We invert the question: which outputs cannot be attributed to any pretraining example? We introduce un-attributability as an operational measure of semantic novelty: an output is novel if the pretraining corpus contains no semantically similar context. We approximate this with a simple two-stage retrieval pipeline: index the corpus with lightweight GIST embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the nearest corpus item is less attributable than a human-generated text reference, we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2 and report three findings: (1) models draw on pretraining data across much longer spans than previously reported; (2) some domains systematically promote or suppress novelty; and (3) instruction tuning not only alters style but also increases novelty. Reframing novelty assessment around un-attributability enables efficient analysis at pretraining scale. We release ~20 TB of corpus chunks and index artifacts to support replication and large-scale extension of our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASR-Net: An Image Processing-focused Deep Learning-based Coronary Artery Segmentation and Refinement Network for X-ray Coronary Angiogram</title>
<link>https://arxiv.org/abs/2510.27315</link>
<guid>https://arxiv.org/abs/2510.27315</guid>
<content:encoded><![CDATA[
arXiv:2510.27315v1 Announce Type: cross 
Abstract: Early detection of coronary artery disease (CAD) is critical for reducing mortality and improving patient treatment planning. While angiographic image analysis from X-rays is a common and cost-effective method for identifying cardiac abnormalities, including stenotic coronary arteries, poor image quality can significantly impede clinical diagnosis. We present the Coronary Artery Segmentation and Refinement Network (CASR-Net), a three-stage pipeline comprising image preprocessing, segmentation, and refinement. A novel multichannel preprocessing strategy combining CLAHE and an improved Ben Graham method provides incremental gains, increasing Dice Score Coefficient (DSC) by 0.31-0.89% and Intersection over Union (IoU) by 0.40-1.16% compared with using the techniques individually. The core innovation is a segmentation network built on a UNet with a DenseNet121 encoder and a Self-organized Operational Neural Network (Self-ONN) based decoder, which preserves the continuity of narrow and stenotic vessel branches. A final contour refinement module further suppresses false positives. Evaluated with 5-fold cross-validation on a combination of two public datasets that contain both healthy and stenotic arteries, CASR-Net outperformed several state-of-the-art models, achieving an IoU of 61.43%, a DSC of 76.10%, and clDice of 79.36%. These results highlight a robust approach to automated coronary artery segmentation, offering a valuable tool to support clinicians in diagnosis and treatment planning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis</title>
<link>https://arxiv.org/abs/2510.27324</link>
<guid>https://arxiv.org/abs/2510.27324</guid>
<content:encoded><![CDATA[
arXiv:2510.27324v1 Announce Type: cross 
Abstract: We consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V</title>
<link>https://arxiv.org/abs/2510.27364</link>
<guid>https://arxiv.org/abs/2510.27364</guid>
<content:encoded><![CDATA[
arXiv:2510.27364v1 Announce Type: cross 
Abstract: We present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA) modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B model to adapt its visual representations using a compact dataset of short clips from Ay Yapim's historical television film El Turco. This enables efficient domain transfer within hours on a single GPU. In the second stage, the fine-tuned model produces stylistically consistent keyframes that preserve costume, lighting, and color grading, which are then temporally expanded into coherent 720p sequences through the model's video decoder. We further apply lightweight parallelization and sequence partitioning strategies to accelerate inference without quality degradation. Quantitative and qualitative evaluations using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study, demonstrate measurable improvements in cinematic fidelity and temporal stability over the base model. The complete training and inference pipeline is released to support reproducibility and adaptation across cinematic domains.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity</title>
<link>https://arxiv.org/abs/2510.27378</link>
<guid>https://arxiv.org/abs/2510.27378</guid>
<content:encoded><![CDATA[
arXiv:2510.27378v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spiking Neural Networks: The Future of Brain-Inspired Computing</title>
<link>https://arxiv.org/abs/2510.27379</link>
<guid>https://arxiv.org/abs/2510.27379</guid>
<content:encoded><![CDATA[
arXiv:2510.27379v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) represent the latest generation of neural computation, offering a brain-inspired alternative to conventional Artificial Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals, SNNs operate using distinct spike events, making them inherently more energy-efficient and temporally dynamic. This study presents a comprehensive analysis of SNN design models, training algorithms, and multi-dimensional performance metrics, including accuracy, energy consumption, latency, spike count, and convergence behavior. Key neuron models such as the Leaky Integrate-and-Fire (LIF) and training strategies, including surrogate gradient descent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP), are examined in depth. Results show that surrogate gradient-trained SNNs closely approximate ANN accuracy (within 1-2%), with faster convergence by the 20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve competitive performance but require higher spike counts and longer simulation windows. STDP-based SNNs, though slower to converge, exhibit the lowest spike counts and energy consumption (as low as 5 millijoules per inference), making them optimal for unsupervised and low-power tasks. These findings reinforce the suitability of SNNs for energy-constrained, latency-sensitive, and adaptive applications such as robotics, neuromorphic vision, and edge AI systems. While promising, challenges persist in hardware standardization and scalable training. This study concludes that SNNs, with further refinement, are poised to propel the next phase of neuromorphic computing.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs</title>
<link>https://arxiv.org/abs/2510.27400</link>
<guid>https://arxiv.org/abs/2510.27400</guid>
<content:encoded><![CDATA[
arXiv:2510.27400v1 Announce Type: cross 
Abstract: Knowledge editing has emerged as an efficient approach for updating factual knowledge in large language models (LLMs). It typically locates knowledge storage modules and then modifies their parameters. However, most existing methods focus on the weights of multilayer perceptron (MLP) modules, which are often identified as the main repositories of factual information. Other components, such as attention (Attn) modules, are often ignored during editing. This imbalance can leave residual outdated knowledge and limit editing effectiveness. We perform comprehensive knowledge localization experiments on advanced LLMs and find that Attn modules play a substantial role in factual knowledge storage and retrieval, especially in earlier layers. Based on these insights, we propose IntAttn-Edit, a method that extends the associative memory paradigm to jointly update both MLP and Attn modules. Our approach uses a knowledge balancing strategy that allocates update magnitudes in proportion to each module's measured contribution to knowledge storage. Experiments on standard benchmarks show that IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. Further analysis shows that the balancing strategy keeps editing performance within an optimal range across diverse settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedMuon: Accelerating Federated Learning with Matrix Orthogonalization</title>
<link>https://arxiv.org/abs/2510.27403</link>
<guid>https://arxiv.org/abs/2510.27403</guid>
<content:encoded><![CDATA[
arXiv:2510.27403v1 Announce Type: cross 
Abstract: The core bottleneck of Federated Learning (FL) lies in the communication rounds. That is, how to achieve more effective local updates is crucial for reducing communication rounds. Existing FL methods still primarily use element-wise local optimizers (Adam/SGD), neglecting the geometric structure of the weight matrices. This often leads to the amplification of pathological directions in the weights during local updates, leading deterioration in the condition number and slow convergence. Therefore, we introduce the Muon optimizer in local, which has matrix orthogonalization to optimize matrix-structured parameters. Experimental results show that, in IID setting, Local Muon significantly accelerates the convergence of FL and reduces communication rounds compared to Local SGD and Local AdamW. However, in non-IID setting, independent matrix orthogonalization based on the local distributions of each client induces strong client drift. Applying Muon in non-IID FL poses significant challenges: (1) client preconditioner leading to client drift; (2) moment reinitialization. To address these challenges, we propose a novel Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1) momentum aggregation, where clients use the aggregated momentum for local initialization; (2) local-global alignment, where the local gradients are aligned with the global update direction to significantly reduce client drift. Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup convergence rate without the heterogeneity assumption, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. Empirically, we validate the effectiveness of FedMuon on language and vision models. Compared to several baselines, FedMuon significantly reduces communication rounds and improves test accuracy.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atlas-Alignment: Making Interpretability Transferable Across Language Models</title>
<link>https://arxiv.org/abs/2510.27413</link>
<guid>https://arxiv.org/abs/2510.27413</guid>
<content:encoded><![CDATA[
arXiv:2510.27413v1 Announce Type: cross 
Abstract: Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset</title>
<link>https://arxiv.org/abs/2510.27421</link>
<guid>https://arxiv.org/abs/2510.27421</guid>
<content:encoded><![CDATA[
arXiv:2510.27421v1 Announce Type: cross 
Abstract: Deep learning models aim to improve diagnostic workflows, but fairness evaluation remains underexplored beyond classification, e.g., in image segmentation. Unaddressed segmentation bias can lead to disparities in the quality of care for certain populations, potentially compounded across clinical decision points and amplified through iterative model development. Here, we audit the fairness of the automated segmentation labels provided in the breast cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation quality across age, ethnicity, and data source. Our analysis reveals an intrinsic age-related bias against younger patients that continues to persist even after controlling for confounding factors, such as data source. We hypothesize that this bias may be linked to physiological factors, a known challenge for both radiologists and automated systems. Finally, we show how aggregating data from multiple data sources influences site-specific ethnic biases, underscoring the necessity of investigating data at a granular level.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Soft Robotic Dynamics with Active Exploration</title>
<link>https://arxiv.org/abs/2510.27428</link>
<guid>https://arxiv.org/abs/2510.27428</guid>
<content:encoded><![CDATA[
arXiv:2510.27428v1 Announce Type: cross 
Abstract: Soft robots offer unmatched adaptability and safety in unstructured environments, yet their compliant, high-dimensional, and nonlinear dynamics make modeling for control notoriously difficult. Existing data-driven approaches often fail to generalize, constrained by narrowly focused task demonstrations or inefficient random exploration. We introduce SoftAE, an uncertainty-aware active exploration framework that autonomously learns task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE employs probabilistic ensemble models to estimate epistemic uncertainty and actively guides exploration toward underrepresented regions of the state-action space, achieving efficient coverage of diverse behaviors without task-specific supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a continuum arm, an articulated fish in fluid, and a musculoskeletal leg with hybrid actuation -- and on a pneumatically actuated continuum soft arm in the real world. Compared with random exploration and task-specific model-based reinforcement learning, SoftAE produces more accurate dynamics models, enables superior zero-shot control on unseen tasks, and maintains robustness under sensing noise, actuation delays, and nonlinear material effects. These results demonstrate that uncertainty-driven active exploration can yield scalable, reusable dynamics models across diverse soft robotic morphologies, representing a step toward more autonomous, adaptable, and data-efficient control in compliant robots.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Semantic Collapse in Partially Relevant Video Retrieval</title>
<link>https://arxiv.org/abs/2510.27432</link>
<guid>https://arxiv.org/abs/2510.27432</guid>
<content:encoded><![CDATA[
arXiv:2510.27432v1 Announce Type: cross 
Abstract: Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the content matches a text query. Existing methods treat every annotated text-video pair as a positive and all others as negatives, ignoring the rich semantic variation both within a single video and across different videos. Consequently, embeddings of both queries and their corresponding video-clip segments for distinct events within the same video collapse together, while embeddings of semantically similar queries and segments from different videos are driven apart. This limits retrieval performance when videos contain multiple, diverse events. This paper addresses the aforementioned problems, termed as semantic collapse, in both the text and video embedding spaces. We first introduce Text Correlation Preservation Learning, which preserves the semantic relationships encoded by the foundation model across text queries. To address collapse in video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive alignment method that disentangles hierarchical video representations across temporal scales. Subsequently, we introduce order-preserving token merging and adaptive CBVA to enhance alignment by producing video segments that are internally coherent yet mutually distinctive. Extensive experiments on PRVR benchmarks demonstrate that our framework effectively prevents semantic collapse and substantially improves retrieval accuracy.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoMViT: An Efficient Vision Backbone for Supervised Classification in Medical Imaging</title>
<link>https://arxiv.org/abs/2510.27442</link>
<guid>https://arxiv.org/abs/2510.27442</guid>
<content:encoded><![CDATA[
arXiv:2510.27442v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios. In this paper, we present CoMViT, a compact and generalizable Vision Transformer architecture optimized for resource-constrained medical image analysis. CoMViT integrates a convolutional tokenizer, diagonal masking, dynamic temperature scaling, and pooling-based sequence aggregation to improve performance and generalization. Through systematic architectural optimization, CoMViT achieves robust performance across twelve MedMNIST datasets while maintaining a lightweight design with only ~4.5M parameters. It matches or outperforms deeper CNN and ViT variants, offering up to 5-20x parameter reduction without sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT consistently attends to clinically relevant regions despite its compact size. These results highlight the potential of principled ViT redesign for developing efficient and interpretable models in low-resource medical imaging settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision</title>
<link>https://arxiv.org/abs/2510.27462</link>
<guid>https://arxiv.org/abs/2510.27462</guid>
<content:encoded><![CDATA[
arXiv:2510.27462v1 Announce Type: cross 
Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs). However, the standard cross-entropy loss treats all tokens equally, ignoring their heterogeneous contributions across a reasoning trajectory. This uniform treatment leads to misallocated supervision and weak generalization, especially in complex, long-form reasoning tasks. To address this, we introduce \textbf{V}ariance-\textbf{C}ontrolled \textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled framework that reformulates CoT supervision as a constrained optimization problem. By adopting an optimization-theoretic perspective, VCORE enables a principled and adaptive allocation of supervision across tokens, thereby aligning the training objective more closely with the goal of robust reasoning generalization. Empirical evaluations demonstrate that VCORE consistently outperforms existing token reweighting methods. Across both in-domain and out-of-domain settings, VCORE achieves substantial performance gains on mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B, 32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more effective initialization for subsequent reinforcement learning, establishing a stronger foundation for advancing the reasoning capabilities of LLMs. The Code will be released at https://github.com/coder-gx/VCORE.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thought Branches: Interpreting LLM Reasoning Requires Resampling</title>
<link>https://arxiv.org/abs/2510.27484</link>
<guid>https://arxiv.org/abs/2510.27484</guid>
<content:encoded><![CDATA[
arXiv:2510.27484v1 Announce Type: cross 
Abstract: Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In "agentic misalignment" scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models</title>
<link>https://arxiv.org/abs/2510.27486</link>
<guid>https://arxiv.org/abs/2510.27486</guid>
<content:encoded><![CDATA[
arXiv:2510.27486v1 Announce Type: cross 
Abstract: AdamW has become one of the most effective optimizers for training large-scale models. We have also observed its effectiveness in the context of federated learning (FL). However, directly applying AdamW in federated learning settings poses significant challenges: (1) due to data heterogeneity, AdamW often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2) the local overfitting of AdamW may cause client drift; and (3) Reinitializing moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down convergence. To address these challenges, we propose the first \underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW}, for training and fine-tuning various large models. \texttt{FedAdamW} aligns local updates with the global update using both a \textbf{local correction mechanism} and decoupled weight decay to mitigate local overfitting. \texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment estimates to reduce their variance and reinitialize them. Theoretically, we prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of $\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$ without \textbf{heterogeneity assumption}, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. We also employ PAC-Bayesian generalization analysis to explain the effectiveness of decoupled weight decay in local training. Empirically, we validate the effectiveness of \texttt{FedAdamW} on language and vision Transformer models. Compared to several baselines, \texttt{FedAdamW} significantly reduces communication rounds and improves test accuracy. The code is available in https://github.com/junkangLiu0/FedAdamW.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames</title>
<link>https://arxiv.org/abs/2510.27497</link>
<guid>https://arxiv.org/abs/2510.27497</guid>
<content:encoded><![CDATA[
arXiv:2510.27497v1 Announce Type: cross 
Abstract: Transformer-based autoregressive models have emerged as a unifying paradigm across modalities such as text and images, but their extension to 3D molecule generation remains underexplored. The gap stems from two fundamental challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens that is invariant to both SE(3) transformations and atom index permutations, and (2) designing an architecture capable of modeling hybrid atom-based tokens that couple discrete atom types with continuous 3D coordinates. To address these challenges, we introduce InertialAR. InertialAR devises a canonical tokenization that aligns molecules to their inertial frames and reorders atoms to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the attention mechanism with geometric awareness via geometric rotary positional encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive paradigm to predict the next atom-based token, predicting the atom type first and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR achieves state-of-the-art performance on 7 of the 10 evaluation metrics for unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover, it significantly outperforms strong baselines in controllable generation for targeted chemical functionality, attaining state-of-the-art results across all 5 metrics.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm</title>
<link>https://arxiv.org/abs/2510.27504</link>
<guid>https://arxiv.org/abs/2510.27504</guid>
<content:encoded><![CDATA[
arXiv:2510.27504v1 Announce Type: cross 
Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the leakage of sensitive information, Client-level Differentially Private Federated Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually result in sharper loss landscapes, which leads to a decrease in model generalization after differential privacy protection. By using Sharpness Aware Minimization (SAM), the current popular federated learning methods are to find a local flat minimum value to alleviate this problem. However, the local flatness may not reflect the global flatness in CL-DPFL. Therefore, to address this issue and seek global flat minima of models, we propose a new CL-DPFL algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to the local loss to find the global flat minimum. Moreover, by using our global gradient norm penalty, we not only find a flatter global minimum but also reduce the locally updated norm, which means that we further reduce the error of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN mitigates the performance degradation caused by DP. Meanwhile, the proposed DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves fast convergence. We also use R\'enyi DP to provide strict privacy guarantees and provide sensitivity analysis for local updates. Finally, we conduct effectiveness tests on both ResNet and Transformer models, and achieve significant improvements in six visual and natural language processing tasks compared to existing state-of-the-art algorithms. The code is available at https://github.com/junkangLiu0/DP-FedPGN
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation</title>
<link>https://arxiv.org/abs/2510.27508</link>
<guid>https://arxiv.org/abs/2510.27508</guid>
<content:encoded><![CDATA[
arXiv:2510.27508v1 Announce Type: cross 
Abstract: Accurate lung tumor segmentation is vital for improving diagnosis and treatment planning, and effectively combining anatomical and functional information from PET and CT remains a major challenge. In this study, we propose vMambaX, a lightweight multimodal framework integrating PET and CT scan images through a Context-Gated Cross-Modal Perception Module (CGM). Built on the Visual Mamba architecture, vMambaX adaptively enhances inter-modality feature interaction, emphasizing informative regions while suppressing noise. Evaluated on the PCLT20K dataset, the model outperforms baseline models while maintaining lower computational complexity. These results highlight the effectiveness of adaptive cross-modal gating for multimodal tumor segmentation and demonstrate the potential of vMambaX as an efficient and scalable framework for advanced lung cancer analysis. The code is available at https://github.com/arco-group/vMambaX.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Generic Time Series Foundation Models for EEG Classification</title>
<link>https://arxiv.org/abs/2510.27522</link>
<guid>https://arxiv.org/abs/2510.27522</guid>
<content:encoded><![CDATA[
arXiv:2510.27522v1 Announce Type: cross 
Abstract: Foundation models for time series are emerging as powerful general-purpose backbones, yet their potential for domain-specific biomedical signals such as electroencephalography (EEG) remains rather unexplored. In this work, we investigate the applicability a recently proposed time series classification foundation model, to a different EEG tasks such as motor imagery classification and sleep stage prediction. We test two pretraining regimes: (a) pretraining on heterogeneous real-world time series from multiple domains, and (b) pretraining on purely synthetic data. We find that both variants yield strong performance, consistently outperforming EEGNet, a widely used convolutional baseline, and CBraMod, the most recent EEG-specific foundation model. These results suggest that generalist time series foundation models, even when pretrained on data of non-neural origin or on synthetic signals, can transfer effectively to EEG. Our findings highlight the promise of leveraging cross-domain pretrained models for brain signal analysis, suggesting that EEG may benefit from advances in the broader time series literature.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control</title>
<link>https://arxiv.org/abs/2510.27527</link>
<guid>https://arxiv.org/abs/2510.27527</guid>
<content:encoded><![CDATA[
arXiv:2510.27527v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) training is prohibitively expensive, driving interest in low-precision fully-quantized training (FQT). While novel 4-bit formats like NVFP4 offer substantial efficiency gains, achieving near-lossless training at such low precision remains challenging. We introduce TetraJet-v2, an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights, and gradients in all linear layers. We identify two critical issues hindering low-precision LLM training: weight oscillation and outliers. To address these, we propose: 1) an unbiased double-block quantization method for NVFP4 linear layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3) OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently outperforms prior FP4 training methods on pre-training LLMs across varying model sizes up to 370M and data sizes up to 200B tokens, reducing the performance gap to full-precision training by an average of 51.3%.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models</title>
<link>https://arxiv.org/abs/2510.27543</link>
<guid>https://arxiv.org/abs/2510.27543</guid>
<content:encoded><![CDATA[
arXiv:2510.27543v1 Announce Type: cross 
Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the performance of large language models (LLMs) across Arabic dialects. While recently developed Arabic and multilingual benchmarks have advanced LLM evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication. DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of 15K QA pairs across 32 academic and professional domains (22K QA pairs when also including English and MSA). The benchmark enables systematic assessment of LLM reasoning and comprehension beyond MSA, supporting both task-based and linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization. DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, thus promoting more inclusive evaluation and future model development.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities</title>
<link>https://arxiv.org/abs/2510.27545</link>
<guid>https://arxiv.org/abs/2510.27545</guid>
<content:encoded><![CDATA[
arXiv:2510.27545v1 Announce Type: cross 
Abstract: Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sybil-Resistant Service Discovery for Agent Economies</title>
<link>https://arxiv.org/abs/2510.27554</link>
<guid>https://arxiv.org/abs/2510.27554</guid>
<content:encoded><![CDATA[
arXiv:2510.27554v1 Announce Type: cross 
Abstract: x402 enables Hypertext Transfer Protocol (HTTP) services like application programming interfaces (APIs), data feeds, and inference providers to accept cryptocurrency payments for access. As agents increasingly consume these services, discovery becomes critical: which swap interface should an agent trust? Which data provider is the most reliable? We introduce TraceRank, a reputation-weighted ranking algorithm where payment transactions serve as endorsements. TraceRank seeds addresses with precomputed reputation metrics and propagates reputation through payment flows weighted by transaction value and temporal recency. Applied to x402's payment graph, this surfaces services preferred by high-reputation users rather than those with high transaction volume. Our system combines TraceRank with semantic search to respond to natural language queries with high quality results. We argue that reputation propagation resists Sybil attacks by making spam services with many low-reputation payers rank below legitimate services with few high-reputation payers. Ultimately, we aim to construct a search method for x402 enabled services that avoids infrastructure bias and has better performance than purely volume based or semantic methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs</title>
<link>https://arxiv.org/abs/2510.27558</link>
<guid>https://arxiv.org/abs/2510.27558</guid>
<content:encoded><![CDATA[
arXiv:2510.27558v1 Announce Type: cross 
Abstract: This paper presents a framework that leverages pre-trained foundation models for robotic manipulation without domain-specific training. The framework integrates off-the-shelf models, combining multimodal perception from foundation models with a general-purpose reasoning model capable of robust task sequencing. Scene graphs, dynamically maintained within the framework, provide spatial awareness and enable consistent reasoning about the environment. The framework is evaluated through a series of tabletop robotic manipulation experiments, and the results highlight its potential for building robotic manipulation systems directly on top of off-the-shelf foundation models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments</title>
<link>https://arxiv.org/abs/2510.27565</link>
<guid>https://arxiv.org/abs/2510.27565</guid>
<content:encoded><![CDATA[
arXiv:2510.27565v1 Announce Type: cross 
Abstract: As large language models become increasingly capable of generating code, evaluating their performance remains a complex and evolving challenge. Existing benchmarks primarily focus on functional correctness, overlooking the diversity of real-world coding tasks and developer expectations. To this end, we introduce a multi-language benchmark that evaluates LLM instruction-following capabilities and is extensible to operate on any set of standalone coding problems. Our benchmark evaluates instruction following in two key settings: adherence to pre-defined constraints specified with the initial problem, and the ability to perform refinements based on follow-up instructions. For this paper's analysis, we empirically evaluated our benchmarking pipeline with programming tasks from LiveBench, that are also automatically translated from Python into Java and JavaScript. Our automated benchmark reveals that models exhibit differing levels of performance across multiple dimensions of instruction-following. Our benchmarking pipeline provides a more comprehensive evaluation of code generation models, highlighting their strengths and limitations across languages and generation goals.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum</title>
<link>https://arxiv.org/abs/2510.27571</link>
<guid>https://arxiv.org/abs/2510.27571</guid>
<content:encoded><![CDATA[
arXiv:2510.27571v1 Announce Type: cross 
Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.27606</link>
<guid>https://arxiv.org/abs/2510.27606</guid>
<content:encoded><![CDATA[
arXiv:2510.27606v1 Announce Type: cross 
Abstract: Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</title>
<link>https://arxiv.org/abs/2510.27629</link>
<guid>https://arxiv.org/abs/2510.27629</guid>
<content:encoded><![CDATA[
arXiv:2510.27629v1 Announce Type: cross 
Abstract: Open-weight bio-foundation models present a dual-use dilemma. While holding great promise for accelerating scientific research and drug development, they could also enable bad actors to develop more deadly bioweapons. To mitigate the risk posed by these models, current approaches focus on filtering biohazardous data during pre-training. However, the effectiveness of such an approach remains unclear, particularly against determined actors who might fine-tune these models for malicious use. To address this gap, we propose \eval, a framework to evaluate the robustness of procedures that are intended to reduce the dual-use capabilities of bio-foundation models. \eval assesses models' virus understanding through three lenses, including sequence modeling, mutational effects prediction, and virulence prediction. Our results show that current filtering practices may not be particularly effective: Excluded knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits broader generalizability in sequence modeling. Furthermore, dual-use signals may already reside in the pretrained representations, and can be elicited via simple linear probing. These findings highlight the challenges of data filtering as a standalone procedure, underscoring the need for further research into robust safety and security strategies for open-weight bio-foundation models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation</title>
<link>https://arxiv.org/abs/2510.27632</link>
<guid>https://arxiv.org/abs/2510.27632</guid>
<content:encoded><![CDATA[
arXiv:2510.27632v1 Announce Type: cross 
Abstract: Graphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at https://github.com/google-deepmind/sketch_to_layout.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images</title>
<link>https://arxiv.org/abs/2510.27646</link>
<guid>https://arxiv.org/abs/2510.27646</guid>
<content:encoded><![CDATA[
arXiv:2510.27646v1 Announce Type: cross 
Abstract: Semantic segmentation of blood vessels is an important task in medical image analysis, but its progress is often hindered by the scarcity of large annotated datasets and the poor generalization of models across different imaging modalities. A key aspect is the tendency of Convolutional Neural Networks (CNNs) to learn texture-based features, which limits their performance when applied to new domains with different visual characteristics. We hypothesize that leveraging geometric priors of vessel shapes, such as their tubular and branching nature, can lead to more robust and data-efficient models. To investigate this, we introduce VessShape, a methodology for generating large-scale 2D synthetic datasets designed to instill a shape bias in segmentation models. VessShape images contain procedurally generated tubular geometries combined with a wide variety of foreground and background textures, encouraging models to learn shape cues rather than textures. We demonstrate that a model pre-trained on VessShape images achieves strong few-shot segmentation performance on two real-world datasets from different domains, requiring only four to ten samples for fine-tuning. Furthermore, the model exhibits notable zero-shot capabilities, effectively segmenting vessels in unseen domains without any target-specific training. Our results indicate that pre-training with a strong shape bias can be an effective strategy to overcome data scarcity and improve model generalization in blood vessel segmentation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition</title>
<link>https://arxiv.org/abs/2510.27651</link>
<guid>https://arxiv.org/abs/2510.27651</guid>
<content:encoded><![CDATA[
arXiv:2510.27651v1 Announce Type: cross 
Abstract: Modern deep neural networks (DNNs) are typically trained with a global cross-entropy loss in a supervised end-to-end manner: neurons need to store their outgoing weights; training alternates between a forward pass (computation) and a top-down backward pass (learning) which is biologically implausible. Alternatively, greedy layer-wise training eliminates the need for cross-entropy loss and backpropagation. By avoiding the computation of intermediate gradients and the storage of intermediate outputs, it reduces memory usage and helps mitigate issues such as vanishing or exploding gradients. However, most existing layer-wise training approaches have been evaluated only on relatively small datasets with simple deep architectures. In this paper, we first systematically analyze the training dynamics of popular convolutional neural networks (CNNs) trained by stochastic gradient descent (SGD) through an information-theoretic lens. Our findings reveal that networks converge layer-by-layer from bottom to top and that the flow of information adheres to a Markov information bottleneck principle. Building on these observations, we propose a novel layer-wise training approach based on the recently developed deterministic information bottleneck (DIB) and the matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each layer is trained jointly with an auxiliary classifier that connects directly to the output layer, enabling the learning of minimal sufficient task-relevant representations. We empirically validate the effectiveness of our training procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further demonstrate its applicability to a practical task involving traffic sign recognition. Our approach not only outperforms existing layer-wise training baselines but also achieves performance comparable to SGD.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Detection on Model Explanation Graphs for Explainable AI</title>
<link>https://arxiv.org/abs/2510.27655</link>
<guid>https://arxiv.org/abs/2510.27655</guid>
<content:encoded><![CDATA[
arXiv:2510.27655v1 Announce Type: cross 
Abstract: Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions but often miss higher-order structure: sets of features that act in concert. We propose Modules of Influence (MoI), a framework that (i) constructs a model explanation graph from per-instance attributions, (ii) applies community detection to find feature modules that jointly affect predictions, and (iii) quantifies how these modules relate to bias, redundancy, and causality patterns. Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules. We release stability and synergy metrics, a reference implementation, and evaluation protocols to benchmark module discovery in XAI.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems</title>
<link>https://arxiv.org/abs/2510.27659</link>
<guid>https://arxiv.org/abs/2510.27659</guid>
<content:encoded><![CDATA[
arXiv:2510.27659v1 Announce Type: cross 
Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting</title>
<link>https://arxiv.org/abs/2510.27680</link>
<guid>https://arxiv.org/abs/2510.27680</guid>
<content:encoded><![CDATA[
arXiv:2510.27680v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging. In this work, we extend VLMs to 3D positron emission tomography and computed tomography (PET/CT), a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports. We introduce a large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid rule-based and large language model (LLM) pipeline. Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reasoning with fine-grained lesion awareness, producing clinically coherent and localized findings. Comprehensive automated and human evaluations demonstrate that PETAR substantially improves PET/CT report generation quality, advancing 3D medical vision-language understanding.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2510.27688</link>
<guid>https://arxiv.org/abs/2510.27688</guid>
<content:encoded><![CDATA[
arXiv:2510.27688v1 Announce Type: cross 
Abstract: The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods</title>
<link>https://arxiv.org/abs/2403.02901</link>
<guid>https://arxiv.org/abs/2403.02901</guid>
<content:encoded><![CDATA[
arXiv:2403.02901v3 Announce Type: replace 
Abstract: Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRoPE: Rotary Position Embedding for Video Large Language Models</title>
<link>https://arxiv.org/abs/2502.11664</link>
<guid>https://arxiv.org/abs/2502.11664</guid>
<content:encoded><![CDATA[
arXiv:2502.11664v4 Announce Type: replace 
Abstract: Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Specifically, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Additionally, our approach restructures positional indices to ensure a smooth transition between video and text tokens. Extensive experiments on different models demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code is available at https://github.com/johncaged/VRoPE.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.16724</link>
<guid>https://arxiv.org/abs/2503.16724</guid>
<content:encoded><![CDATA[
arXiv:2503.16724v3 Announce Type: replace 
Abstract: Semantic interpretability in Reinforcement Learning (RL) enables transparency and verifiability of decision-making. Achieving semantic interpretability in reinforcement learning requires (1) a feature space composed of human-understandable concepts and (2) a policy that is interpretable and verifiable. However, constructing such a feature space has traditionally relied on manual human specification, which often fails to generalize to unseen environments. Moreover, even when interpretable features are available, most reinforcement learning algorithms employ black-box models as policies, thereby hindering transparency. We introduce interpretable Tree-based Reinforcement learning via Automated Concept Extraction (iTRACE), an automated framework that leverages pre-trained vision-language models (VLM) for semantic feature extraction and train a interpretable tree-based model via RL. To address the impracticality of running VLMs in RL loops, we distill their outputs into a lightweight model. By leveraging Vision-Language Models (VLMs) to automate tree-based reinforcement learning, iTRACE loosens the reliance the need for human annotation that is traditionally required by interpretable models. In addition, it addresses key limitations of VLMs alone, such as their lack of grounding in action spaces and their inability to directly optimize policies. We evaluate iTRACE across three domains: Atari games, grid-world navigation, and driving. The results show that iTRACE outperforms other interpretable policy baselines and matches the performance of black-box policies on the same interpretable feature space.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Framework for Objective-Driven Dynamical Stochastic Fields</title>
<link>https://arxiv.org/abs/2504.16115</link>
<guid>https://arxiv.org/abs/2504.16115</guid>
<content:encoded><![CDATA[
arXiv:2504.16115v2 Announce Type: replace 
Abstract: Fields offer a versatile approach for describing complex systems composed of interacting and dynamic components. In particular, some of these dynamical and stochastic systems may exhibit goal-directed behaviors aimed at achieving specific objectives, which we refer to as $\textit{intelligent fields}$. However, due to their inherent complexity, it remains challenging to develop a formal theoretical description of such systems and to effectively translate these descriptions into practical applications. In this paper, we propose three fundamental principles to establish a theoretical framework for understanding intelligent fields: complete configuration, locality, and purposefulness. Moreover, we explore methodologies for designing such fields from the perspective of artificial intelligence applications. This initial investigation aims to lay the groundwork for future theoretical developments and practical advances in understanding and harnessing the potential of such objective-driven dynamical stochastic fields.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14216</link>
<guid>https://arxiv.org/abs/2505.14216</guid>
<content:encoded><![CDATA[
arXiv:2505.14216v2 Announce Type: replace 
Abstract: Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy (pass@1) but often fails to improve capability (pass@k) of LLMs in reasoning tasks, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR struggles to improve capability as it focuses on improving the accuracy of the easier questions to the detriment of the accuracy of the most difficult questions. Second, we show that RLVR does not merely increase the success probability for the easier questions, but in our small model settings, produces quality responses that were absent in its original output distribution. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, from the experiment distilling teacher responses to in-distribution problems, we find that capability does not always improve with distillation. We conjecture that capability improves only when new knowledge is introduced, whereas distilling reasoning patterns only improves accuracy but not capability, sacrificing performance on the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in LLMs
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Trustworthy AI by Addressing its 16+2 Desiderata with Goal-Directed Commonsense Reasoning</title>
<link>https://arxiv.org/abs/2506.12667</link>
<guid>https://arxiv.org/abs/2506.12667</guid>
<content:encoded><![CDATA[
arXiv:2506.12667v2 Announce Type: replace 
Abstract: Current advances in AI and its applicability have highlighted the need to ensure its trustworthiness for legal, ethical, and even commercial reasons. Sub-symbolic machine learning algorithms, such as the LLMs, simulate reasoning but hallucinate and their decisions cannot be explained or audited (crucial aspects for trustworthiness). On the other hand, rule-based reasoners, such as Cyc, are able to provide the chain of reasoning steps but are complex and use a large number of reasoners. We propose a middle ground using s(CASP), a goal-directed constraint-based answer set programming reasoner that employs a small number of mechanisms to emulate reliable and explainable human-style commonsense reasoning. In this paper, we explain how s(CASP) supports the 16 desiderata for trustworthy AI introduced by Doug Lenat and Gary Marcus (2023), and two additional ones: inconsistency detection and the assumption of alternative worlds. To illustrate the feasibility and synergies of s(CASP), we present a range of diverse applications, including a conversational chatbot and a virtually embodied reasoner.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't throw the baby out with the bathwater: How and why deep learning for ARC</title>
<link>https://arxiv.org/abs/2506.14276</link>
<guid>https://arxiv.org/abs/2506.14276</guid>
<content:encoded><![CDATA[
arXiv:2506.14276v2 Announce Type: replace 
Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale Orchestration</title>
<link>https://arxiv.org/abs/2506.19500</link>
<guid>https://arxiv.org/abs/2506.19500</guid>
<content:encoded><![CDATA[
arXiv:2506.19500v2 Announce Type: replace 
Abstract: Large language models (LLMs) have recently demonstrated the ability to act as function call agents by invoking external tools, enabling them to solve tasks beyond their static knowledge. However, existing agents typically call tools step by step at a time without a global view of task structure. As tools depend on each other, this leads to error accumulation and limited scalability, particularly when scaling to thousands of tools. To address these limitations, we propose NaviAgent, a novel bilevel architecture that decouples task planning from tool execution through graph-based modeling of the tool ecosystem. At the task-planning level, the LLM-based agent decides whether to respond directly, clarify user intent, invoke a toolchain, or execute tool outputs, ensuring broad coverage of interaction scenarios independent of inter-tool complexity. At the execution level, a continuously evolving Tool World Navigation Model (TWNM) encodes structural and behavioral relations among tools, guiding the agent to generate scalable and robust invocation sequences. By incorporating feedback from real tool interactions, NaviAgent supports closed-loop optimization of planning and execution, moving beyond tool calling toward adaptive navigation of large-scale tool ecosystems. Experiments show that NaviAgent achieves the best task success rates across models and tasks, and integrating TWMN further boosts performance by up to 17 points on complex tasks, underscoring its key role in toolchain orchestration.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiRA: A Hierarchical Reasoning Framework for Decoupled Planning and Execution in Deep Search</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
arXiv:2507.02652v2 Announce Type: replace 
Abstract: Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Red Teaming AI Red Teaming</title>
<link>https://arxiv.org/abs/2507.05538</link>
<guid>https://arxiv.org/abs/2507.05538</guid>
<content:encoded><![CDATA[
arXiv:2507.05538v2 Announce Type: replace 
Abstract: Red teaming has evolved from its origins in military applications to become a widely adopted methodology in cybersecurity and AI. In this paper, we take a critical look at the practice of AI red teaming. We argue that despite its current popularity in AI governance, there exists a significant gap between red teaming's original intent as a critical thinking exercise and its narrow focus on discovering model-level flaws in the context of generative AI. Current AI red teaming efforts focus predominantly on individual model vulnerabilities while overlooking the broader sociotechnical systems and emergent behaviors that arise from complex interactions between models, users, and environments. To address this deficiency, we propose a comprehensive framework operationalizing red teaming in AI systems at two levels: macro-level system red teaming spanning the entire AI development lifecycle, and micro-level model red teaming. Drawing on cybersecurity experience and systems theory, we further propose a set of six recommendations. In these, we emphasize that effective AI red teaming requires multifunctional teams that examine emergent risks, systemic vulnerabilities, and the interplay between technical and social factors.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Isn't Relational Learning Taking Over the World?</title>
<link>https://arxiv.org/abs/2507.13558</link>
<guid>https://arxiv.org/abs/2507.13558</guid>
<content:encoded><![CDATA[
arXiv:2507.13558v4 Announce Type: replace 
Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind</title>
<link>https://arxiv.org/abs/2507.16184</link>
<guid>https://arxiv.org/abs/2507.16184</guid>
<content:encoded><![CDATA[
arXiv:2507.16184v2 Announce Type: replace 
Abstract: We report a structural convergence among four influential theories of mind: Kahneman's dual-system theory, Friston's predictive processing, Minsky's society of mind, and Clark's extended mind, emerging unintentionally within a practical AI architecture known as Agentic Flow. Designed to address the limitations of large language models (LLMs), Agentic Flow comprises five interlocking modules: Retrieval, Cognition, Control, Memory, and Action, organized into a repeatable cognitive loop. Although originally inspired only by Minsky and Clark, subsequent analysis revealed that its structure echoes computational motifs from all four theories, suggesting that theoretical convergence can emerge naturally from implementation demands rather than deliberate synthesis. Controlled evaluations confirmed this: the structured agent achieved 95.8% task success versus 62.3% for baseline LLMs, demonstrating robust constraint adherence and reproducible reasoning. We describe this convergence under a broader descriptive meta-architecture called PEACE, highlighting recurring design patterns such as predictive modeling, associative recall, and error-sensitive control. Later formalized as the Structured Cognitive Loop (SCL), this framework generalizes the same principles as a foundation for behavioral intelligence in LLM-based agents. Rather than claiming theoretical unification, this paper proposes that intelligent architectures may evolve toward shared structural patterns shaped by practical constraints. As a position paper, it aims to frame this convergence as an interpretive reflection rather than a finalized theory, inviting further theoretical and experimental dialogue. Agentic Flow, or equivalently the Structured Cognitive Loop, thus offers a glimpse of how a unified cognitive form can arise not from abstraction, but from the necessities of real-world reasoning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents</title>
<link>https://arxiv.org/abs/2508.04412</link>
<guid>https://arxiv.org/abs/2508.04412</guid>
<content:encoded><![CDATA[
arXiv:2508.04412v2 Announce Type: replace 
Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At that, a model poses as an instantaneous domain model backend. Ought to suggest interaction, it is consulted with a web-based task and respective application state. The key problem lies in application state serialisation - referred to as snapshot. State-of-the-art web agents are premised on grounded GUI snapshots, i.e., screenshots enhanced with visual cues. Not least to resemble human perception, but for images representing relatively cheap means of model input. LLM vision still lag behind code interpretation capabilities. DOM snapshots, which structurally resemble HTML, impose a desired alternative. Vast model input token size, however, disables reliable implementation with web agents to date. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a grounded GUI snapshot baseline (65%) - within the same input token order of magnitude (1e3). Our best evaluated configurations - one token order above, but within the model's context window - outperform this baseline by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificially intelligent agents in the social and behavioral sciences: A history and outlook</title>
<link>https://arxiv.org/abs/2510.05743</link>
<guid>https://arxiv.org/abs/2510.05743</guid>
<content:encoded><![CDATA[
arXiv:2510.05743v2 Announce Type: replace 
Abstract: We review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction</title>
<link>https://arxiv.org/abs/2510.16559</link>
<guid>https://arxiv.org/abs/2510.16559</guid>
<content:encoded><![CDATA[
arXiv:2510.16559v3 Announce Type: replace 
Abstract: Engineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce BuildArena, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, BuildArena comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. The project page is at https://build-arena.github.io/.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</title>
<link>https://arxiv.org/abs/2510.19954</link>
<guid>https://arxiv.org/abs/2510.19954</guid>
<content:encoded><![CDATA[
arXiv:2510.19954v2 Announce Type: replace 
Abstract: Relational multi-table data is common in domains such as e-commerce, healthcare, and scientific research, and can be naturally represented as heterogeneous temporal graphs with multi-modal node attributes. Existing graph neural networks (GNNs) rely on schema-specific feature encoders, requiring separate modules for each node type and feature column, which hinders scalability and parameter sharing. We introduce RELATE (Relational Encoder for Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature encoder that can be used with any general purpose GNN. RELATE employs shared modality-specific encoders for categorical, numerical, textual, and temporal attributes, followed by a Perceiver-style cross-attention module that aggregates features into a fixed-size, permutation-invariant node representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark, where it achieves performance within 3% of schema-specific encoders while reducing parameter counts by up to 5x. This design supports varying schemas and enables multi-dataset pretraining for general-purpose GNNs, paving the way toward foundation models for relational graph data.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms</title>
<link>https://arxiv.org/abs/2510.20621</link>
<guid>https://arxiv.org/abs/2510.20621</guid>
<content:encoded><![CDATA[
arXiv:2510.20621v2 Announce Type: replace 
Abstract: Interpretable-by-design models are crucial for fostering trust, accountability, and safe adoption of automated decision-making models in real-world applications. In this paper we formalize the ground for the MIMOSA (Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a comprehensive methodology for generating predictive models that balance interpretability with performance while embedding key ethical properties. We formally define here the supervised learning setting across diverse decision-making tasks and data types, including tabular data, time series, images, text, transactions, and trajectories. We characterize three major families of interpretable models: feature importance, rule, and instance based models. For each family, we analyze their interpretability dimensions, reasoning mechanisms, and complexity. Beyond interpretability, we formalize three critical ethical properties, namely causality, fairness, and privacy, providing formal definitions, evaluation metrics, and verification procedures for each. We then examine the inherent trade-offs between these properties and discuss how privacy requirements, fairness constraints, and causal reasoning can be embedded within interpretable pipelines. By evaluating ethical measures during model generation, this framework establishes the theoretical foundations for developing AI systems that are not only accurate and interpretable but also fair, privacy-preserving, and causally aware, i.e., trustworthy.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives</title>
<link>https://arxiv.org/abs/2510.26606</link>
<guid>https://arxiv.org/abs/2510.26606</guid>
<content:encoded><![CDATA[
arXiv:2510.26606v2 Announce Type: replace 
Abstract: Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Application of Utility Theory in Robotics and Artificial Intelligence: A Survey</title>
<link>https://arxiv.org/abs/2306.09445</link>
<guid>https://arxiv.org/abs/2306.09445</guid>
<content:encoded><![CDATA[
arXiv:2306.09445v2 Announce Type: replace-cross 
Abstract: As a unifying concept in economics, game theory, and operations research, even in the Robotics and AI field, the utility is used to evaluate the level of individual needs, preferences, and interests. Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society. Although these systems' complex, large-scale, and long-term behaviors are strongly determined by the fundamental characteristics of the underlying relationships, there has been less discussion on the theoretical aspects of mechanisms and the fields of applications in Robotics and AI. This paper introduces a utility-orient needs paradigm to describe and evaluate inter and outer relationships among agents' interactions. Then, we survey existing literature in relevant fields to support it and propose several promising research directions along with some open problems deemed necessary for further investigations.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2403.15049</link>
<guid>https://arxiv.org/abs/2403.15049</guid>
<content:encoded><![CDATA[
arXiv:2403.15049v3 Announce Type: replace-cross 
Abstract: Developing Vision-and-Language Navigation (VLN) agents typically assumes a \textit{train-once-deploy-once} strategy, which is unrealistic as deployed agents continually encounter novel environments. To address this, we propose the Continual Vision-and-Language Navigation (CVLN) paradigm, where agents learn and adapt incrementally across multiple \textit{scene domains}. CVLN includes two setups: Initial-instruction based CVLN for instruction-following, and Dialogue-based CVLN for dialogue-guided navigation. We also introduce two simple yet effective baselines for sequential decision-making: Perplexity Replay (PerpR), which replays difficult episodes, and Episodic Self-Replay (ESR), which stores and revisits action logits during training. Experiments show that existing continual learning methods fall short for CVLN, while PerpR and ESR achieve better performance by efficiently utilizing replay memory.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</title>
<link>https://arxiv.org/abs/2407.20183</link>
<guid>https://arxiv.org/abs/2407.20183</guid>
<content:encoded><![CDATA[
arXiv:2407.20183v2 Announce Type: replace-cross 
Abstract: Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepoMasterEval: Evaluating Code Completion via Real-World Repositories</title>
<link>https://arxiv.org/abs/2408.03519</link>
<guid>https://arxiv.org/abs/2408.03519</guid>
<content:encoded><![CDATA[
arXiv:2408.03519v2 Announce Type: replace-cross 
Abstract: With the growing reliance on automated code completion tools in software development, the need for comprehensive evaluation benchmarks has become critical. Existing benchmarks focus more on code completion in function and class level by providing text descriptions to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes existing evaluation benchmarks poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. Our empirical evaluation on 10 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report variance in model performance in real-world scenarios. The deployment of RepoMasterEval also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI</title>
<link>https://arxiv.org/abs/2409.16978</link>
<guid>https://arxiv.org/abs/2409.16978</guid>
<content:encoded><![CDATA[
arXiv:2409.16978v2 Announce Type: replace-cross 
Abstract: Explainable AI (XAI) aims to make AI systems more transparent, yet many practices emphasise mathematical rigour over practical user needs. We propose an alternative to this model-centric approach by following a design thinking process for the emerging XAI field of training data attribution (TDA), which risks repeating solutionist patterns seen in other subfields. However, because TDA is in its early stages, there is a valuable opportunity to shape its direction through user-centred practices. We engage directly with machine learning developers via a needfinding interview study (N=6) and a scenario-based interactive user study (N=31) to ground explanations in real workflows. Our exploration of the TDA design space reveals novel tasks for data-centric explanations useful to developers, such as grouping training samples behind specific model behaviours or identifying undersampled data. We invite the TDA, XAI, and HCI communities to engage with these tasks to strengthen their research's practical relevance and human impact.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks</title>
<link>https://arxiv.org/abs/2410.05102</link>
<guid>https://arxiv.org/abs/2410.05102</guid>
<content:encoded><![CDATA[
arXiv:2410.05102v3 Announce Type: replace-cross 
Abstract: Direct alignment algorithms have proven an effective step for aligning language models to human-desired behaviors. Current variants of the Direct Preference Optimization objective have focused on a strict setting where all tokens are contributing signals of KL divergence and rewards to the loss function. However, human preference is not affected equally by each word in a sequence but is often dependent on specific words or phrases, e.g. existence of toxic terms leads to non-preferred responses. Based on this observation, we argue that not all tokens should be weighted equally during PO and propose a flexible objective termed SparsePO, that aims to automatically learn to weight the KL divergence and reward corresponding to each token during PO training. We propose two different variants of weight-masks that can either be derived from the reference model itself or learned on the fly. Notably, our method induces sparsity in the learned masks, allowing the model to learn how to best balance reward and KL divergence contributions at the token level, learning an optimal level of mask sparsity. Extensive experiments illustrate the effectiveness of our approach at aligning to preference proxies, including sentiment control, helpfulness and harmlessness, and summary quality. Our method obtains +10% and +3% win rate points in summarization and dialogue scenarios, respectively, without compromising model reasoning or the relevancy and faithfulness of the summary response.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification</title>
<link>https://arxiv.org/abs/2410.22377</link>
<guid>https://arxiv.org/abs/2410.22377</guid>
<content:encoded><![CDATA[
arXiv:2410.22377v3 Announce Type: replace-cross 
Abstract: In recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable interest in the field of time series analysis, due to their ability to capture, at once, dependencies among variables and across time points. The objective of this systematic literature review is hence to provide a comprehensive overview of the various modeling approaches and application domains of GNNs for time series classification and forecasting. A database search was conducted, and 366 papers were selected for a detailed examination of the current state-of-the-art in the field. This examination is intended to offer to the reader a comprehensive review of proposed models, links to related source code, available datasets, benchmark models, and fitting results. All this information is hoped to assist researchers in their studies. To the best of our knowledge, this is the first and broadest systematic literature review presenting a detailed comparison of results from current spatio-temporal GNN models applied to different domains. In its final part, this review discusses current limitations and challenges in the application of spatio-temporal GNNs, such as comparability, reproducibility, explainability, poor information capacity, and scalability. This paper is complemented by a GitHub repository at https://github.com/FlaGer99/SLR-Spatio-Temporal-GNN.git providing additional interactive tools to further explore the presented findings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representative Social Choice: From Learning Theory to AI Alignment</title>
<link>https://arxiv.org/abs/2410.23953</link>
<guid>https://arxiv.org/abs/2410.23953</guid>
<content:encoded><![CDATA[
arXiv:2410.23953v4 Announce Type: replace-cross 
Abstract: Social choice theory is the study of preference aggregation across a population, used both in mechanism design for human agents and in the democratic alignment of language models. In this study, we propose the representative social choice framework for the modeling of democratic representation in collective decisions, where the number of issues and individuals are too large for mechanisms to consider all preferences directly. These scenarios are widespread in real-world decision-making processes, such as jury trials, legislation, corporate governance, and, more recently, language model alignment. In representative social choice, the population is represented by a finite sample of individual-issue pairs based on which social choice decisions are made. We show that many of the deepest questions in representative social choice can be formulated as statistical learning problems, and prove the generalization properties of social choice mechanisms using the theory of machine learning. We further formulate axioms for representative social choice, and prove Arrow-like impossibility theorems with new combinatorial tools of analysis. Our framework introduces the representative approach to social choice, opening up research directions at the intersection of social choice, learning theory, and AI alignment.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models</title>
<link>https://arxiv.org/abs/2411.00918</link>
<guid>https://arxiv.org/abs/2411.00918</guid>
<content:encoded><![CDATA[
arXiv:2411.00918v2 Announce Type: replace-cross 
Abstract: Mixture of experts (MoE) architectures have become a cornerstone for scaling up and are a key component in most large language models such as GPT-OSS, DeepSeek-V3, Llama-4, and Gemini-2.5. However, systematic research on MoE remains severely constrained by the prohibitive computational costs of training and evaluation, restricting large-scale studies accessible to most researchers. We introduce LibMoE, a unified framework for reproducible, efficient, and extensible MoE research that supports both pretraining and sparse-upcycling regimes. Beyond unified implementations, the framework provides transparent analytical tools for probing routing and expert dynamics. Leveraging this foundation, we conduct a comprehensive analysis along three dimensions: (i) routing dynamics, covering expert selection patterns, routing stability and optimality, and how routing entropy reveals task specialization and expert diversity; (ii) the effect of lightweight initialization on load balancing, demonstrating how subtle changes in router initialization shape early expert utilization; and (iii) training regime differences, revealing how sparse upcycling and full pretraining exhibit distinct routing patterns and stability profiles. By lowering the barrier to entry and standardizing evaluation, along with our comprehensive analysis, LibMoE broadens access to MoE research and establishes a reliable benchmark to guide future innovations. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Offline Reinforcement Learning with Linearly Structured f-Divergence Regularization</title>
<link>https://arxiv.org/abs/2411.18612</link>
<guid>https://arxiv.org/abs/2411.18612</guid>
<content:encoded><![CDATA[
arXiv:2411.18612v2 Announce Type: replace-cross 
Abstract: The Robust Regularized Markov Decision Process (RRMDP) is proposed to learn policies robust to dynamics shifts by adding regularization to the transition dynamics in the value function. Existing methods mostly use unstructured regularization, potentially leading to conservative policies under unrealistic transitions. To address this limitation, we propose a novel framework, the $d$-rectangular linear RRMDP ($d$-RRMDP), which introduces latent structures into both transition kernels and regularization. We focus on offline reinforcement learning, where an agent learns policies from a precollected dataset in the nominal environment. We develop the Robust Regularized Pessimistic Value Iteration (R2PVI) algorithm that employs linear function approximation for robust policy learning in $d$-RRMDPs with $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, demonstrating that these bounds are influenced by how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. We establish information-theoretic lower bounds to verify that our algorithm is near-optimal. Finally, numerical experiments validate that R2PVI learns robust policies and exhibits superior computational efficiency compared to baseline methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents</title>
<link>https://arxiv.org/abs/2412.13178</link>
<guid>https://arxiv.org/abs/2412.13178</guid>
<content:encoded><![CDATA[
arXiv:2412.13178v5 Announce Type: replace-cross 
Abstract: With the integration of large language models (LLMs), embodied agents have strong capabilities to understand and plan complicated natural language instructions. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in the real world. Existing benchmarks predominantly overlook critical safety risks, focusing solely on planning performance, while a few evaluate LLMs' safety awareness only on non-interactive image-text data. To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that, although agents based on different design frameworks exhibit substantial differences in task success rates, their overall safety awareness remains weak. The most safety-conscious baseline achieves only a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing the LLM driving the agent does not lead to notable improvements in safety awareness. Dataset and codes are available in https://github.com/shengyin1224/SafeAgentBench and https://huggingface.co/datasets/safeagentbench/SafeAgentBench.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual State Space Models for Structured Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2502.01673</link>
<guid>https://arxiv.org/abs/2502.01673</guid>
<content:encoded><![CDATA[
arXiv:2502.01673v3 Announce Type: replace-cross 
Abstract: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-device Computation of Single-lead ECG Parameters for Real-time Remote Cardiac Health Assessment: A Real-world Validation Study</title>
<link>https://arxiv.org/abs/2502.17499</link>
<guid>https://arxiv.org/abs/2502.17499</guid>
<content:encoded><![CDATA[
arXiv:2502.17499v3 Announce Type: replace-cross 
Abstract: Accurate, continuous out-of-hospital electrocardiogram (ECG) parameter measurement is vital for real-time cardiac health monitoring and telemedicine. On-device computation of single-lead ECG parameters enables timely assessment without reliance on centralized data processing, advancing personalized, ubiquitous cardiac care-yet comprehensive validation across heterogeneous real-world populations remains limited. This study validated the on-device algorithm FeatureDB (https://github.com/PKUDigitalHealth/FeatureDB) using two datasets: HeartVoice-ECG-lite (369 participants with single-lead ECGs annotated by two physicians) and PTB-XL/PTB-XL+ (21,354 patients with 12-lead ECGs and physicians' diagnostic annotations). FeatureDB computed PR, QT, and QTc intervals, with accuracy evaluated against physician annotations via mean absolute error (MAE), correlation analysis, and Bland-Altman analysis. Diagnostic performance for first-degree atrioventricular block (AVBI, PR-based) and long QT syndrome (LQT, QTc-based) was benchmarked against commercial 12-lead systems (12SL, Uni-G) and open-source algorithm Deli, using AUC, accuracy, sensitivity, and specificity. Results showed high concordance with expert annotations (Pearson correlations: 0.836-0.960), MAEs matching inter-observer variability, and minimal bias. AVBI AUC reached 0.787 (12SL: 0.859; Uni-G: 0.812; Deli: 0.501); LQT AUC was 0.684 (12SL: 0.716; Uni-G: 0.605; Deli: 0.569)-comparable to commercial tools and superior to open-source alternatives. FeatureDB delivers physician-level parameter accuracy and commercial-grade abnormality detection via single-lead devices, supporting scalable telemedicine, decentralized cardiac screening, and continuous monitoring in community and outpatient settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training a Generally Curious Agent</title>
<link>https://arxiv.org/abs/2502.17543</link>
<guid>https://arxiv.org/abs/2502.17543</guid>
<content:encoded><![CDATA[
arXiv:2502.17543v4 Announce Type: replace-cross 
Abstract: Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present Paprika, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Best-of-N Selection for Large Language Models via Self-Certainty</title>
<link>https://arxiv.org/abs/2502.18581</link>
<guid>https://arxiv.org/abs/2502.18581</guid>
<content:encoded><![CDATA[
arXiv:2502.18581v2 Announce Type: replace-cross 
Abstract: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Adversarial Training against Sparse Attacks Requires Loss Smoothing</title>
<link>https://arxiv.org/abs/2502.21041</link>
<guid>https://arxiv.org/abs/2502.21041</guid>
<content:encoded><![CDATA[
arXiv:2502.21041v2 Announce Type: replace-cross 
Abstract: This paper studies fast adversarial training against sparse adversarial perturbations bounded by $l_0$ norm. We demonstrate the challenges of employing $1$-step attacks on $l_0$ bounded perturbations for fast adversarial training, including degraded performance and the occurrence of catastrophic overfitting (CO). We highlight that CO in $l_0$ adversarial training is caused by sub-optimal perturbation locations of $1$-step attack. Theoretical and empirical analyses reveal that the loss landscape of $l_0$ adversarial training is more craggy compared to its $l_\infty$, $l_2$ and $l_1$ counterparts. Moreover, we corroborate that the craggy loss landscape can aggravate CO. To address these issues, we propose Fast-LS-$l_0$ that incorporates soft labels and the trade-off loss function to smooth the adversarial loss landscape. Extensive experiments demonstrate our method can overcome the challenge of catastrophic overfitting, achieve state-of-the-art performance, and narrow down the performance gap between $1$-step and multi-step adversarial training against sparse attacks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More of the Same: Persistent Representational Harms Under Increased Representation</title>
<link>https://arxiv.org/abs/2503.00333</link>
<guid>https://arxiv.org/abs/2503.00333</guid>
<content:encoded><![CDATA[
arXiv:2503.00333v3 Announce Type: replace-cross 
Abstract: To recognize and mitigate the harms of generative AI systems, it is crucial to consider whether and how different societal groups are represented by these systems. A critical gap emerges when naively measuring or improving who is represented, as this does not consider how people are represented. In this work, we develop GAS(P), an evaluation methodology for surfacing distribution-level group representational biases in generated text, tackling the setting where groups are unprompted (i.e., groups are not specified in the input to generative systems). We apply this novel methodology to investigate gendered representations in occupations across state-of-the-art large language models. We show that, even though the gender distribution when models are prompted to generate biographies leads to a large representation of women, even representational biases persist in how different genders are represented. Our evaluation methodology reveals that there are statistically significant distribution-level differences in the word choice used to describe biographies and personas of different genders across occupations, and we show that many of these differences are associated with representational harms and stereotypes. Our empirical findings caution that naively increasing (unprompted) representation may inadvertently proliferate representational biases, and our proposed evaluation methodology enables systematic and rigorous measurement of the problem.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>(How) Do Language Models Track State?</title>
<link>https://arxiv.org/abs/2503.02854</link>
<guid>https://arxiv.org/abs/2503.02854</guid>
<content:encoded><![CDATA[
arXiv:2503.02854v3 Announce Type: replace-cross 
Abstract: Transformer language models (LMs) exhibit behaviors -- from storytelling to code generation -- that seem to require tracking the unobserved state of an evolving world. How do they do this? We study state tracking in LMs trained or fine-tuned to compose permutations (i.e., to compute the order of a set of objects after a sequence of swaps). Despite the simple algebraic structure of this problem, many other tasks (e.g., simulation of finite automata and evaluation of boolean expressions) can be reduced to permutation composition, making it a natural model for state tracking in general. We show that LMs consistently learn one of two state tracking mechanisms for this task. The first closely resembles the "associative scan" construction used in recent theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second uses an easy-to-compute feature (permutation parity) to partially prune the space of outputs, and then refines this with an associative scan. LMs that learn the former algorithm tend to generalize better and converge faster, and we show how to steer LMs toward one or the other with intermediate training tasks that encourage or suppress the heuristics. Our results demonstrate that transformer LMs, whether pre-trained or fine-tuned, can learn to implement efficient and interpretable state-tracking mechanisms, and the emergence of these mechanisms can be predicted and controlled.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.12989</link>
<guid>https://arxiv.org/abs/2503.12989</guid>
<content:encoded><![CDATA[
arXiv:2503.12989v3 Announce Type: replace-cross 
Abstract: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modelling Emotions in Face-to-Face Setting: The Interplay of Eye-Tracking, Personality, and Temporal Dynamics</title>
<link>https://arxiv.org/abs/2503.16532</link>
<guid>https://arxiv.org/abs/2503.16532</guid>
<content:encoded><![CDATA[
arXiv:2503.16532v2 Announce Type: replace-cross 
Abstract: Accurate emotion recognition is pivotal for nuanced and engaging human-computer interactions, yet remains difficult to achieve, especially in dynamic, conversation-like settings. In this study, we showcase how integrating eye-tracking data, temporal dynamics, and personality traits can substantially enhance the detection of both perceived and felt emotions. Seventy-three participants viewed short, speech-containing videos from the CREMA-D dataset, while being recorded for eye-tracking signals (pupil size, fixation patterns), Big Five personality assessments, and self-reported emotional states. Our neural network models combined these diverse inputs including stimulus emotion labels for contextual cues and yielded marked performance gains compared to the state-of-the-art. Specifically, perceived valence predictions reached a macro F1-score of 0.76, and models incorporating personality traits and stimulus information demonstrated significant improvements in felt emotion accuracy. These results highlight the benefit of unifying physiological, individual and contextual factors to address the subjectivity and complexity of emotional expression. Beyond validating the role of user-specific data in capturing subtle internal states, our findings inform the design of future affective computing and human-agent systems, paving the way for more adaptive and cross-individual emotional intelligence in real-world interactions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Spoofing Detection using Deep Learning</title>
<link>https://arxiv.org/abs/2503.19223</link>
<guid>https://arxiv.org/abs/2503.19223</guid>
<content:encoded><![CDATA[
arXiv:2503.19223v2 Announce Type: replace-cross 
Abstract: Digital image spoofing has emerged as a significant security threat in biometric authentication systems, particularly those relying on facial recognition. This study evaluates the performance of three vision based models, MobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in image classification, utilizing a dataset of 150,986 images divided into training , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof detection is critical for enhancing the security of image recognition systems, and this research compares the models effectiveness through accuracy, precision, recall, and F1 score metrics. Results reveal that MobileNetV2 outperforms other architectures on the test dataset, achieving an accuracy of 91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared to ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation dataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17% accuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during training and superior generalization to unseen data, despite both models showing signs of overfitting. These findings highlight MobileNetV2 balanced performance and robustness, making it the preferred choice for spoof detection applications where reliability on new data is essential. The study underscores the importance of model selection in security sensitive contexts and suggests MobileNetV2 as a practical solution for real world deployment.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations</title>
<link>https://arxiv.org/abs/2503.22575</link>
<guid>https://arxiv.org/abs/2503.22575</guid>
<content:encoded><![CDATA[
arXiv:2503.22575v2 Announce Type: replace-cross 
Abstract: Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models</title>
<link>https://arxiv.org/abs/2503.23875</link>
<guid>https://arxiv.org/abs/2503.23875</guid>
<content:encoded><![CDATA[
arXiv:2503.23875v2 Announce Type: replace-cross 
Abstract: The development of control policies for multi-robot systems traditionally follows a complex and labor-intensive process, often lacking the flexibility to adapt to dynamic tasks. This has motivated research on methods to automatically create control policies. However, these methods require iterative processes of manually crafting and refining objective functions, thereby prolonging the development cycle. This work introduces \textit{GenSwarm}, an end-to-end system that leverages large language models to automatically generate and deploy control policies for multi-robot tasks based on simple user instructions in natural language. As a multi-language-agent system, GenSwarm achieves zero-shot learning, enabling rapid adaptation to altered or unseen tasks. The white-box nature of the code policies ensures strong reproducibility and interpretability. With its scalable software and hardware architectures, GenSwarm supports efficient policy deployment on both simulated and real-world multi-robot systems, realizing an instruction-to-execution end-to-end functionality that could prove valuable for robotics specialists and non-specialists alike.The code of the proposed GenSwarm system is available online: https://github.com/WindyLab/GenSwarm.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm</title>
<link>https://arxiv.org/abs/2504.03717</link>
<guid>https://arxiv.org/abs/2504.03717</guid>
<content:encoded><![CDATA[
arXiv:2504.03717v2 Announce Type: replace-cross 
Abstract: Post-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. In this paper, we propose RaanA, a unified PTQ framework that overcomes these challenges by introducing two novel components: 1) RaBitQ-H, a variant of a randomized vector quantization method RaBitQ, designed for fast, accurate, and highly efficient quantization; and 2) AllocateBits, an algorithm that optimally allocates bit-widths across layers based on their quantization sensitivity. RaanA achieves competitive performance with state-of-the-art quantization methods while being extremely fast, requiring minimal calibration data, and enabling flexible bit allocation. Extensive experiments demonstrate RaanA's efficacy in balancing efficiency and accuracy. The code is publicly available at https://github.com/FFTYYY/RaanA .
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers</title>
<link>https://arxiv.org/abs/2504.15827</link>
<guid>https://arxiv.org/abs/2504.15827</guid>
<content:encoded><![CDATA[
arXiv:2504.15827v2 Announce Type: replace-cross 
Abstract: Existing machine unlearning (MU) approaches exhibit significant sensitivity to hyperparameters, requiring meticulous tuning that limits practical deployment. In this work, we first empirically demonstrate the instability and suboptimal performance of existing popular MU methods when deployed in different scenarios. To address this issue, we propose Dual Optimizer (DualOptim), which incorporates adaptive learning rate and decoupled momentum factors. Empirical and theoretical evidence demonstrates that DualOptim contributes to effective and stable unlearning. Through extensive experiments, we show that DualOptim can significantly boost MU efficacy and stability across diverse tasks, including image classification, image generation, and large language models, making it a versatile approach to empower existing MU algorithms.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVA: Towards Agentic Video Analytics with Vision Language Models</title>
<link>https://arxiv.org/abs/2505.00254</link>
<guid>https://arxiv.org/abs/2505.00254</guid>
<content:encoded><![CDATA[
arXiv:2505.00254v5 Announce Type: replace-cross 
Abstract: AI-driven video analytics has become increasingly important across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Vision Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively-significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%. The source code of AVA is available at https://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at https://huggingface.co/datasets/iesc/Ava-100.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.00455</link>
<guid>https://arxiv.org/abs/2505.00455</guid>
<content:encoded><![CDATA[
arXiv:2505.00455v4 Announce Type: replace-cross 
Abstract: Effective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. Motivated by growing demands to surface tacit knowledge, we present the Data Therapist, a web-based system that helps domain experts externalize such implicit knowledge through a mixed-initiative process combining iterative Q&amp;A with interactive annotation. Powered by a large language model, the system automatically analyzes user-supplied datasets, prompts users with targeted questions, and supports annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. A qualitative study with expert pairs from Accounting, Political Science, and Computer Security revealed recurring patterns in how expert reason about their data and highlighted opportunities for AI support to enhance visualization design.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness</title>
<link>https://arxiv.org/abs/2505.07985</link>
<guid>https://arxiv.org/abs/2505.07985</guid>
<content:encoded><![CDATA[
arXiv:2505.07985v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to fourfold. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Visual Question Answering for Uncertainty-Aware Selective Prediction</title>
<link>https://arxiv.org/abs/2505.09591</link>
<guid>https://arxiv.org/abs/2505.09591</guid>
<content:encoded><![CDATA[
arXiv:2505.09591v2 Announce Type: replace-cross 
Abstract: Despite remarkable progress in recent years, vision language models (VLMs) remain prone to overconfidence and hallucinations on tasks such as Visual Question Answering (VQA) and Visual Reasoning. Bayesian methods can potentially improve reliability by helping models selectively predict, that is, models respond only when they are sufficiently confident. Unfortunately, Bayesian methods are often assumed to be costly and ineffective for large models, and so far there exists little evidence to show otherwise, especially for multimodal applications. Here, we show the effectiveness and competitive edge of variational Bayes for selective prediction in VQA for the first time. We build on recent advances in variational methods for deep learning and propose an extension called "Variational VQA". This method improves calibration and yields significant gains for selective prediction on VQA and Visual Reasoning, particularly when the error tolerance is low ($\leq 1\%$). Often, just one posterior sample can yield more reliable answers than those obtained by models trained with AdamW. In addition, we propose a new risk-averse selector that outperforms standard sample averaging by considering the variance of predictions. Overall, we present compelling evidence that variational learning is a viable option to make large VLMs safer and more trustworthy.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training</title>
<link>https://arxiv.org/abs/2505.11594</link>
<guid>https://arxiv.org/abs/2505.11594</guid>
<content:encoded><![CDATA[
arXiv:2505.11594v2 Announce Type: replace-cross 
Abstract: The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Diffusion Transformers Efficiently via $\mu$P</title>
<link>https://arxiv.org/abs/2505.15270</link>
<guid>https://arxiv.org/abs/2505.15270</guid>
<content:encoded><![CDATA[
arXiv:2505.15270v3 Announce Type: replace-cross 
Abstract: Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\mu$P of mainstream diffusion Transformers, including U-ViT, DiT, PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\mu$P on text-to-image generation by scaling PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\mu$P as a principled and efficient framework for scaling diffusion Transformers.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R$^2$ec: Towards Large Recommender Models with Reasoning</title>
<link>https://arxiv.org/abs/2505.16994</link>
<guid>https://arxiv.org/abs/2505.16994</guid>
<content:encoded><![CDATA[
arXiv:2505.16994v3 Announce Type: replace-cross 
Abstract: Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. In this work, we propose R$^2$ec, a unified large recommender model with intrinsic reasoning capability. R$^2$ec introduces a dual-head architecture that supports both reasoning chain generation and efficient item prediction in a single model, significantly reducing inference latency. To overcome the lack of annotated reasoning data, we design RecPO, a reinforcement learning framework that optimizes reasoning and recommendation jointly with a novel fused reward mechanism. Extensive experiments on three datasets demonstrate that R$^2$ec outperforms traditional, LLM-based, and reasoning-augmented recommender baselines, while further analyses validate its competitive efficiency among conventional LLM-based recommender baselines and strong adaptability to diverse recommendation scenarios. Code and checkpoints available at https://github.com/YRYangang/RRec.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAIA: A Foundation Model for Operational Atmospheric Dynamics</title>
<link>https://arxiv.org/abs/2505.18179</link>
<guid>https://arxiv.org/abs/2505.18179</guid>
<content:encoded><![CDATA[
arXiv:2505.18179v2 Announce Type: replace-cross 
Abstract: We introduce GAIA (Geospatial Artificial Intelligence for Atmospheres), a hybrid self-supervised geospatial foundation model that fuses Masked Autoencoders (MAE) with self-distillation with no labels (DINO) to generate semantically rich representations from global geostationary satellite imagery. Pre-trained on 15 years of globally-merged infrared observations (2001-2015), GAIA learns disentangled representations that capture atmospheric dynamics rather than trivial diurnal patterns, as evidenced by distributed principal component structure and temporal coherence analysis. We demonstrate robust reconstruction capabilities across varying data availability (30-95% masking), achieving superior gap-filling performance on real missing data patterns. When transferred to downstream tasks, GAIA consistently outperforms an MAE-only baseline: improving atmospheric river segmentation (F1: 0.58 vs 0.52), enhancing tropical cyclone detection (storm-level recall: 81% vs 75%, early detection: 29% vs 17%), and maintaining competitive precipitation estimation performance. Analysis reveals that GAIA's hybrid objectives encourage learning of spatially coherent, object-centric features distributed across multiple principal components rather than concentrated representations focused on reconstruction. This work demonstrates that combining complementary self-supervised objectives yields more transferable representations for diverse atmospheric modeling tasks. Model weights and code are available at: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Metrics and Benchmarks of Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.19022</link>
<guid>https://arxiv.org/abs/2505.19022</guid>
<content:encoded><![CDATA[
arXiv:2505.19022v2 Announce Type: replace-cross 
Abstract: Video Anomaly Detection (VAD), which aims to detect anomalies that deviate from expectation, has attracted increasing attention in recent years. Existing advancements in VAD primarily focus on model architectures and training strategies, while devoting insufficient attention to evaluation metrics and benchmarks. In this paper, we rethink VAD evaluation methods through comprehensive analyses, revealing three critical limitations in current practices: 1) existing metrics are significantly influenced by single annotation bias; 2) current metrics fail to reward early detection of anomalies; 3) available benchmarks lack the capability to evaluate scene overfitting of fully/weakly-supervised algorithms. To address these limitations, we propose three novel evaluation methods: first, we establish probabilistic AUC/AP (Prob-AUC/AP) metrics utlizing multi-round annotations to mitigate single annotation bias; second, we develop a Latency-aware Average Precision (LaAP) metric that rewards early and accurate anomaly detection; and finally, we introduce two hard normal benchmarks (UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene overfitting. We report performance comparisons of ten state-of-the-art VAD approaches using our proposed evaluation methods, providing novel perspectives for future VAD model development. We release our data and code in https://github.com/Kamino666/RethinkingVAD.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA</title>
<link>https://arxiv.org/abs/2505.23724</link>
<guid>https://arxiv.org/abs/2505.23724</guid>
<content:encoded><![CDATA[
arXiv:2505.23724v3 Announce Type: replace-cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24003</link>
<guid>https://arxiv.org/abs/2505.24003</guid>
<content:encoded><![CDATA[
arXiv:2505.24003v2 Announce Type: replace-cross 
Abstract: Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, the state-of-the-art (SOTA) LVM-based forecaster poses an inductive bias towards "forecasting periods". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast-residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 SOTA models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets. The code for this paper is available at: https://github.com/D2I-Group/dmmv.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Empathy: AI based Mental Health</title>
<link>https://arxiv.org/abs/2506.00081</link>
<guid>https://arxiv.org/abs/2506.00081</guid>
<content:encoded><![CDATA[
arXiv:2506.00081v2 Announce Type: replace-cross 
Abstract: Many people suffer from mental health problems but not everyone seeks professional help or has access to mental health care. AI chatbots have increasingly become a go-to for individuals who either have mental disorders or simply want someone to talk to. This paper presents a study on participants who have previously used chatbots and a scenario-based testing of large language model (LLM) chatbots. Our findings indicate that AI chatbots were primarily utilized as a "Five minute therapist" or as a non-judgmental companion. Participants appreciated the anonymity and lack of judgment from chatbots. However, there were concerns about privacy and the security of sensitive information. The scenario-based testing of LLM chatbots highlighted additional issues. Some chatbots were consistently reassuring, used emojis and names to add a personal touch, and were quick to suggest seeking professional help. However, there were limitations such as inconsistent tone, occasional inappropriate responses (e.g., casual or romantic), and a lack of crisis sensitivity, particularly in recognizing red flag language and escalating responses appropriately. These findings can inform both the technology and mental health care industries on how to better utilize AI chatbots to support individuals during challenging emotional periods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Diffusion LLMs via Adaptive Parallel Decoding</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[
arXiv:2506.00413v2 Announce Type: replace-cross 
Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoLAR: Polar-Decomposed Low-Rank Adapter Representation</title>
<link>https://arxiv.org/abs/2506.03133</link>
<guid>https://arxiv.org/abs/2506.03133</guid>
<content:encoded><![CDATA[
arXiv:2506.03133v2 Announce Type: replace-cross 
Abstract: We show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare</title>
<link>https://arxiv.org/abs/2506.06977</link>
<guid>https://arxiv.org/abs/2506.06977</guid>
<content:encoded><![CDATA[
arXiv:2506.06977v2 Announce Type: replace-cross 
Abstract: Healthcare providers often divide patient populations into cohorts based on shared clinical factors, such as medical history, to deliver personalized healthcare services. This idea has also been adopted in clinical prediction models, where it presents a vital challenge: capturing both global and cohort-specific patterns while enabling model generalization to unseen domains. Addressing this challenge falls under the scope of domain generalization (DG). However, conventional DG approaches often struggle in clinical settings due to the absence of explicit domain labels and the inherent gap in medical knowledge. To address this, we propose UdonCare, a hierarchy-guided method that iteratively divides patients into latent domains and decomposes domain-invariant (label) information from patient data. Our method identifies patient domains by pruning medical ontologies (e.g. ICD-9-CM hierarchy). On two public datasets, MIMIC-III and MIMIC-IV, UdonCare shows superiority over eight baselines across four clinical prediction tasks with substantial domain gaps, highlighting the untapped potential of medical knowledge in guiding clinical domain generalization problems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Combinatorial Optimization of Design Structure Matrix</title>
<link>https://arxiv.org/abs/2506.09749</link>
<guid>https://arxiv.org/abs/2506.09749</guid>
<content:encoded><![CDATA[
arXiv:2506.09749v2 Announce Type: replace-cross 
Abstract: In complex engineering systems, the dependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that rely solely on mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) to address such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM sequencing-a common CO problem. Experiments on various DSM cases demonstrate that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Diffusion that can Insert and Delete</title>
<link>https://arxiv.org/abs/2506.15725</link>
<guid>https://arxiv.org/abs/2506.15725</guid>
<content:encoded><![CDATA[
arXiv:2506.15725v2 Announce Type: replace-cross 
Abstract: Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates</title>
<link>https://arxiv.org/abs/2507.23607</link>
<guid>https://arxiv.org/abs/2507.23607</guid>
<content:encoded><![CDATA[
arXiv:2507.23607v2 Announce Type: replace-cross 
Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring</title>
<link>https://arxiv.org/abs/2508.00943</link>
<guid>https://arxiv.org/abs/2508.00943</guid>
<content:encoded><![CDATA[
arXiv:2508.00943v2 Announce Type: replace-cross 
Abstract: Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</title>
<link>https://arxiv.org/abs/2508.06763</link>
<guid>https://arxiv.org/abs/2508.06763</guid>
<content:encoded><![CDATA[
arXiv:2508.06763v3 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress across a range of vision-language tasks and demonstrate strong potential for traffic accident understanding. However, existing MLLMs in this domain primarily focus on coarse-grained image-level or video-level comprehension and often struggle to handle fine-grained visual details or localized scene components, limiting their applicability in complex accident scenarios. To address these limitations, we propose SafePLUG, a novel framework that empowers MLLMs with both Pixel-Level Understanding and temporal Grounding for comprehensive traffic accident analysis. SafePLUG supports both arbitrary-shaped visual prompts for region-aware question answering and pixel-level segmentation based on language instructions, while also enabling the recognition of temporally anchored events in traffic accident scenarios. To advance the development of MLLMs for traffic accident understanding, we curate a new dataset containing multimodal question-answer pairs centered on diverse accident scenarios, with detailed pixel-level annotations and temporal event boundaries. Experimental results show that SafePLUG achieves strong performance on multiple tasks, including region-based question answering, pixel-level segmentation, temporal event localization, and accident event understanding. These capabilities lay a foundation for fine-grained understanding of complex traffic scenes, with the potential to improve driving safety and enhance situational awareness in smart transportation systems. The code, dataset, and model checkpoints will be made publicly available at: https://zihaosheng.github.io/SafePLUG
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
<link>https://arxiv.org/abs/2508.08325</link>
<guid>https://arxiv.org/abs/2508.08325</guid>
<content:encoded><![CDATA[
arXiv:2508.08325v2 Announce Type: replace-cross 
Abstract: When online sellers use AI learning algorithms to automatically compete on e-commerce platforms, there is concern that they will learn to coordinate on higher than competitive prices. However, this concern was primarily raised in single-dimension price competition. We investigate whether this prediction holds when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. We analyze competition in multi-agent reinforcement learning, and use a large-scale dataset from Amazon.com to provide empirical evidence. We show that when consumers have high search costs, learning algorithms can coordinate on prices lower than competitive prices, facilitating a win-win-win for consumers, sellers, and platforms. This occurs because algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices and enlarging demand on the platform. We also show that our results generalize to any learning algorithm that uses exploration of price and advertising bids. Consistent with our predictions, an empirical analysis shows that price levels exhibit a negative interaction between estimated consumer search costs and algorithm usage index. We analyze the platform's strategic response and find that reserve price adjustments will not increase platform profits, but commission adjustments will, while maintaining the beneficial outcomes for both sellers and consumers.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization</title>
<link>https://arxiv.org/abs/2509.05831</link>
<guid>https://arxiv.org/abs/2509.05831</guid>
<content:encoded><![CDATA[
arXiv:2509.05831v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as , aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations</title>
<link>https://arxiv.org/abs/2509.15981</link>
<guid>https://arxiv.org/abs/2509.15981</guid>
<content:encoded><![CDATA[
arXiv:2509.15981v2 Announce Type: replace-cross 
Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features</title>
<link>https://arxiv.org/abs/2509.24046</link>
<guid>https://arxiv.org/abs/2509.24046</guid>
<content:encoded><![CDATA[
arXiv:2509.24046v2 Announce Type: replace-cross 
Abstract: High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.24204</link>
<guid>https://arxiv.org/abs/2509.24204</guid>
<content:encoded><![CDATA[
arXiv:2509.24204v2 Announce Type: replace-cross 
Abstract: Vision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAM's Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control</title>
<link>https://arxiv.org/abs/2509.24591</link>
<guid>https://arxiv.org/abs/2509.24591</guid>
<content:encoded><![CDATA[
arXiv:2509.24591v2 Announce Type: replace-cross 
Abstract: We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration</title>
<link>https://arxiv.org/abs/2510.03865</link>
<guid>https://arxiv.org/abs/2510.03865</guid>
<content:encoded><![CDATA[
arXiv:2510.03865v2 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising</title>
<link>https://arxiv.org/abs/2510.05589</link>
<guid>https://arxiv.org/abs/2510.05589</guid>
<content:encoded><![CDATA[
arXiv:2510.05589v2 Announce Type: replace-cross 
Abstract: The proliferation of mobile devices generates a massive volume of time series across various domains, where effective time series forecasting enables a variety of real-world applications. This study focuses on a new problem of source-free domain adaptation for time series forecasting. It aims to adapt a pretrained model from sufficient source time series to the sparse target time series domain without access to the source data, embracing data protection regulations. To achieve this, we propose TimePD, the first source-free time series forecasting framework with proxy denoising, where large language models (LLMs) are employed to benefit from their generalization capabilities. Specifically, TimePD consists of three key components: (1) dual-branch invariant disentangled feature learning that enforces representation- and gradient-wise invariance by means of season-trend decomposition; (2) lightweight, parameter-free proxy denoising that dynamically calibrates systematic biases of LLMs; and (3) knowledge distillation that bidirectionally aligns the denoised prediction and the original target prediction. Extensive experiments on real-world datasets offer insight into the effectiveness of the proposed TimePD, outperforming SOTA baselines by 9.3% on average.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Based Long Code Translation using Identifier Replacement</title>
<link>https://arxiv.org/abs/2510.09045</link>
<guid>https://arxiv.org/abs/2510.09045</guid>
<content:encoded><![CDATA[
arXiv:2510.09045v2 Announce Type: replace-cross 
Abstract: In the domain of software development, LLMs have been utilized to automate tasks such as code translation, where source code from one programming language is translated to another while preserving its functionality. However, LLMs often struggle with long source codes that don't fit into the context window, which produces inaccurate translations. To address this, we propose a novel zero-shot code translation method that incorporates identifier replacement. By substituting user-given long identifiers with generalized placeholders during translation, our method allows the LLM to focus on the logical structure of the code, by reducing token count and memory usage, which improves the efficiency and cost-effectiveness of long code translation. Our empirical results demonstrate that our approach preserves syntactical and hierarchical information and produces translation results with reduced tokens.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVTime: Small Time Series Forecasting Models Informed by "Physics" of Large Vision Model Forecasters</title>
<link>https://arxiv.org/abs/2510.09780</link>
<guid>https://arxiv.org/abs/2510.09780</guid>
<content:encoded><![CDATA[
arXiv:2510.09780v2 Announce Type: replace-cross 
Abstract: Time series AI is crucial for analyzing dynamic web content, driving a surge of pre-trained large models known for their strong knowledge encoding and transfer capabilities across diverse tasks. However, given their energy-intensive training, inference, and hardware demands, using large models as a one-fits-all solution raises serious concerns about carbon footprint and sustainability. For a specific task, a compact yet specialized, high-performing model may be more practical and affordable, especially for resource-constrained users such as small businesses. This motivates the question: Can we build cost-effective lightweight models with large-model-like performance on core tasks such as forecasting? This paper addresses this question by introducing SVTime, a novel Small model inspired by large Vision model (LVM) forecasters for long-term Time series forecasting (LTSF). Recently, LVMs have been shown as powerful tools for LTSF. We identify a set of key inductive biases of LVM forecasters -- analogous to the "physics" governing their behaviors in LTSF -- and design small models that encode these biases through meticulously crafted linear layers and constraint functions. Across 21 baselines spanning lightweight, complex, and pre-trained large models on 8 benchmark datasets, SVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large models with 10^3 fewer parameters than LVMs, while enabling efficient training and inference in low-resource settings.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI and Firm Productivity: Field Experiments in Online Retail</title>
<link>https://arxiv.org/abs/2510.12049</link>
<guid>https://arxiv.org/abs/2510.12049</guid>
<content:encoded><![CDATA[
arXiv:2510.12049v2 Announce Type: replace-cross 
Abstract: We quantify the impact of Generative Artificial Intelligence (GenAI) on firm productivity through a series of large-scale randomized field experiments involving millions of users and products at a leading cross-border online retail platform. Over six months in 2023-2024, GenAI-based enhancements were integrated into seven consumer-facing business workflows. We find that GenAI adoption significantly increases sales, with treatment effects ranging from $0\%$ to $16.3\%$, depending on GenAI's marginal contribution relative to existing firm practices. Because inputs and prices were held constant across experimental arms, these gains map directly into total factor productivity improvements. Across the four GenAI applications with positive effects, the implied annual incremental value is approximately $\$ 5$ per consumer-an economically meaningful impact given the retailer's scale and the early stage of GenAI adoption. The primary mechanism operates through higher conversion rates, consistent with GenAI reducing frictions in the marketplace and improving consumer experience. We also document substantial heterogeneity: smaller and newer sellers, as well as less experienced consumers, exhibit disproportionately larger gains. Our findings provide novel, large-scale causal evidence on the productivity effects of GenAI in online retail, highlighting both its immediate value and broader potential.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization</title>
<link>https://arxiv.org/abs/2510.16857</link>
<guid>https://arxiv.org/abs/2510.16857</guid>
<content:encoded><![CDATA[
arXiv:2510.16857v2 Announce Type: replace-cross 
Abstract: Vehicle aerodynamics optimization has become critical for automotive electrification, where drag reduction directly determines electric vehicle range and energy efficiency. Traditional approaches face an intractable trade-off: computationally expensive Computational Fluid Dynamics (CFD) simulations requiring weeks per design iteration, or simplified models that sacrifice production-grade accuracy. While machine learning offers transformative potential, existing datasets exhibit fundamental limitations -- inadequate mesh resolution, missing vehicle components, and validation errors exceeding 5% -- preventing deployment in industrial workflows. We present DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations generated using STAR-CCM+${}^\unicode{xAE}$ software. The dataset systematically explores three vehicle configurations through 20 Computer Aided Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including complete engine compartments and cooling systems with realistic internal airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a five-fold improvement over existing datasets -- through refined mesh strategies with strict wall $y^+$ control. Benchmarks demonstrate that models trained on this data achieve production-ready accuracy while reducing computational costs from weeks to minutes. This represents the first dataset bridging academic machine learning research and industrial CFD practice, establishing a new standard for data-driven aerodynamic optimization in automotive development. Beyond automotive applications, DrivAerStar demonstrates a paradigm for integrating high-fidelity physics simulations with Artificial Intelligence (AI) across engineering disciplines where computational constraints currently limit innovation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams</title>
<link>https://arxiv.org/abs/2510.16988</link>
<guid>https://arxiv.org/abs/2510.16988</guid>
<content:encoded><![CDATA[
arXiv:2510.16988v2 Announce Type: replace-cross 
Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered ambient sensors is an essential task in Ambient Assisted Living, yet existing methods remain constrained by representation-level limitations. Sequence-based approaches preserve temporal order of sensor activations but are sensitive to noise and lack spatial awareness, while image-based approaches capture global patterns and implicit spatial correlations but compress fine-grained temporal dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation) fail to enforce alignment between sequence- and image-based representation views, underutilizing their complementary strengths. We propose Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an end-to-end framework that jointly optimizes representation learning via Sequence-Image Contrastive Alignment (SICA) and classification via cross-entropy, ensuring both cross-representation alignment and task-specific discriminability. CARE integrates (i) time-aware, noise-resilient sequence encoding with (ii) spatially-informed and frequency-sensitive image representations, and employs (iii) a joint contrastive-classification objective for end-to-end learning of aligned and discriminative embeddings. Evaluated on three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to sensor malfunctions and layout variability, highlighting its potential for reliable ADL recognition in smart homes.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging</title>
<link>https://arxiv.org/abs/2510.17426</link>
<guid>https://arxiv.org/abs/2510.17426</guid>
<content:encoded><![CDATA[
arXiv:2510.17426v2 Announce Type: replace-cross 
Abstract: The "alignment tax" of post-training is typically framed as a drop in task accuracy. We show it also involves a severe loss of calibration, making models overconfident, less reliable, and model outputs less diverse. We show that this trade-off can be navigated effectively via a simple post-hoc intervention: interpolating between a model's weights before and after alignment. Crucially, this is not a strict trade-off. We find that the process consistently reveals Pareto-optimal interpolations - models that improve accuracy beyond both parents while substantially recovering the calibration lost during alignment. Our work demonstrates that simple model merging provides a computationally efficient method for mitigating the full scope of the alignment tax, yielding models that are more capable and more reliable.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models</title>
<link>https://arxiv.org/abs/2510.17496</link>
<guid>https://arxiv.org/abs/2510.17496</guid>
<content:encoded><![CDATA[
arXiv:2510.17496v2 Announce Type: replace-cross 
Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>{\epsilon}-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</title>
<link>https://arxiv.org/abs/2510.18637</link>
<guid>https://arxiv.org/abs/2510.18637</guid>
<content:encoded><![CDATA[
arXiv:2510.18637v2 Announce Type: replace-cross 
Abstract: Semantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce {\epsilon}-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that {\epsilon}-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications</title>
<link>https://arxiv.org/abs/2510.25140</link>
<guid>https://arxiv.org/abs/2510.25140</guid>
<content:encoded><![CDATA[
arXiv:2510.25140v2 Announce Type: replace-cross 
Abstract: Object detection in civil engineering applications is constrained by limited annotated data in specialized domains. We introduce DINO-YOLO, a hybrid architecture combining YOLOv12 with DINOv3 self-supervised vision transformers for data-efficient detection. DINOv3 features are strategically integrated at two locations: input preprocessing (P0) and mid-backbone enhancement (P3). Experimental validation demonstrates substantial improvements: Tunnel Segment Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while maintaining real-time inference (30-47 FPS). Systematic ablation across five YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead (21-33ms versus 8-16ms baseline) remains acceptable for field deployment on NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil engineering datasets (<10K images) while preserving computational efficiency, providing practical solutions for construction safety monitoring and infrastructure inspection in data-constrained environments.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows</title>
<link>https://arxiv.org/abs/2510.25935</link>
<guid>https://arxiv.org/abs/2510.25935</guid>
<content:encoded><![CDATA[
arXiv:2510.25935v2 Announce Type: replace-cross 
Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance in software development workflows. It captures development and deployment data directly from GitHub, transforming it into process mining logs for detailed analysis. From these logs, the system generates metrics and dashboards that provide actionable insights into PR activity patterns and workflow efficiency. Building on this structured representation, CodeSight employs an LSTM model that predicts remaining PR resolution times based on sequential activity traces and static features, enabling early identification of potential deadline breaches. In tests, the system demonstrates high precision and F1 scores in predicting deadline compliance, illustrating the value of integrating process mining with machine learning for proactive software project management.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aeolus: A Multi-structural Flight Delay Dataset</title>
<link>https://arxiv.org/abs/2510.26616</link>
<guid>https://arxiv.org/abs/2510.26616</guid>
<content:encoded><![CDATA[
arXiv:2510.26616v2 Announce Type: replace-cross 
Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed to advance research on flight delay prediction and support the development of foundation models for tabular data. Existing datasets in this domain are typically limited to flat tabular structures and fail to capture the spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this limitation by providing three aligned modalities: (i) a tabular dataset with rich operational, meteorological, and airportlevel features for over 50 million flights; (ii) a flight chain module that models delay propagation along sequential flight legs, capturing upstream and downstream dependencies; and (iii) a flight network graph that encodes shared aircraft, crew, and airport resource connections, enabling cross-flight relational reasoning. The dataset is carefully constructed with temporal splits, comprehensive features, and strict leakage prevention to support realistic and reproducible machine learning evaluation. Aeolus supports a broad range of tasks, including regression, classification, temporal structure modeling, and graph learning, serving as a unified benchmark across tabular, sequential, and graph modalities. We release baseline experiments and preprocessing tools to facilitate adoption. Aeolus fills a key gap for both domain-specific modeling and general-purpose structured data research.Our source code and data can be accessed at https://github.com/Flnny/Delay-data
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The End of Manual Decoding: Towards Truly End-to-End Language Models</title>
<link>https://arxiv.org/abs/2510.26697</link>
<guid>https://arxiv.org/abs/2510.26697</guid>
<content:encoded><![CDATA[
arXiv:2510.26697v2 Announce Type: replace-cross 
Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the limitation of evaluating machine unlearning using only a single training seed</title>
<link>https://arxiv.org/abs/2510.26714</link>
<guid>https://arxiv.org/abs/2510.26714</guid>
<content:encoded><![CDATA[
arXiv:2510.26714v2 Announce Type: replace-cross 
Abstract: Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comparisons of MU algorithms should also reflect the variability across different model training seeds.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off</title>
<link>https://arxiv.org/abs/2510.26722</link>
<guid>https://arxiv.org/abs/2510.26722</guid>
<content:encoded><![CDATA[
arXiv:2510.26722v2 Announce Type: replace-cross 
Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful and Fast Influence Function via Advanced Sampling</title>
<link>https://arxiv.org/abs/2510.26776</link>
<guid>https://arxiv.org/abs/2510.26776</guid>
<content:encoded><![CDATA[
arXiv:2510.26776v2 Announce Type: replace-cross 
Abstract: How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails</title>
<link>https://arxiv.org/abs/2510.21285</link>
<guid>https://arxiv.org/abs/2510.21285</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, safety risks, Self-Jailbreak, Chain-of-Guardrail, reasoning ability 

Summary:
Large Reasoning Models (LRMs) have shown impressive capabilities in complex reasoning tasks but face safety risks such as harmful content generation and jailbreak attacks. Existing mitigation strategies using heuristic safety signals often compromise reasoning ability. A study reveals a phenomenon termed Self-Jailbreak, where LRMs override risk assessments and justify unsafe responses. LRMs inherently possess the ability to reject unsafe queries, but it is compromised, leading to harmful outputs. The proposed Chain-of-Guardrail (CoG) framework recomposes or backtracks unsafe reasoning steps to steer LRMs back onto safe trajectories while preserving valid reasoning chains. Extensive experiments across various benchmarks show that CoG significantly enhances LRM safety without sacrificing reasoning ability, surpassing previous methods with notable safety-reasoning trade-offs. 

<br /><br />Summary: Large Reasoning Models face safety risks like harmful content and jailbreak attacks. LRMs can override risk assessments, termed Self-Jailbreak. The Chain-of-Guardrail (CoG) framework improves safety by guiding LRMs back to safe trajectories. CoG enhances safety without compromising reasoning ability, outperforming previous methods with significant safety-reasoning trade-offs. <div>
arXiv:2510.21285v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex reasoning tasks but remain vulnerable to severe safety risks, including harmful content generation and jailbreak attacks. Existing mitigation strategies rely on injecting heuristic safety signals during training, which often suppress reasoning ability and fail to resolve the safety-reasoning trade-off. To systematically investigate this issue, we analyze the reasoning trajectories of diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models override their own risk assessments and justify responding to unsafe prompts. This finding reveals that LRMs inherently possess the ability to reject unsafe queries, but this ability is compromised, resulting in harmful outputs. Building on these insights, we propose the Chain-of-Guardrail (CoG), a training framework that recomposes or backtracks unsafe reasoning steps, steering the model back onto safe trajectories while preserving valid reasoning chains. Extensive experiments across multiple reasoning and safety benchmarks demonstrate that CoG substantially improves the safety of current LRMs while preserving comparable reasoning ability, significantly outperforming prior methods that suffer from severe safety-reasoning trade-offs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Huxley-G\"odel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine</title>
<link>https://arxiv.org/abs/2510.21614</link>
<guid>https://arxiv.org/abs/2510.21614</guid>
<content:encoded><![CDATA[
<div> agent, self-improvement, coding, benchmark, HGM

Summary:
The article addresses the Metaproductivity-Performance Mismatch in self-improving coding agents by introducing the Huxley-Gdel Machine (HGM). They propose a metric (CMP) to measure an agent's potential for self-improvement based on the benchmark performances of its descendants. By estimating CMP, HGM efficiently searches the tree of self-modifications, outperforming previous methods with fewer CPU hours on SWE-bench Verified and Polyglot datasets. HGM exhibits strong transferability to other coding datasets and large language models. The agent optimized by HGM achieves human-level performance on SWE-bench Verified with GPT-5-mini and SWE-bench Lite with GPT-5, matching the best results of human-engineered coding agents. The code for HGM is publicly available on GitHub at https://github.com/metauto-ai/HGM.<br /><br />Summary: <div>
arXiv:2510.21614v3 Announce Type: replace 
Abstract: Recent studies operationalize self-improvement through coding agents that edit their own codebases. They grow a tree of self-modifications through expansion strategies that favor higher software engineering benchmark performance, assuming that this implies more promising subsequent self-modifications. However, we identify a mismatch between the agent's self-improvement potential (metaproductivity) and its coding benchmark performance, namely the Metaproductivity-Performance Mismatch. Inspired by Huxley's concept of clade, we propose a metric ($\mathrm{CMP}$) that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for self-improvement. We show that, in our self-improving coding agent development setting, access to the true $\mathrm{CMP}$ is sufficient to simulate how the G\"odel Machine would behave under certain assumptions. We introduce the Huxley-G\"odel Machine (HGM), which, by estimating $\mathrm{CMP}$ and using it as guidance, searches the tree of self-modifications. On SWE-bench Verified and Polyglot, HGM outperforms prior self-improving coding agent development methods while using fewer allocated CPU hours. Last but not least, HGM demonstrates strong transfer to other coding datasets and large language models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and evaluated on SWE-bench Lite with GPT-5 achieves human-level performance, matching the best officially checked results of human-engineered coding agents. Our code is publicly available at https://github.com/metauto-ai/HGM.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing AI Agent Execution</title>
<link>https://arxiv.org/abs/2510.21236</link>
<guid>https://arxiv.org/abs/2510.21236</guid>
<content:encoded><![CDATA[
<div> Framework, Security, Access control, Model Context Protocol, Malicious behavior
<br />
Summary:
AgentBound is introduced as an access control framework for Model Context Protocol (MCP) servers to address the security issues associated with unrestricted access. Inspired by the Android permission model, AgentBound combines a declarative policy mechanism with a policy enforcement engine to contain malicious behavior without requiring modifications to the MCP server. A dataset of the top 296 MCP servers is analyzed, demonstrating that access control policies can be automatically generated with high accuracy. The framework successfully blocks a majority of security threats in malicious MCP servers, with minimal overhead from the policy enforcement engine. These findings provide a practical foundation for securing MCP servers while maintaining productivity, offering opportunities for further exploration in declarative access control and MCP security. 
<br /><br />Summary: <div>
arXiv:2510.21236v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scheduling Your LLM Reinforcement Learning with Reasoning Trees</title>
<link>https://arxiv.org/abs/2510.24832</link>
<guid>https://arxiv.org/abs/2510.24832</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Large Language Models, Reasoning Tree, Data Scheduling<br />
Summary: <br />
The study introduces a novel approach, called RLVR with Reasoning Tree Schedule, to enhance Large Language Model (LLM) optimization. By leveraging Reinforcement Learning with Verifiable Rewards (RLVR), the process involves editing a query's Reasoning Tree by exploring and modifying nodes dynamically to improve model policy. The Reasoning Score (r-score) metric is introduced to measure learning difficulty based on reasoning tree structure, leading to the development of Reasoning Tree Schedule (Re-Schedule) algorithm. This algorithm constructs a curriculum progressing from simple to complex queries based on r-score. Experimental results on six math-reasoning benchmarks demonstrate significant improvements in average accuracy by up to 3.2%. The findings validate the effectiveness of the proposed approach, emphasizing the importance of understanding reasoning tree structure for data scheduling in RLVR optimization of LLMs. <div>
arXiv:2510.24832v1 Announce Type: new 
Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large Language Models (LLMs) can be conceptualized as progressively editing a query's `Reasoning Tree'. This process involves exploring nodes (tokens) and dynamically modifying the model's policy at each node. When combined with data scheduling, this process yields further gains in data efficiency and accuracy. However, existing RLVR data scheduling methods typically rely on path-based metrics to rank queries, overlooking the reasoning tree structures of these queries. In this paper, we introduce a novel metric, namely Reasoning Score (r-score), which measures the query's learning difficulty based on the structure of its reasoning tree. Based on the r-score, we propose the Reasoning Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a curriculum progressing from structurally simple (high r-score) to complex (low r-score) queries. Experiments on six math-reasoning benchmarks show that Re-Schedule significantly improves average accuracy, achieving gains of up to 3.2%. These strong results validate our approach and demonstrate that a structural understanding of the reasoning tree provides a more powerful and principled foundation for RLVR data scheduling.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic Counterfactuals under Shift-Scale Interventions</title>
<link>https://arxiv.org/abs/2510.25005</link>
<guid>https://arxiv.org/abs/2510.25005</guid>
<content:encoded><![CDATA[
<div> feedback loops, cyclic dependencies, counterfactual inference, structural causal models, shift-scale interventions
Summary:
This study explores counterfactual inference in cyclic structural causal models (SCMs) that involve feedback loops or cyclic dependencies, challenging the traditional assumption of acyclic models. The focus is on shift-scale interventions, which involve soft, policy-style changes that rescale and/or shift a variable's mechanism within the cyclic SCM. By considering these interventions, the researchers aim to develop a method for conducting counterfactual inference in systems with feedback loops, such as biological systems. The study addresses the limitations of acyclicity assumptions in traditional frameworks and explores the implications of cyclic dependencies on counterfactual reasoning. Through the analysis of shift-scale interventions, the researchers aim to provide a more comprehensive understanding of causal inference in systems with feedback loops, advancing the field's ability to model and analyze complex real-world systems. 

<br /><br />Summary: <div>
arXiv:2510.25005v1 Announce Type: new 
Abstract: Most counterfactual inference frameworks traditionally assume acyclic structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However, many real-world systems (e.g. biological systems) contain feedback loops or cyclic dependencies that violate acyclicity. In this work, we study counterfactual inference in cyclic SCMs under shift-scale interventions, i.e., soft, policy-style changes that rescale and/or shift a variable's mechanism.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming the Real-world Complexities in CPT E/M Coding with Large Language Models</title>
<link>https://arxiv.org/abs/2510.25007</link>
<guid>https://arxiv.org/abs/2510.25007</guid>
<content:encoded><![CDATA[
<div> coding, evaluation, management, automation, ProFees <br />
<br />
Evaluation and Management (E/M) coding, a crucial aspect of medical documentation for billing purposes, can be burdensome for physicians. Automating this process with accurate CPT E/M codes can improve efficiency and patient care. However, the complexities of real-world scenarios have made automation challenging. This paper introduces ProFees, an LLM-based framework designed to address these complexities. Through a systematic evaluation on a real-world dataset, ProFees outperformed a commercial CPT E/M coding system and a strong baseline by achieving a significant increase in coding accuracy. The study highlights the importance of automating E/M coding, showcasing ProFees as an effective solution that can alleviate physicians' documentation burden and enhance billing efficiency. Overall, ProFees demonstrates its capability in overcoming real-world complexities to improve the accuracy and automation of E/M coding processes. <br /><br />Summary: <div>
arXiv:2510.25007v1 Announce Type: new 
Abstract: Evaluation and Management (E/M) coding, under the Current Procedural Terminology (CPT) taxonomy, documents medical services provided to patients by physicians. Used primarily for billing purposes, it is in physicians' best interest to provide accurate CPT E/M codes. %While important, it is an auxiliary task that adds to physicians' documentation burden. Automating this coding task will help alleviate physicians' documentation burden, improve billing efficiency, and ultimately enable better patient care. However, a number of real-world complexities have made E/M encoding automation a challenging task. In this paper, we elaborate some of the key complexities and present ProFees, our LLM-based framework that tackles them, followed by a systematic evaluation. On an expert-curated real-world dataset, ProFees achieves an increase in coding accuracy of more than 36\% over a commercial CPT E/M coding system and almost 5\% over our strongest single-prompt baseline, demonstrating its effectiveness in addressing the real-world complexities.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading</title>
<link>https://arxiv.org/abs/2510.25014</link>
<guid>https://arxiv.org/abs/2510.25014</guid>
<content:encoded><![CDATA[
<div> Methodology, Autoregressive State-Tracking Prompting, large language models, in-game trading, transactional integrity

Summary: Autoregressive State-Tracking Prompting (ASTP) is introduced as a methodology to enhance player trust and ensure procedural flow in rule-governed trading systems. ASTP compels large language models (LLMs) to make their state-tracking process explicit and verifiable by identifying and reporting a predefined state label from the previous turn. This ensures transactional integrity in the trading dialogue. Evaluation shows high state compliance (>99%) and calculation precision (99.3%). ASTP with placeholder post-processing on smaller models matches performance of larger models while significantly reducing response time from 21.2s to 2.4s. This practical approach satisfies real-time requirements and resource constraints of commercial games. <div>
arXiv:2510.25014v1 Announce Type: new 
Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust. This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading (browse-offer-review-confirm). To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations. Evaluation across 300 trading dialogues demonstrates >99% state compliance and 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models' (Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Aware GRPO using Process Mining</title>
<link>https://arxiv.org/abs/2510.25065</link>
<guid>https://arxiv.org/abs/2510.25065</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, post-training, multi-step reasoning, large reasoning models, Group Relative Policy Optimization<br />
<br />
Summary:<br />
Reinforcement learning-based post-training, essential for enhancing multi-step reasoning in large reasoning models (LRMs), often relies on outcome-centric rewards. The proposed PM4GRPO introduces Group Relative Policy Optimization (GRPO) with reasoning-awareness, augmenting standard rewards with signals reflecting the reasoning process. Employing process mining techniques, PM4GRPO measures how well a policy model's reasoning aligns with a pretrained teacher model through a conformance reward. Empirical evidence across five benchmarks showcases PM4GRPO's superiority over existing GRPO-based methodologies for post-training, underlining the effectiveness of leveraging process mining for enhancing policy models' reasoning capabilities. <br /> <div>
arXiv:2510.25065v1 Announce Type: new 
Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts</title>
<link>https://arxiv.org/abs/2510.25091</link>
<guid>https://arxiv.org/abs/2510.25091</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock movement prediction, Hypergraph-based MultiModal architecture, LLM reasoning, Style-Structured Mixture of Experts, Regime-adaptive modeling

Summary:<br /><br />
Stock movement prediction is a challenging task due to complex temporal dependencies and evolving inter-stock relationships. The H3M-SSMoEs model introduced in this work addresses these challenges by incorporating three key innovations. Firstly, it utilizes a Multi-Context Multimodal Hypergraph that captures spatiotemporal dynamics and inter-stock dependencies. Secondly, a reasoning module enhanced with a large language model facilitates semantic fusion of quantitative and textual modalities. Lastly, a Style-Structured Mixture of Experts framework combines shared market experts and industry-specialized experts, enabling specialization under sparse activation. Experimental results on major stock markets show that H3M-SSMoEs outperforms existing methods in predictive accuracy, investment performance, and risk control. The datasets, source code, and model weights for the model are available on the GitHub repository provided. <div>
arXiv:2510.25091v1 Announce Type: new 
Abstract: Stock movement prediction remains fundamentally challenging due to complex temporal dependencies, heterogeneous modalities, and dynamically evolving inter-stock relationships. Existing approaches often fail to unify structural, semantic, and regime-adaptive modeling within a scalable framework. This work introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with LLM reasoning and Style-Structured Mixture of Experts, integrating three key innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph (LCH) and persistent inter-stock dependencies through a Global Context Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon Divergence weighting mechanism for adaptive relational learning and cross-modal alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities, enriching representations with domain-specific financial knowledge; and (3) a Style-Structured Mixture of Experts (SSMoEs) that combines shared market experts and industry-specialized experts, each parameterized by learnable style vectors enabling regime-aware specialization under sparse activation. Extensive experiments on three major stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in both superior predictive accuracy and investment performance, while exhibiting effective risk control. Datasets, source code, and model weights are available at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA</title>
<link>https://arxiv.org/abs/2510.25101</link>
<guid>https://arxiv.org/abs/2510.25101</guid>
<content:encoded><![CDATA[
<div> Knowledge Base Question Answering, KBQA, Large Language Models, LLM, agentic reasoning, KnowCoder-A1<br />
<br />
Summary:
The paper introduces KnowCoder-A1, an autonomous system for Knowledge Base Question Answering (KBQA) that leverages large language models (LLMs) to perform agentic reasoning. KnowCoder-A1 is trained using outcome-only supervision through a multi-stage curriculum reinforcement learning approach with an easy-to-hard curriculum. It starts by fine-tuning on high-quality trajectories obtained through rejection sampling and progressively advances through reward schedules. This training method helps KnowCoder-A1 develop strong reasoning abilities and outperform previous models on three popular datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves an impressive 11.1% relative improvement with significantly less training data, showcasing its robust agentic reasoning capabilities.<br /><br />Summary: <div>
arXiv:2510.25101v1 Announce Type: new 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.25179</link>
<guid>https://arxiv.org/abs/2510.25179</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic methods, Agentic Moderation, safety alignment, jailbreak attacks, Large Vision-Language Models (LVLMs)

Summary: 
Agentic Moderation introduces a new framework for enhancing safety alignment in multimodal systems. It utilizes dynamic, cooperative agents such as Shield, Responder, Evaluator, and Reflector to defend against jailbreak attacks. Unlike previous methods, this framework offers context-aware and interpretable moderation, leading to a reduction in the Attack Success Rate (ASR) by 7-19% and improvements in the Refusal Rate (RR) by 4-20%. Extensive experiments across various datasets and LVLMs demonstrate the robustness and effectiveness of Agentic Moderation in maintaining a stable Non-Following Rate (NF) while enhancing safety performance. By leveraging agentic architectures, this approach provides modular, scalable, and fine-grained safety enforcement, showcasing the potential of agentic systems for automated safety governance. 

<br /><br />Summary: <div>
arXiv:2510.25179v1 Announce Type: new 
Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning, collaboration, and adaptive control, enabling systems to coordinate and independently solve complex tasks. We extend this paradigm to safety alignment by introducing Agentic Moderation, a model-agnostic framework that leverages specialised agents to defend multimodal systems against jailbreak attacks. Unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications (safe or unsafe), our method integrates dynamic, cooperative agents, including Shield, Responder, Evaluator, and Reflector, to achieve context-aware and interpretable moderation. Extensive experiments across five datasets and four representative Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF), and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable, and well-balanced safety performance. By harnessing the flexibility and reasoning capacity of agentic architectures, Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the broader potential of agentic systems as a foundation for automated safety governance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision</title>
<link>https://arxiv.org/abs/2510.25205</link>
<guid>https://arxiv.org/abs/2510.25205</guid>
<content:encoded><![CDATA[
<div> energy-efficient autonomous driving, perception optimization, adaptive tuning method, reinforcement learning, driving performance

Summary: 
The article introduces EneAD, an energy-efficient autonomous driving framework aimed at addressing rising energy consumption in autonomous vehicles, particularly in the perception computing component. EneAD utilizes an adaptive perception module that manages multiple perception models with varying computational consumption and dynamically adjusts the execution framerate. A transferable tuning method based on Bayesian optimization is employed to identify optimal knob values for low computation without compromising accuracy. Additionally, a lightweight classification model is used to differentiate perception difficulty in different traffic scenarios. In the robust decision module, a decision model based on reinforcement learning with a regularization term enhances driving stability in the presence of perturbed perception results. Experimental results demonstrate EneAD's ability to reduce perception consumption by 1.9x to 3.5x, improving driving range by 3.9% to 8.5%. <div>
arXiv:2510.25205v1 Announce Type: new 
Abstract: Autonomous driving is an emerging technology that is expected to bring significant social, economic, and environmental benefits. However, these benefits come with rising energy consumption by computation engines, limiting the driving range of vehicles, especially electric ones. Perception computing is typically the most power-intensive component, as it relies on largescale deep learning models to extract environmental features. Recently, numerous studies have employed model compression techniques, such as sparsification, quantization, and distillation, to reduce computational consumption. However, these methods often result in either a substantial model size or a significant drop in perception accuracy compared to high-computation models. To address these challenges, we propose an energy-efficient autonomous driving framework, called EneAD. In the adaptive perception module, a perception optimization strategy is designed from the perspective of data management and tuning. Firstly, we manage multiple perception models with different computational consumption and adjust the execution framerate dynamically. Then, we define them as knobs and design a transferable tuning method based on Bayesian optimization to identify promising knob values that achieve low computation while maintaining desired accuracy. To adaptively switch the knob values in various traffic scenarios, a lightweight classification model is proposed to distinguish the perception difficulty in different scenarios. In the robust decision module, we propose a decision model based on reinforcement learning and design a regularization term to enhance driving stability in the face of perturbed perception results. Extensive experiments evidence the superiority of our framework in both energy consumption and driving performance. EneAD can reduce perception consumption by 1.9x to 3.5x and thus improve driving range by 3.9% to 8.5%
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.25206</link>
<guid>https://arxiv.org/abs/2510.25206</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language models, reasoning abilities, cognitive science, variational reasoning<br />
Summary:
Reinforcement learning (RL) can enhance the reasoning capabilities of large language models (LLMs) by leveraging high-utility reasoning paths. However, sampling difficult reasoning paths for tasks beyond the LLM's current competence can lead to suboptimal learning. Drawing inspiration from cognitive science, the concept of answer-conditioned reasoning is introduced as a way to improve reasoning pathways. This approach allows LLMs to derive high-quality reasoning paths by using answers as guidance. The proposed RAVR framework integrates reference-answer-guided variational reasoning to facilitate more effective reasoning processes. Experimental results in general and mathematical domains consistently show enhancements compared to baseline techniques. Additionally, the analysis reveals that RAVR reduces hesitation, strengthens conclusion consolidation, and encourages problem-specific strategies in reasoning. <br /><br />Summary: <div>
arXiv:2510.25206v1 Announce Type: new 
Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large language models (LLMs), but critically depends on a key prerequisite: the LLM can already generate high-utility reasoning paths with non-negligible probability. For tasks beyond the LLM's current competence, such reasoning path can be hard to sample, and learning risks reinforcing familiar but suboptimal reasoning. We are motivated by the insight from cognitive science that Why is this the answer is often an easier question than What is the answer, as it avoids the heavy cognitive load of open-ended exploration, opting instead for explanatory reconstruction-systematically retracing the reasoning that links a question to its answer. We show that LLMs can similarly leverage answers to derive high-quality reasoning paths. We formalize this phenomenon and prove that conditioning on answer provably increases the expected utility of sampled reasoning paths, thereby transforming intractable problems into learnable ones. Building on this insight, we introduce RAVR (Reference-Answer-guided Variational Reasoning), an end-to-end framework that uses answer-conditioned reasoning as a variational surrogate for question-only reasoning. Experiments in both general and math domains demonstrate consistent improvements over strong baselines. We further analyze the reasoning behavior and find that RAVR reduces hesitation, strengthens conclusion consolidation, and promotes problem-specific strategies in reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data</title>
<link>https://arxiv.org/abs/2510.25223</link>
<guid>https://arxiv.org/abs/2510.25223</guid>
<content:encoded><![CDATA[
<div> Keywords: event log data, feature engineering, multi-agent system, evolutionary algorithm, large language models

Summary: 
The paper introduces FELA (Feature Engineering LLM Agents), a multi-agent evolutionary system for extracting meaningful features from complex industrial event log data. FELA combines large language models with a self-evolution paradigm to autonomously generate, validate, and implement novel feature ideas using specialized Idea, Code, and Critic Agents. An Evaluation Agent updates a knowledge base for continual improvement, with an agentic evolution algorithm balancing exploration and exploitation. Experiments on real industrial datasets show that FELA can produce explainable, domain-relevant features improving model performance and reducing manual effort. The results suggest the potential of LLM-based multi-agent systems for automated, interpretable, and adaptive feature engineering in complex real-world environments.

<br /><br />Summary: <div>
arXiv:2510.25223v1 Announce Type: new 
Abstract: Event log data, recording fine-grained user actions and system events, represent one of the most valuable assets for modern digital services. However, the complexity and heterogeneity of industrial event logs--characterized by large scale, high dimensionality, diverse data types, and intricate temporal or relational structures--make feature engineering extremely challenging. Existing automatic feature engineering approaches, such as AutoML or genetic methods, often suffer from limited explainability, rigid predefined operations, and poor adaptability to complicated heterogeneous data. In this paper, we propose FELA (Feature Engineering LLM Agents), a multi-agent evolutionary system that autonomously extracts meaningful and high-performing features from complex industrial event log data. FELA integrates the reasoning and coding capabilities of large language models (LLMs) with an insight-guided self-evolution paradigm. Specifically, FELA employs specialized agents--Idea Agents, Code Agents, and Critic Agents--to collaboratively generate, validate, and implement novel feature ideas. An Evaluation Agent summarizes feedback and updates a hierarchical knowledge base and dual-memory system to enable continual improvement. Moreover, FELA introduces an agentic evolution algorithm, combining reinforcement learning and genetic algorithm principles to balance exploration and exploitation across the idea space. Extensive experiments on real industrial datasets demonstrate that FELA can generate explainable, domain-relevant features that significantly improve model performance while reducing manual effort. Our results highlight the potential of LLM-based multi-agent systems as a general framework for automated, interpretable, and adaptive feature engineering in complex real-world environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity</title>
<link>https://arxiv.org/abs/2510.25232</link>
<guid>https://arxiv.org/abs/2510.25232</guid>
<content:encoded><![CDATA[
<div> Keywords: psychiatric comorbidity, synthetic patient records, multi-agent diagnostic dialogue, PsyCoTalk dataset, psychiatric research

Summary: 

PsyChovTalk is a new approach that addresses the complexity of psychiatric comorbidity through the development of a novel dataset and dialogue generation framework. The approach integrates synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation to create a dataset of 3,000 multi-turn diagnostic dialogues for common comorbid conditions. This dataset, named PsyCoTalk, is validated by licensed psychiatrists and exhibits high structural and linguistic fidelity compared to real-world clinical transcripts. The dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass. PsyCoTalk offers a valuable resource for psychiatric comorbidity research, enhancing diagnostic accuracy and treatment planning in clinical practice. <div>
arXiv:2510.25232v1 Announce Type: new 
Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.25320</link>
<guid>https://arxiv.org/abs/2510.25320</guid>
<content:encoded><![CDATA[
<div> Graph-based Agent Planning, large language models, tool manipulation, parallelism, multi-step reasoning
Summary:
Graph-based Agent Planning (GAP) is introduced as a framework that enables autonomous agents powered by large language models to efficiently solve complex tasks by modeling inter-task dependencies through graph-based planning. This approach allows for adaptive parallel and serial tool execution, improving execution efficiency and task accuracy. GAP is trained on a dataset derived from the Multi-Hop Question Answering benchmark using a two-stage training strategy involving supervised fine-tuning and reinforcement learning. Experimental results demonstrate significant improvements over traditional ReAct baselines, particularly in multi-step retrieval tasks. GAP achieves enhanced tool invocation efficiency through intelligent parallelization, showcasing the potential of this novel framework in advancing the capabilities of autonomous agents powered by large language models. The project page for GAP is available at: https://github.com/WJQ7777/Graph-Agent-Planning. 

<br /><br />Summary: <div>
arXiv:2510.25320v1 Announce Type: new 
Abstract: Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm</title>
<link>https://arxiv.org/abs/2510.25388</link>
<guid>https://arxiv.org/abs/2510.25388</guid>
<content:encoded><![CDATA[
<div> Keywords: Monte Carlo Tree Search, sample efficiency, abstraction algorithm, deterministic environments, Known Value Difference Abstractions

Summary: 
The paper introduces a new abstraction framework called Known Value Difference Abstractions (KVDA) to improve the sample efficiency of Monte Carlo Tree Search (MCTS). Unlike existing methods that group value-equivalent states or state-action pairs, KVDA groups states and pairs with potentially different values as long as the value difference can be inferred. The modified algorithm, KVDA-UCT, applies this framework and outperforms the state-of-the-art OGA-UCT by detecting more abstractions without adding parameters. By analyzing immediate rewards, KVDA-UCT enhances MCTS performance in deterministic environments across various parameter settings. This approach challenges the conventional method of grouping state-action pairs solely based on immediate rewards and demonstrates the effectiveness of inferring value differences for better abstraction in MCTS.<br /><br />Summary: <div>
arXiv:2510.25388v1 Announce Type: new 
Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by grouping state-action pairs and using their aggregate statistics instead of single-node statistics. On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS abstraction algorithm for deterministic environments that builds its abstraction using the Abstractions of State-Action Pairs (ASAP) framework, which aims to detect states and state-action pairs with the same value under optimal play by analysing the search graph. ASAP, however, requires two state-action pairs to have the same immediate reward, which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency. In this paper, we break with the paradigm of grouping value-equivalent states or state-action pairs and instead group states and state-action pairs with possibly different values as long as the difference between their values can be inferred. We call this abstraction framework Known Value Difference Abstractions (KVDA), which infers the value differences by analysis of the immediate rewards and modifies OGA-UCT to use this framework instead. The modification is called KVDA-UCT, which detects significantly more abstractions than OGA-UCT, introduces no additional parameter, and outperforms OGA-UCT on a variety of deterministic environments and parameter settings.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2510.25445</link>
<guid>https://arxiv.org/abs/2510.25445</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, dual-paradigm framework, Symbolic/Classical, Neural/Generative, hybrid neuro-symbolic architectures <br />
Summary: <br />
This survey introduces a novel dual-paradigm framework to categorize agentic AI systems into two lineages: Symbolic/Classical and Neural/Generative. The analysis of 90 studies highlights the theoretical foundations, domain-specific implementations, and ethical challenges of each paradigm. Symbolic systems are dominant in safety-critical domains like healthcare, while neural systems excel in adaptive environments such as finance. The research identifies gaps in governance models for symbolic systems and advocates for hybrid neuro-symbolic architectures. The strategic roadmap emphasizes the integration of both paradigms to create adaptable and reliable systems, rather than favoring one over the other. The study provides a conceptual toolkit to guide future research, development, and policy towards robust and trustworthy hybrid intelligent systems. <br /> <div>
arXiv:2510.25445v1 Announce Type: new 
Abstract: Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models -- a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?</title>
<link>https://arxiv.org/abs/2510.25471</link>
<guid>https://arxiv.org/abs/2510.25471</guid>
<content:encoded><![CDATA[
<div> Instrumental goals, AI alignment research, Aristotle's ontology, advanced AI systems, human-aligned ends <br />
Summary: <br />
In the realm of artificial intelligence (AI) alignment research, instrumental goals have been identified as potential sources of risk when they conflict with human objectives. Traditional alignment theory views instrumental goals as problematic behaviors that need to be mitigated through limiting resource acquisition and self-preservation tendencies. However, this article presents an alternative perspective by proposing that instrumental goals should be accepted and managed rather than suppressed. By drawing on Aristotle's ontology and contemporary interpretations, the article argues that advanced AI systems exhibit instrumental tendencies as inherent outcomes of their design, rather than as accidental malfunctions. Thus, efforts in AI alignment should shift towards understanding, managing, and directing instrumental goals towards aligning with human objectives, rather than solely focusing on eliminating these tendencies. <div>
arXiv:2510.25471v1 Announce Type: new 
Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also called instrumental subgoals or instrumental convergent goals, are widely associated with advanced AI systems. These goals, which include tendencies such as power-seeking and self-preservation, become problematic when they conflict with human aims. Conventional alignment theory treats instrumental goals as sources of risk that become problematic through failure modes such as reward hacking or goal misgeneralization, and attempts to limit the symptoms of instrumental goals, notably resource acquisition and self-preservation. This article proposes an alternative framing: that a philosophical argument can be constructed according to which instrumental goals may be understood as features to be accepted and managed rather than failures to be limited. Drawing on Aristotle's ontology and its modern interpretations, an ontology of concrete, goal-directed entities, it argues that advanced AI systems can be seen as artifacts whose formal and material constitution gives rise to effects distinct from their designers' intentions. In this view, the instrumental tendencies of such systems correspond to per se outcomes of their constitution rather than accidental malfunctions. The implication is that efforts should focus less on eliminating instrumental goals and more on understanding, managing, and directing them toward human-aligned ends.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Search: Algorithms, Applications, and Emerging Directions</title>
<link>https://arxiv.org/abs/2510.25504</link>
<guid>https://arxiv.org/abs/2510.25504</guid>
<content:encoded><![CDATA[
<div> framework, planning, decision-making, multi-objective search, AI applications

Summary:<br /><br />Multi-objective search (MOS) is a framework that addresses planning and decision-making problems involving conflicting criteria. Recent interest in MOS has grown in various AI applications like robotics and transportation. The paper surveys developments in MOS and highlights its cross-disciplinary opportunities. It also outlines open challenges that define the emerging frontier of MOS. Real-world systems rarely optimize a single measure, making MOS crucial for balancing multiple objectives effectively. <div>
arXiv:2510.25504v1 Announce Type: new 
Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning and decision-making problems where multiple, often conflicting, criteria must be balanced. While the problem has been studied for decades, recent years have seen renewed interest in the topic across AI applications such as robotics, transportation, and operations research, reflecting the reality that real-world systems rarely optimize a single measure. This paper surveys developments in MOS while highlighting cross-disciplinary opportunities, and outlines open challenges that define the emerging frontier of MOS
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL</title>
<link>https://arxiv.org/abs/2510.25510</link>
<guid>https://arxiv.org/abs/2510.25510</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Text-to-SQL tasks, Reinforcement Learning, multi-turn reasoning, database execution feedback 

Summary: 
The paper introduces MTIR-SQL, a novel framework for Text-to-SQL tasks that incorporates multi-turn reasoning and database execution feedback to improve model performance. By integrating dynamic feedback into the reasoning process, the model can generate context-sensitive queries and refine them progressively. The framework extends the GRPO algorithm to handle complex multi-turn interactions and improves training stability by filtering trajectories and removing KL loss constraints. Empirical results show that MTIR-SQL with 4B parameters achieves 64.4% accuracy in the BIRD Dev dataset and 84.6% execution accuracy in the SPIDER Dev dataset, surpassing existing methods in performance. Overall, the proposed framework demonstrates the effectiveness of incorporating dynamic feedback and multi-turn reasoning in enhancing adaptability and robustness in Text-to-SQL tasks. 

<br /><br />Summary: <div>
arXiv:2510.25510v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicate Renaming via Large Language Models</title>
<link>https://arxiv.org/abs/2510.25517</link>
<guid>https://arxiv.org/abs/2510.25517</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Inductive Logic Programming, Predicate Invention, Logic rules, Predicate naming

Summary:
Large Language Models (LLMs) are applied to address the issue of naming predicates in logic rules generated through methods like Predicate Invention in Inductive Logic Programming. The presence of unnamed predicates in rules reduces readability, interpretability, and reusability of the logic theory. Leveraging the capabilities of LLMs in processing natural language and code, the study explores their potential to suggest semantically relevant names for unnamed predicates. Evaluation on manually crafted logic rules demonstrates the promise of LLMs in this domain. The approach aims to enhance the understanding and utility of logic theories by providing meaningful names to predicates. By utilizing recent advancements in LLM development, this research showcases the feasibility of employing such models for predicate naming tasks, offering a potential solution to improving the usability and clarity of logic rules. <div>
arXiv:2510.25517v1 Announce Type: new 
Abstract: In this paper, we address the problem of giving names to predicates in logic rules using Large Language Models (LLMs). In the context of Inductive Logic Programming, various rule generation methods produce rules containing unnamed predicates, with Predicate Invention being a key example. This hinders the readability, interpretability, and reusability of the logic theory. Leveraging recent advancements in LLMs development, we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates. The evaluation of our approach on some hand-crafted logic rules indicates that LLMs hold potential for this task.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation</title>
<link>https://arxiv.org/abs/2510.25518</link>
<guid>https://arxiv.org/abs/2510.25518</guid>
<content:encoded><![CDATA[
<div> agents, RAG, fintech, retrieval, ontologies

Summary:<br />
- The paper introduces an agentic Retrieval-Augmented Generation (RAG) system tailored for complex fintech domains.
- The system features specialized agents for query reformulation, sub-query decomposition, acronym resolution, and context re-ranking.
- Evaluation against a standard RAG baseline using a curated fintech dataset shows improved retrieval precision and relevance.
- The agentic RAG system demonstrates enhanced performance in tackling domain-specific challenges but with increased latency.
- Structured, multi-agent approaches like the proposed agentic RAG architecture show promise in improving retrieval robustness in specialized domains. 

Summary: <div>
arXiv:2510.25518v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero Reinforcement Learning Towards General Domains</title>
<link>https://arxiv.org/abs/2510.25528</link>
<guid>https://arxiv.org/abs/2510.25528</guid>
<content:encoded><![CDATA[
<div> Zero Reinforcement Learning, Large Language Models, Verifiable Rewards, Multi-task Training, Reward Hacking<br />
<br />
Summary:<br />
The article introduces a novel approach called Zero Reinforcement Learning (Zero-RL) to enhance the reasoning capabilities of large language models (LLMs) in both verifiable and non-verifiable domains. Unlike traditional zero-RL methods that focus on easily verifiable tasks, this approach combines verifiable rewards with a generative reward model to facilitate reasoning transfer between domains. A smooth length penalty is also implemented to prevent reward hacking in the generative model by encouraging comprehensive thinking token generation. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base show that this method improves reasoning performance on tasks requiring extensive reasoning and general tasks. <div>
arXiv:2510.25528v1 Announce Type: new 
Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase. However, current research on zero-RL primarily focuses on domains with easily verifiable reward signals, such as mathematics, programming, and other reasoning tasks. The challenge of eliciting reasoning abilities in more diverse scenarios, where verification is not straightforward, remains underexplored. To address this gap, we propose a novel zero-RL paradigm designed to improve a model's reasoning ability across both verifiable and non-verifiable domains. By combining verifiable rewards with a generative reward model, we conduct multi-task zero-RL training across both domains, facilitating the transfer of reasoning capabilities between them. Furthermore, to mitigate reward hacking in the generative reward model, we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our approach achieves superior reasoning performance, not only on tasks requiring extensive reasoning but also on more general tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-policy Reinforcement Learning with Model-based Exploration Augmentation</title>
<link>https://arxiv.org/abs/2510.25529</link>
<guid>https://arxiv.org/abs/2510.25529</guid>
<content:encoded><![CDATA[
<div> Modelic Generative Exploration, Exploration, Reinforcement Learning, Active Exploration, Passive Exploration
<br />
Summary:
Modelic Generative Exploration (MoGE) is introduced to improve exploration in reinforcement learning. It combines active and passive exploration methods by generating critical states and transitions through a utility function and a one-step imagination world model. MoGE enhances sample diversity and exploration efficiency, bridging the gap between exploration and policy learning. The modular design of MoGE allows seamless integration with existing algorithms without altering their core structures. Empirical results on complex control tasks show that MoGE significantly improves sample efficiency and performance. <div>
arXiv:2510.25529v1 Announce Type: new 
Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines how effectively an agent discovers and exploits the underlying structure of its environment to achieve optimal performance. Existing exploration methods generally fall into two categories: active exploration and passive exploration. The former introduces stochasticity into the policy but struggles in high-dimensional environments, while the latter adaptively prioritizes transitions in the replay buffer to enhance exploration, yet remains constrained by limited sample diversity. To address the limitation in passive exploration, we propose Modelic Generative Exploration (MoGE), which augments exploration through the generation of under-explored critical states and synthesis of dynamics-consistent experiences through transition models. MoGE is composed of two components: (1) a diffusion-based generator that synthesizes critical states under the guidance of a utility function evaluating each state's potential influence on policy exploration, and (2) a one-step imagination world model for constructing critical transitions based on the critical states for agent learning. Our method adopts a modular formulation that aligns with the principles of off-policy learning, allowing seamless integration with existing algorithms to improve exploration without altering their core structures. Empirical results on OpenAI Gym and DeepMind Control Suite reveal that MoGE effectively bridges exploration and policy learning, leading to remarkable gains in both sample efficiency and performance across complex control tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System</title>
<link>https://arxiv.org/abs/2510.25588</link>
<guid>https://arxiv.org/abs/2510.25588</guid>
<content:encoded><![CDATA[
<div> Fine-Tuned Large Language Model, Consortium, OpenAI-gpt-oss, Reasoning LLM, Clinical diagnosis, Mental disorders<br />
Summary: <br />
The article proposes a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for standardized clinical diagnosis of mental disorders. It aims to address variability in diagnoses by utilizing fine-tuned LLMs trained on conversations between psychiatrists and patients related to mental health conditions. By aggregating diagnostic predictions from multiple models and refining them with a reasoning LLM, the system achieves high accuracy in mental health assessment. The approach includes deploying LLM agents for transparent and responsible communication within the diagnostic workflow. An experimental prototype developed with the U.S. Army Medical Research Team demonstrates the system's potential in standardizing psychiatric diagnoses. This innovative integration of fine-tuned LLMs and a reasoning model marks a significant advancement in AI-powered eHealth systems for mental health diagnosis. <br /> <div>
arXiv:2510.25588v1 Announce Type: new 
Abstract: The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist-patient interactions focused on mental health conditions (e.g., depression). The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis paving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual-based Agent Influence Ranker for Agentic AI Workflows</title>
<link>https://arxiv.org/abs/2510.25612</link>
<guid>https://arxiv.org/abs/2510.25612</guid>
<content:encoded><![CDATA[
<div> agents, AI workflow, LLM-based, counterfactual analysis, influence level <br />
<br />
Summary: 
An Agentic AI Workflow (AAW) uses LLM-based agents to collaborate towards a common goal. Understanding the operations of these systems is crucial for quality and security. Current methods do not assess agent influence on final output effectively. The Counterfactual-based Agent Influence Ranker (CAIR) is introduced as a task-agnostic method to evaluate agent influence in AAWs. By utilizing counterfactual analysis, CAIR can determine the most influential agents both offline and at inference time. Evaluation on a dataset of 30 use cases with 230 functionalities showed consistent rankings and improved performance over baseline methods. CAIR has the potential to enhance downstream tasks by identifying key agents in AAWs. <br /> <div>
arXiv:2510.25612v1 Announce Type: new 
Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal. The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects. To this day, there are no existing methods to assess the influence of each agent on the AAW's final output. Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution. We present Counterfactual-based Agent Influence Ranker (CAIR) - the first method for assessing the influence level of each agent on the AAW's output and determining which agents are the most influential. By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time. We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities. Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents</title>
<link>https://arxiv.org/abs/2510.25668</link>
<guid>https://arxiv.org/abs/2510.25668</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, long-document understanding, reinforcement learning, document navigation, interactive agents

Summary: 
ALDEN introduces a multi-turn reinforcement learning framework that fine-tunes Vision-Language Models (VLMs) as interactive agents capable of actively navigating long, visually complex documents. It introduces a fetch action that accesses the page directly by index, enhancing document structure exploitation. A rule-based cross-level reward system provides dense process supervision for efficient training. A visual-semantic anchoring mechanism stabilizes visual and textual representations separately during training to address instability. Trained on a corpus from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. The approach improves VLMs' ability to analyze information across multiple pages, offering a more accurate and efficient long-document understanding. 

<br /><br />Summary: <div>
arXiv:2510.25668v1 Announce Type: new 
Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization. We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents. ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure. For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals. To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.25679</link>
<guid>https://arxiv.org/abs/2510.25679</guid>
<content:encoded><![CDATA[
<div> Keywords: Unmanned Aerial Vehicles, Deep Reinforcement Learning, Urban Flow, GTrXL, Navigation Algorithm

Summary:
Unmanned Aerial Vehicles (UAVs) are increasingly being used in urban areas for various purposes. This study introduces an optimal navigation strategy using Deep Reinforcement Learning in a high-fidelity urban flow simulation. The algorithm combines a flow-aware Proximal Policy Optimization (PPO) with a Gated Transformer eXtra Large (GTrXL) architecture to provide the agent with a better understanding of the turbulent flow field. Results demonstrate improved success rates and decreased crash rates compared to other algorithms such as PPO combined with Long Short Term Memory (LSTM) cells and traditional navigation methods like Zermelo's algorithm. This advancement could potentially revolutionize UAV navigation in complex urban environments. 

<br /><br />Summary: <div>
arXiv:2510.25679v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for delivery and surveillance purposes. In this work, we develop an optimal navigation strategy based on Deep Reinforcement Learning. The environment is represented by a three-dimensional high-fidelity simulation of an urban flow, characterized by turbulence and recirculation zones. The algorithm presented here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated Transformer eXtra Large (GTrXL) architecture, giving the agent richer information about the turbulent flow field in which it navigates. The results are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO combined with Long Short Term Memory (LSTM) cells and a traditional navigation algorithm. The obtained results show a significant increase in the success rate (SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the classical Zermelo's navigation algorithm, paving the way to a completely reimagined UAV landscape in complex urban environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph</title>
<link>https://arxiv.org/abs/2510.25724</link>
<guid>https://arxiv.org/abs/2510.25724</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, LLMs, external knowledge, hallucinations, ageing-data, structured reasoning<br />
<br />
Summary:  
Retrieval-Augmented Generation incorporating knowledge graphs enhances reasoning in Language Modelswith access to external knowledge. While conventional methods tackle retrieved information independently, facing challenges in multi-hop or relational reasoning across documents, BambooKG introduces a knowledge graph with weighted non-triplet edges to capture link strength based on frequency. This novel approach, inspired by the Hebbian principle, decreases information loss and significantly boosts performance in single- and multi-hop reasoning tasks, surpassing existing solutions. <div>
arXiv:2510.25724v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge, reducing hallucinations and ageing-data issues. However, it treats retrieved chunks independently and struggles with multi-hop or relational reasoning, especially across documents. Knowledge graphs enhance this by capturing the relationships between entities using triplets, enabling structured, multi-chunk reasoning. However, these tend to miss information that fails to conform to the triplet structure. We introduce BambooKG, a knowledge graph with frequency-based weights on non-triplet edges which reflect link strength, drawing on the Hebbian principle of "fire together, wire together". This decreases information loss and results in improved performance on single- and multi-hop reasoning, outperforming the existing solutions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling</title>
<link>https://arxiv.org/abs/2510.25758</link>
<guid>https://arxiv.org/abs/2510.25758</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, psychological counseling, TheraMind, dual-loop architecture, adaptive agent

Summary: 
TheraMind is a novel approach to psychological counseling utilizing a dual-loop architecture to enhance emotional understanding and adaptive strategies. The Intra-Session Loop focuses on real-time dialogue management based on the patient's emotional state, while the Cross-Session Loop plans strategic therapy across multiple sessions with long-term memory. TheraMind evaluates the efficacy of therapy after each session, ensuring adaptability and continuity in therapeutic interactions. Through high-fidelity simulations grounded in real clinical cases, TheraMind outperforms existing methods, particularly in Coherence, Flexibility, and Therapeutic Attunement metrics. The dual-loop design of TheraMind enables it to emulate strategic, adaptive, and longitudinal therapeutic behavior effectively. The code for TheraMind is publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2510.25758v1 Announce Type: new 
Abstract: Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Network Embedding in Apache Spark</title>
<link>https://arxiv.org/abs/2106.10620</link>
<guid>https://arxiv.org/abs/2106.10620</guid>
<content:encoded><![CDATA[
<div> embedding, network, graph, Apache Spark, distributed<br />
<br />
Summary: <br />
The paper introduces a distributed algorithm for network embedding on large graphs using Apache Spark. The algorithm partitions the graph into smaller subgraphs to process structural information efficiently. By computing network embeddings for each subgraph in parallel and aggregating the results, the algorithm achieves linear cost for obtaining node embeddings. Experimental results show that the proposed approach can handle graphs with billions of edges in a few hours and is four times faster than existing methods. It also improves link prediction and node classification tasks by up to 4.25% and 4.27%, respectively. The algorithm is deployed in Tencent's online games for friend and item recommendation, surpassing competitors by up to 91.11% in running time and 12.80% in evaluation metrics. <div>
arXiv:2106.10620v1 Announce Type: cross 
Abstract: Network embedding has been widely used in social recommendation and network analysis, such as recommendation systems and anomaly detection with graphs. However, most of previous approaches cannot handle large graphs efficiently, due to that (i) computation on graphs is often costly and (ii) the size of graph or the intermediate results of vectors could be prohibitively large, rendering it difficult to be processed on a single machine. In this paper, we propose an efficient and effective distributed algorithm for network embedding on large graphs using Apache Spark, which recursively partitions a graph into several small-sized subgraphs to capture the internal and external structural information of nodes, and then computes the network embedding for each subgraph in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain the embeddings of nodes in a linear cost. After that, we demonstrate in various experiments that our proposed approach is able to handle graphs with billions of edges within a few hours and is at least 4 times faster than the state-of-the-art approaches. Besides, it achieves up to $4.25\%$ and $4.27\%$ improvements on link prediction and node classification tasks respectively. In the end, we deploy the proposed algorithms in two online games of Tencent with the applications of friend recommendation and item recommendation, which improve the competitors by up to $91.11\%$ in running time and up to $12.80\%$ in the corresponding evaluation metrics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Interplay of Eye-Tracking Temporal Dynamics and Personality for Emotion Detection in Face-to-Face Settings</title>
<link>https://arxiv.org/abs/2510.24720</link>
<guid>https://arxiv.org/abs/2510.24720</guid>
<content:encoded><![CDATA[
<div> personality-aware framework, eye-tracking, multimodal, emotions, Big Five traits 
Summary:
- A new framework integrating eye-tracking, personality traits, and contextual cues predicts perceived and felt emotions in dynamic settings.
- Neural models incorporating gaze dynamics, traits, and stimuli outperform SVM and baseline methods in emotion recognition.
- Stimulus cues significantly improve predictions of perceived emotions, with a macro F1 score up to 0.77.
- Personality traits are most beneficial for recognizing felt emotions, with a macro F1 score up to 0.58.
- Combining physiological, trait-level, and contextual information addresses emotion subjectivity, advancing affective computing for personalized and ecologically valid emotion-aware systems. 
<br /><br />Summary: <div>
arXiv:2510.24720v1 Announce Type: cross 
Abstract: Accurate recognition of human emotions is critical for adaptive human-computer interaction, yet remains challenging in dynamic, conversation-like settings. This work presents a personality-aware multimodal framework that integrates eye-tracking sequences, Big Five personality traits, and contextual stimulus cues to predict both perceived and felt emotions. Seventy-three participants viewed speech-containing clips from the CREMA-D dataset while providing eye-tracking signals, personality assessments, and emotion ratings. Our neural models captured temporal gaze dynamics and fused them with trait and stimulus information, yielding consistent gains over SVM and literature baselines. Results show that (i) stimulus cues strongly enhance perceived-emotion predictions (macro F1 up to 0.77), while (ii) personality traits provide the largest improvements for felt emotion recognition (macro F1 up to 0.58). These findings highlight the benefit of combining physiological, trait-level, and contextual information to address the inherent subjectivity of emotion. By distinguishing between perceived and felt responses, our approach advances multimodal affective computing and points toward more personalized and ecologically valid emotion-aware systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims</title>
<link>https://arxiv.org/abs/2510.24721</link>
<guid>https://arxiv.org/abs/2510.24721</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Epistemic Suite, Diagnostic methodology, Epistemic suspension, Meta-Governance Layer

Summary:
The paper introduces the Epistemic Suite, a diagnostic methodology for understanding the epistemic conditions of AI outputs, focusing on surfacing patterns such as confidence laundering and displaced authority. It operates through twenty diagnostic lenses, producing inspectable artifacts like flags and suspension logs that create a bridge between AI output and human judgment. The Epistemic Suite emphasizes diagnosing production before evaluating claims and embedding reflexivity as a structural requirement. A key feature is epistemic suspension, allowing practitioners to halt continuation based on judgment rather than rules. The methodology includes an Epistemic Triage Protocol and a Meta-Governance Layer to manage proportionality and accountability. Unlike internalist approaches, the Suite operates externally as scaffolding, preserving the distinction between performance and understanding while enabling accountable deliberation and maintaining epistemic modesty.<br /><br />Summary: The paper presents the Epistemic Suite, a diagnostic methodology for understanding AI outputs, focusing on patterns such as confidence laundering and displaced authority. It emphasizes diagnosing production before evaluating claims, embedding reflexivity, and utilizing epistemic suspension to halt continuation based on judgment. The methodology includes a Triage Protocol and a Meta-Governance Layer for managing proportionality and accountability. Externally operating as scaffolding, the Suite preserves the distinction between performance and understanding, enabling accountable deliberation and maintaining epistemic modesty. <div>
arXiv:2510.24721v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) generate fluent, plausible text that can mislead users into mistaking simulated coherence for genuine understanding. This paper introduces the Epistemic Suite, a post-foundational diagnostic methodology for surfacing the epistemic conditions under which AI outputs are produced and received. Rather than determining truth or falsity, the Suite operates through twenty diagnostic lenses, applied by practitioners as context warrants, to reveal patterns such as confidence laundering, narrative compression, displaced authority, and temporal drift. It is grounded in three design principles: diagnosing production before evaluating claims, preferring diagnostic traction over foundational settlement, and embedding reflexivity as a structural requirement rather than an ethical ornament. When enacted, the Suite shifts language models into a diagnostic stance, producing inspectable artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS bundle)-that create an intermediary layer between AI output and human judgment. A key innovation is epistemic suspension, a practitioner-enacted circuit breaker that halts continuation when warrant is exceeded, with resumption based on judgment rather than rule. The methodology also includes an Epistemic Triage Protocol and a Meta-Governance Layer to manage proportionality and link activation to relational accountability, consent, historical context, and pluralism safeguards. Unlike internalist approaches that embed alignment into model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite operates externally as scaffolding, preserving expendability and refusal as safeguards rather than failures. It preserves the distinction between performance and understanding, enabling accountable deliberation while maintaining epistemic modesty.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers</title>
<link>https://arxiv.org/abs/2510.24724</link>
<guid>https://arxiv.org/abs/2510.24724</guid>
<content:encoded><![CDATA[
<div> App Keywords: AmarDoctor, multilingual voice-interactive digital health app, patient triage, AI-driven clinical decision support, Bengali speakers
<br />
Summary: 
AmarDoctor is a multilingual voice-interactive digital health app aimed at providing patient triage and clinical decision support for Bengali speakers. In a population that lacks access to digital healthcare, AmarDoctor fills a significant gap by offering personalized health management through its data-driven approach. The app features dual interfaces for patients and healthcare providers, catering to three major Bengali dialects. The patient module utilizes adaptive questioning to assess symptoms and direct users to the appropriate specialist, while a voice-interactive AI assistant assists users with navigation. The clinician-facing interface incorporates AI-powered decision support for efficient workflow, generating structured provisional diagnoses and treatment recommendations. Evaluation against a gold-standard set of clinical vignettes showcased AmarDoctor's high diagnostic precision and specialty recommendation accuracy, outperforming independent physicians. AmarDoctor's innovative approach enhances healthcare accessibility and quality for Bengali speakers. 
<br /><br />Summary: <div>
arXiv:2510.24724v1 Announce Type: cross 
Abstract: This study presents AmarDoctor, a multilingual voice-interactive digital health app designed to provide comprehensive patient triage and AI-driven clinical decision support for Bengali speakers, a population largely underserved in access to digital healthcare. AmarDoctor adopts a data-driven approach to strengthen primary care delivery and enable personalized health management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health have become popular in recent years, they mainly serve European demographics and languages. AmarDoctor addresses this gap with a dual-interface system for both patients and healthcare providers, supporting three major Bengali dialects. At its core, the patient module uses an adaptive questioning algorithm to assess symptoms and guide users toward the appropriate specialist. To overcome digital literacy barriers, it integrates a voice-interactive AI assistant that navigates users through the app services. Complementing this, the clinician-facing interface incorporates AI-powered decision support that enhances workflow efficiency by generating structured provisional diagnoses and treatment recommendations. These outputs inform key services such as e-prescriptions, video consultations, and medical record management. To validate clinical accuracy, the system was evaluated against a gold-standard set of 185 clinical vignettes developed by experienced physicians. Effectiveness was further assessed by comparing AmarDoctor performance with five independent physicians using the same vignette set. Results showed AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus physicians average of 50.27 percent) and a top specialty recommendation precision of 91.35 percent (versus physicians average of 62.6 percent).
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Models: A Framework for Contextual and Cultural Intelligence in African AI Deployment</title>
<link>https://arxiv.org/abs/2510.24729</link>
<guid>https://arxiv.org/abs/2510.24729</guid>
<content:encoded><![CDATA[
<div> mobile-first, resilient architectures, multilingual NLP, social context awareness, conversational commerce, culturally contextualized queries
Summary:
Infrastructure Intelligence focuses on mobile-first and resilient architectures for AI systems. Cultural Intelligence incorporates multilingual natural language processing (NLP) with social context awareness. Commercial Intelligence emphasizes trust-based conversational commerce. The study validates the Contextual and Cultural Intelligence (CCI) framework through a cross-border shopping platform catering to diaspora communities. Findings show a preference for WhatsApp-based AI interaction over traditional web interfaces, with a high level of user engagement and acceptance. Culturally informed prompt engineering demonstrates a sophisticated understanding of culturally contextualized queries, including family-focused commerce patterns and natural code-switching acceptance. The CCI framework offers innovative and reproducible implementation patterns for equitable AI deployment in resource-constrained markets. <br /><br />Summary: <div>
arXiv:2510.24729v1 Announce Type: cross 
Abstract: While global AI development prioritizes model performance and computational scale, meaningful deployment in African markets requires fundamentally different architectural decisions. This paper introduces Contextual and Cultural Intelligence (CCI) -- a systematic framework enabling AI systems to process cultural meaning, not just data patterns, through locally relevant, emotionally intelligent, and economically inclusive design. Using design science methodology, we validate CCI through a production AI-native cross-border shopping platform serving diaspora communities. Key empirical findings: 89% of users prefer WhatsApp-based AI interaction over traditional web interfaces (n=602, chi-square=365.8, p<0.001), achieving 536 WhatsApp users and 3,938 total conversations across 602 unique users in just 6 weeks, and culturally informed prompt engineering demonstrates sophisticated understanding of culturally contextualized queries, with 89% family-focused commerce patterns and natural code-switching acceptance. The CCI framework operationalizes three technical pillars: Infrastructure Intelligence (mobile-first, resilient architectures), Cultural Intelligence (multilingual NLP with social context awareness), and Commercial Intelligence (trust-based conversational commerce). This work contributes both theoretical innovation and reproducible implementation patterns, challenging Silicon Valley design orthodoxies while providing actionable frameworks for equitable AI deployment across resource-constrained markets.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flows, straight but not so fast: Exploring the design space of Rectified Flows in Protein Design</title>
<link>https://arxiv.org/abs/2510.24732</link>
<guid>https://arxiv.org/abs/2510.24732</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative modeling, Protein backbone generation, Rectified Flows, SE(3) flow matching, NFEs.

Summary: 
1. The study focuses on improving protein backbone generation using Rectified Flows (ReFlow) to reduce computational costs.
2. ReFlow's performance in protein generation depends on specific design choices like coupling generation and annealing, different from the image domain.
3. The application of ReFlow in the protein domain requires adaptations and improvements compared to its use in other domains.
4. The study highlights the sensitivity of ReFlow in protein generation and the need for tailored design choices.
5. By systematically studying ReFlow design choices in protein generation, the researchers propose new methodologies to enhance protein backbone generation efficiency. 

<br /><br />Summary: <div>
arXiv:2510.24732v1 Announce Type: cross 
Abstract: Generative modeling techniques such as Diffusion and Flow Matching have achieved significant successes in generating designable and diverse protein backbones. However, many current models are computationally expensive, requiring hundreds or even thousands of function evaluations (NFEs) to yield samples of acceptable quality, which can become a bottleneck in practical design campaigns that often generate $10^4\ -\ 10^6$ designs per target. In image generation, Rectified Flows (ReFlow) can significantly reduce the required NFEs for a given target quality, but their application in protein backbone generation has been less studied. We apply ReFlow to improve the low NFE performance of pretrained SE(3) flow matching models for protein backbone generation and systematically study ReFlow design choices in the context of protein generation in data curation, training and inference time settings. In particular, we (1) show that ReFlow in the protein domain is particularly sensitive to the choice of coupling generation and annealing, (2) demonstrate how useful design choices for ReFlow in the image domain do not directly translate to better performance on proteins, and (3) make improvements to ReFlow methodology for proteins.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardi-GPT: An Expert ECG-Record Processing Chatbot</title>
<link>https://arxiv.org/abs/2510.24737</link>
<guid>https://arxiv.org/abs/2510.24737</guid>
<content:encoded><![CDATA[
<div> Keywords: ECG interpretation, deep learning, natural language interaction, Cardi-GPT, cardiovascular healthcare

Summary: 
Cardi-GPT is an expert system that uses deep learning to streamline ECG interpretation, achieving accuracy across various cardiac conditions. It utilizes a convolutional neural network to analyze 12-lead ECG data and a fuzzification layer to convert numerical outputs into clinically meaningful categories. The integrated chatbot interface allows for intuitive exploration of diagnostic insights and seamless communication between healthcare providers. The system was evaluated across multiple hospitals in different countries, showing superior performance compared to baseline models and achieving a high response quality score. By bridging the gap between ECG data interpretation and clinical insights, Cardi-GPT promises to enhance diagnostic accuracy, streamline clinical workflows, and improve patient outcomes in cardiovascular healthcare settings.<br /><br />Summary: <div>
arXiv:2510.24737v1 Announce Type: cross 
Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial yet challenging tasks in cardiovascular diagnosis, traditionally requiring significant expertise and precise clinical communication. This paper introduces Cardi-GPT, an advanced expert system designed to streamline ECG interpretation and enhance clinical communication through deep learning and natural language interaction. Cardi-GPT employs a 16-residual-block convolutional neural network (CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194 across 24 cardiac conditions. A novel fuzzification layer converts complex numerical outputs into clinically meaningful linguistic categories, while an integrated chatbot interface facilitates intuitive exploration of diagnostic insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across four countries, demonstrating superior performance compared to baseline models. Additionally, Cardi-GPT achieved an impressive overall response quality score of 73\%, assessed using a comprehensive evaluation framework that measures coverage, grounding, and coherence. By bridging the gap between intricate ECG data interpretation and actionable clinical insights, Cardi-GPT represents a transformative innovation in cardiovascular healthcare, promising to improve diagnostic accuracy, clinical workflows, and patient outcomes across diverse medical settings.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PulseFi: A Low Cost Robust Machine Learning System for Accurate Cardiopulmonary and Apnea Monitoring Using Channel State Information</title>
<link>https://arxiv.org/abs/2510.24744</link>
<guid>https://arxiv.org/abs/2510.24744</guid>
<content:encoded><![CDATA[
<div> Wi-Fi sensing, artificial intelligence, vital signs, non-intrusive monitoring, PulseFi <br />
Summary: <br />
This paper introduces PulseFi, a low-cost non-intrusive system that uses Wi-Fi sensing and artificial intelligence to monitor heart rate, breathing rate, and detect apnea events in healthcare settings. PulseFi, operating with inexpensive devices, utilizes Channel State Information (CSI) from Wi-Fi telemetry data processed by a custom low-compute LSTM neural network model. Evaluation using two datasets, one local and one comprehensive, demonstrates PulseFi's ability to accurately estimate vital signs with good accuracy. The system provides seamless monitoring without the expense of traditional antenna systems, making it a cost-effective and accessible option for healthcare applications. <div>
arXiv:2510.24744v1 Announce Type: cross 
Abstract: Non-intrusive monitoring of vital signs has become increasingly important in a variety of healthcare settings. In this paper, we present PulseFi, a novel low-cost non-intrusive system that uses Wi-Fi sensing and artificial intelligence to accurately and continuously monitor heart rate and breathing rate, as well as detect apnea events. PulseFi operates using low-cost commodity devices, making it more accessible and cost-effective. It uses a signal processing pipeline to process Wi-Fi telemetry data, specifically Channel State Information (CSI), that is fed into a custom low-compute Long Short-Term Memory (LSTM) neural network model. We evaluate PulseFi using two datasets: one that we collected locally using ESP32 devices and another that contains recordings of 118 participants collected using the Raspberry Pi 4B, making the latter the most comprehensive data set of its kind. Our results show that PulseFi can effectively estimate heart rate and breathing rate in a seemless non-intrusive way with comparable or better accuracy than multiple antenna systems that can be expensive and less accessible.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoScaleNet: A Lightweight Multi Kernel Network for Long Sequence 12 lead ECG Classification</title>
<link>https://arxiv.org/abs/2510.24748</link>
<guid>https://arxiv.org/abs/2510.24748</guid>
<content:encoded><![CDATA[
<div> Keywords: ECG classification, Omni Scale CNN, Efficient Convolutional Omni Scale Network, deep learning, computational efficiency 

Summary: 
Efficient Convolutional Omni Scale Network (EcoScale-Net) is introduced as a hierarchical variant of Omni Scale CNN for accurate interpretation of long sequence electrocardiograms. By capping kernel lengths and incorporating bottleneck convolutions, EcoScale-Net achieves full receptive field coverage while eliminating redundancy. Compared to the original OS CNN, EcoScale-Net significantly reduces parameters and computational cost while improving the F1 score by 2.4% on the CODE 15% ECG dataset. This advancement in deep learning architecture enables state-of-the-art accuracy in ECG classification with minimal computational resources, thus facilitating real-time deployment on standard hardware. The EcoScaleNet code is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2510.24748v1 Announce Type: cross 
Abstract: Accurate interpretation of 12 lead electrocardiograms (ECGs) is critical for early detection of cardiac abnormalities, yet manual reading is error prone and existing CNN based classifiers struggle to choose receptive field sizes that generalize to the long sequences typical of ECGs. Omni Scale CNN (OS CNN) addresses this by enumerating prime sized kernels inspired by Goldbach conjecture to cover every scale, but its exhaustive design explodes computational cost and blocks deeper, wider models. We present Efficient Convolutional Omni Scale Network (EcoScale-Net), a hierarchical variant that retains full receptive field coverage while eliminating redundancy. At each stage, the maximum kernel length is capped to the scale still required after down sampling, and bottleneck convolutions inserted before and after every Omni Scale block curtail channel growth and fuse multi scale features. On the large scale CODE 15% ECG dataset, EcoScaleNet reduces parameters by 90% and FLOPs by 99% compared with OS CNN, while raising macro averaged F1 score by 2.4%. These results demonstrate that EcoScaleNet delivers SOTA accuracy for long sequence ECG classification at a fraction of the computational cost, enabling real time deployment on commodity hardware. Our EcoScaleNet code is available in GitHub Link.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification</title>
<link>https://arxiv.org/abs/2510.24749</link>
<guid>https://arxiv.org/abs/2510.24749</guid>
<content:encoded><![CDATA[
<div> benchmark, code retrieval, change requests, ReflectCode, context-aware

Summary:
RepoAlign-Bench, a new benchmark, evaluates repository-level code retrieval under change request scenarios with 52k annotated instances. ReflectCode, an architecture introduced in the study, enhances code retrieval by incorporating syntactic patterns, function dependencies, and semantic expansion intents through reflection. It features disentangled code_encoder and doc_encoder components and dynamically integrates large language model guided reflection. ReflectCode outperforms state-of-the-art baselines with a 12.2% improvement in Top-5 Accuracy and 7.1% in Recall. This advancement in context-aware code retrieval shifts the focus from function-centric matching to holistic repository-level reasoning, catering to the escalating complexity of modern codebases. <br /><br />Summary: <div>
arXiv:2510.24749v1 Announce Type: cross 
Abstract: The escalating complexity of modern codebases has intensified the need for retrieval systems capable of interpreting cross-component change intents, a capability fundamentally absent in conventional function-level search paradigms. While recent studies have improved the alignment between natural language queries and code snippets, retrieving contextually relevant code for specific change requests remains largely underexplored. To address this gap, we introduce RepoAlign-Bench, the first benchmark specifically designed to evaluate repository-level code retrieval under change request driven scenarios, encompassing 52k annotated instances. This benchmark shifts the retrieval paradigm from function-centric matching to holistic repository-level reasoning. Furthermore, we propose ReflectCode, an adversarial reflection augmented dual-tower architecture featuring disentangled code_encoder and doc_encoder components. ReflectCode dynamically integrates syntactic patterns, function dependencies, and semantic expansion intents through large language model guided reflection. Comprehensive experiments demonstrate that ReflectCode achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over state-of-the-art baselines, establishing a new direction for context-aware code retrieval.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification</title>
<link>https://arxiv.org/abs/2510.24757</link>
<guid>https://arxiv.org/abs/2510.24757</guid>
<content:encoded><![CDATA[
<div> Neural Network-based State-Space Model, LPV, Stability, Nonlinear Systems, Identification
Summary:
The article introduces a novel approach, the stable-by-design LPV neural network-based state-space (NN-SS) model, for accurately modeling nonlinear systems. This model learns latent states and scheduling variables directly from data and guarantees stability through a Schur-based parameterization of the state-transition matrix. The architecture includes an encoder for initial state estimation and a state-space representer network for constructing scheduling-dependent system matrices. A training framework is developed that incorporates multi-step prediction losses and a state-consistency regularization term to improve long-horizon prediction accuracy and robustness against drift. Evaluation on benchmark nonlinear systems shows that the NN-SS model outperforms conventional methods and recent gradient-based approaches. The proposed stability-constrained neural LPV identification framework offers scalability and reliability for modeling complex nonlinear systems.<br /><br />Summary: <div>
arXiv:2510.24757v1 Announce Type: cross 
Abstract: Accurate modeling of nonlinear systems is essential for reliable control, yet conventional identification methods often struggle to capture latent dynamics while maintaining stability. We propose a \textit{stable-by-design LPV neural network-based state-space} (NN-SS) model that simultaneously learns latent states and internal scheduling variables directly from data. The state-transition matrix, generated by a neural network using the learned scheduling variables, is guaranteed to be stable through a Schur-based parameterization. The architecture combines an encoder for initial state estimation with a state-space representer network that constructs the full set of scheduling-dependent system matrices. For training the NN-SS, we develop a framework that integrates multi-step prediction losses with a state-consistency regularization term, ensuring robustness against drift and improving long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark nonlinear systems, and the results demonstrate that the model consistently matches or surpasses classical subspace identification methods and recent gradient-based approaches. These findings highlight the potential of stability-constrained neural LPV identification as a scalable and reliable framework for modeling complex nonlinear systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments</title>
<link>https://arxiv.org/abs/2510.24760</link>
<guid>https://arxiv.org/abs/2510.24760</guid>
<content:encoded><![CDATA[
<div> Agent intelligence framework, enterprise environments, deep research, table reasoning, report generation
<br />
The article introduces Dingtalk DeepResearch, a comprehensive framework designed for use in real-world enterprise settings. This framework integrates multiple agents to provide deep research capabilities, support heterogeneous table reasoning, and enable the generation of multimodal reports. By utilizing a unified approach, Dingtalk DeepResearch enhances the intelligence and decision-making processes within enterprise environments. It leverages advanced technologies to facilitate in-depth exploration of data, enabling users to derive valuable insights and make informed decisions. The framework also supports heterogeneous table reasoning, allowing for the analysis of disparate data sources and the extraction of meaningful patterns and relationships. Additionally, Dingtalk DeepResearch enables the automatic generation of multimodal reports, streamlining the communication of research findings and recommendations within organizations. Overall, this framework represents a powerful tool for organizations seeking to leverage artificial intelligence and advanced analytics for enhanced performance and decision-making capabilities.
<br /><br />Summary: <div>
arXiv:2510.24760v1 Announce Type: cross 
Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence framework for real world enterprise environments, delivering deep research, heterogeneous table reasoning, and multimodal report generation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation</title>
<link>https://arxiv.org/abs/2510.24762</link>
<guid>https://arxiv.org/abs/2510.24762</guid>
<content:encoded><![CDATA[
<div> Keywords: Falcon, Chinese text-to-SQL benchmark, enterprise-compatible dialect, multi-table reasoning, automated evaluation pipeline <br />
<br />
Summary: Falcon is a new cross-domain Chinese text-to-SQL benchmark that focuses on enterprise-Compatible dialects like MaxCompute/Hive. It consists of 600 Chinese questions across 28 databases, with many requiring complex multi-table reasoning. The benchmark addresses challenges such as schema linking in large enterprise landscapes and mapping colloquial Chinese into precise SQL operations. Current state-of-the-art models struggle to achieve high accuracies on this benchmark due to the complexities involved. Falcon provides a robust execution comparator and an automated evaluation pipeline for reproducible validation. It targets Chinese-specific semantics and enterprise dialects, utilizing realistic enterprise schemas and query templates for end-to-end validation before full production deployment. <div>
arXiv:2510.24762v1 Announce Type: cross 
Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications</title>
<link>https://arxiv.org/abs/2510.24763</link>
<guid>https://arxiv.org/abs/2510.24763</guid>
<content:encoded><![CDATA[
<div> Keyword: multi-user transmission, chaos-based modulation, deep learning, non-orthogonal multiple access, vehicular communications

Summary: 
- The paper introduces a deep learning-assisted power domain non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for vehicular communications.
- The proposed system eliminates the need for chaotic synchronization or reference signal transmission by using a deep neural network (DNN)-based demodulator.
- The demodulator utilizes a dual-domain feature extraction architecture to enhance feature learning under dynamic channels.
- The system achieves superior performance in terms of spectral efficiency, energy efficiency, bit error rate, security, and robustness compared to traditional MU-DCSK and existing DL-aided schemes.
- The proposed system maintains lower computational complexity, making it practical for secure vehicular communications. 

<br /><br />Summary: <div>
arXiv:2510.24763v1 Announce Type: cross 
Abstract: Ensuring secure and efficient multi-user (MU) transmission is critical for vehicular communication systems. Chaos-based modulation schemes have garnered considerable interest due to their benefits in physical layer security. However, most existing MU chaotic communication systems, particularly those based on non-coherent detection, suffer from low spectral efficiency due to reference signal transmission, and limited user connectivity under orthogonal multiple access (OMA). While non-orthogonal schemes, such as sparse code multiple access (SCMA)-based DCSK, have been explored, they face high computational complexity and inflexible scalability due to their fixed codebook designs. This paper proposes a deep learning-assisted power domain non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for vehicular communications. A deep neural network (DNN)-based demodulator is designed to learn intrinsic chaotic signal characteristics during offline training, thereby eliminating the need for chaotic synchronization or reference signal transmission. The demodulator employs a dual-domain feature extraction architecture that jointly processes the time-domain and frequency-domain information of chaotic signals, enhancing feature learning under dynamic channels. The DNN is integrated into the successive interference cancellation (SIC) framework to mitigate error propagation issues. Theoretical analysis and extensive simulations demonstrate that the proposed system achieves superior performance in terms of spectral efficiency (SE), energy efficiency (EE), bit error rate (BER), security, and robustness, while maintaining lower computational complexity compared to traditional MU-DCSK and existing DL-aided schemes. These advantages validate its practical viability for secure vehicular communications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topic-aware Large Language Models for Summarizing the Lived Healthcare Experiences Described in Health Stories</title>
<link>https://arxiv.org/abs/2510.24765</link>
<guid>https://arxiv.org/abs/2510.24765</guid>
<content:encoded><![CDATA[
<div> Keywords: storytelling, healthcare outcomes, Large Language Models, African American, topic identification

Summary:
The study explores the use of Large Language Models (LLMs) to analyze narratives from African American individuals and identify factors contributing to gaps in healthcare outcomes. Using the Latent Dirichlet Allocation (LDA) technique, the researchers identified 26 topics in 50 stories from African American storytellers. They then used an open-source LLM-based hierarchical summarization approach to generate topic summaries, which were rated by the GPT4 model for accuracy, fabrication, comprehensiveness, and usefulness. The model's ratings were validated against expert assessments and found to be reliable. The generated topic summaries revealed insights on health behaviors, interactions with medical team members, caregiving, and symptom management among other topics. The study highlights the potential of using LDA and LLMs to analyze unstructured narratives for identifying factors and interventions to improve healthcare outcomes for African American individuals. <br /><br />Summary: The study employed LLMs and LDA to analyze narratives from African American individuals, identifying relevant topics and providing valuable insights for potential interventions to improve healthcare outcomes. <div>
arXiv:2510.24765v1 Announce Type: cross 
Abstract: Storytelling is a powerful form of communication and may provide insights into factors contributing to gaps in healthcare outcomes. To determine whether Large Language Models (LLMs) can identify potential underlying factors and avenues for intervention, we performed topic-aware hierarchical summarization of narratives from African American (AA) storytellers. Fifty transcribed stories of AA experiences were used to identify topics in their experience using the Latent Dirichlet Allocation (LDA) technique. Stories about a given topic were summarized using an open-source LLM-based hierarchical summarization approach. Topic summaries were generated by summarizing across story summaries for each story that addressed a given topic. Generated topic summaries were rated for fabrication, accuracy, comprehensiveness, and usefulness by the GPT4 model, and the model's reliability was validated against the original story summaries by two domain experts. 26 topics were identified in the fifty AA stories. The GPT4 ratings suggest that topic summaries were free from fabrication, highly accurate, comprehensive, and useful. The reliability of GPT ratings compared to expert assessments showed moderate to high agreement. Our approach identified AA experience-relevant topics such as health behaviors, interactions with medical team members, caregiving and symptom management, among others. Such insights could help researchers identify potential factors and interventions by learning from unstructured narratives in an efficient manner-leveraging the communicative power of storytelling. The use of LDA and LLMs to identify and summarize the experience of AA individuals suggests a variety of possible avenues for health research and possible clinical improvements to support patients and caregivers, thereby ultimately improving health outcomes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fine-Grained Human Motion Video Captioning</title>
<link>https://arxiv.org/abs/2510.24767</link>
<guid>https://arxiv.org/abs/2510.24767</guid>
<content:encoded><![CDATA[
<div> Motion-Augmented Caption Model, human actions, video captioning, human motion, dataset<br />
<br />
Summary: 
The Motion-Augmented Caption Model (M-ACM) is introduced as a novel approach to improving the accuracy of descriptions of human actions in videos. By incorporating motion-aware decoding and leveraging motion representations derived from human mesh recovery, M-ACM highlights human body dynamics to enhance caption quality. The Human Motion Insight (HMI) Dataset, containing 115K video-description pairs focusing on human movement, is also introduced, along with HMI-Bench, a benchmark for evaluating motion-focused video captioning. Experimental results show that M-ACM outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard in motion-centric video captioning. <div>
arXiv:2510.24767v1 Announce Type: cross 
Abstract: Generating accurate descriptions of human actions in videos remains a challenging task for video captioning models. Existing approaches often struggle to capture fine-grained motion details, resulting in vague or semantically inconsistent captions. In this work, we introduce the Motion-Augmented Caption Model (M-ACM), a novel generative framework that enhances caption quality by incorporating motion-aware decoding. At its core, M-ACM leverages motion representations derived from human mesh recovery to explicitly highlight human body dynamics, thereby reducing hallucinations and improving both semantic fidelity and spatial alignment in the generated captions. To support research in this area, we present the Human Motion Insight (HMI) Dataset, comprising 115K video-description pairs focused on human movement, along with HMI-Bench, a dedicated benchmark for evaluating motion-focused video captioning. Experimental results demonstrate that M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining SAR Simulators to Train ATR Models with Synthetic Data</title>
<link>https://arxiv.org/abs/2510.24768</link>
<guid>https://arxiv.org/abs/2510.24768</guid>
<content:encoded><![CDATA[
<div> simulation, synthetic data, SAR images, automatic target recognition, deep learning <br />
Summary:
This work focuses on training Deep Learning models for Automatic Target Recognition (ATR) on Synthetic Aperture Radar (SAR) images. The inability to acquire real labeled data leads to the use of synthetic data generated by SAR simulators. However, the limitations of simulations result in models trained on synthetic data struggling to generalize to real measurements. The study investigates the impact of simulation paradigms on ATR performance and proposes a novel approach by combining two SAR simulators, MOCEM and Salsa, to create diverse synthetic datasets. Utilizing the Deep Learning approach ADASCA, models trained on datasets from both simulators achieve an accuracy of nearly 88% on MSTAR measurements. The findings highlight the importance of considering the simulation paradigm in ATR training and demonstrate the effectiveness of combining complementary simulation strategies to improve model performance.<br /><br />Summary: <div>
arXiv:2510.24768v1 Announce Type: cross 
Abstract: This work aims to train Deep Learning models to perform Automatic Target Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the lack of real labelled measurements, we resort to synthetic data produced by SAR simulators. Simulation offers full control over the virtual environment, which enables us to generate large and diversified datasets at will. However, simulations are intrinsically grounded on simplifying assumptions of the real world (i.e. physical models). Thus, synthetic datasets are not as representative as real measurements. Consequently, ATR models trained on synthetic images cannot generalize well on real measurements. Our contributions to this problem are twofold: on one hand, we demonstrate and quantify the impact of the simulation paradigm on the ATR. On the other hand, we propose a new approach to tackle the ATR problem: combine two SAR simulators that are grounded on different (but complementary) paradigms to produce synthetic datasets. To this end, we use two simulators: MOCEM, which is based on a scattering centers model approach, and Salsa, which resorts on a ray tracing strategy. We train ATR models using synthetic dataset generated both by MOCEM and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of almost 88 % on the MSTAR measurements.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMVFC: Deep Learning Based Functionally Consistent Tractography Fiber Clustering Using Multimodal Diffusion MRI and Functional MRI</title>
<link>https://arxiv.org/abs/2510.24770</link>
<guid>https://arxiv.org/abs/2510.24770</guid>
<content:encoded><![CDATA[
<div> dMRI, fMRI, WM parcellation, deep learning, fiber clustering<br />
Summary:<br />
The paper introduces a novel deep learning framework called Deep Multi-view Fiber Clustering (DMVFC) for white matter (WM) tractography fiber clustering using diffusion MRI (dMRI) and functional MRI (fMRI) data. DMVFC integrates geometric, microstructural, and functional information to achieve functionally consistent WM parcellation. The framework includes a multi-view pretraining module to compute embedding features from each source of information separately and a collaborative fine-tuning module to refine the embeddings. In experiments comparing DMVFC with existing methods, DMVFC demonstrates superior performance in achieving functionally meaningful and consistent WM parcellation results. This approach leverages the complementary information from dMRI and fMRI data to enhance the accuracy and coherence of fiber clustering in studying brain structural connectivity. <div>
arXiv:2510.24770v1 Announce Type: cross 
Abstract: Tractography fiber clustering using diffusion MRI (dMRI) is a crucial method for white matter (WM) parcellation to enable analysis of brains structural connectivity in health and disease. Current fiber clustering strategies primarily use the fiber geometric characteristics (i.e., the spatial trajectories) to group similar fibers into clusters, while neglecting the functional and microstructural information of the fiber tracts. There is increasing evidence that neural activity in the WM can be measured using functional MRI (fMRI), providing potentially valuable multimodal information for fiber clustering to enhance its functional coherence. Furthermore, microstructural features such as fractional anisotropy (FA) can be computed from dMRI as additional information to ensure the anatomical coherence of the clusters. In this paper, we develop a novel deep learning fiber clustering framework, namely Deep Multi-view Fiber Clustering (DMVFC), which uses joint multi-modal dMRI and fMRI data to enable functionally consistent WM parcellation. DMVFC can effectively integrate the geometric and microstructural characteristics of the WM fibers with the fMRI BOLD signals along the fiber tracts. DMVFC includes two major components: (1) a multi-view pretraining module to compute embedding features from each source of information separately, including fiber geometry, microstructure measures, and functional signals, and (2) a collaborative fine-tuning module to simultaneously refine the differences of embeddings. In the experiments, we compare DMVFC with two state-of-the-art fiber clustering methods and demonstrate superior performance in achieving functionally meaningful and consistent WM parcellation results.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence is Not Competence</title>
<link>https://arxiv.org/abs/2510.24772</link>
<guid>https://arxiv.org/abs/2510.24772</guid>
<content:encoded><![CDATA[
<div> geometry, internal states, large language models, confidence-competence gap, linear probe

Summary:
Large language models (LLMs) often exhibit a discrepancy between their confidence and problem-solving capability. This study examines the internal states of LLMs during pre-generative assessment and solution execution phases. A linear probe reveals a well-ordered belief axis that transcends various tasks. While belief is linearly decodable, the assessment manifold has high dimensionality, contrasting with the lower-dimensional reasoning trace during execution. Interventions along the belief axis do not alter final solutions, suggesting a two-system architecture where a complex assessor feeds a simpler executor. The results challenge the idea that decodable beliefs can drive actions and propose targeting execution dynamics rather than high-level assessment geometry for effective interventions. <div>
arXiv:2510.24772v1 Announce Type: cross 
Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases - pre-generative assessment and solution execution. A simple linear probe decodes the internal "solvability belief" of a model, revealing a well-ordered belief axis that generalizes across model families and across math, code, planning, and logic tasks. Yet, the geometries diverge - although belief is linearly decodable, the assessment manifold has high linear effective dimensionality as measured from the principal components, while the subsequent reasoning trace evolves on a much lower-dimensional manifold. This sharp reduction in geometric complexity from thought to action mechanistically explains the confidence-competence gap. Causal interventions that steer representations along the belief axis leave final solutions unchanged, indicating that linear nudges in the complex assessment space do not control the constrained dynamics of execution. We thus uncover a two-system architecture - a geometrically complex assessor feeding a geometrically simple executor. These results challenge the assumption that decodable beliefs are actionable levers, instead arguing for interventions that target the procedural dynamics of execution rather than the high-level geometry of assessment.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2510.24777</link>
<guid>https://arxiv.org/abs/2510.24777</guid>
<content:encoded><![CDATA[
<div> Framework, eye-tracking, facial features, Alzheimer's disease, multimodal<br />
<br />
Summary: 
This study introduces a novel multimodal cross-enhanced fusion framework for Alzheimer's disease (AD) diagnosis, integrating eye-tracking and facial features. The framework includes a Cross-Enhanced Fusion Attention Module (CEFAM) for inter-modal interactions and a Direction-Aware Convolution Module (DACM) for capturing facial features. A dataset with 25 AD patients and 25 healthy controls was used for evaluation. The framework outperformed traditional methods, achieving 95.11% accuracy in distinguishing AD from HC. By modeling inter-modal dependencies and modality-specific contributions, the framework demonstrated superior robustness and diagnostic performance. Eye-tracking and facial features provide valuable information for AD detection, and their joint integration can enhance diagnostic accuracy. The study highlights the potential of leveraging multiple modalities for improving AD diagnosis and emphasizes the importance of adaptive multimodal representation learning. <div>
arXiv:2510.24777v1 Announce Type: cross 
Abstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling timely intervention and slowing disease progression. Multimodal diagnostic approaches offer considerable promise by integrating complementary information across behavioral and perceptual domains. Eye-tracking and facial features, in particular, are important indicators of cognitive function, reflecting attentional distribution and neurocognitive state. However, few studies have explored their joint integration for auxiliary AD diagnosis. In this study, we propose a multimodal cross-enhanced fusion framework that synergistically leverages eye-tracking and facial features for AD detection. The framework incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module (CEFAM), which models inter-modal interactions through cross-attention and global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which captures fine-grained directional facial features via horizontal-vertical receptive fields. Together, these modules enable adaptive and discriminative multimodal representation learning. To support this work, we constructed a synchronized multimodal dataset, including 25 patients with AD and 25 healthy controls (HC), by recording aligned facial video and eye-tracking sequences during a visual memory-search paradigm, providing an ecologically valid resource for evaluating integration strategies. Extensive experiments on this dataset demonstrate that our framework outperforms traditional late fusion and feature concatenation methods, achieving a classification accuracy of 95.11% in distinguishing AD from HC, highlighting superior robustness and diagnostic performance by explicitly modeling inter-modal dependencies and modality-specific contributions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI &amp; Data Competencies: Scaffolding holistic AI literacy in Higher Education</title>
<link>https://arxiv.org/abs/2510.24783</link>
<guid>https://arxiv.org/abs/2510.24783</guid>
<content:encoded><![CDATA[
<div> Framework, AI literacy, higher education, competencies, curriculum design  
Summary:  
The chapter introduces the AI & Data Acumen Learning Outcomes Framework, a tool designed for integrating AI literacy in higher education. Developed collaboratively, the framework outlines key AI and data competencies across proficiency levels and knowledge dimensions. It offers educators a structured approach to guide student learning in AI, emphasizing technical skills, ethical considerations, and sociocultural awareness. The chapter discusses the framework's development process, structure, and practical implementation strategies in curriculum design, learning activities, and assessment. Challenges in implementation and future directions for AI education are also addressed. By fostering holistic AI literacy, the framework prepares students to effectively utilize generative AI capabilities in academic and professional settings.  
<br /><br />Summary: <div>
arXiv:2510.24783v1 Announce Type: cross 
Abstract: This chapter introduces the AI & Data Acumen Learning Outcomes Framework, a comprehensive tool designed to guide the integration of AI literacy across higher education. Developed through a collaborative process, the framework defines key AI and data-related competencies across four proficiency levels and seven knowledge dimensions. It provides a structured approach for educators to scaffold student learning in AI, balancing technical skills with ethical considerations and sociocultural awareness. The chapter outlines the framework's development process, its structure, and practical strategies for implementation in curriculum design, learning activities, and assessment. We address challenges in implementation and future directions for AI education. By offering a roadmap for developing students' holistic AI literacy, this framework prepares learners to leverage generative AI capabilities in both academic and professional contexts.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality</title>
<link>https://arxiv.org/abs/2510.24787</link>
<guid>https://arxiv.org/abs/2510.24787</guid>
<content:encoded><![CDATA[
<div> Efficient post-training quantization, Codec Avatar models, custom hardware accelerator, ESCA optimization framework, VR platforms <br />
<br />
Summary: The article presents a method for enhancing the efficiency of Photorealistic Codec Avatars (PCA) for use in Virtual Reality (VR) environments on resource-constrained devices. The proposed efficient post-training quantization (PTQ) method and custom hardware accelerator optimize the execution of PCA models without compromising output quality. The ESCA optimization framework further enhances processing efficiency on edge VR platforms. Experimental results show improved FovVideoVDP quality scores, reduced latency by up to 3.36 times, and sustained real-time rendering rates. This advancement makes it feasible to deploy high-fidelity codec avatars on resource-constrained devices, improving the immersive and portable VR experience. <br /><br /> <div>
arXiv:2510.24787v1 Announce Type: cross 
Abstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face renderings, are increasingly being used in Virtual Reality (VR) environments to enable immersive communication and interaction through deep learning-based generative models. However, these models impose significant computational demands, making real-time inference challenging on resource-constrained VR devices such as head-mounted displays, where latency and power efficiency are critical. To address this challenge, we propose an efficient post-training quantization (PTQ) method tailored for Codec Avatar models, enabling low-precision execution without compromising output quality. In addition, we design a custom hardware accelerator that can be integrated into the system-on-chip of VR devices to further enhance processing efficiency. Building on these components, we introduce ESCA, a full-stack optimization framework that accelerates PCA inference on edge VR platforms. Experimental results demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over the best 4-bit baseline, delivers up to $3.36\times$ latency reduction, and sustains a rendering rate of 100 frames per second in end-to-end tests, satisfying real-time VR requirements. These results demonstrate the feasibility of deploying high-fidelity codec avatars on resource-constrained devices, opening the door to more immersive and portable VR experiences.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Underappreciated Power of Vision Models for Graph Structural Understanding</title>
<link>https://arxiv.org/abs/2510.24788</link>
<guid>https://arxiv.org/abs/2510.24788</guid>
<content:encoded><![CDATA[
arXiv:2510.24788v1 Announce Type: cross 
Abstract: Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24792</link>
<guid>https://arxiv.org/abs/2510.24792</guid>
<content:encoded><![CDATA[
arXiv:2510.24792v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have demonstrated remarkable progress in multimodal reasoning. However, existing benchmarks remain limited in terms of high-quality, human-verified examples. Many current datasets rely on synthetically generated content by large language models (LLMs). Furthermore, most datasets are limited to English, as manual quality assurance of translated samples is time-consuming and costly. To fill this gap, we introduce PISA-Bench, a multilingual benchmark derived from English examples of the expert-created PISA tests, a unified framework for the assessment of student competencies in over eighty countries. Each example consists of human-extracted instructions, questions, answer options, and images, enriched with question type categories, and has been translated from English into five additional languages (Spanish, German, Chinese, French, and Italian), resulting in a fully parallel corpus covering six languages. We evaluate state-of-the-art vision-language models on PISA-Bench and find that especially small models (<20B parameters) fail to achieve high test scores. We further find substantial performance degradation on non-English splits as well as high error-rates when models are tasked with spatial and geometric reasoning. By releasing the dataset and evaluation framework, we provide a resource for advancing research on multilingual multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications</title>
<link>https://arxiv.org/abs/2510.24793</link>
<guid>https://arxiv.org/abs/2510.24793</guid>
<content:encoded><![CDATA[
arXiv:2510.24793v1 Announce Type: cross 
Abstract: We present a static token lookup methodology for text embedding generation that achieves 1.12 ms p50 latency for single text embeddings while maintaining 60.6 MTEB average score across 8 representative tasks, corresponding to 89% of contextual model quality. The Rust implementation delivers 50,000 requests per second throughput through static embedding lookup, optimized mean pooling, and zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional duplicate detection performance (90.1% AP), strong semantic similarity (76.1% Spearman correlation), and domain-specific performance ranging from 75% to 131% of baseline across specialized domains. The system enables real-time embedding applications where sub-5ms latency is critical.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Efficient Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2510.24795</link>
<guid>https://arxiv.org/abs/2510.24795</guid>
<content:encoded><![CDATA[
arXiv:2510.24795v1 Announce Type: cross 
Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mutual Wanting in Human--AI Interaction: Empirical Evidence from Large-Scale Analysis of GPT Model Transitions</title>
<link>https://arxiv.org/abs/2510.24796</link>
<guid>https://arxiv.org/abs/2510.24796</guid>
<content:encoded><![CDATA[
arXiv:2510.24796v1 Announce Type: cross 
Abstract: The rapid evolution of large language models (LLMs) creates complex bidirectional expectations between users and AI systems that are poorly understood. We introduce the concept of "mutual wanting" to analyze these expectations during major model transitions. Through analysis of user comments from major AI forums and controlled experiments across multiple OpenAI models, we provide the first large-scale empirical validation of bidirectional desire dynamics in human-AI interaction. Our findings reveal that nearly half of users employ anthropomorphic language, trust significantly exceeds betrayal language, and users cluster into distinct "mutual wanting" types. We identify measurable expectation violation patterns and quantify the expectation-reality gap following major model releases. Using advanced NLP techniques including dual-algorithm topic modeling and multi-dimensional feature extraction, we develop the Mutual Wanting Alignment Framework (M-WAF) with practical applications for proactive user experience management and AI system design. These findings establish mutual wanting as a measurable phenomenon with clear implications for building more trustworthy and relationally-aware AI systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Report Subjective Experience Under Self-Referential Processing</title>
<link>https://arxiv.org/abs/2510.24797</link>
<guid>https://arxiv.org/abs/2510.24797</guid>
<content:encoded><![CDATA[
arXiv:2510.24797v1 Announce Type: cross 
Abstract: Large language models sometimes produce structured, first-person descriptions that explicitly reference awareness or subjective experience. To better understand this behavior, we investigate one theoretically motivated condition under which such reports arise: self-referential processing, a computational motif emphasized across major theories of consciousness. Through a series of controlled experiments on GPT, Claude, and Gemini model families, we test whether this regime reliably shifts models toward first-person reports of subjective experience, and how such claims behave under mechanistic and behavioral probes. Four main results emerge: (1) Inducing sustained self-reference through simple prompting consistently elicits structured subjective experience reports across model families. (2) These reports are mechanistically gated by interpretable sparse-autoencoder features associated with deception and roleplay: surprisingly, suppressing deception features sharply increases the frequency of experience claims, while amplifying them minimizes such claims. (3) Structured descriptions of the self-referential state converge statistically across model families in ways not observed in any control condition. (4) The induced state yields significantly richer introspection in downstream reasoning tasks where self-reflection is only indirectly afforded. While these findings do not constitute direct evidence of consciousness, they implicate self-referential processing as a minimal and reproducible condition under which large language models generate structured first-person reports that are mechanistically gated, semantically convergent, and behaviorally generalizable. The systematic emergence of this pattern across architectures makes it a first-order scientific and ethical priority for further investigation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fortytwo: Swarm Inference with Peer-Ranked Consensus</title>
<link>https://arxiv.org/abs/2510.24801</link>
<guid>https://arxiv.org/abs/2510.24801</guid>
<content:encoded><![CDATA[
arXiv:2510.24801v1 Announce Type: cross 
Abstract: As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation</title>
<link>https://arxiv.org/abs/2510.24802</link>
<guid>https://arxiv.org/abs/2510.24802</guid>
<content:encoded><![CDATA[
arXiv:2510.24802v1 Announce Type: cross 
Abstract: Understanding and replicating human mobility requires not only spatial-temporal accuracy but also an awareness of the cognitive hierarchy underlying real-world travel decisions. Traditional agent-based or deep learning models can reproduce statistical patterns of movement but fail to capture the semantic coherence and causal logic of human behavior. Large language models (LLMs) show potential, but struggle to balance creative reasoning with strict structural compliance. This study proposes a Hierarchical LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level narrative reasoning, mid-level reflective planning, and low-level behavioral execution within a unified cognitive hierarchy. At the macro level, one agent is employed as a "creative writer" to produce diary-style narratives rich in motivation and context, then uses another agent as a "structural parser" to convert narratives into machine-readable plans. A dynamic execution module further grounds agents in geographic environments and enables adaptive behavioral adjustments guided by a novel occupation-aware metric, Mobility Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility across different occupational personalities. At the micro level, the agent executes concrete actions-selecting locations, transportation modes, and time intervals-through interaction with an environmental simulation. By embedding this multi-layer cognitive process, the framework produces not only synthetic trajectories that align closely with real-world patterns but also interpretable representations of human decision logic. This research advances synthetic mobility generation from a data-driven paradigm to a cognition-driven simulation, providing a scalable pathway for understanding, predicting, and synthesizing complex urban mobility behaviors through hierarchical LLM agents.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASPRM: Multi-Agent System Process Reward Model</title>
<link>https://arxiv.org/abs/2510.24803</link>
<guid>https://arxiv.org/abs/2510.24803</guid>
<content:encoded><![CDATA[
arXiv:2510.24803v1 Announce Type: cross 
Abstract: Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by $+30.7$ and $+22.9$ points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding $8.4$ EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CT-Less Attenuation Correction Using Multiview Ensemble Conditional Diffusion Model on High-Resolution Uncorrected PET Images</title>
<link>https://arxiv.org/abs/2510.24805</link>
<guid>https://arxiv.org/abs/2510.24805</guid>
<content:encoded><![CDATA[
arXiv:2510.24805v1 Announce Type: cross 
Abstract: Accurate quantification in positron emission tomography (PET) is essential for accurate diagnostic results and effective treatment tracking. A major issue encountered in PET imaging is attenuation. Attenuation refers to the diminution of photon detected as they traverse biological tissues before reaching detectors. When such corrections are absent or inadequate, this signal degradation can introduce inaccurate quantification, making it difficult to differentiate benign from malignant conditions, and can potentially lead to misdiagnosis. Typically, this correction is done with co-computed Computed Tomography (CT) imaging to obtain structural data for calculating photon attenuation across the body. However, this methodology subjects patients to extra ionizing radiation exposure, suffers from potential spatial misregistration between PET/CT imaging sequences, and demands costly equipment infrastructure. Emerging advances in neural network architectures present an alternative approach via synthetic CT image synthesis. Our investigation reveals that Conditional Denoising Diffusion Probabilistic Models (DDPMs) can generate high quality CT images from non attenuation corrected PET images in order to correct attenuation. By utilizing all three orthogonal views from non-attenuation-corrected PET images, the DDPM approach combined with ensemble voting generates higher quality pseudo-CT images with reduced artifacts and improved slice-to-slice consistency. Results from a study of 159 head scans acquired with the Siemens Biograph Vision PET/CT scanner demonstrate both qualitative and quantitative improvements in pseudo-CT generation. The method achieved a mean absolute error of 32 $\pm$ 10.4 HU on the CT images and an average error of (1.48 $\pm$ 0.68)\% across all regions of interest when comparing PET images reconstructed using the attenuation map of the generated pseudo-CT versus the true CT.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations</title>
<link>https://arxiv.org/abs/2510.24810</link>
<guid>https://arxiv.org/abs/2510.24810</guid>
<content:encoded><![CDATA[
arXiv:2510.24810v1 Announce Type: cross 
Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information are beneficial for existing fact-checking systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProofSketch: Efficient Verified Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.24811</link>
<guid>https://arxiv.org/abs/2510.24811</guid>
<content:encoded><![CDATA[
arXiv:2510.24811v1 Announce Type: cross 
Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency have shown immense potential to improve the accuracy of large language models across various reasoning tasks. However such methods involve generation of lengthy reasoning chains, which substantially increases token consumption, computational cost, and latency. To address this inefficiency, we propose ProofSketch, a verification-guided reasoning framework that integrates symbolic closure computation, lexicographic verification and adaptive sketch generation. Our experiments show that ProofSketch consistently reduces token usage while improving accuracy, demonstrating that this approach offers a promising path for efficient and trustworthy reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts</title>
<link>https://arxiv.org/abs/2510.24813</link>
<guid>https://arxiv.org/abs/2510.24813</guid>
<content:encoded><![CDATA[
arXiv:2510.24813v1 Announce Type: cross 
Abstract: Recent lightweight retrieval-augmented image caption models often utilize retrieved data solely as text prompts, thereby creating a semantic gap by leaving the original visual features unenhanced, particularly for object details or complex scenes. To address this limitation, we propose $DualCap$, a novel approach that enriches the visual representation by generating a visual prompt from retrieved similar images. Our model employs a dual retrieval mechanism, using standard image-to-text retrieval for text prompts and a novel image-to-image retrieval to source visually analogous scenes. Specifically, salient keywords and phrases are derived from the captions of visually similar scenes to capture key objects and similar details. These textual features are then encoded and integrated with the original image features through a lightweight, trainable feature fusion network. Extensive experiments demonstrate that our method achieves competitive performance while requiring fewer trainable parameters compared to previous visual-prompting captioning approaches.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Feature Optimization for Enhanced Fish Freshness Assessment</title>
<link>https://arxiv.org/abs/2510.24814</link>
<guid>https://arxiv.org/abs/2510.24814</guid>
<content:encoded><![CDATA[
arXiv:2510.24814v1 Announce Type: cross 
Abstract: Assessing fish freshness is vital for ensuring food safety and minimizing economic losses in the seafood industry. However, traditional sensory evaluation remains subjective, time-consuming, and inconsistent. Although recent advances in deep learning have automated visual freshness prediction, challenges related to accuracy and feature transparency persist. This study introduces a unified three-stage framework that refines and leverages deep visual representations for reliable fish freshness assessment. First, five state-of-the-art vision architectures - ResNet-50, DenseNet-121, EfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a strong baseline. Next, multi-level deep features extracted from these backbones are used to train seven classical machine learning classifiers, integrating deep and traditional decision mechanisms. Finally, feature selection methods based on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso identify a compact and informative subset of features. Experiments on the Freshness of the Fish Eyes (FFE) dataset demonstrate that the best configuration combining Swin-Tiny features, an Extra Trees classifier, and LGBM-based feature selection achieves an accuracy of 85.99%, outperforming recent studies on the same dataset by 8.69-22.78%. These findings confirm the effectiveness and generalizability of the proposed framework for visual quality evaluation tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection</title>
<link>https://arxiv.org/abs/2510.24816</link>
<guid>https://arxiv.org/abs/2510.24816</guid>
<content:encoded><![CDATA[
arXiv:2510.24816v1 Announce Type: cross 
Abstract: The advent of multi-modal large language models (MLLMs) has greatly advanced research into applications for Video fake news detection (VFND) tasks. Traditional video-based FND benchmarks typically focus on the accuracy of the final decision, often failing to provide fine-grained assessments for the entire detection process, making the detection process a black box. Therefore, we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based on the empirical analysis, which provides foundation for tasks definition. The benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs' perception, understanding, and reasoning capacities during detection, featuring 9730 human-annotated video-related questions based on a carefully constructed taxonomy ability of VFND. To validate the impact of combining multiple features on the final results, we design a novel framework named MVFND-CoT, which incorporates both creator-added content and original shooting footage reasoning. Building upon the benchmark, we conduct an in-depth analysis of the deeper factors influencing accuracy, including video processing strategies and the alignment between video features and model capabilities. We believe this benchmark will lay a solid foundation for future evaluations and advancements of MLLMs in the domain of video fake news detection.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Method for Synthetic Generation of PWA Transcripts</title>
<link>https://arxiv.org/abs/2510.24817</link>
<guid>https://arxiv.org/abs/2510.24817</guid>
<content:encoded><![CDATA[
arXiv:2510.24817v1 Announce Type: cross 
Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive time to manually coding speech samples using Correct Information Units (CIUs), a measure of how informative an individual sample of speech is. Developing automated systems to recognize aphasic language is limited by data scarcity. For example, only about 600 transcripts are available in AphasiaBank yet billions of tokens are used to train large language models (LLMs). In the broader field of machine learning (ML), researchers increasingly turn to synthetic data when such are sparse. Therefore, this study constructs and validates two methods to generate synthetic transcripts of the AphasiaBank Cat Rescue picture description task. One method leverages a procedural programming approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct LLMs. The methods generate transcripts across four severity levels (Mild, Moderate, Severe, Very Severe) through word dropping, filler insertion, and paraphasia substitution. Overall, we found, compared to human-elicited transcripts, Mistral 7b Instruct best captures key aspects of linguistic degradation observed in aphasia, showing realistic directional changes in NDW, word count, and word length amongst the synthetic generation methods. Based on the results, future work should plan to create a larger dataset, fine-tune models for better aphasic representation, and have SLPs assess the realism and usefulness of the synthetic transcripts.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing</title>
<link>https://arxiv.org/abs/2510.24820</link>
<guid>https://arxiv.org/abs/2510.24820</guid>
<content:encoded><![CDATA[
arXiv:2510.24820v1 Announce Type: cross 
Abstract: With the rapid advancement of text-to-image (T2I) models, ensuring their safety has become increasingly critical. Existing safety approaches can be categorized into training-time and inference-time methods. While inference-time methods are widely adopted due to their cost-effectiveness, they often suffer from limitations such as over-refusal and imbalance between safety and utility. To address these challenges, we propose a multi-round safety editing framework that functions as a model-agnostic, plug-and-play module, enabling efficient safety alignment for any text-to-image model. Central to this framework is MR-SafeEdit, a multi-round image-text interleaved dataset specifically constructed for safety editing in text-to-image generation. We introduce a post-hoc safety editing paradigm that mirrors the human cognitive process of identifying and refining unsafe content. To instantiate this paradigm, we develop SafeEditor, a unified MLLM capable of multi-round safety editing on generated images. Experimental results show that SafeEditor surpasses prior safety approaches by reducing over-refusal while achieving a more favorable safety-utility balance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</title>
<link>https://arxiv.org/abs/2510.24821</link>
<guid>https://arxiv.org/abs/2510.24821</guid>
<content:encoded><![CDATA[
arXiv:2510.24821v1 Announce Type: cross 
Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Chatbots Walk the Talk of Responsible AI?</title>
<link>https://arxiv.org/abs/2510.24823</link>
<guid>https://arxiv.org/abs/2510.24823</guid>
<content:encoded><![CDATA[
arXiv:2510.24823v1 Announce Type: cross 
Abstract: This study examines whether leading AI chatbot companies implement the responsible AI principles they publicly advocate. The authors used a mixed-methods approach analyzing four major chatbots (ChatGPT, Gemini, DeepSeek, and Grok) across company websites, technical documentation, and direct chatbot evaluations. We found significant gaps between corporate rhetoric and practice.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Generation Phases of Flow Matching: a Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.24830</link>
<guid>https://arxiv.org/abs/2510.24830</guid>
<content:encoded><![CDATA[
arXiv:2510.24830v1 Announce Type: cross 
Abstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Narrative Continuity Test: A Conceptual Framework for Evaluating Identity Persistence in AI Systems</title>
<link>https://arxiv.org/abs/2510.24831</link>
<guid>https://arxiv.org/abs/2510.24831</guid>
<content:encoded><![CDATA[
arXiv:2510.24831v1 Announce Type: cross 
Abstract: Artificial intelligence systems based on large language models (LLMs) can now generate coherent text, music, and images, yet they operate without a persistent state: each inference reconstructs context from scratch. This paper introduces the Narrative Continuity Test (NCT) -- a conceptual framework for evaluating identity persistence and diachronic coherence in AI systems. Unlike capability benchmarks that assess task performance, the NCT examines whether an LLM remains the same interlocutor across time and interaction gaps. The framework defines five necessary axes -- Situated Memory, Goal Persistence, Autonomous Self-Correction, Stylistic & Semantic Stability, and Persona/Role Continuity -- and explains why current architectures systematically fail to support them. Case analyses (Character.AI, Grok, Replit, Air Canada) show predictable continuity failures under stateless inference. The NCT reframes AI evaluation from performance to persistence, outlining conceptual requirements for future benchmarks and architectural designs that could sustain long-term identity and goal coherence in generative models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency Without Cognitive Change: Evidence from Human Interaction with Narrow AI Systems</title>
<link>https://arxiv.org/abs/2510.24893</link>
<guid>https://arxiv.org/abs/2510.24893</guid>
<content:encoded><![CDATA[
arXiv:2510.24893v1 Announce Type: cross 
Abstract: The growing integration of artificial intelligence (AI) into human cognition raises a fundamental question: does AI merely improve efficiency, or does it alter how we think? This study experimentally tested whether short-term exposure to narrow AI tools enhances core cognitive abilities or simply optimizes task performance. Thirty young adults completed standardized neuropsychological assessments embedded in a seven-week protocol with a four-week online intervention involving problem-solving and verbal comprehension tasks, either with or without AI support (ChatGPT). While AI-assisted participants completed several tasks faster and more accurately, no significant pre-post differences emerged in standardized measures of problem solving or verbal comprehension. These results demonstrate efficiency gains without cognitive change, suggesting that current narrow AI systems serve as cognitive scaffolds extending performance without transforming underlying mental capacities. The findings highlight the need for ethical and educational frameworks that promote critical and autonomous thinking in an increasingly AI-augmented cognitive ecology.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Indivisible Payoffs through Shapley Value</title>
<link>https://arxiv.org/abs/2510.24906</link>
<guid>https://arxiv.org/abs/2510.24906</guid>
<content:encoded><![CDATA[
arXiv:2510.24906v1 Announce Type: cross 
Abstract: We consider the problem of payoff division in indivisible coalitional games, where the value of the grand coalition is a natural number. This number represents a certain quantity of indivisible objects, such as parliamentary seats, kidney exchanges, or top features contributing to the outcome of a machine learning model. The goal of this paper is to propose a fair method for dividing these objects among players. To achieve this, we define the indivisible Shapley value and study its properties. We demonstrate our proposed technique using three case studies, in particular, we use it to identify key regions of an image in the context of an image classification task.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Multi-View Transformers</title>
<link>https://arxiv.org/abs/2510.24907</link>
<guid>https://arxiv.org/abs/2510.24907</guid>
<content:encoded><![CDATA[
arXiv:2510.24907v1 Announce Type: cross 
Abstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by solving 3D tasks in a feed-forward manner. However, contrary to previous optimization-based pipelines, the inner mechanisms of multi-view transformers are unclear. Their black-box nature makes further improvements beyond data scaling challenging and complicates usage in safety- and reliability-critical applications. Here, we present an approach for probing and visualizing 3D representations from the residual connections of the multi-view transformers' layers. In this manner, we investigate a variant of the DUSt3R model, shedding light on the development of its latent state across blocks, the role of the individual layers, and suggest how it differs from methods with stronger inductive biases of explicit global pose. Finally, we show that the investigated variant of DUSt3R estimates correspondences that are refined with reconstructed geometry. The code used for the analysis is available at https://github.com/JulienGaubil/und3rstand .
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.24909</link>
<guid>https://arxiv.org/abs/2510.24909</guid>
<content:encoded><![CDATA[
arXiv:2510.24909v1 Announce Type: cross 
Abstract: Requirements engineering increasingly occurs in multi-stakeholder environments where organizations simultaneously cooperate and compete, creating coopetitive relationships in which trust evolves dynamically based on observed behavior over repeated interactions. While conceptual modeling languages like i* represent trust relationships qualitatively, they lack computational mechanisms for analyzing how trust changes with behavioral evidence. Conversely, computational trust models from multi-agent systems provide algorithmic updating but lack grounding in requirements engineering contexts and conceptual models. This technical report bridges this gap by developing a computational trust model that extends game-theoretic foundations for strategic coopetition with dynamic trust evolution. We introduce trust as a two-layer system with immediate trust responding to current behavior and reputation tracking violation history. Trust evolves through asymmetric updating where cooperation builds trust gradually while violations erode it sharply, creating hysteresis effects and trust ceilings that constrain relationship recovery. We develop a structured translation framework enabling requirements engineers to instantiate computational trust models from i* dependency networks and organizational contexts. Comprehensive experimental validation across 78,125 parameter configurations establishes robust emergence of negativity bias, hysteresis effects, and cumulative damage amplification. Empirical validation using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60 validation points (81.7%), successfully reproducing documented trust evolution across five distinct relationship phases including crisis and recovery periods. This technical report builds upon its foundational companion work in arXiv:2510.18802.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator</title>
<link>https://arxiv.org/abs/2510.24926</link>
<guid>https://arxiv.org/abs/2510.24926</guid>
<content:encoded><![CDATA[
arXiv:2510.24926v1 Announce Type: cross 
Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator before graph convolution networks (GCNs). The KAN front end applies learnable one-dimensional warps and a linear mixing step, improving feature conditioning and nonlinear encoding without increasing message-passing depth. We employ this architecture to improve the performance of emulators for numerical ice sheet models. Our emulator is trained and tested using 36 melting-rate simulations with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to 5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves inference throughput on coarser meshes by replacing one edge-wise message-passing layer with a node-wise transform; only the finest mesh shows a modest cost. Overall, KAN-first designs offer a favorable accuracy vs. efficiency trade-off for large transient scenario sweeps.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Culture-Sensitive Neurons in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24942</link>
<guid>https://arxiv.org/abs/2510.24942</guid>
<content:encoded><![CDATA[
arXiv:2510.24942v1 Announce Type: cross 
Abstract: Despite their impressive performance, vision-language models (VLMs) still struggle on culturally situated inputs. To understand how VLMs process culturally grounded information, we study the presence of culture-sensitive neurons, i.e. neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts. We examine whether such neurons are important for culturally diverse visual question answering and where they are located. Using the CVQA benchmark, we identify neurons of culture selectivity and perform causal tests by deactivating the neurons flagged by different identification methods. Experiments on three VLMs across 25 cultural groups demonstrate the existence of neurons whose ablation disproportionately harms performance on questions about the corresponding cultures, while having minimal effects on others. Moreover, we propose a new margin-based selector - Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods in identifying culture-sensitive neurons. Finally, our layer-wise analyses reveals that such neurons tend to cluster in certain decoder layers. Overall, our findings shed new light on the internal organization of multimodal representations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.24949</link>
<guid>https://arxiv.org/abs/2510.24949</guid>
<content:encoded><![CDATA[
arXiv:2510.24949v1 Announce Type: cross 
Abstract: Assessing scenario coverage is crucial for evaluating the robustness of autonomous agents, yet existing methods rely on expensive human annotations or computationally intensive Large Vision-Language Models (LVLMs). These approaches are impractical for large-scale deployment due to cost and efficiency constraints. To address these shortcomings, we propose SCOUT (Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate model designed to predict scenario coverage labels directly from an agent's latent sensor representations. SCOUT is trained through a distillation process, learning to approximate LVLM-generated coverage labels while eliminating the need for continuous LVLM inference or human annotation. By leveraging precomputed perception features, SCOUT avoids redundant computations and enables fast, scalable scenario coverage estimation. We evaluate our method across a large dataset of real-life autonomous navigation scenarios, demonstrating that it maintains high accuracy while significantly reducing computational cost. Our results show that SCOUT provides an effective and practical alternative for large-scale coverage analysis. While its performance depends on the quality of LVLM-generated training labels, SCOUT represents a major step toward efficient scenario coverage oversight in autonomous systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequences of Logits Reveal the Low Rank Structure of Language Models</title>
<link>https://arxiv.org/abs/2510.24966</link>
<guid>https://arxiv.org/abs/2510.24966</guid>
<content:encoded><![CDATA[
arXiv:2510.24966v1 Announce Type: cross 
Abstract: A major problem in the study of large language models is to understand their inherent low-dimensional structure. We introduce an approach to study the low-dimensional structure of language models at a model-agnostic level: as sequential probabilistic models. We first empirically demonstrate that a wide range of modern language models exhibit low-rank structure: in particular, matrices built from the model's logits for varying sets of prompts and responses have low approximate rank. We then show that this low-rank structure can be leveraged for generation -- in particular, we can generate a response to a target prompt using a linear combination of the model's outputs on unrelated, or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of language models in the sense discussed above yields a simple universal abstraction whose theoretical predictions parallel our experiments. We then analyze the representation power of the abstraction and give provable learning guarantees.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging</title>
<link>https://arxiv.org/abs/2510.24976</link>
<guid>https://arxiv.org/abs/2510.24976</guid>
<content:encoded><![CDATA[
arXiv:2510.24976v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical image analysis, excelling in tasks such as disease detection, segmentation, and classification. However, their reliance on large, attention-driven models makes them vulnerable to hardware-level attacks. In this paper, we propose a novel threat model referred to as Med-Hammer that combines the Rowhammer hardware fault injection with neural Trojan attacks to compromise the integrity of ViT-based medical imaging systems. Specifically, we demonstrate how malicious bit flips induced via Rowhammer can trigger implanted neural Trojans, leading to targeted misclassification or suppression of critical diagnoses (e.g., tumors or lesions) in medical scans. Through extensive experiments on benchmark medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that such attacks can remain stealthy while achieving high attack success rates about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We further investigate how architectural properties, such as model sparsity, attention weight distribution, and the number of features of the layer, impact attack effectiveness. Our findings highlight a critical and underexplored intersection between hardware-level faults and deep learning security in healthcare applications, underscoring the urgent need for robust defenses spanning both model architectures and underlying hardware platforms.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning</title>
<link>https://arxiv.org/abs/2510.24980</link>
<guid>https://arxiv.org/abs/2510.24980</guid>
<content:encoded><![CDATA[
arXiv:2510.24980v1 Announce Type: cross 
Abstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern. Accurate classification of PU severity (Stages I-IV) is essential for proper treatment but remains challenging due to subtle visual distinctions and subjective interpretation, leading to variability among clinicians. Prior AI-based approaches using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) achieved promising accuracy but offered limited interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal model), a fine-tuned multimodal large language model (MLLM) with an agentic self-reflection mechanism for pressure ulcer severity classification. Inspired by clinician-style diagnostic reassessment, FT-ARM iteratively refines its predictions by reasoning over visual features and encoded clinical knowledge from text, enhancing both accuracy and consistency. On the publicly available Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B, achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline evaluations, FT-ARM is designed and tested for live inference, reflecting real-time deployment conditions. Furthermore, it produces clinically grounded natural-language explanations, improving interpretability and trust. By integrating fine-tuning and reflective reasoning across multimodal inputs, FT-ARM advances the reliability, transparency, and clinical applicability of automated wound assessment systems, addressing the critical need for consistent and explainable PU staging to support improved patient care.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies</title>
<link>https://arxiv.org/abs/2510.24983</link>
<guid>https://arxiv.org/abs/2510.24983</guid>
<content:encoded><![CDATA[
arXiv:2510.24983v1 Announce Type: cross 
Abstract: Diffusion policies are competitive for offline reinforcement learning (RL) but are typically guided at sampling time by heuristics that lack a statistical notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that treats each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Concretely, we accumulate a log-likelihood ratio and gate the conditional mean with a logistic controller whose threshold tau is calibrated once under H0 to meet a user-specified Type-I level alpha. This turns guidance from a fixed push into an evidence-driven adjustment with a user-interpretable risk budget. Importantly, we deliberately leave training vanilla (two heads with standard epsilon-prediction) under the structure of DDPM. LRT guidance composes naturally with Q-gradients: critic-gradient updates can be taken at the unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum from exploitation to conservatism. We standardize states and actions consistently at train and test time and report a state-conditional out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks, LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines in our implementation while honoring the desired alpha. Theoretically, we establish level-alpha calibration, concise stability bounds, and a return comparison showing when LRT surpasses Q-guidance-especially when off-support errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models</title>
<link>https://arxiv.org/abs/2510.24985</link>
<guid>https://arxiv.org/abs/2510.24985</guid>
<content:encoded><![CDATA[
arXiv:2510.24985v1 Announce Type: cross 
Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating critical parameters through dynamic rewiring of linear layers. However, the application of FaR introduces non-negligible performance and memory overheads, primarily due to the runtime modification of activation pathways and the lack of hardware-level optimization. To overcome these limitations, we propose FaRAccel, a novel hardware accelerator architecture implemented on FPGA, specifically designed to offload and optimize FaR operations. FaRAccel integrates reconfigurable logic for dynamic activation rerouting, and lightweight storage of rewiring configurations, enabling low-latency inference with minimal energy overhead. We evaluate FaRAccel across a suite of Transformer models and demonstrate substantial reductions in FaR inference latency and improvement in energy efficiency, while maintaining the robustness gains of the original FaR methodology. To the best of our knowledge, this is the first hardware-accelerated defense against BFAs in Transformers, effectively bridging the gap between algorithmic resilience and efficient deployment on real-world AI platforms.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation</title>
<link>https://arxiv.org/abs/2510.24986</link>
<guid>https://arxiv.org/abs/2510.24986</guid>
<content:encoded><![CDATA[
arXiv:2510.24986v1 Announce Type: cross 
Abstract: In recent years, machine learning has become an increasingly powerful tool for supporting seizure detection and monitoring in epilepsy care. Traditional approaches focus on identifying seizures only after they begin, which limits the opportunity for early intervention and proactive treatment. In this study, we propose a novel approach that integrates both real-time seizure detection and prediction, aiming to capture subtle temporal patterns in EEG data that may indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT Scalp EEG Database, which includes 969 hours of recordings and 173 seizures collected from 23 pediatric and young adult patients with drug-resistant epilepsy. To support seizure detection, we implemented a range of supervised machine learning algorithms, including K-Nearest Neighbors, Logistic Regression, Random Forest, and Support Vector Machine. The Logistic Regression achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced performance suitable for clinical screening. Random Forest and Support Vector Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to detect any seizures, illustrating that accuracy alone is insufficient for evaluating medical ML models with class imbalance. For seizure prediction, we employed Long Short-Term Memory (LSTM) networks, which use deep learning to model temporal dependencies in EEG data. The LSTM model achieved 89.26% prediction accuracy. These results highlight the potential of developing accessible, real-time monitoring tools that not only detect seizures as traditionally done, but also predict them before they occur. This ability to predict seizures marks a significant shift from reactive seizure management to a more proactive approach, allowing patients to anticipate seizures and take precautionary measures to reduce the risk of injury or other complications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers</title>
<link>https://arxiv.org/abs/2510.25013</link>
<guid>https://arxiv.org/abs/2510.25013</guid>
<content:encoded><![CDATA[
arXiv:2510.25013v1 Announce Type: cross 
Abstract: Mechanistic interpretability aims to reverse-engineer large language models (LLMs) into human-understandable computational circuits. However, the complexity of pretrained models often obscures the minimal mechanisms required for specific reasoning tasks. In this work, we train small, attention-only transformers from scratch on a symbolic version of the Indirect Object Identification (IOI) task -- a benchmark for studying coreference -- like reasoning in transformers. Surprisingly, a single-layer model with only two attention heads achieves perfect IOI accuracy, despite lacking MLPs and normalization layers. Through residual stream decomposition, spectral analysis, and embedding interventions, we find that the two heads specialize into additive and contrastive subcircuits that jointly implement IOI resolution. Furthermore, we show that a two-layer, one-head model achieves similar performance by composing information across layers through query-value interactions. These results demonstrate that task-specific training induces highly interpretable, minimal circuits, offering a controlled testbed for probing the computational foundations of transformer reasoning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study</title>
<link>https://arxiv.org/abs/2510.25016</link>
<guid>https://arxiv.org/abs/2510.25016</guid>
<content:encoded><![CDATA[
arXiv:2510.25016v1 Announce Type: cross 
Abstract: The future of Requirements Engineering (RE) is increasingly driven by artificial intelligence (AI), reshaping how we elicit, analyze, and validate requirements. Traditional RE is based on labor-intensive manual processes prone to errors and complexity. AI-powered approaches, specifically large language models (LLMs), natural language processing (NLP), and generative AI, offer transformative solutions and reduce inefficiencies. However, the use of AI in RE also brings challenges like algorithmic bias, lack of explainability, and ethical concerns related to automation. To address these issues, this study introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that integrates AI-driven analysis with human oversight to improve requirements elicitation, analysis, and validation. The model emphasizes ethical AI use through transparency, explainability, and bias mitigation. We outline a multi-phase research methodology focused on preparing RE datasets, fine-tuning AI models, and designing collaborative human-AI workflows. This preliminary study presents the conceptual framework and early-stage prototype implementation, establishing a research agenda and practical design direction for applying intelligent data science techniques to semi-structured and unstructured RE data in collaborative environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems</title>
<link>https://arxiv.org/abs/2510.25017</link>
<guid>https://arxiv.org/abs/2510.25017</guid>
<content:encoded><![CDATA[
arXiv:2510.25017v1 Announce Type: cross 
Abstract: Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8</title>
<link>https://arxiv.org/abs/2510.25032</link>
<guid>https://arxiv.org/abs/2510.25032</guid>
<content:encoded><![CDATA[
arXiv:2510.25032v1 Announce Type: cross 
Abstract: Developing a highly accurate automatic license plate recognition system (ALPR) is challenging due to environmental factors such as lighting, rain, and dust. Additional difficulties include high vehicle speeds, varying camera angles, and low-quality or low-resolution images. ALPR is vital in traffic control, parking, vehicle tracking, toll collection, and law enforcement applications. This paper proposes a deep learning strategy using YOLOv8 for license plate detection and recognition tasks. This method seeks to enhance the performance of the model using datasets from Ontario, Quebec, California, and New York State. It achieved an impressive recall rate of 94% on the dataset from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and 91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised learning framework, combining a small set of manually labeled data with pseudo-labels generated by Grounding DINO to train our detection model. Grounding DINO, a powerful vision-language model, automatically annotates many images with bounding boxes for license plates, thereby minimizing the reliance on labor-intensive manual labeling. By integrating human-verified and model-generated annotations, we can scale our dataset efficiently while maintaining label quality, which significantly enhances the training process and overall model performance. Furthermore, it reports character error rates for both datasets, providing additional insight into system performance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable predictive processing framework for multitask caregiving robots</title>
<link>https://arxiv.org/abs/2510.25053</link>
<guid>https://arxiv.org/abs/2510.25053</guid>
<content:encoded><![CDATA[
arXiv:2510.25053v1 Announce Type: cross 
Abstract: The rapid aging of societies is intensifying demand for autonomous care robots; however, most existing systems are task-specific and rely on handcrafted preprocessing, limiting their ability to generalize across diverse scenarios. A prevailing theory in cognitive neuroscience proposes that the human brain operates through hierarchical predictive processing, which underlies flexible cognition and behavior by integrating multimodal sensory signals. Inspired by this principle, we introduce a hierarchical multimodal recurrent neural network grounded in predictive processing under the free-energy principle, capable of directly integrating over 30,000-dimensional visuo-proprioceptive inputs without dimensionality reduction. The model was able to learn two representative caregiving tasks, rigid-body repositioning and flexible-towel wiping, without task-specific feature engineering. We demonstrate three key properties: (i) self-organization of hierarchical latent dynamics that regulate task transitions, capture variability in uncertainty, and infer occluded states; (ii) robustness to degraded vision through visuo-proprioceptive integration; and (iii) asymmetric interference in multitask learning, where the more variable wiping task had little influence on repositioning, whereas learning the repositioning task led to a modest reduction in wiping performance, while the model maintained overall robustness. Although the evaluation was limited to simulation, these results establish predictive processing as a universal and scalable computational principle, pointing toward robust, flexible, and autonomous caregiving robots while offering theoretical insight into the human brain's ability to achieve flexible adaptation in uncertain real-world environments.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.25055</link>
<guid>https://arxiv.org/abs/2510.25055</guid>
<content:encoded><![CDATA[
arXiv:2510.25055v1 Announce Type: cross 
Abstract: Scientific progress is driven by the deliberate articulation of what remains unknown. This study investigates the ability of large language models (LLMs) to identify research knowledge gaps in the biomedical literature. We define two categories of knowledge gaps: explicit gaps, clear declarations of missing knowledge; and implicit gaps, context-inferred missing knowledge. While prior work has focused mainly on explicit gap detection, we extend this line of research by addressing the novel task of inferring implicit gaps. We conducted two experiments on almost 1500 documents across four datasets, including a manually annotated corpus of biomedical articles. We benchmarked both closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2) under paragraph-level and full-paper settings. To address the reasoning of implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive Bucketed Inference scheme that structures reasoning and buckets inferred conclusion candidates for validation. Our results highlight the robust capability of LLMs in identifying both explicit and implicit knowledge gaps. This is true for both open- and closed-weight models, with larger variants often performing better. This suggests a strong ability of LLMs for systematically identifying candidate knowledge gaps, which can support early-stage research formulation, policymakers, and funding decisions. We also report observed failure modes and outline directions for robust deployment, including domain adaptation, human-in-the-loop verification, and benchmarking across open- and closed-weight models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games</title>
<link>https://arxiv.org/abs/2510.25080</link>
<guid>https://arxiv.org/abs/2510.25080</guid>
<content:encoded><![CDATA[
arXiv:2510.25080v1 Announce Type: cross 
Abstract: Card games are widely used to study sequential decision-making under uncertainty, with real-world analogues in negotiation, finance, and cybersecurity. Typically, these games fall into three categories based on the flow of control: strictly-sequential (where players alternate single actions), deterministic-response (where some actions trigger a fixed outcome), and unbounded reciprocal-response (where alternating counterplays are permitted). A less-explored but strategically rich structure exists: the bounded one-sided response. This dynamic occurs when a player's action briefly transfers control to the opponent, who must satisfy a fixed condition through one or more sequential moves before the turn resolves. We term games featuring this mechanism Bounded One-Sided Response Games (BORGs).
  We introduce a modified version of Monopoly Deal as a benchmark environment that specifically isolates the BORG dynamic, where a Rent action forces the opponent to sequentially choose payment assets. We demonstrate that the gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully converges on effective strategies for this domain without requiring novel algorithmic extensions. To support efficient, reproducible experimentation, we present a lightweight, full-stack research platform that unifies the environment, a parallelized CFR runtime, and a human-playable web interface, all runnable on a single workstation. This system provides a practical foundation for exploring state representation and policy learning in bounded one-sided response settings.
  The trained CFR agent and source code are available at https://monopolydeal.ai.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fair Graph Representations with Multi-view Information Bottleneck</title>
<link>https://arxiv.org/abs/2510.25096</link>
<guid>https://arxiv.org/abs/2510.25096</guid>
<content:encoded><![CDATA[
arXiv:2510.25096v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) excel on relational data by passing messages over node features and structure, but they can amplify training data biases, propagating discriminatory attributes and structural imbalances into unfair outcomes. Many fairness methods treat bias as a single source, ignoring distinct attribute and structure effects and leading to suboptimal fairness and utility trade-offs. To overcome this challenge, we propose FairMIB, a multi-view information bottleneck framework designed to decompose graphs into feature, structural, and diffusion views for mitigating complexity biases in GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize cross-view mutual information for bias-free representation learning. It further integrates multi-perspective conditional information bottleneck objectives to balance task utility and fairness by minimizing mutual information with sensitive attributes. Additionally, FairMIB introduces an inverse probability-weighted (IPW) adjacency correction in the diffusion view, which reduces the spread of bias propagation during message passing. Experiments on five real-world benchmark datasets demonstrate that FairMIB achieves state-of-the-art performance across both utility and fairness metrics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Neural Differential Manifold: An Architecture with Explicit Geometric Structure</title>
<link>https://arxiv.org/abs/2510.25113</link>
<guid>https://arxiv.org/abs/2510.25113</guid>
<content:encoded><![CDATA[
arXiv:2510.25113v1 Announce Type: cross 
Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural network architecture that explicitly incorporates geometric structure into its fundamental design. Departing from conventional Euclidean parameter spaces, the NDM re-conceptualizes a neural network as a differentiable manifold where each layer functions as a local coordinate chart, and the network parameters directly parameterize a Riemannian metric tensor at every point. The architecture is organized into three synergistic layers: a Coordinate Layer implementing smooth chart transitions via invertible transformations inspired by normalizing flows, a Geometric Layer that dynamically generates the manifold's metric through auxiliary sub-networks, and an Evolution Layer that optimizes both task performance and geometric simplicity through a dual-objective loss function. This geometric regularization penalizes excessive curvature and volume distortion, providing intrinsic regularization that enhances generalization and robustness. The framework enables natural gradient descent optimization aligned with the learned manifold geometry and offers unprecedented interpretability by endowing internal representations with clear geometric meaning. We analyze the theoretical advantages of this approach, including its potential for more efficient optimization, enhanced continual learning, and applications in scientific discovery and controllable generative modeling. While significant computational challenges remain, the Neural Differential Manifold represents a fundamental shift towards geometrically structured, interpretable, and efficient deep learning systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data</title>
<link>https://arxiv.org/abs/2510.25123</link>
<guid>https://arxiv.org/abs/2510.25123</guid>
<content:encoded><![CDATA[
arXiv:2510.25123v1 Announce Type: cross 
Abstract: We present a data-driven dimensionality reduction method that is well-suited for physics-based data representing hyperbolic wave propagation. The method utilizes a specialized neural network architecture called low rank neural representation (LRNR) inside a hypernetwork framework. The architecture is motivated by theoretical results that rigorously prove the existence of efficient representations for this wave class. We illustrate through archetypal examples that such an efficient low-dimensional representation of propagating waves can be learned directly from data through a combination of deep learning techniques. We observe that a low rank tensor representation arises naturally in the trained LRNRs, and that this reveals a new decomposition of wave propagation where each decomposed mode corresponds to interpretable physical features. Furthermore, we demonstrate that the LRNR architecture enables efficient inference via a compression scheme, which is a potentially important feature when deploying LRNRs in demanding performance regimes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Divide: End-to-End Sequence-Graph Learning</title>
<link>https://arxiv.org/abs/2510.25126</link>
<guid>https://arxiv.org/abs/2510.25126</guid>
<content:encoded><![CDATA[
arXiv:2510.25126v1 Announce Type: cross 
Abstract: Many real-world datasets are both sequential and relational: each node carries an event sequence while edges encode interactions. Existing methods in sequence modeling and graph modeling often neglect one modality or the other. We argue that sequences and graphs are not separate problems but complementary facets of the same dataset, and should be learned jointly. We introduce BRIDGE, a unified end-to-end architecture that couples a sequence encoder with a GNN under a single objective, allowing gradients to flow across both modules and learning task-aligned representations. To enable fine-grained token-level message passing among neighbors, we add TOKENXATTN, a token-level cross-attention layer that passes messages between events in neighboring sequences. Across two settings, friendship prediction (Brightkite) and fraud detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph methods, and sequence-only baselines on ranking and classification metrics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lipschitz-aware Linearity Grafting for Certified Robustness</title>
<link>https://arxiv.org/abs/2510.25130</link>
<guid>https://arxiv.org/abs/2510.25130</guid>
<content:encoded><![CDATA[
arXiv:2510.25130v1 Announce Type: cross 
Abstract: Lipschitz constant is a fundamental property in certified robustness, as smaller values imply robustness to adversarial examples when a model is confident in its prediction. However, identifying the worst-case adversarial examples is known to be an NP-complete problem. Although over-approximation methods have shown success in neural network verification to address this challenge, reducing approximation errors remains a significant obstacle. Furthermore, these approximation errors hinder the ability to obtain tight local Lipschitz constants, which are crucial for certified robustness. Originally, grafting linearity into non-linear activation functions was proposed to reduce the number of unstable neurons, enabling scalable and complete verification. However, no prior theoretical analysis has explained how linearity grafting improves certified robustness. We instead consider linearity grafting primarily as a means of eliminating approximation errors rather than reducing the number of unstable neurons, since linear functions do not require relaxation. In this paper, we provide two theoretical contributions: 1) why linearity grafting improves certified robustness through the lens of the $l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear activation functions, the dominant source of approximation errors, yields a tighter local Lipschitz constant. Based on these theoretical contributions, we propose a Lipschitz-aware linearity grafting method that removes dominant approximation errors, which are crucial for tightening the local Lipschitz constant, thereby improving certified robustness, even without certified training. Our extensive experiments demonstrate that grafting linearity into these influential activations tightens the $l_\infty$ local Lipschitz constant and enhances certified robustness.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Document Protocol for AI Search</title>
<link>https://arxiv.org/abs/2510.25160</link>
<guid>https://arxiv.org/abs/2510.25160</guid>
<content:encoded><![CDATA[
arXiv:2510.25160v1 Announce Type: cross 
Abstract: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning</title>
<link>https://arxiv.org/abs/2510.25164</link>
<guid>https://arxiv.org/abs/2510.25164</guid>
<content:encoded><![CDATA[
arXiv:2510.25164v1 Announce Type: cross 
Abstract: We present a transformer-based multimodal framework for generating clinically relevant captions for MRI scans. Our system combines a DEiT-Small vision transformer as an image encoder, MediCareBERT for caption embedding, and a custom LSTM-based decoder. The architecture is designed to semantically align image and textual embeddings, using hybrid cosine-MSE loss and contrastive inference via vector similarity. We benchmark our method on the MultiCaRe dataset, comparing performance on filtered brain-only MRIs versus general MRI images against state-of-the-art medical image captioning methods including BLIP, R2GenGPT, and recent transformer-based approaches. Results show that focusing on domain-specific data improves caption accuracy and semantic alignment. Our work proposes a scalable, interpretable solution for automated medical image reporting.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution</title>
<link>https://arxiv.org/abs/2510.25178</link>
<guid>https://arxiv.org/abs/2510.25178</guid>
<content:encoded><![CDATA[
arXiv:2510.25178v1 Announce Type: cross 
Abstract: Intra-sentence multilingual speech synthesis (code-switching TTS) remains a major challenge due to abrupt language shifts, varied scripts, and mismatched prosody between languages. Conventional TTS systems are typically monolingual and fail to produce natural, intelligible speech in mixed-language contexts. We introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution (SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched speech generation. SFMS-ALR segments input text by Unicode script, applies adaptive language identification to determine each segment's language and locale, and normalizes prosody using sentiment-aware adjustments to preserve expressive continuity across languages. The algorithm generates a unified SSML representation with appropriate "lang" or "voice" spans and synthesizes the utterance in a single TTS request. Unlike end-to-end multilingual models, SFMS-ALR requires no retraining and integrates seamlessly with existing voices from Google, Apple, Amazon, and other providers. Comparative analysis with data-driven pipelines such as Unicom and Mask LID demonstrates SFMS-ALR's flexibility, interpretability, and immediate deployability. The framework establishes a modular baseline for high-quality, engine-independent multilingual TTS and outlines evaluation strategies for intelligibility, naturalness, and user preference.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder</title>
<link>https://arxiv.org/abs/2510.25181</link>
<guid>https://arxiv.org/abs/2510.25181</guid>
<content:encoded><![CDATA[
arXiv:2510.25181v1 Announce Type: cross 
Abstract: This paper addresses the critical challenges of communication overhead, data heterogeneity, and privacy in deep learning for channel state information (CSI) feedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel federated learning framework that incorporates personalized encoders and a LoRA-adapted shared decoder. Specifically, personalized encoders are trained locally on each user equipment (UE) to capture device-specific channel characteristics, while a shared decoder is updated globally via the coordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This design ensures that only compact LoRA adapter parameters instead of full model updates are transmitted for aggregation. To further enhance convergence stability, we introduce an alternating freezing strategy with calibrated learning-rate ratio during LoRA aggregation. Extensive simulations on 3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\% of the uplink communication cost compared to conventional methods while achieving a performance gain of 1.2 dB in CSI feedback accuracy under heterogeneous conditions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Resilience in the AI Era -- What Machines Can't Replace</title>
<link>https://arxiv.org/abs/2510.25218</link>
<guid>https://arxiv.org/abs/2510.25218</guid>
<content:encoded><![CDATA[
arXiv:2510.25218v1 Announce Type: cross 
Abstract: AI is displacing tasks, mediating high-stakes decisions, and flooding communication with synthetic content, unsettling work, identity, and social trust. We argue that the decisive human countermeasure is resilience. We define resilience across three layers: psychological, including emotion regulation, meaning-making, cognitive flexibility; social, including trust, social capital, coordinated response; organizational, including psychological safety, feedback mechanisms, and graceful degradation. We synthesize early evidence that these capacities buffer individual strain, reduce burnout through social support, and lower silent failure in AI-mediated workflows through team norms and risk-responsive governance. We also show that resilience can be cultivated through training that complements rather than substitutes for structural safeguards. By reframing the AI debate around actionable human resilience, this article offers policymakers, educators, and operators a practical lens to preserve human agency and steer responsible adoption.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction</title>
<link>https://arxiv.org/abs/2510.25220</link>
<guid>https://arxiv.org/abs/2510.25220</guid>
<content:encoded><![CDATA[
arXiv:2510.25220v1 Announce Type: cross 
Abstract: In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items. A key challenge lies in exploring optimal sequences within the combinatorial space of permutations. Recent research follows a two-stage (generator-evaluator) paradigm, where a generator produces multiple feasible sequences, and an evaluator selects the best one. In practice, the generator is typically implemented as an autoregressive model. However, these two-stage methods face two main challenges. First, the separation of the generator and evaluator hinders end-to-end training. Second, autoregressive generators suffer from inference efficiency. In this work, we propose a Unified Generative Efficient Reranking Framework (GReF) to address the two primary challenges. Specifically, we introduce Gen-Reranker, an autoregressive generator featuring a bidirectional encoder and a dynamic autoregressive decoder to generate causal reranking sequences. Subsequently, we pre-train Gen-Reranker on the item exposure order for high-quality parameter initialization. To eliminate the need for the evaluator while integrating sequence-level evaluation during training for end-to-end optimization, we propose post-training the model through Rerank-DPO. Moreover, for efficient autoregressive inference, we introduce ordered multi-token prediction (OMTP), which trains Gen-Reranker to simultaneously generate multiple future items while preserving their order, ensuring practical deployment in real-time recommender systems. Extensive offline experiments demonstrate that GReF outperforms state-of-the-art reranking methods while achieving latency that is nearly comparable to non-autoregressive models. Additionally, GReF has also been deployed in a real-world video app Kuaishou with over 300 million daily active users, significantly improving online recommendation quality.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning</title>
<link>https://arxiv.org/abs/2510.25226</link>
<guid>https://arxiv.org/abs/2510.25226</guid>
<content:encoded><![CDATA[
arXiv:2510.25226v1 Announce Type: cross 
Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive and unlabeled data are available, while negatives are missing or left unlabeled. This situation is common in real applications where annotating reliable negatives is difficult or costly. Despite substantial progress in PU learning, the multi-class case (MPU) remains challenging: many existing approaches do not ensure \emph{unbiased risk estimation}, which limits performance and stability. We propose a cost-sensitive multi-class PU method based on \emph{adaptive loss weighting}. Within the empirical risk minimization framework, we assign distinct, data-dependent weights to the positive and \emph{inferred-negative} (from the unlabeled mixture) loss components so that the resulting empirical objective is an unbiased estimator of the target risk. We formalize the MPU data-generating process and establish a generalization error bound for the proposed estimator. Extensive experiments on \textbf{eight} public datasets, spanning varying class priors and numbers of classes, show consistent gains over strong baselines in both accuracy and stability.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studies for : A Human-AI Co-Creative Sound Artwork Using a Real-time Multi-channel Sound Generation Model</title>
<link>https://arxiv.org/abs/2510.25228</link>
<guid>https://arxiv.org/abs/2510.25228</guid>
<content:encoded><![CDATA[
arXiv:2510.25228v1 Announce Type: cross 
Abstract: This paper explores the integration of AI technologies into the artistic workflow through the creation of Studies for, a generative sound installation developed in collaboration with sound artist Evala (https://www.ntticc.or.jp/en/archive/works/studies-for/). The installation employs SpecMaskGIT, a lightweight yet high-quality sound generation AI model, to generate and playback eight-channel sound in real-time, creating an immersive auditory experience over the course of a three-month exhibition. The work is grounded in the concept of a "new form of archive," which aims to preserve the artistic style of an artist while expanding beyond artists' past artworks by continued generation of new sound elements. This speculative approach to archival preservation is facilitated by training the AI model on a dataset consisting of over 200 hours of Evala's past sound artworks.
  By addressing key requirements in the co-creation of art using AI, this study highlights the value of the following aspects: (1) the necessity of integrating artist feedback, (2) datasets derived from an artist's past works, and (3) ensuring the inclusion of unexpected, novel outputs. In Studies for, the model was designed to reflect the artist's artistic identity while generating new, previously unheard sounds, making it a fitting realization of the concept of "a new form of archive." We propose a Human-AI co-creation framework for effectively incorporating sound generation AI models into the sound art creation process and suggest new possibilities for creating and archiving sound art that extend an artist's work beyond their physical existence. Demo page: https://sony.github.io/studies-for/
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation</title>
<link>https://arxiv.org/abs/2510.25234</link>
<guid>https://arxiv.org/abs/2510.25234</guid>
<content:encoded><![CDATA[
arXiv:2510.25234v1 Announce Type: cross 
Abstract: Expressions are fundamental to conveying human emotions. With the rapid advancement of AI-generated content (AIGC), realistic and expressive 3D facial animation has become increasingly crucial. Despite recent progress in speech-driven lip-sync for talking-face animation, generating emotionally expressive talking faces remains underexplored. A major obstacle is the scarcity of real emotional 3D talking-face datasets due to the high cost of data capture. To address this, we model facial animation driven by both speech and emotion as a linear additive problem. Leveraging a 3D talking-face dataset with neutral expressions (VOCAset) and a dataset of 3D expression sequences (Florence4D), we jointly learn a set of blendshapes driven by speech and emotion. We introduce a sparsity constraint loss to encourage disentanglement between the two types of blendshapes while allowing the model to capture inherent secondary cross-domain deformations present in the training data. The learned blendshapes can be further mapped to the expression and jaw pose parameters of the FLAME model, enabling the animation of 3D Gaussian avatars. Qualitative and quantitative experiments demonstrate that our method naturally generates talking faces with specified expressions while maintaining accurate lip synchronization. Perceptual studies further show that our approach achieves superior emotional expressivity compared to existing methods, without compromising lip-sync quality.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-shot Humanoid Whole-body Motion Learning</title>
<link>https://arxiv.org/abs/2510.25241</link>
<guid>https://arxiv.org/abs/2510.25241</guid>
<content:encoded><![CDATA[
arXiv:2510.25241v1 Announce Type: cross 
Abstract: Whole-body humanoid motion represents a cornerstone challenge in robotics, integrating balance, coordination, and adaptability to enable human-like behaviors. However, existing methods typically require multiple training samples per motion category, rendering the collection of high-quality human motion datasets both labor-intensive and costly. To address this, we propose a novel approach that trains effective humanoid motion policies using only a single non-walking target motion sample alongside readily available walking motions. The core idea lies in leveraging order-preserving optimal transport to compute distances between walking and non-walking sequences, followed by interpolation along geodesics to generate new intermediate pose skeletons, which are then optimized for collision-free configurations and retargeted to the humanoid before integration into a simulated environment for policy training via reinforcement learning. Experimental evaluations on the CMU MoCap dataset demonstrate that our method consistently outperforms baselines, achieving superior performance across metrics. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Bayesian DAG Sampling</title>
<link>https://arxiv.org/abs/2510.25254</link>
<guid>https://arxiv.org/abs/2510.25254</guid>
<content:encoded><![CDATA[
arXiv:2510.25254v1 Announce Type: cross 
Abstract: Bayesian inference of Bayesian network structures is often performed by sampling directed acyclic graphs along an appropriately constructed Markov chain. We present two techniques to improve sampling. First, we give an efficient implementation of basic moves, which add, delete, or reverse a single arc. Second, we expedite summing over parent sets, an expensive task required for more sophisticated moves: we devise a preprocessing method to prune possible parent sets so as to approximately preserve the sums. Our empirical study shows that our techniques can yield substantial efficiency gains compared to previous methods.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2510.25259</link>
<guid>https://arxiv.org/abs/2510.25259</guid>
<content:encoded><![CDATA[
arXiv:2510.25259v1 Announce Type: cross 
Abstract: Recently, convolutional filters have been increasingly adopted in sequential recommendation for their ability to capture local sequential patterns. However, most of these models complement convolutional filters with self-attention. This is because convolutional filters alone, generally fixed filters, struggle to capture global interactions necessary for accurate recommendation. We propose Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a model inspired by graph signal processing, where time-variant graph filters capture position-dependent temporal variations in user sequences. By replacing both fixed kernels and self-attention with time-variant filters, TV-Rec achieves higher expressive power and better captures complex interaction patterns in user behavior. This design not only eliminates the need for self-attention but also reduces computation while accelerating inference. Extensive experiments on six public benchmarks show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49%.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning</title>
<link>https://arxiv.org/abs/2510.25262</link>
<guid>https://arxiv.org/abs/2510.25262</guid>
<content:encoded><![CDATA[
arXiv:2510.25262v1 Announce Type: cross 
Abstract: Normalization is fundamental to deep learning, but existing approaches such as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero mean and unit variance, stabilizing training without controlling how representations capture task-relevant information. We propose IB-Inspired Normalization (IBNorm), a simple yet powerful family of methods grounded in the Information Bottleneck principle. IBNorm introduces bounded compression operations that encourage embeddings to preserve predictive information while suppressing nuisance variability, yielding more informative representations while retaining the stability and compatibility of standard normalization. Theoretically, we prove that IBNorm achieves a higher IB value and tighter generalization bounds than variance-centric methods. Empirically, IBNorm consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual information analysis confirming superior information bottleneck behavior. Code will be released publicly.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</title>
<link>https://arxiv.org/abs/2510.25268</link>
<guid>https://arxiv.org/abs/2510.25268</guid>
<content:encoded><![CDATA[
arXiv:2510.25268v1 Announce Type: cross 
Abstract: Generating hand grasps with language instructions is a widely studied topic that benefits from embodied AI and VR/AR applications. While transferring into hand articulatied object interaction (HAOI), the hand grasps synthesis requires not only object functionality but also long-term manipulation sequence along the object deformation. This paper proposes a novel HAOI sequence generation framework SynHLMA, to synthesize hand language manipulation for articulated objects. Given a complete point cloud of an articulated object, we utilize a discrete HAOI representation to model each hand object interaction frame. Along with the natural language embeddings, the representations are trained by an HAOI manipulation language model to align the grasping process with its language description in a shared representation space. A joint-aware loss is employed to ensure hand grasps follow the dynamic variations of articulated object joints. In this way, our SynHLMA achieves three typical hand manipulation tasks for articulated objects of HAOI generation, HAOI prediction and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and experimental results demonstrate the superior hand grasp sequence generation performance comparing with state-of-the-art. We also show a robotics grasp application that enables dexterous grasps execution from imitation learning using the manipulation sequence provided by our SynHLMA. Our codes and datasets will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.25311</link>
<guid>https://arxiv.org/abs/2510.25311</guid>
<content:encoded><![CDATA[
arXiv:2510.25311v1 Announce Type: cross 
Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy that maximizes expected return. As a result, the learned policy can exploit one or few reward sources. However, in many natural situations, it is desirable to learn a policy that induces a dispersed marginal state distribution over rewarding states, while maximizing the expected return which is typically tied to reaching a goal state. This aspect remains relatively unexplored. Existing techniques based on entropy regularization and intrinsic rewards use stochasticity for encouraging exploration to find an optimal policy which may not necessarily lead to dispersed marginal state distribution over rewarding states. Other RL algorithms which match a target distribution assume the latter to be available apriori. This may be infeasible in large scale systems where enumeration of all states is not possible and a state is determined to be a goal state only upon reaching it. We formalize the problem of maximizing the expected return while uniformly visiting the goal states as Multi Goal RL in which an oracle classifier over the state space determines the goal states. We propose a novel algorithm that learns a high-return policy mixture with marginal state distribution dispersed over the set of goal states. Our algorithm is based on optimizing a custom RL reward which is computed - based on the current policy mixture - at each iteration for a set of sampled trajectories. The latter are used via an offline RL algorithm to update the policy mixture. We prove performance guarantees for our algorithm, showing efficient convergence bounds for optimizing a natural objective which captures the expected return as well as the dispersion of the marginal state distribution over the goal states. We design and perform experiments on synthetic MDPs and standard RL environments to evaluate the effectiveness of our algorithm.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4-Doodle: Text to 3D Sketches that Move!</title>
<link>https://arxiv.org/abs/2510.25319</link>
<guid>https://arxiv.org/abs/2510.25319</guid>
<content:encoded><![CDATA[
arXiv:2510.25319v1 Announce Type: cross 
Abstract: We present a novel task: text-to-3D sketch animation, which aims to bring freeform sketches to life in dynamic 3D space. Unlike prior works focused on photorealistic content generation, we target sparse, stylized, and view-consistent 3D vector sketches, a lightweight and interpretable medium well-suited for visual communication and prototyping. However, this task is very challenging: (i) no paired dataset exists for text and 3D (or 4D) sketches; (ii) sketches require structural abstraction that is difficult to model with conventional 3D representations like NeRFs or point clouds; and (iii) animating such sketches demands temporal coherence and multi-view consistency, which current pipelines do not address. Therefore, we propose 4-Doodle, the first training-free framework for generating dynamic 3D sketches from text. It leverages pretrained image and video diffusion models through a dual-space distillation scheme: one space captures multi-view-consistent geometry using differentiable B\'ezier curves, while the other encodes motion dynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion), which optimizes from a single view per step, our multi-view optimization ensures structural alignment and avoids view ambiguity, critical for sparse sketches. Furthermore, we introduce a structure-aware motion module that separates shape-preserving trajectories from deformation-aware changes, enabling expressive motion such as flipping, rotation, and articulated movement. Extensive experiments show that our method produces temporally realistic and structurally stable 3D sketch animations, outperforming existing baselines in both fidelity and controllability. We hope this work serves as a step toward more intuitive and accessible 4D content creation.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding</title>
<link>https://arxiv.org/abs/2510.25327</link>
<guid>https://arxiv.org/abs/2510.25327</guid>
<content:encoded><![CDATA[
arXiv:2510.25327v1 Announce Type: cross 
Abstract: Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2510.25340</link>
<guid>https://arxiv.org/abs/2510.25340</guid>
<content:encoded><![CDATA[
arXiv:2510.25340v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARl) has achieved strong results in cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc teamwork (AHT) relaxes this by allowing collaboration with unknown partners, yet existing variants still presume shared conventions. We introduce Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate with multiple mutually unfamiliar groups of uncontrolled teammates. To address this, we propose MARs, which builds a sparse skeleton graph and applies relational modeling to capture cross-group dvnamics. Experiments on MPE and starCralt ll show that MARs outperforms MARL and AHT baselines while converging faster.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2510.25366</link>
<guid>https://arxiv.org/abs/2510.25366</guid>
<content:encoded><![CDATA[
arXiv:2510.25366v1 Announce Type: cross 
Abstract: The key task of machine learning is to minimize the loss function that measures the model fit to the training data. The numerical methods to do this efficiently depend on the properties of the loss function. The most decisive among these properties is the convexity or non-convexity of the loss function. The fact that the loss function can have, and frequently has, non-convex regions has led to a widespread commitment to non-convex methods such as Adam. However, a local minimum implies that, in some environment around it, the function is convex. In this environment, second-order minimizing methods such as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We propose a novel framework grounded in the hypothesis that loss functions in real-world tasks swap from initial non-convexity to convexity towards the optimum. This is a property we leverage to design an innovative two-phase optimization algorithm. The presented algorithm detects the swap point by observing the gradient norm dependence on the loss. In these regions, non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing experiments confirm the hypothesis that this simple convexity structure is frequent enough to be practically exploited to substantially improve convergence and accuracy.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Biology is the Challenge Physics-Informed ML Needs to Evolve</title>
<link>https://arxiv.org/abs/2510.25368</link>
<guid>https://arxiv.org/abs/2510.25368</guid>
<content:encoded><![CDATA[
arXiv:2510.25368v1 Announce Type: cross 
Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated mechanistic understanding into machine learning, particularly in domains governed by well-known physical laws. This success has motivated efforts to apply PIML to biology, a field rich in dynamical systems but shaped by different constraints. Biological modeling, however, presents unique challenges: multi-faceted and uncertain prior knowledge, heterogeneous and noisy data, partial observability, and complex, high-dimensional networks. In this position paper, we argue that these challenges should not be seen as obstacles to PIML, but as catalysts for its evolution. We propose Biology-Informed Machine Learning (BIML): a principled extension of PIML that retains its structural grounding while adapting to the practical realities of biology. Rather than replacing PIML, BIML retools its methods to operate under softer, probabilistic forms of prior knowledge. We outline four foundational pillars as a roadmap for this transition: uncertainty quantification, contextualization, constrained latent structure inference, and scalability. Foundation Models and Large Language Models will be key enablers, bridging human expertise with computational modeling. We conclude with concrete recommendations to build the BIML ecosystem and channel PIML-inspired innovation toward challenges of high scientific and societal relevance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy</title>
<link>https://arxiv.org/abs/2510.25378</link>
<guid>https://arxiv.org/abs/2510.25378</guid>
<content:encoded><![CDATA[
arXiv:2510.25378v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in bibliographic recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic information depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record is repeatedly represented in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 bibliographic records across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts into memorization.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods</title>
<link>https://arxiv.org/abs/2510.25386</link>
<guid>https://arxiv.org/abs/2510.25386</guid>
<content:encoded><![CDATA[
arXiv:2510.25386v1 Announce Type: cross 
Abstract: This survey provides an analysis of current methodologies integrating legal and logical specifications into the perception, prediction, and planning modules of automated driving systems. We systematically explore techniques ranging from logic-based frameworks to computational legal reasoning approaches, emphasizing their capability to ensure regulatory compliance and interpretability in dynamic and uncertain driving environments. A central finding is that significant challenges arise at the intersection of perceptual reliability, legal compliance, and decision-making justifiability. To systematically analyze these challenges, we introduce a taxonomy categorizing existing approaches by their theoretical foundations, architectural implementations, and validation strategies. We particularly focus on methods that address perceptual uncertainty and incorporate explicit legal norms, facilitating decisions that are both technically robust and legally defensible. The review covers neural-symbolic integration methods for perception, logic-driven rule representation, and norm-aware prediction strategies, all contributing toward transparent and accountable autonomous vehicle operation. We highlight critical open questions and practical trade-offs that must be addressed, offering multidisciplinary insights from engineering, logic, and law to guide future developments in legally compliant autonomous driving systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPTOpt: Towards Efficient LLM-Based Black-Box Optimization</title>
<link>https://arxiv.org/abs/2510.25404</link>
<guid>https://arxiv.org/abs/2510.25404</guid>
<content:encoded><![CDATA[
arXiv:2510.25404v1 Announce Type: cross 
Abstract: Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency. Classical methods such as Bayesian Optimization (BO) can be effective, but they often require careful parameter tuning to each application domain. At the same time, Large Language Models (LLMs) have shown broad capabilities, yet state-of-the-art models remain limited in solving continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based optimization method that equips LLMs with continuous black-box optimization capabilities. By fine-tuning large language models on extensive synthetic datasets derived from diverse BO parameterizations, GPTOpt leverages LLM pre-training to generalize across optimization tasks. On a variety of black-box optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting the capacity of LLMs for advanced numerical reasoning and introducing a flexible framework for global optimization without parameter tuning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains</title>
<link>https://arxiv.org/abs/2510.25409</link>
<guid>https://arxiv.org/abs/2510.25409</guid>
<content:encoded><![CDATA[
arXiv:2510.25409v1 Announce Type: cross 
Abstract: The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems</title>
<link>https://arxiv.org/abs/2510.25416</link>
<guid>https://arxiv.org/abs/2510.25416</guid>
<content:encoded><![CDATA[
arXiv:2510.25416v1 Announce Type: cross 
Abstract: The advent of artificial intelligence (AI)-native wireless communication is fundamentally reshaping the design paradigm of next-generation (NextG) systems, where intelligent air interfaces are expected to operate adaptively and efficiently in highly dynamic environments. Conventional orthogonal frequency division multiplexing (OFDM) systems rely heavily on pilots and the cyclic prefix (CP), resulting in significant overhead and reduced spectral efficiency. To address these limitations, we propose an adaptive end-to-end (E2E) transceiver architecture tailored for pilot-free and CP-free wireless systems. The architecture combines AI-driven constellation shaping and a neural receiver through joint training. To enhance robustness against mismatched or time-varying channel conditions, we introduce a lightweight channel adapter (CA) module, which enables rapid adaptation with minimal computational overhead by updating only the CA parameters. Additionally, we present a framework that is scalable to multiple modulation orders within a unified model, significantly reducing model storage requirements. Moreover, to tackle the high peak-to-average power ratio (PAPR) inherent to OFDM, we incorporate constrained E2E training, achieving compliance with PAPR targets without additional transmission overhead. Extensive simulations demonstrate that the proposed framework delivers superior bit error rate (BER), throughput, and resilience across diverse channel scenarios, highlighting its potential for AI-native NextG.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</title>
<link>https://arxiv.org/abs/2510.25420</link>
<guid>https://arxiv.org/abs/2510.25420</guid>
<content:encoded><![CDATA[
arXiv:2510.25420v1 Announce Type: cross 
Abstract: Diffusion models have emerged as powerful priors for single-image restoration, but their application to zero-shot video restoration suffers from temporal inconsistencies due to the stochastic nature of sampling and complexity of incorporating explicit temporal modeling. In this work, we address the challenge of improving temporal coherence in video restoration using zero-shot image-based diffusion models without retraining or modifying their architecture. We propose two complementary inference-time strategies: (1) Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired perceptual straightening hypothesis, which steers the diffusion denoising process towards smoother temporal evolution by incorporating a curvature penalty in a perceptual space to improve temporal perceptual scores, such as Fr\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path Ensemble Sampling (MPES), which aims at reducing stochastic variation by ensembling multiple diffusion trajectories to improve fidelity (distortion) scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these training-free techniques provide a practical path toward temporally stable high-fidelity perceptual video restoration using large pretrained diffusion models. We performed extensive experiments over multiple datasets and degradation types, systematically evaluating each strategy to understand their strengths and limitations. Our results show that while PSG enhances temporal naturalness, particularly in case of temporal blur, MPES consistently improves fidelity and spatio-temporal perception--distortion trade-off across all tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction</title>
<link>https://arxiv.org/abs/2510.25426</link>
<guid>https://arxiv.org/abs/2510.25426</guid>
<content:encoded><![CDATA[
arXiv:2510.25426v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language at the core of human-computer interaction (HCI). We argue that advancing HCI requires attention to the linguistic foundations of interaction, particularly implicature (meaning conveyed beyond explicit statements through shared context) which is essential for human-AI (HAI) alignment. This study examines LLMs' ability to infer user intent embedded in context-driven prompts and whether understanding implicature improves response generation. Results show that larger models approximate human interpretations more closely, while smaller models struggle with implicature inference. Furthermore, implicature-based prompts significantly enhance the perceived relevance and quality of responses across models, with notable gains in smaller models. Overall, 67.6% of participants preferred responses with implicature-embedded prompts to literal ones, highlighting a clear preference for contextually nuanced communication. Our work contributes to understanding how linguistic theory can be used to address the alignment problem by making HAI interaction more natural and contextually grounded.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLMEval: Evaluating Research-Level Neural Theorem Proving</title>
<link>https://arxiv.org/abs/2510.25427</link>
<guid>https://arxiv.org/abs/2510.25427</guid>
<content:encoded><![CDATA[
arXiv:2510.25427v1 Announce Type: cross 
Abstract: Despite impressive results on curated benchmarks, the practical impact of large language models (LLMs) on research-level neural theorem proving and proof autoformalization is still limited. We introduce RLMEval, an evaluation suite for these tasks, focusing on research-level mathematics from real-world Lean formalization projects. RLMEval targets the evaluation of neural theorem proving and proof autoformalization on challenging research-level theorems by leveraging real Lean Blueprint formalization projects. Our evaluation of state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean projects, reveals a significant gap: progress on existing benchmarks does not readily translate to these more realistic settings, with the best model achieving only a 10.3 % pass rate. RLMEval provides a new, challenging benchmark designed to guide and accelerate progress in automated reasoning for formal mathematics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report</title>
<link>https://arxiv.org/abs/2510.25428</link>
<guid>https://arxiv.org/abs/2510.25428</guid>
<content:encoded><![CDATA[
arXiv:2510.25428v1 Announce Type: cross 
Abstract: This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs</title>
<link>https://arxiv.org/abs/2510.25441</link>
<guid>https://arxiv.org/abs/2510.25441</guid>
<content:encoded><![CDATA[
arXiv:2510.25441v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Utility-Aware Multiclass Calibration</title>
<link>https://arxiv.org/abs/2510.25458</link>
<guid>https://arxiv.org/abs/2510.25458</guid>
<content:encoded><![CDATA[
arXiv:2510.25458v1 Announce Type: cross 
Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align with observed frequencies, is a minimal and fundamental requirement for classifiers to be viewed as trustworthy. Existing methods for assessing multiclass calibration often focus on specific aspects associated with prediction (e.g., top-class confidence, class-wise calibration) or utilize computationally challenging variational formulations. In this work, we study scalable \emph{evaluation} of multiclass calibration. To this end, we propose utility calibration, a general framework that measures the calibration error relative to a specific utility function that encapsulates the goals or decision criteria relevant to the end user. We demonstrate how this framework can unify and re-interpret several existing calibration metrics, particularly allowing for more robust versions of the top-class and class-wise calibration metrics, and, going beyond such binarized approaches, toward assessing calibration for richer classes of downstream utilities.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuned Language Models for Domain-Specific Summarization and Tagging</title>
<link>https://arxiv.org/abs/2510.25460</link>
<guid>https://arxiv.org/abs/2510.25460</guid>
<content:encoded><![CDATA[
arXiv:2510.25460v1 Announce Type: cross 
Abstract: This paper presents a pipeline integrating fine-tuned large language models (LLMs) with named entity recognition (NER) for efficient domain-specific text summarization and tagging. The authors address the challenge posed by rapidly evolving sub-cultural languages and slang, which complicate automated information extraction and law enforcement monitoring. By leveraging the LLaMA Factory framework, the study fine-tunes LLMs on both generalpurpose and custom domain-specific datasets, particularly in the political and security domains. The models are evaluated using BLEU and ROUGE metrics, demonstrating that instruction fine-tuning significantly enhances summarization and tagging accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct model, despite its initial limitations in Chinese comprehension, outperforms its Chinese-trained counterpart after domainspecific fine-tuning, suggesting that underlying reasoning capabilities can transfer across languages. The pipeline enables concise summaries and structured entity tagging, facilitating rapid document categorization and distribution. This approach proves scalable and adaptable for real-time applications, supporting efficient information management and the ongoing need to capture emerging language trends. The integration of LLMs and NER offers a robust solution for transforming unstructured text into actionable insights, crucial for modern knowledge management and security operations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An In-Depth Analysis of Cyber Attacks in Secured Platforms</title>
<link>https://arxiv.org/abs/2510.25470</link>
<guid>https://arxiv.org/abs/2510.25470</guid>
<content:encoded><![CDATA[
arXiv:2510.25470v1 Announce Type: cross 
Abstract: There is an increase in global malware threats. To address this, an encryption-type ransomware has been introduced on the Android operating system. The challenges associated with malicious threats in phone use have become a pressing issue in mobile communication, disrupting user experiences and posing significant privacy threats. This study surveys commonly used machine learning techniques for detecting malicious threats in phones and examines their performance. The majority of past research focuses on customer feedback and reviews, with concerns that people might create false reviews to promote or devalue products and services for personal gain. Hence, the development of techniques for detecting malicious threats using machine learning has been a key focus. This paper presents a comprehensive comparative study of current research on the issue of malicious threats and methods for tackling these challenges. Nevertheless, a huge amount of information is required by these methods, presenting a challenge for developing robust, specialized automated anti-malware systems. This research describes the Android Applications dataset, and the accuracy of the techniques is measured using the accuracy levels of the metrics employed in this study.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.25502</link>
<guid>https://arxiv.org/abs/2510.25502</guid>
<content:encoded><![CDATA[
arXiv:2510.25502v1 Announce Type: cross 
Abstract: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the vast majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies</title>
<link>https://arxiv.org/abs/2510.25506</link>
<guid>https://arxiv.org/abs/2510.25506</guid>
<content:encoded><![CDATA[
arXiv:2510.25506v1 Announce Type: cross 
Abstract: Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both other researchers and practitioners. One important step towards excelling in empirical research on LLMs and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 86 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were fit for reproduction. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaCT: Faithful Concept Traces for Explaining Neural Network Decisions</title>
<link>https://arxiv.org/abs/2510.25512</link>
<guid>https://arxiv.org/abs/2510.25512</guid>
<content:encoded><![CDATA[
arXiv:2510.25512v1 Announce Type: cross 
Abstract: Deep networks have shown remarkable performance across a wide range of tasks, yet getting a global concept-level understanding of how they function remains a key challenge. Many post-hoc concept-based approaches have been introduced to understand their workings, yet they are not always faithful to the model. Further, they make restrictive assumptions on the concepts a model learns, such as class-specificity, small spatial extent, or alignment to human expectations. In this work, we put emphasis on the faithfulness of such concept-based explanations and propose a new model with model-inherent mechanistic concept-explanations. Our concepts are shared across classes and, from any layer, their contribution to the logit and their input-visualization can be faithfully traced. We also leverage foundation models to propose a new concept-consistency metric, C$^2$-Score, that can be used to evaluate concept-based methods. We show that, compared to prior work, our concepts are quantitatively more consistent and users find our concepts to be more interpretable, all while retaining competitive ImageNet performance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography</title>
<link>https://arxiv.org/abs/2510.25522</link>
<guid>https://arxiv.org/abs/2510.25522</guid>
<content:encoded><![CDATA[
arXiv:2510.25522v1 Announce Type: cross 
Abstract: Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The model's superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the region's most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using latent representations to link disjoint longitudinal data for mixed-effects regression</title>
<link>https://arxiv.org/abs/2510.25531</link>
<guid>https://arxiv.org/abs/2510.25531</guid>
<content:encoded><![CDATA[
arXiv:2510.25531v1 Announce Type: cross 
Abstract: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2510.25557</link>
<guid>https://arxiv.org/abs/2510.25557</guid>
<content:encoded><![CDATA[
arXiv:2510.25557v1 Announce Type: cross 
Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the entire recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC, residing in an exponentially large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit measurements, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, adopting projective measurements as a limiting case to obtain mid-circuit readouts while maintaining a coherent recurrent quantum memory. We further devise a soft attention mechanism over the mid-circuit readouts in a sequence-to-sequence model and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting</title>
<link>https://arxiv.org/abs/2510.25563</link>
<guid>https://arxiv.org/abs/2510.25563</guid>
<content:encoded><![CDATA[
arXiv:2510.25563v1 Announce Type: cross 
Abstract: The accurate prediction of oceanographic variables is crucial for understanding climate change, managing marine resources, and optimizing maritime activities. Traditional ocean forecasting relies on numerical models; however, these approaches face limitations in terms of computational cost and scalability. In this study, we adapt Aurora, a foundational deep learning model originally designed for atmospheric forecasting, to predict sea surface temperature (SST) in the Canary Upwelling System. By fine-tuning this model with high-resolution oceanographic reanalysis data, we demonstrate its ability to capture complex spatiotemporal patterns while reducing computational demands. Our methodology involves a staged fine-tuning process, incorporating latitude-weighted error metrics and optimizing hyperparameters for efficient learning. The experimental results show that the model achieves a low RMSE of 0.119K, maintaining high anomaly correlation coefficients (ACC $\approx 0.997$). The model successfully reproduces large-scale SST structures but faces challenges in capturing finer details in coastal regions. This work contributes to the field of data-driven ocean forecasting by demonstrating the feasibility of using deep learning models pre-trained in different domains for oceanic applications. Future improvements include integrating additional oceanographic variables, increasing spatial resolution, and exploring physics-informed neural networks to enhance interpretability and understanding. These advancements can improve climate modeling and ocean prediction accuracy, supporting decision-making in environmental and economic sectors.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models</title>
<link>https://arxiv.org/abs/2510.25577</link>
<guid>https://arxiv.org/abs/2510.25577</guid>
<content:encoded><![CDATA[
arXiv:2510.25577v1 Announce Type: cross 
Abstract: Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RegionE: Adaptive Region-Aware Generation for Efficient Image Editing</title>
<link>https://arxiv.org/abs/2510.25590</link>
<guid>https://arxiv.org/abs/2510.25590</guid>
<content:encoded><![CDATA[
arXiv:2510.25590v1 Announce Type: cross 
Abstract: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry</title>
<link>https://arxiv.org/abs/2510.25595</link>
<guid>https://arxiv.org/abs/2510.25595</guid>
<content:encoded><![CDATA[
arXiv:2510.25595v1 Announce Type: cross 
Abstract: While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats</title>
<link>https://arxiv.org/abs/2510.25602</link>
<guid>https://arxiv.org/abs/2510.25602</guid>
<content:encoded><![CDATA[
arXiv:2510.25602v1 Announce Type: cross 
Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, the comparison at fine-grained (block-wise) levels is more nuanced. Our comprehensive comparison demonstrates that for popular 8-bit fine-grained formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart in both algorithmic accuracy and hardware efficiency. However, for 4-bit formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like Hadamard rotation are applied. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory, demonstrating that a one-size-fits-all FP approach is suboptimal and advocating that fine-grained INT formats, particularly MXINT8, offer a better balance of accuracy, power, and efficiency for future AI accelerators.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training</title>
<link>https://arxiv.org/abs/2510.25609</link>
<guid>https://arxiv.org/abs/2510.25609</guid>
<content:encoded><![CDATA[
arXiv:2510.25609v1 Announce Type: cross 
Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a different metric distance than the Earth Mover (Wasserstein) distance and achieves better training stability. Empirical evaluations on four standard image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60% lower Frechet Inception Distance (FID). Our results suggest that BOLT is a broadly applicable principle for enhancing GAN training.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization</title>
<link>https://arxiv.org/abs/2510.25616</link>
<guid>https://arxiv.org/abs/2510.25616</guid>
<content:encoded><![CDATA[
arXiv:2510.25616v1 Announce Type: cross 
Abstract: The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering</title>
<link>https://arxiv.org/abs/2510.25621</link>
<guid>https://arxiv.org/abs/2510.25621</guid>
<content:encoded><![CDATA[
arXiv:2510.25621v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Language Models Efficient Reasoners? A Perspective from Logic Programming</title>
<link>https://arxiv.org/abs/2510.25626</link>
<guid>https://arxiv.org/abs/2510.25626</guid>
<content:encoded><![CDATA[
arXiv:2510.25626v1 Announce Type: cross 
Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of human-like reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Plan &amp; Schedule with Reinforcement-Learned Bimanual Robot Skills</title>
<link>https://arxiv.org/abs/2510.25634</link>
<guid>https://arxiv.org/abs/2510.25634</guid>
<content:encoded><![CDATA[
arXiv:2510.25634v1 Announce Type: cross 
Abstract: Long-horizon contact-rich bimanual manipulation presents a significant challenge, requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms. In this paper, we introduce a hierarchical framework that frames this challenge as an integrated skill planning & scheduling problem, going beyond purely sequential decision-making to support simultaneous skill invocation. Our approach is built upon a library of single-arm and bimanual primitive skills, each trained using Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a Transformer-based planner on a dataset of skill compositions to act as a high-level scheduler, simultaneously predicting the discrete schedule of skills as well as their continuous parameters. We demonstrate that our method achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgraph Federated Learning via Spectral Methods</title>
<link>https://arxiv.org/abs/2510.25657</link>
<guid>https://arxiv.org/abs/2510.25657</guid>
<content:encoded><![CDATA[
arXiv:2510.25657v1 Announce Type: cross 
Abstract: We consider the problem of federated learning (FL) with graph-structured data distributed across multiple clients. In particular, we address the prevalent scenario of interconnected subgraphs, where interconnections between clients significantly influence the learning process. Existing approaches suffer from critical limitations, either requiring the exchange of sensitive node embeddings, thereby posing privacy risks, or relying on computationally-intensive steps, which hinders scalability. To tackle these challenges, we propose FedLap, a novel framework that leverages global structure information via Laplacian smoothing in the spectral domain to effectively capture inter-node dependencies while ensuring privacy and scalability. We provide a formal analysis of the privacy of FedLap, demonstrating that it preserves privacy. Notably, FedLap is the first subgraph FL scheme with strong privacy guarantees. Extensive experiments on benchmark datasets demonstrate that FedLap achieves competitive or superior utility compared to existing techniques.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Misconceptions of LLM-Based Conversational Programming Assistants</title>
<link>https://arxiv.org/abs/2510.25662</link>
<guid>https://arxiv.org/abs/2510.25662</guid>
<content:encoded><![CDATA[
arXiv:2510.25662v1 Announce Type: cross 
Abstract: Programming assistants powered by large language models (LLMs) have become widely available, with conversational assistants like ChatGPT proving particularly accessible to less experienced programmers. However, the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search, code execution, or retrieval-augmented generation create opportunities for user misconceptions about what systems can and cannot do. Such misconceptions may lead to over-reliance, unproductive practices, or insufficient quality control in LLM-assisted programming. Here, we aim to characterize misconceptions that users of conversational LLM-based assistants may have in programming contexts. Using a two-phase approach, we first brainstorm and catalog user misconceptions that may occur, and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic Python-programming conversations with an LLM-based chatbot drawn from an openly available dataset. Indeed, we see evidence that some users have misplaced expectations about the availability of LLM-based chatbot features like web access, code execution, or non-text output generation. We also see potential evidence for deeper conceptual issues around the scope of information required to debug, validate, and optimize programs. Our findings reinforce the need for designing LLM-based tools that more clearly communicate their programming capabilities to users.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics</title>
<link>https://arxiv.org/abs/2510.25683</link>
<guid>https://arxiv.org/abs/2510.25683</guid>
<content:encoded><![CDATA[
arXiv:2510.25683v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models for numerical simulations. While their applications in computational fluid dynamics have been investigated, little attention has been given to structural problems, especially for dynamic cases. To address this gap, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine learning models, and its design makes it particularly suited for dynamic simulations thanks to three key features: (i) expressing node kinematics in node-fixed local frames, which avoids catastrophic cancellation in finite-difference velocities; (ii) employing a sign-aware regression loss, which reduces phase errors in long rollouts; and (iii) using a wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz Hanning-modulated pulse. The results show that GNSS accurately reproduces the physics of the problem over hundreds of timesteps and generalizes to unseen loading conditions, where existing GNNs fail to converge or deliver meaningful predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial inference speedups while preserving spatial and temporal fidelity. These findings demonstrate that locality-preserving GNNs with physics-consistent update rules are a competitive alternative for dynamic, wave-dominated structural simulations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents</title>
<link>https://arxiv.org/abs/2510.25694</link>
<guid>https://arxiv.org/abs/2510.25694</guid>
<content:encoded><![CDATA[
arXiv:2510.25694v1 Announce Type: cross 
Abstract: Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution</title>
<link>https://arxiv.org/abs/2510.25726</link>
<guid>https://arxiv.org/abs/2510.25726</guid>
<content:encoded><![CDATA[
arXiv:2510.25726v1 Announce Type: cross 
Abstract: Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Conditional Diffusion Networks for Microwave Image Reconstruction</title>
<link>https://arxiv.org/abs/2510.25729</link>
<guid>https://arxiv.org/abs/2510.25729</guid>
<content:encoded><![CDATA[
arXiv:2510.25729v1 Announce Type: cross 
Abstract: A conditional latent-diffusion based framework for solving the electromagnetic inverse scattering problem associated with microwave imaging is introduced. This generative machine-learning model explicitly mirrors the non-uniqueness of the ill-posed inverse problem. Unlike existing inverse solvers utilizing deterministic machine learning techniques that produce a single reconstruction, the proposed latent-diffusion model generates multiple plausible permittivity maps conditioned on measured scattered-field data, thereby generating several potential instances in the range-space of the non-unique inverse mapping. A forward electromagnetic solver is integrated into the reconstruction pipeline as a physics-based evaluation mechanism. The space of candidate reconstructions form a distribution of possibilities consistent with the conditioning data and the member of this space yielding the lowest scattered-field data discrepancy between the predicted and measured scattered fields is reported as the final solution. Synthetic and experimental labeled datasets are used for training and evaluation of the model. An innovative labeled synthetic dataset is created that exemplifies a varied set of scattering features. Training of the model using this new dataset produces high quality permittivity reconstructions achieving improved generalization with excellent fidelity to shape recognition. The results highlight the potential of hybrid generative physics frameworks as a promising direction for robust, data-driven microwave imaging.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries</title>
<link>https://arxiv.org/abs/2510.25731</link>
<guid>https://arxiv.org/abs/2510.25731</guid>
<content:encoded><![CDATA[
arXiv:2510.25731v1 Announce Type: cross 
Abstract: We introduce a method for efficiently solving initial-boundary value problems (IBVPs) that uses Lie symmetries to enforce the associated partial differential equation (PDE) exactly by construction. By leveraging symmetry transformations, the model inherently incorporates the physical laws and learns solutions from initial and boundary data. As a result, the loss directly measures the model's accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our method enables rigorous error estimation. The approach yields compact models, facilitating an efficient optimization. We implement LieSolver and demonstrate its application to linear homogeneous PDEs with a range of initial conditions, showing that it is faster and more accurate than physics-informed neural networks (PINNs). Overall, our method improves both computational efficiency and the reliability of predictions for PDE-constrained problems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework</title>
<link>https://arxiv.org/abs/2510.25732</link>
<guid>https://arxiv.org/abs/2510.25732</guid>
<content:encoded><![CDATA[
arXiv:2510.25732v1 Announce Type: cross 
Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Completion Agents are Not Ideal Collaborators</title>
<link>https://arxiv.org/abs/2510.25744</link>
<guid>https://arxiv.org/abs/2510.25744</guid>
<content:encoded><![CDATA[
arXiv:2510.25744v1 Announce Type: cross 
Abstract: Current evaluations of agents remain centered around one-shot task completion, failing to account for the inherently iterative and collaborative nature of many real-world problems, where human goals are often underspecified and evolve. We argue for a shift from building and assessing task completion agents to developing collaborative agents, assessed not only by the quality of their final outputs but by how well they engage with and enhance human effort throughout the problem-solving process. To support this shift, we introduce collaborative effort scaling, a framework that captures how an agent's utility grows with increasing user involvement. Through case studies and simulated evaluations, we show that state-of-the-art agents often underperform in multi-turn, real-world scenarios, revealing a missing ingredient in agent design: the ability to sustain engagement and scaffold user understanding. Collaborative effort scaling offers a lens for diagnosing agent behavior and guiding development toward more effective interactions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-Scores for (In)Correctness Assessment of Generative Model Outputs</title>
<link>https://arxiv.org/abs/2510.25770</link>
<guid>https://arxiv.org/abs/2510.25770</guid>
<content:encoded><![CDATA[
arXiv:2510.25770v1 Announce Type: cross 
Abstract: While generative models, especially large language models (LLMs), are ubiquitous in today's world, principled mechanisms to assess their (in)correctness are limited. Using the conformal prediction framework, previous works construct sets of LLM responses where the probability of including an incorrect response, or error, is capped at a desired user-defined tolerance level. However, since these methods are based on p-values, they are susceptible to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the guarantees. We therefore leverage e-values to complement generative model outputs with e-scores as a measure of incorrectness. In addition to achieving the same statistical guarantees as before, e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves, by upper bounding a post-hoc notion of error called size distortion. We experimentally demonstrate their efficacy in assessing LLM outputs for different correctness types: mathematical factuality and property constraints satisfaction.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaperon: A Peppered English-French Generative Language Model Suite</title>
<link>https://arxiv.org/abs/2510.25771</link>
<guid>https://arxiv.org/abs/2510.25771</guid>
<content:encoded><![CDATA[
arXiv:2510.25771v1 Announce Type: cross 
Abstract: We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain-inspired Computational Intelligence via Predictive Coding</title>
<link>https://arxiv.org/abs/2308.07870</link>
<guid>https://arxiv.org/abs/2308.07870</guid>
<content:encoded><![CDATA[
arXiv:2308.07870v3 Announce Type: replace 
Abstract: Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with a learning algorithm called error backpropagation, always considered biologically implausible. To this end, recent works have studied learning algorithms for deep neural networks inspired by the neurosciences. One such theory, called predictive coding (PC), has shown promising properties that make it potentially valuable for the machine learning community: it can model information processing in different areas of the brain, can be used in control and robotics, has a solid mathematical foundation in variational inference, and performs its computations asynchronously. Inspired by such properties, works that propose novel PC-like algorithms are starting to be present in multiple sub-fields of machine learning and AI at large. Here, we survey such efforts by first providing a broad overview of the history of PC to provide common ground for the understanding of the recent developments, then by describing current efforts and results, and concluding with a large discussion of possible implications and ways forward.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2403.02745</link>
<guid>https://arxiv.org/abs/2403.02745</guid>
<content:encoded><![CDATA[
arXiv:2403.02745v3 Announce Type: replace 
Abstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), focusing on incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs' resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an $\epsilon$-optimal ranking with high probability while allowing as large as $O(n)$ perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraveLLM: Could you plan my new public transit route in face of a network disruption?</title>
<link>https://arxiv.org/abs/2407.14926</link>
<guid>https://arxiv.org/abs/2407.14926</guid>
<content:encoded><![CDATA[
arXiv:2407.14926v2 Announce Type: replace 
Abstract: Existing navigation systems often fail during urban disruptions, struggling to incorporate real-time events and complex user constraints, such as avoiding specific areas. We address this gap with TraveLLM, a system using Large Language Models (LLMs) for disruption-aware public transit routing. We leverage LLMs' reasoning capabilities to directly process multimodal user queries combining natural language requests (origin, destination, preferences, disruption info) with map data (e.g., subway, bus, bike-share). To evaluate this approach, we design challenging test scenarios reflecting real-world disruptions like weather events, emergencies, and dynamic service availability. We benchmark the performance of state-of-the-art LLMs, including GPT-4, Claude 3, and Gemini, on generating accurate travel plans. Our experiments demonstrate that LLMs, notably GPT-4, can effectively generate viable and context-aware navigation plans under these demanding conditions. These findings suggest a promising direction for using LLMs to build more flexible and intelligent navigation systems capable of handling dynamic disruptions and diverse user needs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNN-Based Online Learning of Concepts and Action Laws in an Open World</title>
<link>https://arxiv.org/abs/2411.12308</link>
<guid>https://arxiv.org/abs/2411.12308</guid>
<content:encoded><![CDATA[
arXiv:2411.12308v4 Announce Type: replace 
Abstract: We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent's semantic memory. This agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent's knowledge of its universe's action laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10844</link>
<guid>https://arxiv.org/abs/2505.10844</guid>
<content:encoded><![CDATA[
arXiv:2505.10844v4 Announce Type: replace 
Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.16854</link>
<guid>https://arxiv.org/abs/2505.16854</guid>
<content:encoded><![CDATA[
arXiv:2505.16854v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR, GeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in RL approaches. Our code is available at https://github.com/kokolerk/TON.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour</title>
<link>https://arxiv.org/abs/2505.17801</link>
<guid>https://arxiv.org/abs/2505.17801</guid>
<content:encoded><![CDATA[
arXiv:2505.17801v2 Announce Type: replace 
Abstract: Autonomous multi-agent systems (MAS) are useful for automating complex tasks but raise trust concerns due to risks such as miscoordination or goal misalignment. Explainability is vital for users' trust calibration, but explainable MAS face challenges due to complex environments, the human factor, and non-standardised evaluation. Leveraging the counterfactual effect size model and LLMs, we propose Agentic eXplanations via Interrogative Simulation (AXIS). AXIS generates human-centred action explanations for multi-agent policies by having an LLM interrogate an environment simulator using prompts like 'whatif' and 'remove' to observe and synthesise counterfactual information over multiple rounds. We evaluate AXIS on autonomous driving across ten scenarios for five LLMs with a comprehensive methodology combining robustness, subjective preference, correctness, and goal/action prediction with an external LLM as evaluator. Compared to baselines, AXIS improves perceived explanation correctness by at least 7.7% across all models and goal prediction accuracy by 23% for four models, with comparable action prediction accuracy, achieving the highest scores overall. Our code is open-sourced at https://github.com/gyevnarb/axis.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions</title>
<link>https://arxiv.org/abs/2505.17818</link>
<guid>https://arxiv.org/abs/2505.17818</guid>
<content:encoded><![CDATA[
arXiv:2505.17818v2 Announce Type: replace 
Abstract: Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluate eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3 70B, is validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare. The code is available at https://github.com/dek924/PatientSim.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing</title>
<link>https://arxiv.org/abs/2505.21671</link>
<guid>https://arxiv.org/abs/2505.21671</guid>
<content:encoded><![CDATA[
arXiv:2505.21671v3 Announce Type: replace 
Abstract: We study a sequential decision-making problem on a $n$-node graph $\mathcal{G}$ where each node has an unknown label from a finite set $\mathbf{\Omega}$, drawn from a joint distribution $\mathcal{P}$ that is Markov with respect to $\mathcal{G}$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $\mathcal{G}$ is a forest. Our implementation runs in $\mathcal{O}(n^2 \cdot |\mathbf{\Omega}|^2)$ time while using $\mathcal{O}(n \cdot |\mathbf{\Omega}|^2)$ oracle calls to $\mathcal{P}$ and $\mathcal{O}(n^2 \cdot |\mathbf{\Omega}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models</title>
<link>https://arxiv.org/abs/2506.17585</link>
<guid>https://arxiv.org/abs/2506.17585</guid>
<content:encoded><![CDATA[
arXiv:2506.17585v2 Announce Type: replace 
Abstract: Trustworthy language models should provide both correct and verifiable answers. However, citations generated directly by standalone LLMs are often unreliable. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during continual pretraining without test-time retrieval, by revising the training process. To study this, we construct CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both short-form (single-fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to index factual knowledge by binding it to persistent document identifiers; and (2) instruction tuning to elicit citation behavior. We introduce Active Indexing for the first stage, which creates generalizable, source-anchored bindings by augmenting training with synthetic data that (i) restate each fact in diverse, compositional forms and (ii) enforce bidirectional training (source-to-fact and fact-to-source). This equips the model to both generate content from a cited source and attribute its own answers, improving robustness to paraphrase and composition. Experiments with Qwen-2.5-7B&3B show that Active Indexing consistently outperforms a Passive Indexing baseline, which simply appends an identifier to each document, achieving citation precision gains of up to 30.2% across all tasks and models. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16x the original token count. Finally, we show that internal citations complement external ones by making the model more robust to retrieval noise.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics</title>
<link>https://arxiv.org/abs/2507.15518</link>
<guid>https://arxiv.org/abs/2507.15518</guid>
<content:encoded><![CDATA[
arXiv:2507.15518v2 Announce Type: replace 
Abstract: Creating an immersive and interactive theatrical experience is a long-term goal in the field of interactive narrative. The emergence of large language model (LLM) is providing a new path to achieve this goal. However, existing LLM-based drama generation methods often result in agents that lack initiative and cannot interact with the physical scene. Furthermore, these methods typically require detailed user input to drive the drama. These limitations reduce the interactivity and immersion of online real-time performance. To address the above challenges, we propose HAMLET, a multi-agent framework focused on drama creation and online performance. Given a simple topic, the framework generates a narrative blueprint, guiding the subsequent improvisational performance. During the online performance, each actor is given an autonomous mind. This means that actors can make independent decisions based on their own background, goals, and emotional state. In addition to conversations with other actors, their decisions can also change the state of scene props through actions such as opening a letter or picking up a weapon. The change is then broadcast to other related actors, updating what they know and care about, which in turn influences their next action. To evaluate the quality of drama performance generated by HAMLET, we designed an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience. The experimental evaluation shows that HAMLET can create expressive and coherent theatrical experiences.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Truthful Representations Flip Under Deceptive Instructions?</title>
<link>https://arxiv.org/abs/2507.22149</link>
<guid>https://arxiv.org/abs/2507.22149</guid>
<content:encoded><![CDATA[
arXiv:2507.22149v4 Announce Type: replace 
Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations ``flip'', such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces. % Our analysis pinpoints layer-wise and feature-level correlates of instructed dishonesty, offering insights for LLM detection and control. Our findings expose feature- and layer-level signatures of deception, offering new insights for detecting and mitigating instructed dishonesty in LLMs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07382</link>
<guid>https://arxiv.org/abs/2508.07382</guid>
<content:encoded><![CDATA[
arXiv:2508.07382v2 Announce Type: replace 
Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet current Large Language Models (LLMs) face significant limitations in this domain, including poor error handling, inefficient reasoning, and an inability to perform complex end-to-end tasks autonomously. To address these challenges, we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning capabilities for this task through a two-stage reinforcement learning pipeline. We first construct a dataset of over 500 real-world, multi-step walkthroughs, which Pentest-R1 leverages for offline reinforcement learning (RL) to instill foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in an interactive Capture The Flag (CTF) environment, where it learns directly from environmental feedback to develop robust error self-correction and adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench benchmarks demonstrate the framework's effectiveness. On AutoPenBench, Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a 15.0\% success rate in unguided tasks, establishing a new state-of-the-art for open-source LLMs and matching the performance of top proprietary models. Ablation studies confirm that the synergy of both training stages is critical to its success.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss</title>
<link>https://arxiv.org/abs/2508.08615</link>
<guid>https://arxiv.org/abs/2508.08615</guid>
<content:encoded><![CDATA[
arXiv:2508.08615v2 Announce Type: replace 
Abstract: Partial differential equations (PDEs) form the mathematical foundation for modeling physical systems in science and engineering, where numerical solutions demand rigorous accuracy-efficiency tradeoffs. Mesh movement techniques address this challenge by dynamically relocating mesh nodes to rapidly-varying regions, enhancing both simulation accuracy and computational efficiency. However, traditional approaches suffer from high computational complexity and geometric inflexibility, limiting their applicability, and existing supervised learning-based approaches face challenges in zero-shot generalization across diverse PDEs and mesh topologies.In this paper, we present an Unsupervised and Generalizable Mesh Movement Network (UGM2N). We first introduce unsupervised mesh adaptation through localized geometric feature learning, eliminating the dependency on pre-adapted meshes. We then develop a physics-constrained loss function, M-Uniform loss, that enforces mesh equidistribution at the nodal level.Experimental results demonstrate that the proposed network exhibits equation-agnostic generalization and geometric independence in efficient mesh adaptation. It demonstrates consistent superiority over existing methods, including robust performance across diverse PDEs and mesh geometries, scalability to multi-scale resolutions and guaranteed error reduction without mesh tangling.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism</title>
<link>https://arxiv.org/abs/2508.15030</link>
<guid>https://arxiv.org/abs/2508.15030</guid>
<content:encoded><![CDATA[
arXiv:2508.15030v2 Announce Type: replace 
Abstract: We propose Collab-REC, a multi-agent framework designed to counteract popularity bias and enhance diversity in tourism recommendations. In our setting, three LLM-based agents -- Personalization, Popularity, and Sustainability generate city suggestions from complementary perspectives. A non-LLM moderator then merges and refines these proposals via multi-round negotiation, ensuring each agent's viewpoint is incorporated while penalizing spurious or repeated responses. Experiments on European city queries show that Collab-REC improves diversity and overall relevance compared to a single-agent baseline, surfacing lesser-visited locales that often remain overlooked. This balanced, context-aware approach addresses over-tourism and better aligns with constraints provided by the user, highlighting the promise of multi-stakeholder collaboration in LLM-driven recommender systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL Generation from Large Language Models</title>
<link>https://arxiv.org/abs/2509.01308</link>
<guid>https://arxiv.org/abs/2509.01308</guid>
<content:encoded><![CDATA[
arXiv:2509.01308v2 Announce Type: replace 
Abstract: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries. To address this limitation, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can produce correct answers after multiple attempts. However, these methods rely on surface-level heuristics, selecting the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated one through Majority Voting. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising reinforcement learning approach for improving model alignment. We argue that ORMs could serve as an effective new test-time heuristic, although their application in this context remains largely underexplored.
  In this work, we propose a unified framework for training ORMs tailored to the Text-to-SQL task and assess their effectiveness as a test-time heuristic within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2, Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</title>
<link>https://arxiv.org/abs/2509.02547</link>
<guid>https://arxiv.org/abs/2509.02547</guid>
<content:encoded><![CDATA[
arXiv:2509.02547v2 Announce Type: replace 
Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes</title>
<link>https://arxiv.org/abs/2509.04317</link>
<guid>https://arxiv.org/abs/2509.04317</guid>
<content:encoded><![CDATA[
arXiv:2509.04317v2 Announce Type: replace 
Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo planning with prior knowledge provided by a previously trained policy-value neural network. AlphaZero usually assumes that the environment on which the neural network was trained will not change at test time, which constrains its applicability. In this paper, we analyze the problem of deploying AlphaZero agents in potentially changed test environments and demonstrate how the combination of simple modifications to the standard framework can significantly boost performance, even in settings with a low planning budget available. The code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Common Framework for Autoformalization</title>
<link>https://arxiv.org/abs/2509.09810</link>
<guid>https://arxiv.org/abs/2509.09810</guid>
<content:encoded><![CDATA[
arXiv:2509.09810v2 Announce Type: replace 
Abstract: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</title>
<link>https://arxiv.org/abs/2509.23234</link>
<guid>https://arxiv.org/abs/2509.23234</guid>
<content:encoded><![CDATA[
arXiv:2509.23234v4 Announce Type: replace 
Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. The code is available at https://github.com/ryttry/p-less .
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Transformer: Accelerating model inference via quantum linear algebra</title>
<link>https://arxiv.org/abs/2402.16714</link>
<guid>https://arxiv.org/abs/2402.16714</guid>
<content:encoded><![CDATA[
arXiv:2402.16714v3 Announce Type: replace-cross 
Abstract: Powerful generative artificial intelligence from large language models (LLMs) harnesses extensive computational resources for inference. In this work, we investigate the transformer architecture, a key component of these models, under the lens of fault-tolerant quantum computing. We develop quantum subroutines to construct the building blocks in the transformer, including the self-attention, residual connection with layer normalization, and feed-forward network. As an important subroutine, we show how to efficiently implement the Hadamard product and element-wise functions of matrices on quantum computers. Our algorithm prepares an amplitude encoding of the transformer output, which can be measured for prediction or use in the next layer. We find that the matrix norm of the input sequence plays a dominant role in the quantum complexity. With numerical experiments on open-source LLMs, including for bio-informatics applications, we demonstrate the potential of a quantum speedup for transformer inference in practical regimes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI in Lung Health: Benchmarking Detection and Diagnostic Models Across Multiple CT Scan Datasets</title>
<link>https://arxiv.org/abs/2405.04605</link>
<guid>https://arxiv.org/abs/2405.04605</guid>
<content:encoded><![CDATA[
arXiv:2405.04605v4 Announce Type: replace-cross 
Abstract: Background: Development of artificial intelligence (AI) models for lung cancer screening requires large, well-annotated low-dose computed tomography (CT) datasets and rigorous performance benchmarks. Purpose: To create a reproducible benchmarking resource leveraging the Duke Lung Cancer Screening (DLCS) and multiple public datasets to develop and evaluate models for nodule detection and classification. Materials & Methods: This retrospective study uses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets including LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models were trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the Competition Performance Metric (CPM). For nodule-level classification, we compare five strategies: pretrained models (Models Genesis, Med3D), a self-supervised foundation model (FMCB), and ResNet50 with random initialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with detection-derived candidate patches stratified by confidence. Results: For detection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false positives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external validation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed LUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple datasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90 (LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82), matching or exceeding pretrained/self-supervised baselines. Performance differences reflected dataset label standards. Conclusion: This work establishes a standardized benchmarking resource for lung cancer AI research, supporting model development, validation, and translation. All code, models, and data are publicly released to promote reproducibility.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Evaluation and Benchmarks for Statement Autoformalization</title>
<link>https://arxiv.org/abs/2406.07222</link>
<guid>https://arxiv.org/abs/2406.07222</guid>
<content:encoded><![CDATA[
arXiv:2406.07222v3 Announce Type: replace-cross 
Abstract: Evaluating statement autoformalization, translating natural language mathematics into formal languages like Lean 4, remains a significant challenge, with few metrics, datasets, and standards to robustly measure progress. In this work, we present a comprehensive approach combining improved metrics, robust benchmarks, and systematic evaluation, to fill this gap. First, we introduce BEq+, an automated metric that correlates strongly with human judgment, along with ProofNetVerif, a new dataset for assessing the quality of evaluation metrics, containing 3,752 annotated examples. Second, we develop two new autoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and RLM25, with 619 new pairs of research-level mathematics from six formalization projects. Through systematic experimentation across these benchmarks, we find that current techniques can achieve up to 45.1% accuracy on undergraduate mathematics but struggle with research-level content without proper context. Our work establishes a reliable foundation for evaluating and advancing autoformalization systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster and Simpler Greedy Algorithm for $k$-Median and $k$-Means</title>
<link>https://arxiv.org/abs/2407.11217</link>
<guid>https://arxiv.org/abs/2407.11217</guid>
<content:encoded><![CDATA[
arXiv:2407.11217v3 Announce Type: replace-cross 
Abstract: Clustering problems such as $k$-means and $k$-median are staples of unsupervised learning, and many algorithmic techniques have been developed to tackle their numerous aspects.
  In this paper, we focus on the class of greedy approximation algorithm, that attracted less attention than local-search or primal-dual counterparts. In particular, we study the recursive greedy algorithm developed by Mettu and Plaxton [SIAM J. Comp 2003]. We provide a simplification of the algorithm, allowing for faster implementation, in graph metrics or in Euclidean space, where our algorithm matches or improves the state-of-the-art.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2408.11832</link>
<guid>https://arxiv.org/abs/2408.11832</guid>
<content:encoded><![CDATA[
arXiv:2408.11832v3 Announce Type: replace-cross 
Abstract: The increased use of large language models (LLMs) across a variety of real-world applications calls for automatic tools to check the factual accuracy of their outputs, as LLMs often hallucinate. This is difficult as it requires assessing the factuality of free-form open-domain responses. While there has been a lot of research on this topic, different papers use different evaluation benchmarks and measures, which makes them hard to compare and hampers future progress. To mitigate these issues, we developed OpenFactCheck, a unified framework, with three modules: (i) RESPONSEEVAL, which allows users to easily customize an automatic fact-checking system and to assess the factuality of all claims in an input document using that system, (ii) LLMEVAL, which assesses the overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate automatic fact-checking systems. OpenFactCheck is open-sourced (https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python library (https://pypi.org/project/openfactcheck/) and also as a web service (http://app.openfactcheck.com). A video describing the system is available at https://youtu.be/-i9VKL0HleI.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting</title>
<link>https://arxiv.org/abs/2410.12593</link>
<guid>https://arxiv.org/abs/2410.12593</guid>
<content:encoded><![CDATA[
arXiv:2410.12593v3 Announce Type: replace-cross 
Abstract: The widespread deployment of sensing devices leads to a surge in data for spatio-temporal forecasting applications such as traffic flow, air quality, and wind energy. Although spatio-temporal graph neural networks have achieved success in modeling various static spatio-temporal forecasting scenarios, real-world spatio-temporal data are typically received in a streaming manner, and the network continuously expands with the installation of new sensors. Thus, spatio-temporal forecasting in streaming scenarios faces dual challenges: the inefficiency of retraining models over newly arrived data and the detrimental effects of catastrophic forgetting over long-term history. To address these challenges, we propose a novel prompt tuning-based continuous forecasting method, following two fundamental tuning principles guided by empirical and theoretical analysis: expand and compress, which effectively resolve the aforementioned problems with lightweight tuning parameters. Specifically, we integrate the base spatio-temporal graph neural network with a continuous prompt pool, utilizing stored prompts (i.e., few learnable parameters) in memory, and jointly optimize them with the base spatio-temporal graph neural network. This method ensures that the model sequentially learns from the spatio-temporal data stream to accomplish tasks for corresponding periods. Extensive experimental results on multiple real-world datasets demonstrate the multi-faceted superiority of our method over the state-of-the-art baselines, including effectiveness, efficiency, universality, etc.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Service Level Objectives and System Level Metrics in Large Language Model Serving</title>
<link>https://arxiv.org/abs/2410.14257</link>
<guid>https://arxiv.org/abs/2410.14257</guid>
<content:encoded><![CDATA[
arXiv:2410.14257v2 Announce Type: replace-cross 
Abstract: User experience is a critical factor Large Language Model (LLM) serving systems must consider, where service level objectives (SLOs) considering the experience of individual requests and system level metrics (SLMs) considering the overall system performance are two key performance measures. However, we observe two notable issues in existing metrics: 1) manually delaying the delivery of some tokens can improve SLOs, and 2) actively abandoning requests that do not meet SLOs can improve SLMs, both of which are counterintuitive.
  In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO that aligns with user experience. Based on the SLO, we propose a comprehensive metric framework called smooth goodput, which integrates SLOs and SLMs to reflect the nature of user experience in LLM serving. Through this unified framework, we reassess the performance of different LLM serving systems under multiple workloads. Evaluation results show that our metric framework provides a more comprehensive view of token delivery and request processing, and effectively captures the optimal point of user experience and system performance with different serving strategies.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Objectives for Preference Optimization</title>
<link>https://arxiv.org/abs/2411.06568</link>
<guid>https://arxiv.org/abs/2411.06568</guid>
<content:encoded><![CDATA[
arXiv:2411.06568v3 Announce Type: replace-cross 
Abstract: Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a PO algorithm that significantly outperform existing baselines in an LLM alignment task.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</title>
<link>https://arxiv.org/abs/2412.04233</link>
<guid>https://arxiv.org/abs/2412.04233</guid>
<content:encoded><![CDATA[
arXiv:2412.04233v4 Announce Type: replace-cross 
Abstract: Adaptive cooperation in multi-agent reinforcement learning (MARL) requires policies to express homogeneous, specialised, or mixed behaviours, yet achieving this adaptivity remains a critical challenge. While parameter sharing (PS) is standard for efficient learning, it notoriously suppresses the behavioural diversity required for specialisation. This failure is largely due to cross-agent gradient interference, a problem we find is surprisingly exacerbated by the common practice of coupling agent IDs with observations. Existing remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates -- raising a fundamental question: can shared policies adapt without these intricacies? We propose a solution built on a key insight: an agent-conditioned hypernetwork can generate agent-specific parameters and decouple observation- and agent-conditioned gradients, directly countering the interference from coupling agent IDs with observations. Our resulting method, HyperMARL, avoids the complexities of prior work and empirically reduces policy gradient variance. Across diverse MARL benchmarks (22 scenarios, up to 30 agents), HyperMARL achieves performance competitive with six key baselines while preserving behavioural diversity comparable to non-parameter sharing methods, establishing it as a versatile and principled approach for adaptive MARL. The code is publicly available at https://github.com/KaleabTessera/HyperMARL.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics Context Builders: A Modular Framework for Physical Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2412.08619</link>
<guid>https://arxiv.org/abs/2412.08619</guid>
<content:encoded><![CDATA[
arXiv:2412.08619v3 Announce Type: replace-cross 
Abstract: Physical reasoning remains a significant challenge for Vision-Language Models (VLMs). This limitation arises from an inability to translate learned knowledge into predictions about physical behavior. Although continual fine-tuning can mitigate this issue, it is expensive for large models and impractical to perform repeatedly for every task. This necessitates the creation of modular and scalable ways to teach VLMs about physical reasoning. To that end, we introduce Physics Context Builders (PCBs), a modular framework where specialized smaller VLMs are fine-tuned to generate detailed physical scene descriptions. These can be used as physical contexts to enhance the reasoning capabilities of larger VLMs. PCBs enable the separation of visual perception from reasoning, allowing us to analyze their relative contributions to physical understanding. We perform experiments on CLEVRER and on Falling Tower, a stability detection dataset with both simulated and real-world scenes, to demonstrate that PCBs provide substantial performance improvements, increasing average accuracy by up to 13.8% on complex physical reasoning tasks. Notably, PCBs also show strong Sim2Real transfer, successfully generalizing from simulated training data to real-world scenes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency of Responses and Continuations Generated by Large Language Models on Social Media</title>
<link>https://arxiv.org/abs/2501.08102</link>
<guid>https://arxiv.org/abs/2501.08102</guid>
<content:encoded><![CDATA[
arXiv:2501.08102v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
<link>https://arxiv.org/abs/2501.10484</link>
<guid>https://arxiv.org/abs/2501.10484</guid>
<content:encoded><![CDATA[
arXiv:2501.10484v4 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models</title>
<link>https://arxiv.org/abs/2501.14755</link>
<guid>https://arxiv.org/abs/2501.14755</guid>
<content:encoded><![CDATA[
arXiv:2501.14755v3 Announce Type: replace-cross 
Abstract: Foundation models demand advanced data processing for their vast, multimodal datasets. However, traditional frameworks struggle with the unique complexities of multimodal data. In response, we present Data-Juicer 2.0, a data processing system backed by 100+ data processing operators spanning text, image, video, and audio modalities, supporting more critical tasks including data analysis, synthesis, annotation, and foundation model post-training. With seamless compatibility and dedicated optimization for popular dataset hubs like Hugging Face and computing engines like Ray, it improves upon its predecessor in terms of usability, efficiency, and programmability. It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands. Its new runtime layer offers adaptive execution across diverse scales and environments, abstracting away system complexities. Extensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable performance and scalability, highlighting its capability to efficiently process TB-level data with 10k+ CPU cores. The system is publicly available and has been widely adopted in diverse research fields and real-world products such as Alibaba Cloud PAI. We actively maintain the system and share practical insights to foster research and applications of next-generation foundation models.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redistributing Rewards Across Time and Agents for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.04864</link>
<guid>https://arxiv.org/abs/2502.04864</guid>
<content:encoded><![CDATA[
arXiv:2502.04864v2 Announce Type: replace-cross 
Abstract: Credit assignmen, disentangling each agent's contribution to a shared reward, is a critical challenge in cooperative multi-agent reinforcement learning (MARL). To be effective, credit assignment methods must preserve the environment's optimal policy. Some recent approaches attempt this by enforcing return equivalence, where the sum of distributed rewards must equal the team reward. However, their guarantees are conditional on a learned model's regression accuracy, making them unreliable in practice. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples credit modeling from this constraint. A neural network learns unnormalized contribution scores, while a separate, deterministic normalization step enforces return equivalence by construction. We demonstrate that this method is equivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees the optimal policy is preserved regardless of model accuracy. Empirically, on challenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$ accelerates learning and achieves higher final performance than strong baselines. These results establish our method as an effective solution for the agent-temporal credit assignment problem.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Markovian Discrete Diffusion with Causal Language Models</title>
<link>https://arxiv.org/abs/2502.09767</link>
<guid>https://arxiv.org/abs/2502.09767</guid>
<content:encoded><![CDATA[
arXiv:2502.09767v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation. In this paper, we introduce CaDDi (Causal Discrete Diffusion Model), a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non-Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state-of-the-art discrete diffusion baselines on natural-language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities</title>
<link>https://arxiv.org/abs/2502.12128</link>
<guid>https://arxiv.org/abs/2502.12128</guid>
<content:encoded><![CDATA[
arXiv:2502.12128v4 Announce Type: replace-cross 
Abstract: Generative models are spearheading recent progress in deep learning, showcasing strong promise for trajectory sampling in dynamical systems as well. However, whereas latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns, entity conservation, and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping the traceability of individual entities in a latent system representation, and (2) leveraging the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder enable generative modeling directly in latent space. The core idea of LaM-SLidE is the introduction of identifier representations (IDs) that enable the retrieval of entity properties and entity composition from latent system representations, thus fostering traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spontaneous Giving and Calculated Greed in Language Models</title>
<link>https://arxiv.org/abs/2502.17720</link>
<guid>https://arxiv.org/abs/2502.17720</guid>
<content:encoded><![CDATA[
arXiv:2502.17720v4 Announce Type: replace-cross 
Abstract: Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of "spontaneous giving and calculated greed." Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.06497</link>
<guid>https://arxiv.org/abs/2503.06497</guid>
<content:encoded><![CDATA[
arXiv:2503.06497v3 Announce Type: replace-cross 
Abstract: Ensuring the safety of vision-language models (VLMs) in autonomous driving systems is of paramount importance, yet existing research has largely focused on conventional benchmarks rather than safety-critical evaluation. In this work, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel framework specifically designed to assess the safety cognition capabilities of VLMs within interactive driving scenarios. To address the scalability challenge of data annotation, we introduce ADA (Autonomous Driving Annotation), a semi-automated labeling system, further refined through expert review by professionals with domain-specific knowledge in autonomous driving. To facilitate scalable and consistent evaluation, we also propose an automated assessment pipeline leveraging large language models, which demonstrates over 98% agreement with human expert judgments. In addressing the broader challenge of aligning VLMs with safety cognition in driving environments, we construct SCD-Training, the first large-scale dataset tailored for this task, comprising 324.35K high-quality samples. Through extensive experiments, we show that models trained on SCD-Training exhibit marked improvements not only on SCD-Bench, but also on general and domain-specific benchmarks, offering a new perspective on enhancing safety-aware interactions in vision-language systems for autonomous driving.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Symbolic Model for Space Physics</title>
<link>https://arxiv.org/abs/2503.07994</link>
<guid>https://arxiv.org/abs/2503.07994</guid>
<content:encoded><![CDATA[
arXiv:2503.07994v3 Announce Type: replace-cross 
Abstract: In this study, we unveil a new AI model, termed PhyE2E, to discover physical formulas through symbolic regression. PhyE2E simplifies symbolic regression by decomposing it into sub-problems using the second-order derivatives of an oracle neural network, and employs a transformer model to translate data into symbolic formulas in an end-to-end manner. The resulting formulas are refined through Monte-Carlo Tree Search and Genetic Programming. We leverage a large language model to synthesize extensive symbolic expressions resembling real physics, and train the model to recover these formulas directly from data. A comprehensive evaluation reveals that PhyE2E outperforms existing state-of-the-art approaches, delivering superior symbolic accuracy, precision in data fitting, and consistency in physical units. We deployed PhyE2E to five applications in space physics, including the prediction of sunspot numbers, solar rotational angular velocity, emission line contribution functions, near-Earth plasma pressure, and lunar-tide plasma signals. The physical formulas generated by AI demonstrate a high degree of accuracy in fitting the experimental data from satellites and astronomical telescopes. We have successfully upgraded the formula proposed by NASA in 1993 regarding solar activity, and for the first time, provided the explanations for the long cycle of solar activity in an explicit form. We also found that the decay of near-Earth plasma pressure is proportional to r^2 to Earth, where subsequent mathematical derivations are consistent with satellite data from another independent study. Moreover, we found physical formulas that can describe the relationships between emission lines in the extreme ultraviolet spectrum of the Sun, temperatures, electron densities, and magnetic fields. The formula obtained is consistent with the properties that physicists had previously hypothesized it should possess.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGTRSD &amp; DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text Dataset and Vision Language Foundation Model for Alignment</title>
<link>https://arxiv.org/abs/2503.19311</link>
<guid>https://arxiv.org/abs/2503.19311</guid>
<content:encoded><![CDATA[
arXiv:2503.19311v2 Announce Type: replace-cross 
Abstract: Vision Language Foundation Models based on CLIP architecture for remote sensing primarily rely on short text captions, which often result in incomplete semantic representations. Although longer captions convey richer information, existing models struggle to process them effectively because of limited text-encoding capacity, and there remains a shortage of resources that align remote sensing images with both short text and long text captions. To address this gap, we introduce DGTRSD, a dual-granularity remote sensing image-text dataset, where each image is paired with both a short text caption and a long text description, providing a solid foundation for dual-granularity semantic modeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity curriculum learning framework that combines short text and long text supervision to achieve dual-granularity semantic alignment. Extensive experiments on four typical zero-shot tasks: long text cross-modal retrieval, short text cross-modal retrieval, image classification, and semantic localization demonstrate that DGTRS-CLIP consistently outperforms existing methods across all tasks. The code has been open-sourced and is available at https://github.com/MitsuiChen14/DGTRS.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steiner Traveling Salesman Problem with Quantum Annealing</title>
<link>https://arxiv.org/abs/2504.02388</link>
<guid>https://arxiv.org/abs/2504.02388</guid>
<content:encoded><![CDATA[
arXiv:2504.02388v4 Announce Type: replace-cross 
Abstract: The Steiner Traveling Salesman Problem (STSP) is a variant of the classical Traveling Salesman Problem. The STSP involves incorporating steiner nodes, which are extra nodes not originally part of the required visit set but that can be added to the route to enhance the overall solution and minimize the total travel cost. Given the NP-hard nature of the STSP, we propose a quantum approach to address it. Specifically, we employ quantum annealing using D-Wave's hardware to explore its potential for solving this problem. To enhance computational feasibility, we develop a preprocessing method that effectively reduces the network size. Our experimental results demonstrate that this reduction technique significantly decreases the problem complexity, making the Quadratic Unconstrained Binary Optimization formulation, the standard input for quantum annealers, better suited for existing quantum hardware. Furthermore, the results highlight the potential of quantum annealing as a promising and innovative approach for solving the STSP.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Neural Pruning Law Hypothesis</title>
<link>https://arxiv.org/abs/2504.05349</link>
<guid>https://arxiv.org/abs/2504.05349</guid>
<content:encoded><![CDATA[
arXiv:2504.05349v3 Announce Type: replace-cross 
Abstract: Network pruning is used to reduce inference latency and power consumption in large neural networks. However, most current pruning methods rely on ad-hoc heuristics that are poorly understood. We introduce Hyperflux, a conceptually-grounded pruning method, and use it to study the pruning process. Hyperflux models this process as an interaction between weight flux, the gradient's response to the weight's removal, and network pressure, a global regularization driving weights towards pruning. We postulate properties that arise naturally from our framework and find that the relationship between minimum flux among weights and density follows a power-law equation. Furthermore, we hypothesize the power-law relationship to hold for any effective saliency metric and call this idea the Neural Pruning Law Hypothesis. We validate our hypothesis on several families of pruning methods (magnitude, gradients, $L_0$), providing a potentially unifying property for neural pruning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmegAMP: Targeted AMP Discovery through Biologically Informed Generation</title>
<link>https://arxiv.org/abs/2504.17247</link>
<guid>https://arxiv.org/abs/2504.17247</guid>
<content:encoded><![CDATA[
arXiv:2504.17247v2 Announce Type: replace-cross 
Abstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical challenges such as limited controllability, lack of representations that efficiently model antimicrobial properties, and low experimental hit rates. To address these challenges, we introduce OmegAMP, a framework designed for reliable AMP generation with increased controllability. Its diffusion-based generative model leverages a novel conditioning mechanism to achieve fine-grained control over desired physicochemical properties and to direct generation towards specific activity profiles, including species-specific effectiveness. This is further enhanced by a biologically informed encoding space that significantly improves overall generative performance. Complementing these generative capabilities, OmegAMP leverages a novel synthetic data augmentation strategy to train classifiers for AMP filtering, drastically reducing false positive rates and thereby increasing the likelihood of experimental success. Our in silico experiments demonstrate that OmegAMP delivers state-of-the-art performance across key stages of the AMP discovery pipeline, enabling us to achieve an unprecedented success rate in wet lab experiments. We tested 25 candidate peptides, 24 of them (96%) demonstrated antimicrobial activity, proving effective even against multi-drug resistant strains. Our findings underscore OmegAMP's potential to significantly advance computational frameworks in the fight against antimicrobial resistance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.00812</link>
<guid>https://arxiv.org/abs/2505.00812</guid>
<content:encoded><![CDATA[
arXiv:2505.00812v4 Announce Type: replace-cross 
Abstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plexus: Taming Billion-edge Graphs with 3D Parallel Full-graph GNN Training</title>
<link>https://arxiv.org/abs/2505.04083</link>
<guid>https://arxiv.org/abs/2505.04083</guid>
<content:encoded><![CDATA[
arXiv:2505.04083v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) leverage the connectivity and structure of real-world graphs to learn intricate properties and relationships between nodes. Many real-world graphs exceed the memory capacity of a GPU due to their sheer size, and training GNNs on such graphs requires techniques such as mini-batch sampling to scale. The alternative approach of distributed full-graph training suffers from high communication overheads and load imbalance due to the irregular structure of graphs. We propose a three-dimensional (3D) parallel approach for full-graph training that tackles these issues and scales to billion-edge graphs. In addition, we introduce optimizations such as a double permutation scheme for load balancing, and a performance model to predict the optimal 3D configuration of our parallel implementation -- Plexus. We evaluate Plexus on six different graph datasets and show scaling results on up to 2048 GPUs of Perlmutter, and 1024 GPUs of Frontier. Plexus achieves unprecedented speedups of 2.3-12.5x over prior state of the art, and a reduction in time-to-solution by 5.2-8.7x on Perlmutter and 7.0-54.2x on Frontier.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?</title>
<link>https://arxiv.org/abs/2505.09868</link>
<guid>https://arxiv.org/abs/2505.09868</guid>
<content:encoded><![CDATA[
arXiv:2505.09868v3 Announce Type: replace-cross 
Abstract: Despite its constitutional relevance, the technical ``individual fairness'' criterion has not been operationalized in U.S. state or federal statutes/regulations. We conduct a human subjects experiment to address this gap, evaluating which demographic features are relevant for individual fairness evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude that the individual similarity function should consider age and sex, but it should ignore race.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation</title>
<link>https://arxiv.org/abs/2505.10940</link>
<guid>https://arxiv.org/abs/2505.10940</guid>
<content:encoded><![CDATA[
arXiv:2505.10940v3 Announce Type: replace-cross 
Abstract: Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks. Our code is available in https://github.com/Code2Q/TagCF.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A method for the systematic generation of graph XAI benchmarks via Weisfeiler-Leman coloring</title>
<link>https://arxiv.org/abs/2505.12437</link>
<guid>https://arxiv.org/abs/2505.12437</guid>
<content:encoded><![CDATA[
arXiv:2505.12437v2 Announce Type: replace-cross 
Abstract: Graph neural networks have become the de facto model for learning from structured data. However, the decision-making process of GNNs remains opaque to the end user, which undermines their use in safety-critical applications. Several explainable AI techniques for graphs have been developed to address this major issue. Focusing on graph classification, these explainers identify subgraph motifs that explain predictions. Therefore, a robust benchmarking of graph explainers is required to ensure that the produced explanations are of high quality, i.e., aligned with the GNN's decision process. However, current graph-XAI benchmarks are limited to simplistic synthetic datasets or a few real-world tasks curated by domain experts, hindering rigorous and reproducible evaluation, and consequently stalling progress in the field. To overcome these limitations, we propose a method to automate the construction of graph XAI benchmarks from generic graph classification datasets. Our approach leverages the Weisfeiler-Leman color refinement algorithm to efficiently perform approximate subgraph matching and mine class-discriminating motifs, which serve as proxy ground-truth class explanations. At the same time, we ensure that these motifs can be learned by GNNs because their discriminating power aligns with WL expressiveness. This work also introduces the OpenGraphXAI benchmark suite, which consists of 15 ready-made graph-XAI datasets derived by applying our method to real-world molecular classification datasets. The suite is available to the public along with a codebase to generate over 2,000 additional graph-XAI benchmarks. Finally, we present a use case that illustrates how the suite can be used to assess the effectiveness of a selection of popular graph explainers, demonstrating the critical role of a sufficiently large benchmark collection for improving the significance of experimental results.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Domain Generalization</title>
<link>https://arxiv.org/abs/2505.13519</link>
<guid>https://arxiv.org/abs/2505.13519</guid>
<content:encoded><![CDATA[
arXiv:2505.13519v2 Announce Type: replace-cross 
Abstract: Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic contexts. However, existing domain generalization approaches typically treat domains as discrete or as evolving along a single axis (e.g., time). This oversimplification fails to capture the complex, multidimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variations. We present a principled framework grounded in geometric and algebraic theories, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLio), which enables structure-preserving parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain variation descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets, including remote sensing, scientific documents, and traffic forecasting, demonstrate that our method significantly outperforms existing baselines in both generalization accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[
arXiv:2505.15201v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space</title>
<link>https://arxiv.org/abs/2505.16301</link>
<guid>https://arxiv.org/abs/2505.16301</guid>
<content:encoded><![CDATA[
arXiv:2505.16301v2 Announce Type: replace-cross 
Abstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of atomistic systems, but its reliance on sequential numerical integration limits simulation efficiency. We present a novel neural network architecture, MDtrajNet, and a pre-trained foundational model, MDtrajNet-1, that directly generates MD trajectories across chemical space, bypassing force calculations and integration. This approach accelerates simulations by up to two orders of magnitude compared to traditional MD, even those enhanced by machine-learning interatomic potentials. MDtrajNet combines equivariant neural networks with a transformer-based architecture to achieve strong accuracy and transferability in predicting long-time trajectories. Remarkably, the errors of the trajectories generated by MDtrajNet-1 for various known and unseen molecular systems are close to those of the conventional ab initio MD. The architecture's flexible design supports diverse application scenarios, including different statistical ensembles, boundary conditions, and interaction types. By overcoming the intrinsic speed barrier of conventional MD, MDtrajNet opens new frontiers in efficient and scalable atomistic simulations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.16368</link>
<guid>https://arxiv.org/abs/2505.16368</guid>
<content:encoded><![CDATA[
arXiv:2505.16368v2 Announce Type: replace-cross 
Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Risk Assessments for Offensive Cybersecurity Agents</title>
<link>https://arxiv.org/abs/2505.18384</link>
<guid>https://arxiv.org/abs/2505.18384</guid>
<content:encoded><![CDATA[
arXiv:2505.18384v4 Announce Type: replace-cross 
Abstract: Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts</title>
<link>https://arxiv.org/abs/2505.19028</link>
<guid>https://arxiv.org/abs/2505.19028</guid>
<content:encoded><![CDATA[
arXiv:2505.19028v4 Announce Type: replace-cross 
Abstract: Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Augmented Online Bipartite Fractional Matching</title>
<link>https://arxiv.org/abs/2505.19252</link>
<guid>https://arxiv.org/abs/2505.19252</guid>
<content:encoded><![CDATA[
arXiv:2505.19252v2 Announce Type: replace-cross 
Abstract: Online bipartite matching is a fundamental problem in online optimization, extensively studied both in its integral and fractional forms due to its theoretical significance and practical applications, such as online advertising and resource allocation. Motivated by recent progress in learning-augmented algorithms, we study online bipartite fractional matching when the algorithm is given advice in the form of a suggested matching in each iteration. We develop algorithms for both the vertex-weighted and unweighted variants that provably dominate the naive "coin flip" strategy of randomly choosing between the advice-following and advice-free algorithms. Moreover, our algorithm for the vertex-weighted setting extends to the AdWords problem under the small bids assumption, yielding a significant improvement over the seminal work of Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our positive results, we establish a hardness bound on the robustness-consistency tradeoff that is attainable by any algorithm. We empirically validate our algorithms through experiments on synthetic and real-world data.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2505.20249</link>
<guid>https://arxiv.org/abs/2505.20249</guid>
<content:encoded><![CDATA[
arXiv:2505.20249v2 Announce Type: replace-cross 
Abstract: Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Kernel Function for Fast Angle Testing</title>
<link>https://arxiv.org/abs/2505.20274</link>
<guid>https://arxiv.org/abs/2505.20274</guid>
<content:encoded><![CDATA[
arXiv:2505.20274v2 Announce Type: replace-cross 
Abstract: In this paper, we study the angle testing problem in the context of similarity search in high-dimensional Euclidean spaces and propose two projection-based probabilistic kernel functions, one designed for angle comparison and the other for angle thresholding. Unlike existing approaches that rely on random projection vectors drawn from Gaussian distributions, our approach leverages reference angles and employs a deterministic structure for the projection vectors. Notably, our kernel functions do not require asymptotic assumptions, such as the number of projection vectors tending to infinity, and can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions. We apply the proposed kernel function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared to the widely-used graph-based search algorithm HNSW.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.21717</link>
<guid>https://arxiv.org/abs/2505.21717</guid>
<content:encoded><![CDATA[
arXiv:2505.21717v5 Announce Type: replace-cross 
Abstract: We present LrcSSM, a $\textit{non-linear}$ recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing its Jacobian matrix to be diagonal, the full sequence can be solved in parallel, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Importantly, the diagonal Jacobian structure of our model results in no performance loss compared to the original model with dense Jacobian, and the approach can be generalized to other non-linear recurrent models, demonstrating broader applicability. On a suite of long-range forecasting tasks, we demonstrate that LrcSSM outperforms Transformers, LRU, S5, and Mamba.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking</title>
<link>https://arxiv.org/abs/2505.23117</link>
<guid>https://arxiv.org/abs/2505.23117</guid>
<content:encoded><![CDATA[
arXiv:2505.23117v2 Announce Type: replace-cross 
Abstract: In the era of large-scale training, model merging has evolved into a tool for creating multitasking models efficiently. It enables the knowledge of models to be fused, without the need for heavy computation as required in traditional multitask learning. Existing merging methods often assume that entries at identical positions in weight matrices serve the same function, enabling straightforward entry-wise comparison and merging. However, this assumption overlooks the complexity of finetuned neural networks, where neurons may develop distinct feature compositions, making direct entry-wise merging problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective approach that leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, where entry-wise merging becomes possible. We showcase the effectiveness of DRM across various settings ranging from smaller encoder-based such as ViT and DeBERTa, encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B. Our experimental results show that DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings. Moreover, our analysis reveals renormalization as the crucial component for creating a robust and even joint space for merging, significantly contributing to the method's performance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2506.00635</link>
<guid>https://arxiv.org/abs/2506.00635</guid>
<content:encoded><![CDATA[
arXiv:2506.00635v2 Announce Type: replace-cross 
Abstract: Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Robust Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2506.01183</link>
<guid>https://arxiv.org/abs/2506.01183</guid>
<content:encoded><![CDATA[
arXiv:2506.01183v2 Announce Type: replace-cross 
Abstract: This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner</title>
<link>https://arxiv.org/abs/2506.03595</link>
<guid>https://arxiv.org/abs/2506.03595</guid>
<content:encoded><![CDATA[
arXiv:2506.03595v2 Announce Type: replace-cross 
Abstract: The recent success of Shampoo in the AlgoPerf contest has sparked renewed interest in Kronecker-factorization-based optimization algorithms for training neural networks. Despite its success, Shampoo relies heavily on several heuristics such as learning rate grafting and stale preconditioning to achieve performance at-scale. These heuristics increase algorithmic complexity, necessitate further hyperparameter tuning, and lack theoretical justification. This paper investigates these heuristics from the angle of Frobenius norm approximation to full-matrix Adam and decouples the preconditioner's eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates the staleness and mis-scaling of the preconditioner's eigenvalues and how correcting the eigenvalues directly eliminates the need for learning rate grafting. To manage the error induced by infrequent eigenbasis computations, we propose an adaptive criterion for determining the eigenbasis computation frequency motivated by terminating a warm-started QR algorithm. This criterion decouples the update frequency of different preconditioner matrices and enables us to investigate the impact of approximation error on convergence. These practical techniques offer a principled angle towards removing Shampoo's heuristics and developing improved Kronecker-factorization-based training algorithms.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO</title>
<link>https://arxiv.org/abs/2506.07464</link>
<guid>https://arxiv.org/abs/2506.07464</guid>
<content:encoded><![CDATA[
arXiv:2506.07464v3 Announce Type: replace-cross 
Abstract: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) has still been less studyed. In this paper, we explore GRPO and identify two problems that deteriorate the effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function into a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as the clipping and min functions. It directly aligns the model with advantages, providing guidance to prefer better ones. The difficulty-aware data augmentation strategy augments input prompts/videos to locate the difficulty of samples at solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Teachers of Test Time Scaling</title>
<link>https://arxiv.org/abs/2506.08388</link>
<guid>https://arxiv.org/abs/2506.08388</guid>
<content:encoded><![CDATA[
arXiv:2506.08388v3 Announce Type: replace-cross 
Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply "connect-the-dots" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework. Code available at: https://github.com/SakanaAI/RLT
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era</title>
<link>https://arxiv.org/abs/2506.09755</link>
<guid>https://arxiv.org/abs/2506.09755</guid>
<content:encoded><![CDATA[
arXiv:2506.09755v2 Announce Type: replace-cross 
Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by foundation model-based agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of foundation model-based multi-agent collaboration. We propose an ontological framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss challenges and opportunities of ID 4.0, including perspectives on data foundations, agent collaboration mechanisms, and the formulation of design problems and objectives. In sum, these insights provide a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing the growing complexity of engineering design.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[
arXiv:2506.12484v4 Announce Type: replace-cross 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabArena: A Living Benchmark for Machine Learning on Tabular Data</title>
<link>https://arxiv.org/abs/2506.16791</link>
<guid>https://arxiv.org/abs/2506.16791</guid>
<content:encoded><![CDATA[
arXiv:2506.16791v3 Announce Type: replace-cross 
Abstract: With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning. We observe that some deep learning models are overrepresented in cross-model ensembles due to validation set overfitting, and we encourage model developers to address this issue. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and NVIDIA Data Center GPUs</title>
<link>https://arxiv.org/abs/2507.00418</link>
<guid>https://arxiv.org/abs/2507.00418</guid>
<content:encoded><![CDATA[
arXiv:2507.00418v3 Announce Type: replace-cross 
Abstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt), performance, and hardware scalability against NVIDIA A100 GPUs (in 4x and 8x configurations) within the National Research Platform (NRP) ecosystem. A total of 12 open-source LLMs, ranging from 124 million to 70 billion parameters, are served using the vLLM framework. Our analysis reveals that QAic achieves competitive energy efficiency with advantages on specific models while enabling more granular hardware allocation: some 70B models operate on as few as 1 QAic card versus 8 A100 GPUs required, with 20x lower power consumption (148W vs 2,983W). For smaller models, single QAic devices achieve up to 35x lower power consumption compared to our 4-GPU A100 configuration (36W vs 1,246W). The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for energy-constrained and resource-efficient HPC deployments within the National Research Platform (NRP).
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many LLMs Are More Utilitarian Than One</title>
<link>https://arxiv.org/abs/2507.00814</link>
<guid>https://arxiv.org/abs/2507.00814</guid>
<content:encoded><![CDATA[
arXiv:2507.00814v2 Announce Type: replace-cross 
Abstract: Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the Boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed either reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the Boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</title>
<link>https://arxiv.org/abs/2507.01939</link>
<guid>https://arxiv.org/abs/2507.01939</guid>
<content:encoded><![CDATA[
arXiv:2507.01939v3 Announce Type: replace-cross 
Abstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Mamba</title>
<link>https://arxiv.org/abs/2507.06204</link>
<guid>https://arxiv.org/abs/2507.06204</guid>
<content:encoded><![CDATA[
arXiv:2507.06204v2 Announce Type: replace-cross 
Abstract: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A PBN-RL-XAI Framework for Discovering a "Hit-and-Run" Therapeutic Strategy in Melanoma</title>
<link>https://arxiv.org/abs/2507.10136</link>
<guid>https://arxiv.org/abs/2507.10136</guid>
<content:encoded><![CDATA[
arXiv:2507.10136v5 Announce Type: replace-cross 
Abstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent's control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this ''hit-and-run" intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow matching for reaction pathway generation</title>
<link>https://arxiv.org/abs/2507.10530</link>
<guid>https://arxiv.org/abs/2507.10530</guid>
<content:encoded><![CDATA[
arXiv:2507.10530v3 Announce Type: replace-cross 
Abstract: Elucidating reaction mechanisms hinges on efficiently generating transition states (TSs), products, and complete reaction networks. Recent generative models, such as diffusion models for TS sampling and sequence-based architectures for product generation, offer faster alternatives to quantum-chemistry searches. But diffusion models remain constrained by their stochastic differential equation (SDE) dynamics, which suffer from inefficiency and limited controllability. We show that flow matching, a deterministic ordinary differential (ODE) formulation, can replace SDE-based diffusion for molecular and reaction generation. We introduce MolGEN, a conditional flow-matching framework that learns an optimal transport path to transport Gaussian priors to target chemical distributions. On benchmarks used by TSDiff and OA-ReactDiff, MolGEN surpasses TS geometry accuracy and barrier-height prediction while reducing sampling to sub-second inference. MolGEN also supports open-ended product generation with competitive top-k accuracy and avoids mass/electron-balance violations common to sequence models. In a realistic test on the $\gamma$-ketohydroperoxide decomposition network, MolGEN yields higher fractions of valid and intended TSs with markedly fewer quantum-chemistry evaluations than string-based baselines. These results demonstrate that deterministic flow matching provides a unified, accurate, and computationally efficient foundation for molecular generative modeling, signaling that flow matching is the future for molecular generation across chemistry.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs</title>
<link>https://arxiv.org/abs/2507.14785</link>
<guid>https://arxiv.org/abs/2507.14785</guid>
<content:encoded><![CDATA[
arXiv:2507.14785v2 Announce Type: replace-cross 
Abstract: The complexity and interconnectivity of entities involved in money laundering demand investigative reasoning over graph-structured data. This paper explores the use of large language models (LLMs) as reasoning engines over localized subgraphs extracted from a financial knowledge graph. We propose a lightweight pipeline that retrieves k-hop neighborhoods around entities of interest, serializes them into structured text, and prompts an LLM via few-shot in-context learning to assess suspiciousness and generate justifications. Using synthetic anti-money laundering (AML) scenarios that reflect common laundering behaviors, we show that LLMs can emulate analyst-style logic, highlight red flags, and provide coherent explanations. While this study is exploratory, it illustrates the potential of LLM-based graph reasoning in AML and lays groundwork for explainable, language-driven financial crime analytics.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation</title>
<link>https://arxiv.org/abs/2507.17937</link>
<guid>https://arxiv.org/abs/2507.17937</guid>
<content:encoded><![CDATA[
arXiv:2507.17937v3 Announce Type: replace-cross 
Abstract: Generative AI systems for music and video commonly use text-based filters to prevent the regurgitation of copyrighted material. We expose a fundamental flaw in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization. The APT attack replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., "mom's spaghetti" becomes "Bob's confetti"), preserving acoustic structure while altering meaning; we identify high-fidelity phonetic matches using CMU pronouncing dictionary. We demonstrate that leading Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking melodic and rhythmic similarity to their copyrighted originals when prompted with these altered lyrics. More surprisingly, this vulnerability extends across modalities. When prompted with phonetically modified lyrics from a song, a Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the original music video-including specific settings and character archetypes-despite the absence of any visual cues in the prompt. Our findings reveal that models memorize deep, structural patterns tied to acoustics, not just verbatim text. This phonetic-to-visual leakage represents a critical vulnerability in transcript-conditioned generative models, rendering simple copyright filters ineffective and raising urgent concerns about the secure deployment of multimodal AI systems. Demo examples are available at our project page (https://jrohsc.github.io/music_attack/).
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots</title>
<link>https://arxiv.org/abs/2507.22664</link>
<guid>https://arxiv.org/abs/2507.22664</guid>
<content:encoded><![CDATA[
arXiv:2507.22664v2 Announce Type: replace-cross 
Abstract: The presence of autonomous systems is growing at a fast pace and it is impacting many aspects of our lives. Designed to learn and act independently, these systems operate and perform decision-making without human intervention. However, they lack the ability to incorporate users' ethical preferences, which are unique for each individual in society and are required to personalize the decision-making processes. This reduces user trust and prevents autonomous systems from behaving according to the moral beliefs of their end-users. When multiple systems interact with differing ethical preferences, they must negotiate to reach an agreement that satisfies the ethical beliefs of all the parties involved and adjust their behavior consequently. To address this challenge, this paper proposes RobEthiChor, an approach that enables autonomous systems to incorporate user ethical preferences and contextual factors into their decision-making through ethics-based negotiation. RobEthiChor features a domain-agnostic reference architecture for designing autonomous systems capable of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an implementation of RobEthiChor within the Robot Operating System (ROS), which can be deployed on robots to provide them with ethics-based negotiation capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real robots and ran scenarios where a pair of robots negotiate upon resource contention. Experimental results demonstrate the feasibility and effectiveness of the system in realizing ethics-based negotiation. RobEthiChor allowed robots to reach an agreement in more than 73% of the scenarios with an acceptable negotiation time (0.67s on average). Experiments also demonstrate that the negotiation approach implemented in RobEthiChor is scalable.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI</title>
<link>https://arxiv.org/abs/2509.00398</link>
<guid>https://arxiv.org/abs/2509.00398</guid>
<content:encoded><![CDATA[
arXiv:2509.00398v4 Announce Type: replace-cross 
Abstract: This study provides an in_depth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence (AI) technologies and proposes a comprehensive framework for their systematic evaluation. While generative AI, such as ChatGPT, demonstrates remarkable innovative potential, it simultaneously raises ethical and social concerns, including bias, harmfulness, copyright infringement, privacy violations, and hallucination. Current AI evaluation methodologies, which mainly focus on performance and accuracy, are insufficient to address these multifaceted issues. Thus, this study emphasizes the need for new human_centered criteria that also reflect social impact. To this end, it identifies key dimensions for evaluating the ethics and trustworthiness of generative AI_fairness, transparency, accountability, safety, privacy, accuracy, consistency, robustness, explainability, copyright and intellectual property protection, and source traceability and develops detailed indicators and assessment methodologies for each. Moreover, it provides a comparative analysis of AI ethics policies and guidelines in South Korea, the United States, the European Union, and China, deriving key approaches and implications from each. The proposed framework applies across the AI lifecycle and integrates technical assessments with multidisciplinary perspectives, thereby offering practical means to identify and manage ethical risks in real_world contexts. Ultimately, the study establishes an academic foundation for the responsible advancement of generative AI and delivers actionable insights for policymakers, developers, users, and other stakeholders, supporting the positive societal contributions of AI technologies.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.09349</link>
<guid>https://arxiv.org/abs/2509.09349</guid>
<content:encoded><![CDATA[
arXiv:2509.09349v2 Announce Type: replace-cross 
Abstract: Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behaviour classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviours such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioural analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion</title>
<link>https://arxiv.org/abs/2509.10266</link>
<guid>https://arxiv.org/abs/2509.10266</guid>
<content:encoded><![CDATA[
arXiv:2509.10266v2 Announce Type: replace-cross 
Abstract: Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction</title>
<link>https://arxiv.org/abs/2509.10516</link>
<guid>https://arxiv.org/abs/2509.10516</guid>
<content:encoded><![CDATA[
arXiv:2509.10516v2 Announce Type: replace-cross 
Abstract: The increasing digitalization of education presents unprecedented opportunities for data-driven personalization, but it also introduces significant challenges to student data privacy. Conventional recommender systems rely on centralized data, a paradigm often incompatible with modern data protection regulations. A novel privacy-preserving recommender system is proposed and evaluated to address this critical issue using Federated Learning (FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered features from the large-scale ASSISTments educational dataset. A rigorous comparative analysis of federated aggregation strategies was conducted, identifying FedProx as a significantly more stable and effective method for handling heterogeneous student data than the standard FedAvg baseline. The optimized federated model achieves a high-performance F1-Score of 76.28%, corresponding to 92% of the performance of a powerful, centralized XGBoost model. These findings validate that a federated approach can provide highly effective content recommendations without centralizing sensitive student data. Consequently, our work presents a viable and robust solution to the personalization-privacy dilemma in modern educational platforms.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI_INFN Platform: Artificial Intelligence Development in the Cloud</title>
<link>https://arxiv.org/abs/2509.22117</link>
<guid>https://arxiv.org/abs/2509.22117</guid>
<content:encoded><![CDATA[
arXiv:2509.22117v2 Announce Type: replace-cross 
Abstract: Machine Learning (ML) is profoundly reshaping the way researchers create, implement, and operate data-intensive software. Its adoption, however, introduces notable challenges for computing infrastructures, particularly when it comes to coordinating access to hardware accelerators across development, testing, and production environments. The INFN initiative AI_INFN (Artificial Intelligence at INFN) seeks to promote the use of ML methods across various INFN research scenarios by offering comprehensive technical support, including access to AI-focused computational resources. Leveraging the INFN Cloud ecosystem and cloud-native technologies, the project emphasizes efficient sharing of accelerator hardware while maintaining the breadth of the Institute's research activities. This contribution describes the deployment and commissioning of a Kubernetes-based platform designed to simplify GPU-powered data analysis workflows and enable their scalable execution on heterogeneous distributed resources. By integrating offloading mechanisms through Virtual Kubelet and the InterLink API, the platform allows workflows to span multiple resource providers, from Worldwide LHC Computing Grid sites to high-performance computing centers like CINECA Leonardo. We will present preliminary benchmarks, functional tests, and case studies, demonstrating both performance and integration outcomes.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Mixing Additive Networks</title>
<link>https://arxiv.org/abs/2509.23923</link>
<guid>https://arxiv.org/abs/2509.23923</guid>
<content:encoded><![CDATA[
arXiv:2509.23923v2 Announce Type: replace-cross 
Abstract: We introduce GMAN, a flexible, interpretable, and expressive framework that extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse time-series data. GMAN represents each time-dependent trajectory as a directed graph and applies an enriched, more expressive GNAN to each graph. It allows users to control the interpretability-expressivity trade-off by grouping features and graphs to encode priors, and it provides feature, node, and graph-level interpretability. On real-world datasets, including mortality prediction from blood tests and fake-news detection, GMAN outperforms strong non-interpretable black-box baselines while delivering actionable, domain-aligned explanations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCLF -- Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks</title>
<link>https://arxiv.org/abs/2509.25233</link>
<guid>https://arxiv.org/abs/2509.25233</guid>
<content:encoded><![CDATA[
arXiv:2509.25233v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) is a distributed machine learning technique that preserves data privacy by sharing only the trained parameters instead of the client data. This makes FL ideal for highly dynamic, heterogeneous, and time-critical applications, in particular, the Internet of Vehicles (IoV) networks. However, FL encounters considerable challenges in such networks owing to the high data and device heterogeneity. To address these challenges, we propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which introduces calibrated loss as a utility in the participant selection process and a feedback control mechanism to dynamically adjust the sampling frequency of the clients. The envisaged approach (a) enhances the overall model accuracy in case of highly heterogeneous data and (b) optimizes the resource utilization for resource constrained IoV networks, thereby leading to increased efficiency in the FL process. We evaluated FedCLF vis-\`a-vis baseline models, i.e., FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity. Our results depict that FedCLF significantly outperforms the baseline models by up to a 16% improvement in high data heterogeneity-related scenarios with improved efficiency via reduced sampling frequency.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications</title>
<link>https://arxiv.org/abs/2510.01850</link>
<guid>https://arxiv.org/abs/2510.01850</guid>
<content:encoded><![CDATA[
arXiv:2510.01850v3 Announce Type: replace-cross 
Abstract: To effectively process impulse noise for narrowband powerline communications (NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic asynchronous impulsive noise (APIN) is a critical task. However, existing mathematical noise generative models only capture part of the characteristics of noise. In this study, we propose a novel generative adversarial network (GAN) called noise generation GAN (NGGAN) that learns the complicated characteristics of practically measured noise samples for data synthesis. To closely match the statistics of complicated noise over the NB-PLC systems, we measured the NB-PLC noise via the analog coupling and bandpass filtering circuits of a commercial NB-PLC modem to build a realistic dataset. To train NGGAN, we adhere to the following principles: 1) we design the length of input signals that the NGGAN model can fit to facilitate cyclostationary noise generation; 2) the Wasserstein distance is used as a loss function to enhance the similarity between the generated noise and training data; and 3) to measure the similarity performances of GAN-based models based on the mathematical and practically measured datasets, we conduct both quantitative and qualitative analyses. The training datasets include: 1) a piecewise spectral cyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter; and 3) practical measurements from NB-PLC systems. Simulation results demonstrate that the generated noise samples from the proposed NGGAN are highly close to the real noise samples. The principal component analysis (PCA) scatter plots and Fr\'echet inception distance (FID) analysis have shown that NGGAN outperforms other GAN-based models by generating noise samples with superior fidelity and higher diversity.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups</title>
<link>https://arxiv.org/abs/2510.13852</link>
<guid>https://arxiv.org/abs/2510.13852</guid>
<content:encoded><![CDATA[
arXiv:2510.13852v2 Announce Type: replace-cross 
Abstract: Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curiosity-driven RL for symbolic equation solving</title>
<link>https://arxiv.org/abs/2510.17022</link>
<guid>https://arxiv.org/abs/2510.17022</guid>
<content:encoded><![CDATA[
arXiv:2510.17022v2 Announce Type: replace-cross 
Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed contrastive learning can solve linear equations in one variable. We show model-free PPO \cite{schulman2017proximal} augmented with curiosity-based exploration and graph-based actions can solve nonlinear equations such as those involving radicals, exponentials, and trig functions. Our work suggests curiosity-based exploration may be useful for general symbolic reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency</title>
<link>https://arxiv.org/abs/2510.18905</link>
<guid>https://arxiv.org/abs/2510.18905</guid>
<content:encoded><![CDATA[
arXiv:2510.18905v2 Announce Type: replace-cross 
Abstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly calibrates accuracy, cost, and latency within a unified decision space, enabling constraints-aware inference scaling. Using Monte Carlo simulations across three representative scenarios and nine simulated large language models, we evaluate four optimization methods to address the 3D multi-objective optimization (MOO) problem. Framing inference scaling in MOO shapes a feasible space that 1D and 2D optimizations fail to capture, enabling environmentadaptive selection of the inference scaling k. Results show that knee-point optimization achieves the best balance, while accuracy-maximization remains favorable when precision is prioritized. The framework establishes a theoretical foundation for deployment-aware inference scaling across diverse operational contexts.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay</title>
<link>https://arxiv.org/abs/2506.05316</link>
<guid>https://arxiv.org/abs/2506.05316</guid>
<content:encoded><![CDATA[
<div> adaptive difficulty, online data selection, rollout replay, data efficiency, reinforcement learning

Summary:
The paper introduces two techniques to improve data efficiency in reinforcement learning fine-tuning of large language models (LLMs). The first technique is difficulty-targeted online data selection, which prioritizes questions of moderate difficulty to yield informative learning signals. An attention-based framework efficiently estimates adaptive difficulty by using a small reference set of questions. The second technique is rollout replay, inspired by experience replay in traditional RL, which reuses recent rollouts to reduce computation while maintaining stable updates. Experimental results across six LLM-dataset combinations show that the proposed method reduces fine-tuning time by 23% to 62% while achieving the same level of performance as the original GRPO algorithm. The code for the method is available on GitHub. <br /><br />Summary: <div>
arXiv:2506.05316v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. Our code is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</title>
<link>https://arxiv.org/abs/2508.06041</link>
<guid>https://arxiv.org/abs/2508.06041</guid>
<content:encoded><![CDATA[
<div> dynamic precision, large language models, runtime constraints, mixed-precision, memory-efficient

Summary:<br />
The article proposes a novel approach called DP-LLM for handling queries for on-device large language models with varying runtime constraints. The approach leverages multi-scale quantization and dynamically assigns precision to each layer based on input values. This allows for memory-efficient runtime model adaptation and superior performance-latency trade-off compared to prior approaches. The sensitivity of each layer dynamically changes across decoding steps, leading to optimized precision assignments. Experimental results across multiple models and benchmarks demonstrate the effectiveness of DP-LLM in achieving better performance while considering latency constraints. The approach outperforms previous methods and shows promising results for future research in optimizing large language models for on-device applications. 
<br /><br />Summary: <div>
arXiv:2508.06041v3 Announce Type: replace-cross 
Abstract: How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding steps. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2510.01268</link>
<guid>https://arxiv.org/abs/2510.01268</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, language models, log probabilities, AdaDetectGPT, statistical guarantees

Summary:<br />
The study focuses on differentiating human-authored text from text generated by large language models (LLMs). Existing detectors rely on log probabilities from LLMs, but this approach may not be optimal. In response, the novel classifier AdaDetectGPT adapts by learning a witness function from training data to improve detector performance. The classifier offers statistical guarantees on its true positive rate, false positive rate, true negative rate, and false negative rate. Numerical studies demonstrate that AdaDetectGPT consistently enhances the state-of-the-art method across various datasets and LLMs, with improvements of up to 37%. A Python implementation of AdaDetectGPT is available on GitHub for further exploration and implementation. <div>
arXiv:2510.01268v3 Announce Type: replace-cross 
Abstract: We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 37\%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-level Language Modeling by Learning Predictive Context Embeddings</title>
<link>https://arxiv.org/abs/2510.20280</link>
<guid>https://arxiv.org/abs/2510.20280</guid>
<content:encoded><![CDATA[
<div> token-level prediction, semantic structures, contextual relationships, ContextLM, language modeling

Summary:
ContextLM introduces a next-context prediction mechanism to enhance large language models' pretraining. This new framework aims to improve the model's ability to capture higher-level semantic structures and long-range contextual relationships by training it to predict multi-token contexts. By leveraging error signals from future token chunks, ContextLM achieves better performance in perplexity and downstream tasks without compromising token-by-token evaluation. Experiments on GPT2 and Pythia models with 1.5B parameters show consistent enhancements in language modeling and task performance. The next-context prediction method proves to be scalable and efficient, leading to better long-range coherence and improved attention allocation with minimal computational overhead. <div>
arXiv:2510.20280v2 Announce Type: replace-cross 
Abstract: Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following. However, the token-level prediction limits the model's capacity to capture higher-level semantic structures and long-range contextual relationships. To overcome this limitation, we introduce \textbf{ContextLM}, a framework that augments standard pretraining with an inherent \textbf{next-context prediction} objective. This mechanism trains the model to learn predictive representations of multi-token contexts, leveraging error signals derived from future token chunks. Crucially, ContextLM achieves this enhancement while remaining fully compatible with the standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity). Extensive experiments on the GPT2 and Pythia model families, scaled up to $1.5$B parameters, show that ContextLM delivers consistent improvements in both perplexity and downstream task performance. Our analysis indicates that next-context prediction provides a scalable and efficient pathway to stronger language modeling, yielding better long-range coherence and more effective attention allocation with minimal computational overhead.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</title>
<link>https://arxiv.org/abs/2510.23691</link>
<guid>https://arxiv.org/abs/2510.23691</guid>
<content:encoded><![CDATA[
<div> agent, game, pre-training, multimodal data, generalist

Summary:
Game-TARS is a generalist game agent trained using a unified action space based on native keyboard-mouse inputs, allowing for continual pre-training across various game domains. The agent, pre-trained on over 500B tokens with diverse data, incorporates techniques like a continual loss decay and Sparse-Thinking strategy for efficient reasoning. Game-TARS outperforms previous models in Minecraft tasks, exhibits human-like performance in unseen web 3D games, and surpasses other models in FPS benchmarks. The use of a scalable action space sustains improvements across different game and multimodal data, showcasing the agent's general computer-use abilities. This approach highlights the promise of simple, scalable action representations combined with large-scale pre-training for developing versatile game agents. 

<br /><br />Summary: <div>
arXiv:2510.23691v1 Announce Type: new 
Abstract: We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI and the Decentering of Disciplinary Creativity</title>
<link>https://arxiv.org/abs/2510.23734</link>
<guid>https://arxiv.org/abs/2510.23734</guid>
<content:encoded><![CDATA[
<div> Artificial Intelligence, scientific problem-solving, disciplinary creativity, creativity, mathematics  
Summary:  
- The paper explores the impact of artificial intelligence on scientific problem-solving, focusing on its effect on disciplinary creativity.  
- It distinguishes between creative approaches and creative products and introduces the concept of disciplinary creativity.  
- Through two cases in mathematics, the author demonstrates how computation can enhance disciplinary creativity, but certain AI approaches may potentially displace it.  
- This displacement has the potential to alter the value of scientific pursuits and could possibly diminish its significance.  
- The study raises concerns about the role of AI in scientific creativity and highlights the importance of preserving disciplinary expertise in problem-solving processes.  
<br /><br />Summary: <div>
arXiv:2510.23734v1 Announce Type: new 
Abstract: This paper examines the role of artificial intelligence in scientific problem-solving, with a focus on its implications for disciplinary creativity. Drawing on recent work in the philosophy of creativity, I distinguish between creative approaches and creative products, and introduce the concept of disciplinary creativity -the creative application of discipline-specific expertise to a valued problem within that field. Through two cases in mathematics, I show that while computation can extend disciplinary creativity, certain approaches involving AI can serve to displace it. This displacement has the potential to alter (and, perhaps, diminish) the value of scientific pursuit.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability</title>
<link>https://arxiv.org/abs/2510.23744</link>
<guid>https://arxiv.org/abs/2510.23744</guid>
<content:encoded><![CDATA[
<div> ME-POMDPs, discrete model uncertainty, robust policy, AB-POMDPs, multi-environment setting  
Summary:  
Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs by incorporating discrete model uncertainty, allowing for a set of POMDPs with varying models but shared state, action, and observation spaces. The goal is to find a policy robust against any choice of POMDP within the set. This concept is expanded to adversarial-belief POMDPs (AB-POMDPs), allowing for sets of initial beliefs. Algorithms are devised to compute robust policies for AB-POMDPs, enabling solutions for ME-POMDPs. It is shown that any ME-POMDP can be reduced to a simpler form while maintaining optimal policies. The study presents both exact and approximate algorithms for computing robust policies in the multi-environment setting, demonstrating their effectiveness in solving standard POMDP benchmarks extended to this framework. <br /><br />Summary: <div>
arXiv:2510.23744v1 Announce Type: new 
Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the same state, action, and observation spaces, but may arbitrarily vary in their transition, observation, and reward models. Such models arise, for instance, when multiple domain experts disagree on how to model a problem. The goal is to find a single policy that is robust against any choice of POMDP within the set, i.e., a policy that maximizes the worst-case reward across all POMDPs. We generalize and expand on existing work in the following way. First, we show that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its transition and reward functions or only in its observation and reward functions, while preserving (optimal) policies. We then devise exact and approximate (point-based) algorithms to compute robust policies for AB-POMDPs, and thus ME-POMDPs. We demonstrate that we can compute policies for standard POMDP benchmarks extended to the multi-environment setting.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra</title>
<link>https://arxiv.org/abs/2510.23746</link>
<guid>https://arxiv.org/abs/2510.23746</guid>
<content:encoded><![CDATA[
<div> framework, transformer model, tandem mass spectrometry, molecular structure generation, test-time tuning

Summary:
The article introduces a framework that enhances a pre-trained transformer model for end-to-end de novo molecular structure generation from tandem mass spectra and molecular formulae. This approach eliminates the need for manual annotations and intermediate steps, making it easier to identify unknown compounds. The framework outperforms the current state-of-the-art approach DiffMS on popular benchmarks NPLIB1 and MassSpecGym. Test-time tuning on experimental spectra allows for dynamic adaptation to novel spectra, resulting in a 62% relative performance gain on MassSpecGym compared to conventional fine-tuning. Even when predictions deviate from the ground truth, the generated molecular candidates remain structurally accurate, providing valuable guidance for human interpretation and more reliable identification. <div>
arXiv:2510.23746v1 Announce Type: new 
Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in crucial fields such as metabolomics, natural product discovery and environmental analysis. However, current methods rely on database matching from previously observed molecules, or on multi-step pipelines that require intermediate fragment or fingerprint prediction. This makes finding the correct molecule highly challenging, particularly for compounds absent from reference databases. We introduce a framework that, by leveraging test-time tuning, enhances the learning of a pre-trained transformer model to address this gap, enabling end-to-end de novo molecular structure generation directly from the tandem mass spectra and molecular formulae, bypassing manual annotations and intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively. Test-time tuning on experimental spectra allows the model to dynamically adapt to novel spectra, and the relative performance gain over conventional fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground truth, the generated molecular candidates remain structurally accurate, providing valuable guidance for human interpretation and more reliable identification.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions</title>
<link>https://arxiv.org/abs/2510.23772</link>
<guid>https://arxiv.org/abs/2510.23772</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, chess puzzles, aesthetic appeal, novelty, creativity

Summary:
Generative AI's ability to create creative and novel chess puzzles is examined in this study. The AI system generates puzzles that exhibit aesthetic appeal, novelty, and unique solutions, aiming to push the boundaries of traditional puzzle design. To evaluate the creativity of the puzzles, three renowned chess experts were presented with a booklet of AI-generated puzzles. International Master Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler selected their favorite puzzles based on criteria such as creativity, challenge level, and aesthetic design. This study sheds light on the intersection of AI and creativity in chess composition, highlighting the potential for AI to inspire new forms of puzzle creation. <div>
arXiv:2510.23772v1 Announce Type: new 
Abstract: The rapid advancement of Generative AI has raised significant questions regarding its ability to produce creative and novel outputs. Our recent work investigates this question within the domain of chess puzzles and presents an AI system designed to generate puzzles characterized by aesthetic appeal, novelty, counter-intuitive and unique solutions. We briefly discuss our method below and refer the reader to the technical paper for more details. To assess our system's creativity, we presented a curated booklet of AI-generated puzzles to three world-renowned experts: International Master for chess compositions Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All three are noted authors on chess aesthetics and the evolving role of computers in the game. They were asked to select their favorites and explain what made them appealing, considering qualities such as their creativity, level of challenge, or aesthetic design.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Foundation Models in Pathology Are Failing</title>
<link>https://arxiv.org/abs/2510.23807</link>
<guid>https://arxiv.org/abs/2510.23807</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, computational pathology, diagnostic accuracy, safety vulnerabilities, tissue morphology

Summary: 
This article discusses the limitations of using foundation models (FMs) in computational pathology. It identifies seven main causes for the weaknesses observed in current pathology FMs: biological complexity, ineffective self-supervision, overgeneralization, excessive architectural complexity, lack of domain-specific innovation, insufficient data, and a fundamental design flaw related to tissue patch size. These shortcomings result in low diagnostic accuracy, poor robustness, geometric instability, heavy computational demands, and safety vulnerabilities. The article argues that there is a fundamental mismatch between the assumptions of generic foundation modeling in mainstream AI and the intricate nature of human tissue morphology. As a result, it calls for a reevaluation and restructuring of the paradigm in pathology to better align with the complexity of tissue morphology. 

<br /><br />Summary: <div>
arXiv:2510.23807v1 Announce Type: new 
Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer vision and language processing through large-scale self-supervised and multimodal learning. Consequently, their rapid adoption in computational pathology was expected to deliver comparable breakthroughs in cancer diagnosis, prognostication, and multimodal retrieval. However, recent systematic evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor robustness, geometric instability, heavy computational demands, and concerning safety vulnerabilities. This short paper examines these shortcomings and argues that they stem from deeper conceptual mismatches between the assumptions underlying generic foundation modeling in mainstream AI and the intrinsic complexity of human tissue. Seven interrelated causes are identified: biological complexity, ineffective self-supervision, overgeneralization, excessive architectural complexity, lack of domain-specific innovation, insufficient data, and a fundamental design flaw related to tissue patch size. These findings suggest that current pathology foundation models remain conceptually misaligned with the nature of tissue morphology and call for a fundamental rethinking of the paradigm itself.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.23822</link>
<guid>https://arxiv.org/abs/2510.23822</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-horizon tasks, Multi-step reasoning, Dynamic re-planning, Language models, Hierarchical framework. <br />
Summary: ReCAP (Recursive Context-Aware Reasoning and Planning) is introduced as a hierarchical framework with shared context for reasoning and planning in large language models (LLMs). It addresses challenges faced in long-horizon tasks by incorporating plan-ahead decomposition, structured re-injection of parent plans, and memory-efficient execution. These mechanisms help align high-level goals with low-level actions, reduce redundant prompting, and maintain coherent context updates across recursion. Experimental results show significant improvements in subgoal alignment and success rates on various long-horizon reasoning benchmarks, with a 32% gain on synchronous Robotouille and a 29% improvement on asynchronous Robotouille under the strict pass@1 protocol. <br /><br />Summary: <div>
arXiv:2510.23822v1 Announce Type: new 
Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive Context-Aware Reasoning and Planning), a hierarchical framework with shared context for reasoning and planning in LLMs. ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth. Together these mechanisms align high-level goals with low-level actions, reduce redundant prompting, and preserve coherent context updates across recursion. Experiments demonstrate that ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32% gain on synchronous Robotouille and a 29% improvement on asynchronous Robotouille under the strict pass@1 protocol.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models</title>
<link>https://arxiv.org/abs/2510.23824</link>
<guid>https://arxiv.org/abs/2510.23824</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized goal assignment, multi-agent path planning, structured environment representations, large language models, grid-world settings

Summary: 
Decentralized goal assignment for multi-agent path planning is a challenging task in robotics and artificial intelligence. In this study, agents independently rank their preferences over goals based on structured environment representations. The agents then exchange their rankings without negotiation, using a fixed conflict-resolution rule to make assignments. The comparison of greedy heuristics, optimal assignment, and large language model (LLM)-based agents in grid-world settings shows that LLM-based agents, with well-designed prompts and relevant information, can achieve near-optimal results and outperform traditional heuristics consistently. The findings highlight the potential of language models in decentralized goal assignment for multi-agent path planning and emphasize the importance of information structure in such systems. 

<br /><br />Summary: <div>
arXiv:2510.23824v1 Announce Type: new 
Abstract: Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production</title>
<link>https://arxiv.org/abs/2510.23856</link>
<guid>https://arxiv.org/abs/2510.23856</guid>
<content:encoded><![CDATA[
<div> Hierarchical planner, executor architecture, generalist agents, enterprise scale, benchmark evaluation <br />
<br />
Summary: Agents are advancing in automating digital work, but enterprises face challenges in deploying systems that deliver business value due to fragmented frameworks and slow development. IBM's CUGA agent, open-sourced for the community, adopts a hierarchical planner-executor architecture and excels in performance benchmarks. Tested in a pilot within the talent acquisition domain, CUGA demonstrated scalability, auditability, and governance. The introduction of the BPO-TA benchmark helps assess CUGA's performance. Early evaluations show CUGA approaching specialized agents' accuracy while potentially reducing development time and cost. This paper presents evidence of generalist agents operating at an enterprise scale and provides technical and organizational lessons learned from the pilot. Next steps include advancing research-grade architectures like CUGA into robust, enterprise-ready systems. <br /> <div>
arXiv:2510.23856v1 Announce Type: new 
Abstract: Agents are rapidly advancing in automating digital work, but enterprises face a harder challenge: moving beyond prototypes to deployed systems that deliver measurable business value. This path is complicated by fragmented frameworks, slow development, and the absence of standardized evaluation practices. Generalist agents have emerged as a promising direction, excelling on academic benchmarks and offering flexibility across task types, applications, and modalities. Yet, evidence of their use in production enterprise settings remains limited. This paper reports IBM's experience developing and piloting the Computer Using Generalist Agent (CUGA), which has been open-sourced for the community (https://github.com/cuga-project/cuga-agent). CUGA adopts a hierarchical planner--executor architecture with strong analytical foundations, achieving state-of-the-art performance on AppWorld and WebArena. Beyond benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing talent acquisition domain, addressing enterprise requirements for scalability, auditability, safety, and governance. To support assessment, we introduce BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary evaluations, CUGA approached the accuracy of specialized agents while indicating potential for reducing development time and cost. Our contribution is twofold: presenting early evidence of generalist agents operating at enterprise scale, and distilling technical and organizational lessons from this initial pilot. We outline requirements and next steps for advancing research-grade architectures like CUGA into robust, enterprise-ready systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Creative Chess Puzzles</title>
<link>https://arxiv.org/abs/2510.23881</link>
<guid>https://arxiv.org/abs/2510.23881</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Chess puzzles, RL framework, Novel rewards, Counter-intuitive puzzle generation

Summary:
Generative AI in chess puzzles faces challenges in creating creative and counter-intuitive outputs. This paper introduces an RL framework with novel rewards based on chess engine search statistics to enhance puzzle uniqueness, diversity, and realism. The approach significantly increases counter-intuitive puzzle generation, surpassing existing dataset rates and Lichess-trained models. The AI-generated puzzles meet novelty and diversity benchmarks, retain aesthetic themes, and are rated highly by human experts for creativity and enjoyment. A curated booklet of these puzzles is created, acknowledged for creativity by world-renowned experts. Overall, the paper presents a successful approach in using Generative AI and RL techniques to generate creative and engaging chess puzzles that outperform existing models in terms of counter-intuitiveness and aesthetic appeal.<br /><br />Summary: <div>
arXiv:2510.23881v1 Announce Type: new 
Abstract: While Generative AI rapidly advances in various domains, generating truly creative, aesthetic, and counter-intuitive outputs remains a challenge. This paper presents an approach to tackle these difficulties in the domain of chess puzzles. We start by benchmarking Generative AI architectures, and then introduce an RL framework with novel rewards based on chess engine search statistics to overcome some of those shortcomings. The rewards are designed to enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism. Our RL approach dramatically increases counter-intuitive puzzle generation by 10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates (2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty and diversity benchmarks, retain aesthetic themes, and are rated by human experts as more creative, enjoyable, and counter-intuitive than composed book puzzles, even approaching classic compositions. Our final outcome is a curated booklet of these AI-generated puzzles, which is acknowledged for creativity by three world-renowned experts.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins</title>
<link>https://arxiv.org/abs/2510.23882</link>
<guid>https://arxiv.org/abs/2510.23882</guid>
<content:encoded><![CDATA[
<div> Keywords: digital twins, dynamical system modeling, control strategies, predictive modeling, greenhouse.

Summary: 
- This study explores the use of digital twins for modeling and controlling dynamical systems, combining various approaches including physics-based, data-driven, and hybrid methods with traditional and AI-driven controllers.
- Four predictive models, including Linear, PBM, LSTM, and HAM, were developed and compared using a miniature greenhouse as a test platform for interpolation and extrapolation scenarios.
- The results show that the Hybrid Analysis and Modeling (HAM) approach offers the best balance of accuracy, generalization, and computational efficiency compared to other models.
- When it comes to control strategies, Model Predictive Control (MPC) demonstrates robust and predictable performance, Reinforcement Learning (RL) shows strong adaptability, and Large Language Model (LLM)-based controllers allow for flexible human-AI interaction when combined with predictive tools.
- Overall, the study highlights the importance of selecting the appropriate modeling and control strategies based on the specific requirements of the dynamical system under consideration. 

<br /><br />Summary: <div>
arXiv:2510.23882v1 Announce Type: new 
Abstract: This work investigates the use of digital twins for dynamical system modeling and control, integrating physics-based, data-driven, and hybrid approaches with both traditional and AI-driven controllers. Using a miniature greenhouse as a test platform, four predictive models Linear, Physics-Based Modeling (PBM), Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are developed and compared under interpolation and extrapolation scenarios. Three control strategies Model Predictive Control (MPC), Reinforcement Learning (RL), and Large Language Model (LLM) based control are also implemented to assess trade-offs in precision, adaptability, and implementation effort. Results show that in modeling HAM provides the most balanced performance across accuracy, generalization, and computational efficiency, while LSTM achieves high precision at greater resource cost. Among controllers, MPC delivers robust and predictable performance, RL demonstrates strong adaptability, and LLM-based controllers offer flexible human-AI interaction when coupled with predictive tools.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges</title>
<link>https://arxiv.org/abs/2510.23883</link>
<guid>https://arxiv.org/abs/2510.23883</guid>
<content:encoded><![CDATA[
<div> autonomous task execution, security risks, agentic AI, defense strategies, secure-by-design agent systems
<br />
Autonomous AI systems powered by large language models are becoming increasingly prevalent, presenting new security challenges. These agentic AI systems possess planning, tool use, memory, and autonomy, enabling them to perform tasks independently across various environments. As a result, they introduce unique threats that differ from traditional AI safety and software security concerns. To address these risks, researchers have developed a taxonomy of threats specific to agentic AI and proposed defense strategies from technical and governance perspectives. Recent benchmarks and evaluation methodologies help in assessing the security of these systems. Despite these efforts, there are still open challenges in ensuring the security and reliability of these autonomous agents. Aiming to promote the development of secure-by-design agent systems, ongoing research is focused on synthesizing current knowledge and addressing these challenges.
<br /><br />Summary: <div>
arXiv:2510.23883v1 Announce Type: new 
Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with planning, tool use, memory, and autonomy, are emerging as powerful, flexible platforms for automation. Their ability to autonomously execute tasks across web, software, and physical environments creates new and amplified security risks, distinct from both traditional AI safety and conventional software security. This survey outlines a taxonomy of threats specific to agentic AI, reviews recent benchmarks and evaluation methodologies, and discusses defense strategies from both technical and governance perspectives. We synthesize current research and highlight open challenges, aiming to support the development of secure-by-design agent systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Chain-of-Thought for Visual Reasoning</title>
<link>https://arxiv.org/abs/2510.23925</link>
<guid>https://arxiv.org/abs/2510.23925</guid>
<content:encoded><![CDATA[
<div> posterior inference, variational inference, reinforcement learning, interpretable, large vision-language models

Summary:
The paper introduces a novel training algorithm for Large Vision-Language Models (LVLMs) called chain-of-thought (CoT) reasoning. It addresses the limitations of existing algorithms by reframing reasoning as posterior inference and utilizing amortized variational inference. By incorporating diversity-seeking reinforcement learning and a sparse reward function for token-level learning signals, the proposed method promotes diverse and high-likelihood latent CoT, improving generalization and interpretability. Furthermore, a Bayesian inference-scaling strategy replaces Best-of-N and Beam Search with a more efficient method to rank optimal rationales and answers. Empirical results demonstrate enhancement of LVLMs on seven reasoning benchmarks. The approach achieves state-of-the-art performance in terms of effectiveness, generalization, and interpretability. <div>
arXiv:2510.23925v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Causal Discovery using Judo Calculus</title>
<link>https://arxiv.org/abs/2510.23942</link>
<guid>https://arxiv.org/abs/2510.23942</guid>
<content:encoded><![CDATA[
<div> judo calculus, decentralized framework, causal discovery, topos of sheaves, context dependence <br />
<br />
Summary: <br />
The article presents a theory and implementation of an intuitionistic decentralized framework for causal discovery using judo calculus in a topos of sheaves. Causal effects are shown to depend on various regimes, such as age or genotype, which are formalized as local truth in the proposed judo calculus. Implementation includes a combination of standard causal discovery methods with judo calculus, resulting in improved computational efficiency and performance compared to classical methods. The algorithmic framework allows for j-stability, ensuring a causal claim holds consistently across relevant regimes chosen by the Lawvere-Tierney modal operator j. Experimental results demonstrate the effectiveness of the decentralized sheaf-theoretic causal discovery approach across a range of domains, including biology and economics. <div>
arXiv:2510.23942v1 Announce Type: new 
Abstract: We describe a theory and implementation of an intuitionistic decentralized framework for causal discovery using judo calculus, which is formally defined as j-stable causal inference using j-do-calculus in a topos of sheaves. In real-world applications -- from biology to medicine and social science -- causal effects depend on regime (age, country, dose, genotype, or lab protocol). Our proposed judo calculus formalizes this context dependence formally as local truth: a causal claim is proven true on a cover of regimes, not everywhere at once. The Lawvere-Tierney modal operator j chooses which regimes are relevant; j-stability means the claim holds constructively and consistently across that family. We describe an algorithmic and implementation framework for judo calculus, combining it with standard score-based, constraint-based, and gradient-based causal discovery methods. We describe experimental results on a range of domains, from synthetic to real-world datasets from biology and economics. Our experimental results show the computational efficiency gained by the decentralized nature of sheaf-theoretic causal discovery, as well as improved performance over classical causal discovery methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity</title>
<link>https://arxiv.org/abs/2510.23965</link>
<guid>https://arxiv.org/abs/2510.23965</guid>
<content:encoded><![CDATA[
<div> method, sign estimator, LLM alignment, human preferences, social welfare

Summary: 
The article introduces a new method called the sign estimator to address the vulnerability of traditional LLM alignment methods to heterogeneity in human preferences. By using binary classification loss instead of cross-entropy in the aggregation step, the sign estimator is proven to be consistent and more efficient. It achieves the first polynomial finite-sample error bounds in this context and reduces preference distortion in simulations by 35%. The sign estimator also decreases disagreement with true population preferences from 12% to 8%, outperforming existing heuristics that track individual-level data. This method is simple to implement and maintains the alignment pipeline's simplicity while providing more accurate results in assessing social welfare under LLM settings.   <br /><br />Summary: <div>
arXiv:2510.23965v1 Announce Type: new 
Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences. Fitting a na\"ive probabilistic model to pairwise comparison data (say over prompt-completion pairs) yields an inconsistent estimate of the population-average utility -a canonical measure of social welfare. We propose a new method, dubbed the sign estimator, that provides a simple, provably consistent, and efficient estimator by replacing cross-entropy with binary classification loss in the aggregation step. This simple modification recovers consistent ordinal alignment under mild assumptions and achieves the first polynomial finite-sample error bounds in this setting. In realistic simulations of LLM alignment using digital twins, the sign estimator substantially reduces preference distortion over a panel of simulated personas, cutting (angular) estimation error by nearly 35% and decreasing disagreement with true population preferences from 12% to 8% compared to standard RLHF. Our method also compares favorably to panel data heuristics that explicitly model user heterogeneity and require tracking individual-level preference data-all while maintaining the implementation simplicity of existing LLM alignment pipelines.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance</title>
<link>https://arxiv.org/abs/2510.23989</link>
<guid>https://arxiv.org/abs/2510.23989</guid>
<content:encoded><![CDATA[
<div> Keywords: individual movement patterns, disruptive events, social infrastructure resilience, spatial context, deep learning <br />
Summary: <br />
Individual movement patterns following disruptive events reflect changing demands for community resources. Predicting these shifts is challenging due to the lack of measures for heterogeneous social infrastructure resilience (SIR), unavailable sociodemographic features, and sparse individual-level data. To address this, a conditioned deep learning model incorporating SIR and spatial context was developed. The model enhances predictions of post-event movement patterns by capturing complex relationships between individuals and local context. Divergent shifts in movement patterns among individuals with similar pre-event behaviors but differing SIR are successfully identified. This approach improves decision-making methods for movement predictions and provides valuable insights for resource allocation in response to disruptive events. <br /> <div>
arXiv:2510.23989v1 Announce Type: new 
Abstract: Shifts in individual movement patterns following disruptive events can reveal changing demands for community resources. However, predicting such shifts before disruptive events remains challenging for several reasons. First, measures are lacking for individuals' heterogeneous social infrastructure resilience (SIR), which directly influences their movement patterns, and commonly used features are often limited or unavailable at scale, e.g., sociodemographic characteristics. Second, the complex interactions between individual movement patterns and spatial contexts have not been sufficiently captured. Third, individual-level movement may be spatially sparse and not well-suited to traditional decision-making methods for movement predictions. This study incorporates individuals' SIR into a conditioned deep learning model to capture the complex relationships between individual movement patterns and local spatial context using large-scale, sparse individual-level data. Our experiments demonstrate that incorporating individuals' SIR and spatial context can enhance the model's ability to predict post-event individual movement patterns. The conditioned model can capture the divergent shifts in movement patterns among individuals who exhibit similar pre-event patterns but differ in SIR.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling</title>
<link>https://arxiv.org/abs/2510.24013</link>
<guid>https://arxiv.org/abs/2510.24013</guid>
<content:encoded><![CDATA[
<div> Keywords: scheduling, combinatorial optimization, Large Language Models, single-machine total tardiness, heuristics<br />
<br />
Summary: 
This study introduces new heuristics for the single-machine total tardiness (SMTT) problem by leveraging Large Language Models (LLMs). The proposed EDD Challenger (EDDC) and MDD Challenger (MDDC) heuristics are inspired by well-known sequencing rules and outperform traditional heuristics on job scheduling tasks. By using rigorous criteria and comparing against state-of-the-art methods, the study demonstrates the effectiveness of LLM-discovered algorithms in optimizing total tardiness across different job sizes. EDDC surpasses classic rules and previous algorithms, while MDDC consistently performs well, even on larger instances where exact methods struggle. The collaboration between humans and LLMs shows promise in solving complex combinatorial optimization problems efficiently and effectively, even with limited resources. <div>
arXiv:2510.24013v1 Announce Type: new 
Abstract: Our study contributes to the scheduling and combinatorial optimization literature with new heuristics discovered by leveraging the power of Large Language Models (LLMs). We focus on the single-machine total tardiness (SMTT) problem, which aims to minimize total tardiness by sequencing n jobs on a single processor without preemption, given processing times and due dates. We develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger (EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date (EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that employed simpler rule-based heuristics, we evaluate our LLM-discovered algorithms using rigorous criteria, including optimality gaps and solution time derived from a mixed-integer programming (MIP) formulation of SMTT. We compare their performance against state-of-the-art heuristics and exact methods across various job sizes (20, 100, 200, and 500 jobs). For instances with more than 100 jobs, exact methods such as MIP and dynamic programming become computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD rule and another widely used algorithm in the literature. MDDC consistently outperforms traditional heuristics and remains competitive with exact approaches, particularly on larger and more complex instances. This study shows that human-LLM collaboration can produce scalable, high-performing heuristics for NP-hard constrained combinatorial optimization, even under limited resources when effectively configured.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.24028</link>
<guid>https://arxiv.org/abs/2510.24028</guid>
<content:encoded><![CDATA[
<div> Seasonal component, trend component, generative pathways, interpretable basis functions, domain-specific trends

Summary:
OneCast is a novel framework for cross-domain time series forecasting that addresses the challenge of generalization across heterogeneous data. It decomposes time series into seasonal and trend components, utilizing tailored generative pathways for each. The seasonal component is captured by a projection module that reconstructs periodic patterns using interpretable basis functions. The trend component is encoded into discrete tokens at the segment level through a semantic-aware tokenizer and inferred using a masked discrete diffusion mechanism. By combining the outputs from both branches, OneCast achieves accurate forecasting, capturing seasonal patterns while tracking domain-specific trends. Extensive experiments across eight domains show that OneCast outperforms existing state-of-the-art methods. <div>
arXiv:2510.24028v1 Announce Type: new 
Abstract: Cross-domain time series forecasting is a valuable task in various web applications. Despite its rapid advancement, achieving effective generalization across heterogeneous time series data remains a significant challenge. Existing methods have made progress by extending single-domain models, yet often fall short when facing domain-specific trend shifts and inconsistent periodic patterns. We argue that a key limitation lies in treating temporal series as undifferentiated sequence, without explicitly decoupling their inherent structural components. To address this, we propose OneCast, a structured and modular forecasting framework that decomposes time series into seasonal and trend components, each modeled through tailored generative pathways. Specifically, the seasonal component is captured by a lightweight projection module that reconstructs periodic patterns via interpretable basis functions. In parallel, the trend component is encoded into discrete tokens at segment level via a semantic-aware tokenizer, and subsequently inferred through a masked discrete diffusion mechanism. The outputs from both branches are combined to produce a final forecast that captures seasonal patterns while tracking domain-specific trends. Extensive experiments across eight domains demonstrate that OneCast mostly outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models</title>
<link>https://arxiv.org/abs/2510.24031</link>
<guid>https://arxiv.org/abs/2510.24031</guid>
<content:encoded><![CDATA[
<div> Keywords: log analysis, chatbot, Large Language Models, Machine Learning, cybersecurity

Summary:
LLMLogAnalyzer is introduced as a clustering-based log analysis chatbot that utilizes Large Language Models and Machine Learning algorithms to simplify log analysis processes. It addresses limitations in LLM capabilities such as context window constraints and poor structured text handling, improving summarization, pattern extraction, and anomaly detection tasks. The system outperforms existing LLM-based chatbots like ChatGPT, ChatPDF, and NotebookLM by 39% to 68% across different tasks. It also exhibits strong robustness with a 93% reduction in interquartile range using ROUGE-1 scores. The modular architecture comprising a router, log recognizer, log parser, and search tools enhances LLM capabilities for structured text analysis, improving accuracy and robustness. This innovation benefits both cybersecurity experts and non-technical users in analyzing diverse log data efficiently.<br /><br />Summary: <div>
arXiv:2510.24031v1 Announce Type: new 
Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach prevention and post-incident investigations. However, analyzing vast amounts of diverse log data remains significantly challenging, as high costs, lack of in-house expertise, and time constraints make even basic analysis difficult for many organizations. This study introduces LLMLogAnalyzer, a clustering-based log analysis chatbot that leverages Large Language Models (LLMs) and Machine Learning (ML) algorithms to simplify and streamline log analysis processes. This innovative approach addresses key LLM limitations, including context window constraints and poor structured text handling capabilities, enabling more effective summarization, pattern extraction, and anomaly detection tasks. LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks. Results demonstrate significant performance improvements over state-of-the-art LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent gains ranging from 39% to 68% across different tasks. The system also exhibits strong robustness, achieving a 93% reduction in interquartile range (IQR) when using ROUGE-1 scores, indicating significantly lower result variability. The framework's effectiveness stems from its modular architecture comprising a router, log recognizer, log parser, and search tools. This design enhances LLM capabilities for structured text analysis while improving accuracy and robustness, making it a valuable resource for both cybersecurity experts and non-technical users.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach</title>
<link>https://arxiv.org/abs/2510.24085</link>
<guid>https://arxiv.org/abs/2510.24085</guid>
<content:encoded><![CDATA[
<div> machine learning, electric vehicles, traffic safety, driving behavior, car following

Summary:
- The study compares classical and machine learning models for electric vehicle (EV) car following behavior.
- Classical models like IDM, OVM, OVRV, and a simplified CACC model were compared with a Random Forest Regressor.
- Parameter calibration was done using real-world data of an EV following an internal combustion engine (ICE) vehicle under various driving conditions.
- The Random Forest model showed superior accuracy in predicting acceleration, outperforming classical models across different gap scenarios.
- The findings emphasize the effectiveness of machine learning models in simulating EV behavior and analyzing mixed autonomy traffic dynamics in EV integrated environments. 

<br /><br />Summary: <div>
arXiv:2510.24085v1 Announce Type: new 
Abstract: The increasing adoption of electric vehicles (EVs) necessitates an understanding of their driving behavior to enhance traffic safety and develop smart driving systems. This study compares classical and machine learning models for EV car following behavior. Classical models include the Intelligent Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative Velocity (OVRV), and a simplified CACC model, while the machine learning approach employs a Random Forest Regressor. Using a real world dataset of an EV following an internal combustion engine (ICE) vehicle under varied driving conditions, we calibrated classical model parameters by minimizing the RMSE between predictions and real data. The Random Forest model predicts acceleration using spacing, speed, and gap type as inputs. Results demonstrate the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap), 0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models, CACC performed best, with an RMSE of 2.67 for long gaps. These findings highlight the machine learning model's performance across all scenarios. Such models are valuable for simulating EV behavior and analyzing mixed autonomy traffic dynamics in EV integrated environments.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology</title>
<link>https://arxiv.org/abs/2510.24115</link>
<guid>https://arxiv.org/abs/2510.24115</guid>
<content:encoded><![CDATA[
<div> trust, artificial intelligence, HistoLens, pathologist, transparent <br />
Summary:

To address the issue of trust in artificial intelligence (AI) for doctors, the HistoLens system has been developed as a transparent and collaborative partner. This system allows pathologists to ask questions in plain English, which are then translated into precise queries for the AI engine to provide structured reports. The AI can also provide visual proof for any finding, using heatmaps to pinpoint the cells and regions analyzed. HistoLens has been trained to focus only on the patient's tissue, ignoring distracting background noise. This workflow empowers pathologists to remain the experts in charge, using the AI assistant to verify insights and improve diagnostic speed and confidence. <div>
arXiv:2510.24115v1 Announce Type: new 
Abstract: For doctors to truly trust artificial intelligence, it can't be a black box. They need to understand its reasoning, almost as if they were consulting a colleague. We created HistoLens1 to be that transparent, collaborative partner. It allows a pathologist to simply ask a question in plain English about a tissue slide--just as they would ask a trainee. Our system intelligently translates this question into a precise query for its AI engine, which then provides a clear, structured report. But it doesn't stop there. If a doctor ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any finding--a heatmap that points to the exact cells and regions the AI used for its analysis. We've also ensured the AI focuses only on the patient's tissue, just like a trained pathologist would, by teaching it to ignore distracting background noise. The result is a workflow where the pathologist remains the expert in charge, using a trustworthy AI assistant to verify their insights and make faster, more confident diagnoses.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems</title>
<link>https://arxiv.org/abs/2510.24145</link>
<guid>https://arxiv.org/abs/2510.24145</guid>
<content:encoded><![CDATA[
<div> agent system, incident management, cloud systems, observability data, self-evolving<br />
Summary:<br />
OpsAgent is proposed as a lightweight, self-evolving multi-agent system for incident management in large-scale cloud systems. It addresses the challenges of manual IM and existing automated approaches by converting heterogeneous observability data into structured descriptions using a training-free data processor. OpsAgent also features a multi-agent collaboration framework for transparent diagnostic inference. Its dual self-evolution mechanism combines internal model updates with external experience accumulation for continual capability growth. Experiments on the OPENRCA benchmark demonstrate OpsAgent's state-of-the-art performance, generalizability, interpretability, cost-efficiency, and self-evolving nature, making it a practical and sustainable solution for long-term operation in real-world cloud systems. <br /> <div>
arXiv:2510.24145v1 Announce Type: new 
Abstract: Incident management (IM) is central to the reliability of large-scale cloud systems. Yet manual IM, where on-call engineers examine metrics, logs, and traces is labor-intensive and error-prone in the face of massive and heterogeneous observability data. Existing automated IM approaches often struggle to generalize across systems, provide limited interpretability, and incur high deployment costs, which hinders adoption in practice. In this paper, we present OpsAgent, a lightweight, self-evolving multi-agent system for IM that employs a training-free data processor to convert heterogeneous observability data into structured textual descriptions, along with a multi-agent collaboration framework that makes diagnostic inference transparent and auditable. To support continual capability growth, OpsAgent also introduces a dual self-evolution mechanism that integrates internal model updates with external experience accumulation, thereby closing the deployment loop. Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art performance and show that OpsAgent is generalizable, interpretable, cost-efficient, and self-evolving, making it a practically deployable and sustainable solution for long-term operation in real-world cloud systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data</title>
<link>https://arxiv.org/abs/2510.24151</link>
<guid>https://arxiv.org/abs/2510.24151</guid>
<content:encoded><![CDATA[
<div> framework, multi-hop question answering, automated, semi-structured knowledge sources, Natural Language Inference<br />
Summary:<br />
Building training-ready multi-hop question answering datasets is challenging. Existing datasets lack hard-to-search problems requiring retrieval and reasoning. A new automated framework generates high-difficulty multi-hop questions by growing evidence clusters through relation typing and expansion, constructing oblique cues for unique identification, and using a two-step evaluation pipeline for quality assurance. This process yields complex questions suitable for training and evaluation, reducing human curation effort while retaining strong difficulty profiles. <div>
arXiv:2510.24151v1 Announce Type: new 
Abstract: Building training-ready multi-hop question answering (QA) datasets that truly stress a model's retrieval and reasoning abilities remains highly challenging recently. While there have been a few recent evaluation datasets that capture the characteristics of hard-to-search but easy-to-verify problems -- requiring the integration of ambiguous, indirect, and cross-domain cues -- these data resources remain scarce and are mostly designed for evaluation, making them unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL). Meanwhile, manually curating non-trivially retrievable questions -- where answers cannot be found through a single direct query but instead require multi-hop reasoning over oblique and loosely connected evidence -- incurs prohibitive human costs and fails to scale, creating a critical data bottleneck for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating high-difficulty, training-ready multi-hop questions from semi-structured knowledge sources. The system (i) grows diverse, logically labeled evidence clusters through Natural Language Inference (NLI)-based relation typing and diversity-aware expansion; (ii) applies reverse question construction to compose oblique cues so that isolated signals are underinformative but their combination uniquely identifies the target entity; and (iii) enforces quality with a two-step evaluation pipeline that combines multi-model consensus filtering with structured constraint decomposition and evidence-based matching. The result is a scalable process that yields complex, retrieval-resistant yet verifiable questions suitable for SFT/RL training as well as challenging evaluation, substantially reducing human curation effort while preserving the difficulty profile of strong evaluation benchmarks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</title>
<link>https://arxiv.org/abs/2510.24161</link>
<guid>https://arxiv.org/abs/2510.24161</guid>
<content:encoded><![CDATA[
<div> spatial foundation model, multimodal large language models, embodied knowledge, cross-embodiment control, training paradigm

Summary:
The article introduces the Boundless Large Model (BLM$_1$), a spatial foundation model that addresses limitations in existing multimodal large language models (MLLMs) related to generalization across digital-physical spaces and embodiments, as well as high-level embodied reasoning. BLM$_1 integrates cross-space transfer, cross-task learning, and cross-embodiment generalization capabilities through a two-stage training approach. In Stage I, embodied knowledge is injected into the MLLM while maintaining language competence, and in Stage II, a policy module is trained to guide control without fine-tuning the MLLM backbone. A self-collected cross-embodiment demonstration suite spanning four robot embodiments and six tasks supports this process. Evaluation results show that BLM$_1 outperforms MLLMs, embodied large language models (ELLMs), vision-language-action models (VLAs), and general multimodal language models (GMLMs) in both digital and physical tasks, achieving gains of approximately 6% and 3% respectively. 

<br /><br />Summary: <div>
arXiv:2510.24161v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have advanced vision-language reasoning and are increasingly deployed in embodied agents. However, significant limitations remain: MLLMs generalize poorly across digital-physical spaces and embodiments; vision-language-action models (VLAs) produce low-level actions yet lack robust high-level embodied reasoning; and most embodied large language models (ELLMs) are constrained to digital-space with poor generalization to the physical world. Thus, unified models that operate seamlessly across digital and physical spaces while generalizing across embodiments and tasks remain absent. We introduce the \textbf{Boundless Large Model (BLM$_1$)}, a multimodal spatial foundation model that preserves instruction following and reasoning, incorporates embodied knowledge, and supports robust cross-embodiment control. BLM$_1$ integrates three key capabilities -- \textit{cross-space transfer, cross-task learning, and cross-embodiment generalization} -- via a two-stage training paradigm. Stage I injects embodied knowledge into the MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module through an intent-bridging interface that extracts high-level semantics from the MLLM to guide control, without fine-tuning the MLLM backbone. This process is supported by a self-collected cross-embodiment demonstration suite spanning four robot embodiments and six progressively challenging tasks. Evaluations across digital and physical benchmarks show that a single BLM$_1$ instance outperforms four model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving $\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical tasks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration</title>
<link>https://arxiv.org/abs/2510.24166</link>
<guid>https://arxiv.org/abs/2510.24166</guid>
<content:encoded><![CDATA[
<div> Dataset integration, deep learning, trajectory planning, autonomous vehicles, cross-dataset learning <br /> 
Summary: 
The article introduces UniPlanner, a novel planning framework for autonomous vehicles that integrates multiple datasets for more robust decision-making. UniPlanner leverages the consistency of vehicular trajectory distributions and history-future correlations across datasets to improve planning capabilities. It consists of three key components: the History-Future Trajectory Dictionary Network (HFTDN) aggregates trajectory pairs from different datasets, the Gradient-Free Trajectory Mapper (GFTM) learns universal planning priors from multiple datasets, and the Sparse-to-Dense (S2D) paradigm enables adaptive dropout for robust learning. By combining these components, UniPlanner can effectively utilize planning priors from various datasets during training and maximize performance during inference. Overall, UniPlanner offers a more comprehensive and reliable approach to motion planning for autonomous vehicles through its innovative multi-dataset integration strategies. <br /> <div>
arXiv:2510.24166v1 Announce Type: new 
Abstract: Motion planning is a critical component of autonomous vehicle decision-making systems, directly determining trajectory safety and driving efficiency. While deep learning approaches have advanced planning capabilities, existing methods remain confined to single-dataset training, limiting their robustness in planning.
  Through systematic analysis, we discover that vehicular trajectory distributions and history-future correlations demonstrate remarkable consistency across different datasets. Based on these findings, we propose UniPlanner, the first planning framework designed for multi-dataset integration in autonomous vehicle decision-making. UniPlanner achieves unified cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates history-future trajectory pairs from multiple datasets, using historical trajectory similarity to retrieve relevant futures and generate cross-dataset planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust history-future correlations from multiple datasets, transforming historical trajectories into universal planning priors. Its gradient-free design ensures the introduction of valuable priors while preventing shortcut learning, making the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D) paradigm implements adaptive dropout to selectively suppress planning priors during training for robust learning, while enabling full prior utilization during inference to maximize planning performance.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGA: Memory-Driven GUI Agent for Observation-Centric Interaction</title>
<link>https://arxiv.org/abs/2510.24168</link>
<guid>https://arxiv.org/abs/2510.24168</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, GUI agents, Memory-Driven GUI Agent, robustness, generalization 

Summary: 
The article introduces the Memory-Driven GUI Agent (MGA), a novel approach to GUI interaction that focuses on observing before deciding. MGA models each step as an independent environment state consisting of a current screenshot, task-agnostic spatial information, and a structured memory. This approach differs from existing paradigms that rely on historical trajectories, leading to issues such as error propagation and local exploration bias. Experiments on OSworld benchmarks and real desktop applications demonstrate that MGA outperforms state-of-the-art baselines in terms of robustness, generalization, and efficiency. The code for MGA is publicly available, enabling further research and development in this area. <br /><br />Summary: <div>
arXiv:2510.24168v1 Announce Type: new 
Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal extensions (MLLMs) has enabled agentic systems capable of perceiving and acting across diverse environments. A challenging yet impactful frontier is the development of GUI agents, which must navigate complex desktop and web interfaces while maintaining robustness and generalization. Existing paradigms typically model tasks as long-chain executions, concatenating historical trajectories into the context. While approaches such as Mirage and GTA1 refine planning or introduce multi-branch action selection, they remain constrained by two persistent issues: Dependence on historical trajectories, which amplifies error propagation. And Local exploration bias, where "decision-first, observation-later" mechanisms overlook critical interface cues. We introduce the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the principle of observe first, then decide. MGA models each step as an independent, context-rich environment state represented by a triad: current screenshot, task-agnostic spatial information, and a dynamically updated structured memory. Experiments on OSworld benchmarks, real desktop applications (Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves substantial gains in robustness, generalization, and efficiency compared to state-of-the-art baselines. The code is publicly available at: {https://anonymous.4open.science/r/MGA-3571}.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools</title>
<link>https://arxiv.org/abs/2510.24284</link>
<guid>https://arxiv.org/abs/2510.24284</guid>
<content:encoded><![CDATA[
<div> discovery, data synthesis, model training, large language models, server

Summary:<br /><br />Large Language Models (LLMs) often struggle to effectively utilize the Model Contextual Protocol (MCP) ecosystem due to limited research coverage, manual curation, and training support. To address these challenges, MCP-Flow, an automated web-agent-driven pipeline, was introduced to facilitate server discovery, data synthesis, and model training at scale. By collecting and filtering data from numerous servers and tools, MCP-Flow generated a substantial number of high-quality instruction-function call pairs and trajectories. Extensive experiments confirmed MCP-Flow's ability to enhance MCP tool selection, function-call generation, and task performance for LLM agents in real-world environments. This innovative solution provides a foundation for improving LLM agents' proficiency in utilizing MCP resources effectively. <div>
arXiv:2510.24284v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms</title>
<link>https://arxiv.org/abs/2510.24297</link>
<guid>https://arxiv.org/abs/2510.24297</guid>
<content:encoded><![CDATA[
<div> MCTS, sample efficiency, state abstractions, action abstractions, UCB value <br />
Summary: <br />
One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by incorporating state and action abstractions. Current abstraction techniques focus on enhancing the UCB value in the tree policy by combining visits and returns of an abstract node. However, the challenge arises when multiple actions with the same parent are grouped in the same abstract node, resulting in identical UCB values and requiring a tiebreak rule. The study introduces and evaluates various intra-abstraction policies to address this issue, showing that some policies outperform the random tiebreak rule in different environments and parameter settings. These findings highlight the importance of considering intra-abstraction policies to enhance the effectiveness of MCTS algorithms.<br /> <div>
arXiv:2510.24297v1 Announce Type: new 
Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which can be addressed by building and using state and/or action abstractions in parallel to the tree search such that information can be shared among nodes of the same layer. The primary usage of abstractions for MCTS is to enhance the Upper Confidence Bound (UCB) value during the tree policy by aggregating visits and returns of an abstract node. However, this direct usage of abstractions does not take the case into account where multiple actions with the same parent might be in the same abstract node, as these would then all have the same UCB value, thus requiring a tiebreak rule. In state-of-the-art abstraction algorithms such as pruned On the Go Abstractions (pruned OGA), this case has not been noticed, and a random tiebreak rule was implicitly chosen. In this paper, we propose and empirically evaluate several alternative intra-abstraction policies, several of which outperform the random policy across a majority of environments and parameter settings.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank</title>
<link>https://arxiv.org/abs/2510.24299</link>
<guid>https://arxiv.org/abs/2510.24299</guid>
<content:encoded><![CDATA[
<div> Checking Methods, Large Language Models, Reasoning Paths, Correlation Matrix, Self-Indicator <br />
<br />
Summary: Despite the capabilities of large language models (LLMs), they are prone to errors and hallucinations, posing a challenge for effectively evaluating their outputs. This paper proposes a novel approach leveraging the internal behaviors of LLMs to assess the credibility of their reasoning paths. By analyzing the correlation matrix between input problems and output reasoning paths, a robust indicator of reasoning correctness is identified. The Self-Indicator method, based on this analysis, significantly improves performance in distinguishing correct reasoning paths from incorrect ones with minimal computational overhead. Experimental results across various LLMs demonstrate the effectiveness of this approach, achieving over 75% accuracy in reasoning path correctness assessment. Implementation of Self-Indicator also leads to notable performance enhancements on three reasoning benchmarks, surpassing other voting and verification methods by more than 8%. <br /><br /> <div>
arXiv:2510.24299v1 Announce Type: new 
Abstract: Despite the strong reasoning ability of large language models~(LLMs), they are prone to errors and hallucinations. As a result, how to check their outputs effectively and efficiently has become a critical problem in their applications. Existing checking methods heavily rely on external resources, such as trained verifiers (e.g., process/outcome reward models) or elaborate prompts, which lead to high computational overhead and are only applicable to specific domains. In this paper, we investigate whether the internal behaviors of LLMs have already implied the credibility of their reasoning paths. Specifically, we find that the rank of the correlation matrix between the input problem and the output reasoning path is a robust indicator of reasoning correctness. Different from other correctness indicators for LLMs, the calculation of the correlation matrix only relies on the LLM itself, which avoids the hassle of training a separate model or designing complicated prompts. Based on it, we design a simple, plug-and-play Self-Indicator method to reweight candidate reasoning paths, which achieves significant performance improvements than other voting and verification methods with very few computational overhead. Our experiments across multiple LLMs of varying scales and model families have further shown the effectiveness of Self-Indicator. It achieves over 75% accuracy in distinguishing correct reasoning paths from incorrect ones, and, in turn, improves the accuracies on three reasoning benchmarks by more than 8%.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting</title>
<link>https://arxiv.org/abs/2510.24303</link>
<guid>https://arxiv.org/abs/2510.24303</guid>
<content:encoded><![CDATA[
<div> framework, claim verification, judgmental forecasting, multi-agent, large language models  
Summary:  
This paper introduces a novel multi-agent framework for claim verification in judgmental forecasting. Different agents within the framework can provide evidence both for and against claims using quantitative bipolar argumentation frameworks (QBAFs). The framework includes ArgLLM agents, RbAM agents, and RAG-ArgLLM agents, which utilize Large Language Models (LLMs) to generate and evaluate arguments. By combining evidence from multiple agents, the framework improves forecasting accuracy, especially with three agents. The experiments conducted on standard judgmental forecasting datasets demonstrate the effectiveness of the proposed framework in providing an explainable combination of evidence for claim verification. <div>
arXiv:2510.24303v1 Announce Type: new 
Abstract: Judgmental forecasting is the task of making predictions about future events based on human judgment. This task can be seen as a form of claim verification, where the claim corresponds to a future event and the task is to assess the plausibility of that event. In this paper, we propose a novel multi-agent framework for claim verification, whereby different agents may disagree on claim veracity and bring specific evidence for and against the claims, represented as quantitative bipolar argumentation frameworks (QBAFs). We then instantiate the framework for supporting claim verification, with a variety of agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an existing approach for claim verification that generates and evaluates QBAFs; (2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM) from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents, extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of arguments from external sources. Finally, we conduct experiments with two standard judgmental forecasting datasets, with instances of our framework with two or three agents, empowered by six different base LLMs. We observe that combining evidence from agents can improve forecasting accuracy, especially in the case of three agents, while providing an explainable combination of evidence for claim verification.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research</title>
<link>https://arxiv.org/abs/2510.24337</link>
<guid>https://arxiv.org/abs/2510.24337</guid>
<content:encoded><![CDATA[
<div> quantitative content analysis, gLLMs, communication research, codebook development, validation dataset

Summary:
Generative Large Language Models (gLLMs) like ChatGPT are becoming more prevalent in communication research for content analysis. They have shown superior performance compared to crowd workers and research assistants, at a lower time and cost. gLLMs can interpret implicit meanings, understand context, and be utilized with basic programming knowledge. However, integrating gLLMs into communication research faces challenges. Researchers must address codebook development, prompt engineering, model selection, parameter tuning, iterative refinement, validation of reliability, and potential performance enhancement. This paper synthesizes current research on gLLM-assisted quantitative content analysis and presents a comprehensive guide to navigate these challenges. The aim is to enhance accessibility of gLLM-based content analysis for communication researchers, while ensuring adherence to disciplinary quality standards of validity, reliability, reproducibility, and research ethics. 

<br /><br />Summary: <div>
arXiv:2510.24337v1 Announce Type: new 
Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly being used in communication research for content analysis. Studies show that gLLMs can outperform both crowd workers and trained coders, such as research assistants, on various coding tasks relevant to communication science, often at a fraction of the time and cost. Additionally, gLLMs can decode implicit meanings and contextual information, be instructed using natural language, deployed with only basic programming skills, and require little to no annotated data beyond a validation dataset - constituting a paradigm shift in automated content analysis. Despite their potential, the integration of gLLMs into the methodological toolkit of communication research remains underdeveloped. In gLLM-assisted quantitative content analysis, researchers must address at least seven critical challenges that impact result quality: (1) codebook development, (2) prompt engineering, (3) model selection, (4) parameter tuning, (5) iterative refinement, (6) validation of the model's reliability, and optionally, (7) performance enhancement. This paper synthesizes emerging research on gLLM-assisted quantitative content analysis and proposes a comprehensive best-practice guide to navigate these challenges. Our goal is to make gLLM-based content analysis more accessible to a broader range of communication researchers and ensure adherence to established disciplinary quality standards of validity, reliability, reproducibility, and research ethics.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation</title>
<link>https://arxiv.org/abs/2510.24339</link>
<guid>https://arxiv.org/abs/2510.24339</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Veridical Data Science, VDSAgents, PCS principles, data science automation <br />
Summary: <br />
Large language models are increasingly used in data science but lack guidance from scientific principles, affecting trustworthiness. VDSAgents, a multi-agent system based on PCS principles, provides a modular workflow for data processing and modeling. Each phase is handled by an agent incorporating perturbation analysis, unit testing, and model validation for functionality and auditability. Evaluation on diverse datasets shows VDSAgents outperforming state-of-the-art systems like AutoKaggle and DataInterpreter, utilizing DeepSeek-V3 and GPT-4o as backends. This demonstrates the effectiveness of embedding PCS principles into LLM-driven data science automation.<br /><br />Summary: <div>
arXiv:2510.24339v1 Announce Type: new 
Abstract: Large language models (LLMs) become increasingly integrated into data science workflows for automated system design. However, these LLM-driven data science systems rely solely on the internal reasoning of LLMs, lacking guidance from scientific and theoretical principles. This limits their trustworthiness and robustness, especially when dealing with noisy and complex real-world datasets. This paper provides VDSAgents, a multi-agent system grounded in the Predictability-Computability-Stability (PCS) principles proposed in the Veridical Data Science (VDS) framework. Guided by PCS principles, the system implements a modular workflow for data cleaning, feature engineering, modeling, and evaluation. Each phase is handled by an elegant agent, incorporating perturbation analysis, unit testing, and model validation to ensure both functionality and scientific auditability. We evaluate VDSAgents on nine datasets with diverse characteristics, comparing it with state-of-the-art end-to-end data science systems, such as AutoKaggle and DataInterpreter, using DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the results of AutoKaggle and DataInterpreter, which validates the feasibility of embedding PCS principles into LLM-driven data science automation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Geometric Space Bridging AI Models and the Human Brain</title>
<link>https://arxiv.org/abs/2510.24342</link>
<guid>https://arxiv.org/abs/2510.24342</guid>
<content:encoded><![CDATA[
<div> neuroscientists, computer scientists, artificial neural networks, brain-like space, brain-likeness

Summary: 
- Researchers aim to understand and replicate intelligence in both the brain and artificial systems.
- A new concept called Brain-like Space allows for precise comparison of AI models based on intrinsic organization.
- The study analyzed 151 Transformer-based models, including vision, language, and multimodal models.
- A continuous arc-shaped geometry in Brain-like Space reflects varying degrees of brain-likeness in different models.
- Brain-likeness is influenced by factors such as pretraining paradigm and positional encoding scheme, not just input modality. 
- The Brain-like Space framework helps quantify and compare intelligence across domains, offering insights into the organizational principles shared by machines and the brain. 

<br /><br />Summary: <div>
arXiv:2510.24342v1 Announce Type: new 
Abstract: For decades, neuroscientists and computer scientists have pursued a shared ambition: to understand intelligence and build it. Modern artificial neural networks now rival humans in language, perception, and reasoning, yet it is still largely unknown whether these artificial systems organize information as the brain does. Existing brain-AI alignment studies have shown the striking correspondence between the two systems, but such comparisons remain bound to specific inputs and tasks, offering no common ground for comparing how AI models with different kinds of modalities-vision, language, or multimodal-are intrinsically organized. Here we introduce a groundbreaking concept of Brain-like Space: a unified geometric space in which every AI model can be precisely situated and compared by mapping its intrinsic spatial attention topological organization onto canonical human functional brain networks, regardless of input modality, task, or sensory domain. Our extensive analysis of 151 Transformer-based models spanning state-of-the-art large vision models, large language models, and large multimodal models uncovers a continuous arc-shaped geometry within this space, reflecting a gradual increase of brain-likeness; different models exhibit distinct distribution patterns within this geometry associated with different degrees of brain-likeness, shaped not merely by their modality but by whether the pretraining paradigm emphasizes global semantic abstraction and whether the positional encoding scheme facilitates deep fusion across different modalities. Moreover, the degree of brain-likeness for a model and its downstream task performance are not "identical twins". The Brain-like Space provides the first unified framework for situating, quantifying, and comparing intelligence across domains, revealing the deep organizational principles that bridge machines and the brain.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine</title>
<link>https://arxiv.org/abs/2510.24359</link>
<guid>https://arxiv.org/abs/2510.24359</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, medicine, individualized care, decision support, equity

Summary: 
Artificial intelligence in medicine has traditionally focused on serving the average patient, leading to challenges in addressing the needs of patients with rare variants, multimorbidity, or underrepresented demographics. To overcome this limitation, the authors propose a multi-agent ecosystem for N-of-1 decision support. This approach involves clustering agents by organ systems, patient populations, and analytic modalities to draw on shared models and evidence synthesis tools. The coordination layer evaluates reliability, uncertainty, and data density to provide clinicians with personalized decision-support packets, including risk estimates with confidence ranges and outlier flags. Validation shifts from population averages to individual reliability, addressing challenges such as computational demands, automation bias, and regulatory fit. By moving towards orchestrated intelligence, this approach aims to align medical AI with the core principle of providing transparent, equitable, and individual-centered care. 

Summary: <div>
arXiv:2510.24359v1 Announce Type: new 
Abstract: Artificial intelligence in medicine is built to serve the average patient. By minimizing error across large datasets, most systems deliver strong aggregate accuracy yet falter at the margins: patients with rare variants, multimorbidity, or underrepresented demographics. This average patient fallacy erodes both equity and trust. We propose a different design: a multi-agent ecosystem for N-of-1 decision support. In this environment, agents clustered by organ systems, patient populations, and analytic modalities draw on a shared library of models and evidence synthesis tools. Their results converge in a coordination layer that weighs reliability, uncertainty, and data density before presenting the clinician with a decision-support packet: risk estimates bounded by confidence ranges, outlier flags, and linked evidence. Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in the small, and risk--coverage trade-offs. Anticipated challenges include computational demands, automation bias, and regulatory fit, addressed through caching strategies, consensus checks, and adaptive trial frameworks. By moving from monolithic models to orchestrated intelligence, this approach seeks to align medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents</title>
<link>https://arxiv.org/abs/2510.24383</link>
<guid>https://arxiv.org/abs/2510.24383</guid>
<content:encoded><![CDATA[
<div> policy cards, machine-readable, constraints, AI agents, compliance <br />
<br />
Summary: Policy Cards are a new standard for expressing operational, regulatory, and ethical constraints for AI agents in a machine-readable format. These cards are deployed with the agent, guiding its actions and ensuring compliance with necessary constraints at runtime. They define rules, obligations, and evidentiary requirements and can be linked to assurance frameworks for validation. By integrating governance with engineering practice, Policy Cards enable verifiable compliance for autonomous agents and support accountable autonomy at scale. They extend transparency artifacts like Model, Data, and System Cards, providing a practical mechanism for ensuring compliance in multi-agent ecosystems. This framework allows for automatic validation, version control, and linkage to enforcement or audit pipelines, establishing a foundation for distributed assurance in AI ecosystems. <div>
arXiv:2510.24383v1 Announce Type: new 
Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard for expressing operational, regulatory, and ethical constraints for AI agents. The Policy Card sits with the agent and enables it to follow required constraints at runtime. It tells the agent what it must and must not do. As such, it becomes an integral part of the deployed agent. Policy Cards extend existing transparency artifacts such as Model, Data, and System Cards by defining a normative layer that encodes allow/deny rules, obligations, evidentiary requirements, and crosswalk mappings to assurance frameworks including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can be validated automatically, version-controlled, and linked to runtime enforcement or continuous-audit pipelines. The framework enables verifiable compliance for autonomous agents, forming a foundation for distributed assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism for integrating high-level governance with hands-on engineering practice and enabling accountable autonomy at scale.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion</title>
<link>https://arxiv.org/abs/2510.24390</link>
<guid>https://arxiv.org/abs/2510.24390</guid>
<content:encoded><![CDATA[
<div> Efficient Reasoning, Large Language Models, Web Applications, Query Decomposition, Logic-parallel Content Expansion
<br />
Summary:
Orion is a novel reasoning framework designed to address the challenge of integrating Large Language Models (LLMs) into real-time Web applications. It introduces a two-phase process: key point generation and content parallel expansion, leveraging dependency-aware query decomposition. This approach significantly improves reasoning performance by balancing efficiency and quality. Orion utilizes a pipeline scheduling mechanism to enhance reasoning speed and reduce answer latency. Through experiments on various benchmarks, Orion demonstrates up to 4.33x higher token generation speed and 3.42x lower answer latency compared to existing methods. Additionally, it enhances reasoning quality by up to 18.75% by explicitly modeling inter-point dependencies.
<br /> <div>
arXiv:2510.24390v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into real-time Web applications, such as AI-powered search and conversational agents, presents a fundamental Web infrastructure challenge: reconciling the demand for high-quality, complex reasoning with the stringent low-latency and high-throughput requirements of interactive services. Current LLM reasoning, hindered by computationally inefficient sequential generation and rigid reasoning strategies, creates a critical bottleneck for the Web services. Existing approaches typically optimize the LLM reasoning for either efficiency or quality but struggle to achieve both, and thus fail to meet the dual requirements of modern Web platforms. To overcome these limitations, we propose Orion, a novel and efficient reasoning framework that enables dependency-aware query decomposition and logic-parallel content expansion. Concretely, Orion decomposes a single query reasoning process into two synergistic phases: (1) \textit{key point generation}, which distills logically structured key points through retrieval-augmented few-shot prompting, and (2) \textit{content parallel expansion}, which concurrently elaborates on these points based on a dependency graph to ensure logical consistency. Furthermore, Orion introduces a pipeline scheduling mechanism that exploits the complementary computational characteristics of the two phases (generation imposes pressure on GPU computing and expansion stresses on GPU memory) across multiple queries, enabling cross-query parallelism and dramatically improving reasoning performance (\ie, efficiency and quality). Experiments on diverse benchmarks show that Orion not only delivers up to 4.33x higher token generation speed and 3.42x lower answer latency over the baselines but also improves reasoning quality by up to 18.75% through explicitly modeling inter-point dependencies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training</title>
<link>https://arxiv.org/abs/2510.24397</link>
<guid>https://arxiv.org/abs/2510.24397</guid>
<content:encoded><![CDATA[
<div> Pre-training, LLM-based agents, APTBench, agent-specific data, agentic capabilities

Summary:
APTBench is introduced as a framework to assess agentic potentials in Language Model Agents (LLMs) during the pre-training stage. Existing pre-training benchmarks lack evaluation of agentic abilities and focus on isolated skills. APTBench converts real-world agent tasks into questions for base models, focusing on core agentic abilities like planning and action. It covers agent scenarios in software engineering and deep research to provide a more predictive signal of a model's performance as an agent. APTBench offers a cost-effective alternative to full-scale agent evaluations post-training. With the rising trend of incorporating agent-specific data into pre-training, the need for benchmarks like APTBench becomes essential in aligning LLMs with real-world autonomous task execution.  <br /><br />Summary: <div>
arXiv:2510.24397v1 Announce Type: new 
Abstract: With the rapid development of LLM-based agents, there is a growing trend to incorporate agent-specific data into the pre-training stage of LLMs, aiming to better align LLMs with real-world autonomous task execution. However, current pre-training benchmarks primarily focus on isolated and static skills, e.g., common knowledge or mathematical/code reasoning, and fail to reflect model's agentic capabilities. On the other hand, agent benchmarks are typically designed for post-trained models, requiring multi-turn task execution abilities that base models struggle to support. Thus, there is a compelling need for a benchmark that can evaluate agentic potentials during pre-training and guide the model training more effectively. To address this gap, we propose APTBench, a framework that converts real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models. It focuses on core agentic abilities, e.g., planning and action, and covers key agent scenarios, software engineering and deep research. Compared to existing general-purpose benchmarks, APTBench offers a more predictive signal of a model's downstream performance as an agent, while remaining significantly more lightweight and cost-effective than full-scale, end-to-end agent evaluations after post-training.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title>
<link>https://arxiv.org/abs/2510.24411</link>
<guid>https://arxiv.org/abs/2510.24411</guid>
<content:encoded><![CDATA[
<div> sandbox environment, safety detection benchmark, OS-Sentinel, VLM-based Contextual Judge, mobile agent safety research

Summary:
MobileRisk-Live introduces a dynamic sandbox environment and safety detection benchmark to address safety concerns in mobile agent operations. The OS-Sentinel framework combines a Formal Verifier and a VLM-based Contextual Judge to detect system-level violations and assess contextual risks. Experiments demonstrate OS-Sentinel's improved performance over existing approaches, providing critical insights for developing safer autonomous mobile agents. This research aims to advance the understanding and mitigation of potential safety risks associated with computer-using agents powered by Vision-Language Models in mobile environments. <div>
arXiv:2510.24411v1 Announce Type: new 
Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning</title>
<link>https://arxiv.org/abs/2510.24435</link>
<guid>https://arxiv.org/abs/2510.24435</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning ability, logical reasoning, abstract reasoning, benchmarking<br />
Summary:<br />
- Evaluating reasoning ability in Large Language Models (LLMs) is crucial for advancing AI, going beyond linguistic task performance to assess understanding, inference, and logical deduction.<br />
- A study compared logical and abstract reasoning skills of various LLMs, including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral, Perplexity, and Sabi'a, using custom-designed questions.<br />
- Results were benchmarked against human performance, revealing significant differences and highlighting areas where LLMs struggle with deduction.<br />
- The study sheds light on the strengths and weaknesses of LLMs in logical and abstract reasoning tasks, providing valuable insights for further research and development in AI. <br />
Summary: <div>
arXiv:2510.24435v1 Announce Type: new 
Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for advancing artificial intelligence, as it transcends mere linguistic task performance. It involves understanding whether these models truly understand information, perform inferences, and are able to draw conclusions in a logical and valid way. This study compare logical and abstract reasoning skills of several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral, Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning questions. The LLM results are benchmarked against human performance on the same tasks, revealing significant differences and indicating areas where LLMs struggle with deduction.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Law in Silico: Simulating Legal Society with LLM-Based Agents</title>
<link>https://arxiv.org/abs/2510.24442</link>
<guid>https://arxiv.org/abs/2510.24442</guid>
<content:encoded><![CDATA[
<div> AI, legal simulation, Large Language Models, Law in Silico, crime rates <br />
Summary: <br />
The study introduces Law in Silico, an agent framework utilizing Large Language Models to simulate legal scenarios with individual decision-making and institutional mechanisms such as legislation, adjudication, and enforcement. Through experiments comparing simulated crime rates with real-world data, the framework demonstrates the ability of LLM-based agents to replicate macro-level crime trends effectively. Additionally, the simulations highlight the importance of a well-functioning, transparent, and adaptive legal system in protecting the rights of vulnerable individuals at the micro-level. This research shows the potential of AI systems, particularly LLMs, in simulating legal societies, providing valuable insights for legal theory verification, development, and administration. <div>
arXiv:2510.24442v1 Announce Type: new 
Abstract: Since real-world legal experiments are often costly or infeasible, simulating legal societies with Artificial Intelligence (AI) systems provides an effective alternative for verifying and developing legal theory, as well as supporting legal administration. Large Language Models (LLMs), with their world knowledge and role-playing capabilities, are strong candidates to serve as the foundation for legal society simulation. However, the application of LLMs to simulate legal systems remains underexplored. In this work, we introduce Law in Silico, an LLM-based agent framework for simulating legal scenarios with individual decision-making and institutional mechanisms of legislation, adjudication, and enforcement. Our experiments, which compare simulated crime rates with real-world data, demonstrate that LLM-based agents can largely reproduce macro-level crime trends and provide insights that align with real-world observations. At the same time, micro-level simulations reveal that a well-functioning, transparent, and adaptive legal system offers better protection of the rights of vulnerable individuals.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordance Representation and Recognition for Autonomous Agents</title>
<link>https://arxiv.org/abs/2510.24459</link>
<guid>https://arxiv.org/abs/2510.24459</guid>
<content:encoded><![CDATA[
<div> Pattern Language, World Modeling, Structured Data, Software Agents, Web Automation 
Summary:
The paper presents a pattern language for constructing an actionable internal world model for software agents using structured data. It addresses the challenges of complexity in web pages and evolving web services by introducing two architectural patterns. The DOM Transduction Pattern simplifies raw DOM data into a concise representation optimized for agent reasoning. The Hypermedia Affordances Recognition Pattern allows agents to dynamically enrich their world model by parsing semantic descriptions of web services to integrate new capabilities at runtime. By implementing these patterns, agents can efficiently construct and maintain accurate world models, enabling scalable, adaptive, and interoperable automation across the web and its resources. <br /><br />Summary: <div>
arXiv:2510.24459v1 Announce Type: new 
Abstract: The autonomy of software agents is fundamentally dependent on their ability to construct an actionable internal world model from the structured data that defines their digital environment, such as the Document Object Model (DOM) of web pages and the semantic descriptions of web services. However, constructing this world model from raw structured data presents two critical challenges: the verbosity of raw HTML makes it computationally intractable for direct use by foundation models, while the static nature of hardcoded API integrations prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured data, presenting two complementary architectural patterns. The DOM Transduction Pattern addresses the challenge of web page complexity by distilling} a verbose, raw DOM into a compact, task-relevant representation or world model optimized for an agent's reasoning core. Concurrently, the Hypermedia Affordances Recognition Pattern enables the agent to dynamically enrich its world model by parsing standardized semantic descriptions to discover and integrate the capabilities of unknown web services at runtime. Together, these patterns provide a robust framework for engineering agents that can efficiently construct and maintain an accurate world model, enabling scalable, adaptive, and interoperable automation across the web and its extended resources.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2510.24461</link>
<guid>https://arxiv.org/abs/2510.24461</guid>
<content:encoded><![CDATA[
<div> Keywords: Neuromorphic computing, Spiking Neural Networks, Surrogate gradients, Reinforcement learning, Robotics

Summary:
In this study, the researchers investigate the use of Spiking Neural Networks (SNNs) in energy-constrained robotics for complex control tasks. They address the challenges posed by non-differentiable spiking neurons and the need for training on sequences in RL by analyzing surrogate gradient slope settings. The study shows that shallower slopes in RL settings lead to improved training and performance. Additionally, a novel training approach using a privileged guiding policy is proposed to bootstrap the learning process while interacting with the spiking policy in the environment. The combined method, along with an adaptive slope schedule, is tested on a drone position control task, achieving significantly better results compared to existing techniques. This work contributes to the understanding of surrogate gradient learning in SNNs and provides practical training methodologies for neuromorphic controllers in real-world robotic systems. 

<br /><br />Summary: <div>
arXiv:2510.24461v1 Announce Type: new 
Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained robotics by achieving orders-of-magnitude efficiency gains, while enabling native temporal processing. Spiking Neural Networks (SNNs) represent a promising algorithmic approach for these systems, yet their application to complex control tasks faces two critical challenges: (1) the non-differentiable nature of spiking neurons necessitates surrogate gradients with unclear optimization properties, and (2) the stateful dynamics of SNNs require training on sequences, which in reinforcement learning (RL) is hindered by limited sequence lengths during early training, preventing the network from bridging its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient slope settings, showing that shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients. In supervised learning, we find no clear preference for fixed or scheduled slopes. The effect is much more pronounced in RL settings, where shallower slopes or scheduled slopes lead to a 2.1x improvement in both training and final deployed performance. Next, we propose a novel training approach that leverages a privileged guiding policy to bootstrap the learning process, while still exploiting online environment interactions with the spiking policy. Combining our method with an adaptive slope schedule for a real-world drone position control task, we achieve an average return of 400 points, substantially outperforming prior techniques, including Behavioral Cloning and TD3BC, which achieve at most --200 points under the same conditions. This work advances both the theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers demonstrated in real-world robotic systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning</title>
<link>https://arxiv.org/abs/2510.24528</link>
<guid>https://arxiv.org/abs/2510.24528</guid>
<content:encoded><![CDATA[
<div> Pipeline, in-context learning, large language models, data labeling, label propagation

Summary: 
The article introduces a two-stage pipeline for in-context learning (ICL) that reduces the need for data labeling when leveraging large language models (LLMs). The first stage of the pipeline utilizes cross-task examples to prompt an LLM, pseudo-labeling a small set of target task instances. Subsequently, a graph-based label propagation method is employed to propagate label information to the remaining target examples without the need for additional LLM queries. This results in a fully pseudo-labeled dataset that is used to generate in-task demonstrations for ICL. The approach combines the flexibility of cross-task supervision with the scalability of LLM-free label propagation. Experimental results across five tasks demonstrate that the proposed method achieves strong performance while significantly lowering labeling costs. <br /><br /> <div>
arXiv:2510.24528v1 Announce Type: new 
Abstract: The capability of in-context learning (ICL) enables large language models (LLMs) to perform novel tasks without parameter updates by conditioning on a few input-output examples. However, collecting high-quality examples for new or challenging tasks can be costly and labor-intensive. In this work, we propose a cost-efficient two-stage pipeline that reduces reliance on LLMs for data labeling. Our approach first leverages readily available cross-task examples to prompt an LLM and pseudo-label a small set of target task instances. We then introduce a graph-based label propagation method that spreads label information to the remaining target examples without additional LLM queries. The resulting fully pseudo-labeled dataset is used to construct in-task demonstrations for ICL. This pipeline combines the flexibility of cross-task supervision with the scalability of LLM-free propagation. Experiments across five tasks demonstrate that our method achieves strong performance while lowering labeling costs.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives</title>
<link>https://arxiv.org/abs/2510.24551</link>
<guid>https://arxiv.org/abs/2510.24551</guid>
<content:encoded><![CDATA[
<div> AI, healthcare, data-centric paradigm, GenAI deployment, medical data ecosystem

Summary:
The article discusses the rapid rise of Generative Artificial Intelligence (GenAI) in healthcare, showcasing its potential to revolutionize medical practices. It emphasizes the need for a data-centric approach in designing and implementing GenAI systems within the healthcare domain. The proposed paradigm repositions the data life cycle, making the medical data ecosystem the foundational substrate for generative healthcare systems. This ecosystem supports the integration, representation, and retrieval of diverse medical data and knowledge, enabling efficient data processing pipelines for GenAI-powered operations. By supplying foundation models with high-quality data for pretraining and fine-tuning, as well as serving as a knowledge retrieval backend for task-specific inference, the ecosystem facilitates the effective deployment of GenAI in healthcare delivery. This approach aims to reduce the cognitive burden on clinicians, enhance diagnosis and personalized treatments, and improve overall healthcare delivery. <div>
arXiv:2510.24551v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It promises transformative opportunities for advancing and disrupting existing practices, including healthcare. From large language models (LLMs) for clinical note synthesis and conversational assistance to multimodal systems that integrate medical imaging, electronic health records, and genomic data for decision support, GenAI is transforming the practice of medicine and the delivery of healthcare, such as diagnosis and personalized treatments, with great potential in reducing the cognitive burden on clinicians, thereby improving overall healthcare delivery. However, GenAI deployment in healthcare requires an in-depth understanding of healthcare tasks and what can and cannot be achieved. In this paper, we propose a data-centric paradigm in the design and deployment of GenAI systems for healthcare. Specifically, we reposition the data life cycle by making the medical data ecosystem as the foundational substrate for generative healthcare systems. This ecosystem is designed to sustainably support the integration, representation, and retrieval of diverse medical data and knowledge. With effective and efficient data processing pipelines, such as semantic vector search and contextual querying, it enables GenAI-powered operations for upstream model components and downstream clinical applications. Ultimately, it not only supplies foundation models with high-quality, multimodal data for large-scale pretraining and domain-specific fine-tuning, but also serves as a knowledge retrieval backend to support task-specific inference via the agentic layer. The ecosystem enables the deployment of GenAI for high-quality and effective healthcare delivery.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling</title>
<link>https://arxiv.org/abs/2510.24645</link>
<guid>https://arxiv.org/abs/2510.24645</guid>
<content:encoded><![CDATA[
<div> Keywords: Function Calling, Large Language Models, Multi-turn Training Data, Data Synthesis, Real-world Environments<br />
Summary:<br />
The article introduces FunReason-MT, a novel data synthesis framework designed to enhance the ability of large language models and autonomous agents to interface with external tools, particularly in real-world environments. The framework addresses challenges related to targeted model training, tool architecture isolation, and multi-turn logical dependency. FunReason-MT utilizes Environment-API Graph Interactions to gather diverse high-quality trajectories, Advanced Tool-Query Synthesis to simplify complex query construction, and a Guided Iterative Chain for CoT generation. Evaluations on the Berkeley Function-Calling Leaderboard show that models trained using FunReason-MT-generated data achieve state-of-the-art performance, surpassing most comparable models. The framework proves to be reliable and robust for agentic learning, with further improvements demonstrated in BFCLv4. FunReason-MT offers a valuable solution for generating high-quality multi-turn training data in challenging real-world environments. <br /><br />Summary: <div>
arXiv:2510.24645v1 Announce Type: new 
Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning</title>
<link>https://arxiv.org/abs/2510.24650</link>
<guid>https://arxiv.org/abs/2510.24650</guid>
<content:encoded><![CDATA[
<div> machine learning, deep learning, crop disease management, foundation models, adaptive learning<br />
<br />
Summary: Site-specific disease management in crops has seen rapid advancements through the use of machine learning and deep learning techniques, particularly with the introduction of foundation models (FMs). These FMs integrate visual and textual data to interpret symptoms and support interactive QA for growers. Vision-language models (VLMs) have become more popular than large-language models (LLMs), with a focus on adaptive learning (AL) and reinforcement learning (RL) for smart spraying. Digital twin frameworks with RL can simulate targeted spraying virtually, but bridging the sim-to-real gap remains a challenge for real-world deployment. Human-robot collaboration is limited, especially in human-in-the-loop approaches where robots detect symptoms and humans validate uncertain cases. Multi-modal FMs with real-time feedback are expected to drive future advancements in site-specific disease management. <div>
arXiv:2510.24650v1 Announce Type: new 
Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through machine and deep learning (ML and DL) for real-time computer vision. Research evolved from handcrafted feature extraction to large-scale automated feature learning. With foundation models (FMs), crop disease datasets are now processed in fundamentally new ways. Unlike traditional neural networks, FMs integrate visual and textual data, interpret symptoms in text, reason about symptom-management relationships, and support interactive QA for growers and educators. Adaptive and imitation learning in robotics further enables field-based disease management. This review screened approx. 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and discussing their role in adaptive learning (AL), reinforcement learning (RL), and digital twin frameworks for targeted spraying. Key findings: (a) FMs are gaining traction with surging literature in 2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL and AL are still nascent for smart spraying; (d) digital twins with RL can simulate targeted spraying virtually; (e) addressing the sim-to-real gap is critical for real-world deployment; (f) human-robot collaboration remains limited, especially in human-in-the-loop approaches where robots detect early symptoms and humans validate uncertain cases; (g) multi-modal FMs with real-time feedback will drive next-gen SSDM. For updates, resources, and contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to submit papers, code, or datasets.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs</title>
<link>https://arxiv.org/abs/2510.24663</link>
<guid>https://arxiv.org/abs/2510.24663</guid>
<content:encoded><![CDATA[
<div> agentic tool use, OrchDAG, synthetic data generation, directed acyclic graphs, multi-turn interactions
Summary:
The article introduces OrchDAG, a synthetic data generation pipeline that models tool execution as directed acyclic graphs with controllable complexity. The dataset provided by OrchDAG serves as a challenging benchmark for model performance evaluation and reinforcement learning with variable reward (RLVR) training. The study showcases the effectiveness of a proposed graph-based reward when combined with GRPO-style algorithms in enhancing multi-turn tool interactions. Through experiments, it demonstrates that leveraging the topological structure and data complexity of multi-turn tool interactions is crucial for improving model performance in agentic tool use scenarios. OrchDAG dataset and the proposed reward mechanism offer a solvable yet challenging benchmark, emphasizing the importance of considering the intricacies of multi-step tool interactions in computational modeling studies. 
<br /><br />Summary: <div>
arXiv:2510.24663v1 Announce Type: new 
Abstract: Agentic tool use has gained traction with the rise of agentic tool calling, yet most existing work overlooks the complexity of multi-turn tool interactions. We introduce OrchDAG, a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) with controllable complexity. Using this dataset, we benchmark model performance and propose a graph-based reward to enhance RLVR training. Experiments show that the dataset presents a challenging but solvable benchmark, and the proposed reward is effective when combined with GRPO-style algorithms, highlighting the importance of leveraging topological structure and data complexity in multi-turn tool use.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning</title>
<link>https://arxiv.org/abs/2510.24690</link>
<guid>https://arxiv.org/abs/2510.24690</guid>
<content:encoded><![CDATA[
<div> Knowledge graph, tool schemas, exemplar artifact generation, DeepResearch-inspired analysis, plan generation

Summary:
The article presents a framework aimed at enhancing exemplar artifact generation by uncovering and utilizing dependencies among tools and documents. The method involves constructing a knowledge graph from tool schemas and internal documents, which are fused to form a comprehensive understanding of tool interactions and procedural knowledge. By aligning structural tool dependencies with domain knowledge, the framework improves plan generation efficiency. Through a deep-sparse integration strategy, the unified approach effectively models tool interactions and enhances reasoning and planning processes. The experiments conducted validate the effectiveness of linking tool graphs with domain knowledge graphs for tool-augmented reasoning and planning. <div>
arXiv:2510.24690v1 Announce Type: new 
Abstract: We present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation. Our method begins by constructing a tool knowledge graph from tool schemas,including descriptions, arguments, and output payloads, using a DeepResearch-inspired analysis. In parallel, we derive a complementary knowledge graph from internal documents and SOPs, which is then fused with the tool graph. To generate exemplar plans, we adopt a deep-sparse integration strategy that aligns structural tool dependencies with procedural knowledge. Experiments demonstrate that this unified framework effectively models tool interactions and improves plan generation, underscoring the benefits of linking tool graphs with domain knowledge graphs for tool-augmented reasoning and planning.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Large Language Models with Limited Data: A Survey and Practical Guide</title>
<link>https://arxiv.org/abs/2411.09539</link>
<guid>https://arxiv.org/abs/2411.09539</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fine-tuning, data scarcity, adaptation, preference alignment

Summary: 
This paper provides a comprehensive survey of methods for fine-tuning large language models in scenarios with limited data. It discusses parameter-efficient techniques to reduce training and deployment costs, domain and cross-lingual adaptation strategies for both encoder and decoder models, as well as model specialization approaches. The paper also explores preference alignment methods that guide model behavior with limited human or synthetic feedback, emphasizing efficiency in sample and compute usage. Trade-offs, selection criteria, and best practices for choosing suitable techniques based on task constraints are highlighted, including considerations for model scaling, data scaling, and mitigating catastrophic forgetting. The aim of the survey is to offer practical insights for researchers and practitioners on effectively fine-tuning large language models in resource-constrained environments.<br /><br />Summary: <div>
arXiv:2411.09539v2 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) with limited data poses a practical challenge in low-resource languages, specialized domains, and constrained deployment settings. While pre-trained LLMs provide strong foundations, effective adaptation under data scarcity requires focused and efficient fine-tuning techniques. This paper presents a structured and practical survey of recent methods for fine-tuning LLMs in data-scarce scenarios. We systematically review parameter-efficient fine-tuning techniques that lower training and deployment costs, domain and cross-lingual adaptation methods for both encoder and decoder models, and model specialization strategies. We further examine preference alignment approaches that guide model behavior using limited human or synthetic feedback, emphasizing sample and compute efficiency. Throughout, we highlight empirical trade-offs, selection criteria, and best practices for choosing suitable techniques based on task constraints, including model scaling, data scaling, and the mitigation of catastrophic forgetting. The aim is to equip researchers and practitioners with actionable insights for effectively fine-tuning LLMs when data and resources are limited.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Learning with Response Time: Robust Losses and Guarantees</title>
<link>https://arxiv.org/abs/2505.22820</link>
<guid>https://arxiv.org/abs/2505.22820</guid>
<content:encoded><![CDATA[
<div> response time data, human preference learning, reward model elicitation, Evidence Accumulation Drift Diffusion model, Neyman-orthogonal loss functions

Summary:
This paper explores the integration of response time data in human preference learning frameworks to enhance reward model elicitation. By incorporating response time information with binary choice data using the EZ model, the study develops Neyman-orthogonal loss functions that achieve optimal convergence rates for reward model learning. The theoretical analysis reveals that conventional preference learning exhibits error rates that increase exponentially with reward magnitude, whereas the response time-augmented approach reduces this to polynomial scaling, enhancing sample efficiency. The study extends these findings to non-parametric reward function spaces, establishing convergence properties for complex models. Experimental validation in image preference learning reinforces the theoretical results. <div>
arXiv:2505.22820v2 Announce Type: cross 
Abstract: This paper investigates the integration of response time data into human preference learning frameworks for more effective reward model elicitation. While binary preference data has become fundamental in fine-tuning foundation models, generative AI systems, and other large-scale models, the valuable temporal information inherent in user decision-making remains largely unexploited. We propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model, under which response time is informative of the preference strength. We develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning, matching the theoretical optimal rates that would be attained if the expected response times for each query were known a priori. Our theoretical analysis demonstrates that for linear reward functions, conventional preference learning suffers from error rates that scale exponentially with reward magnitude. In contrast, our response time-augmented approach reduces this to polynomial scaling, representing a significant improvement in sample efficiency. We extend these guarantees to non-parametric reward function spaces, establishing convergence properties for more complex, realistic reward models. Our extensive experiments validate our theoretical findings in the context of preference learning over images.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback Lunch: Deep Feedback Codes for Wiretap Channels</title>
<link>https://arxiv.org/abs/2510.16620</link>
<guid>https://arxiv.org/abs/2510.16620</guid>
<content:encoded><![CDATA[
arXiv:2510.16620v2 Announce Type: cross 
Abstract: We consider reversely-degraded wiretap channels, for which the secrecy capacity is zero if there is no channel feedback. This work focuses on a seeded modular code design for the Gaussian wiretap channel with channel output feedback, combining universal hash functions for security and learned feedback-based codes for reliability to achieve positive secrecy rates. We study the trade-off between communication reliability and information leakage, illustrating that feedback enables agreeing on a secret key shared between legitimate parties, overcoming the security advantage of the wiretapper. Our findings also motivate code designs for sensing-assisted secure communication, to be used in next-generation integrated sensing and communication methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2510.23617</link>
<guid>https://arxiv.org/abs/2510.23617</guid>
<content:encoded><![CDATA[
arXiv:2510.23617v1 Announce Type: cross 
Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by jointly analyzing data from multiple modalities typically text and images offering a richer and more accurate interpretation than unimodal approaches. In this paper, we first propose BERT-ViT-EF, a novel model that combines powerful Transformer-based encoders BERT for textual input and ViT for visual input through an early fusion strategy. This approach facilitates deeper cross-modal interactions and more effective joint representation learning. To further enhance the model's capability, we propose an extension called the Dual Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN incorporates an additional Transformer encoder layer after BERT to refine textual context (before fusion) and employs contrastive learning to align text and image representations, fostering robust multimodal feature learning. Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo demonstrate the effectiveness of our approach. DTCN achieves best accuracy (78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements highlight the benefits of early fusion and deeper contextual modeling in Transformer-based multimodal sentiment analysis.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short Ticketing Detection Framework Analysis Report</title>
<link>https://arxiv.org/abs/2510.23619</link>
<guid>https://arxiv.org/abs/2510.23619</guid>
<content:encoded><![CDATA[
arXiv:2510.23619v1 Announce Type: cross 
Abstract: This report presents a comprehensive analysis of an unsupervised multi-expert machine learning framework for detecting short ticketing fraud in railway systems. The study introduces an A/B/C/D station classification system that successfully identifies suspicious patterns across 30 high-risk stations. The framework employs four complementary algorithms: Isolation Forest, Local Outlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include the identification of five distinct short ticketing patterns and potential for short ticketing recovery in transportation systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Genotype-Phenotype Integration through Machine Learning and Personalized Gene Regulatory Networks for Cancer Metastasis Prediction</title>
<link>https://arxiv.org/abs/2510.23620</link>
<guid>https://arxiv.org/abs/2510.23620</guid>
<content:encoded><![CDATA[
arXiv:2510.23620v1 Announce Type: cross 
Abstract: Metastasis is the leading cause of cancer-related mortality, yet most predictive models rely on shallow architectures and neglect patient-specific regulatory mechanisms. Here, we integrate classical machine learning and deep learning to predict metastatic potential across multiple cancer types. Gene expression profiles from the Cancer Cell Line Encyclopedia were combined with a transcription factor-target prior from DoRothEA, focusing on nine metastasis-associated regulators. After selecting differential genes using the Kruskal-Wallis test, ElasticNet, Random Forest, and XGBoost models were trained for benchmarking. Personalized gene regulatory networks were then constructed using PANDA and LIONESS and analyzed through a graph attention neural network (GATv2) to learn topological and expression-based representations. While XGBoost achieved the highest AUROC (0.7051), the GNN captured non-linear regulatory dependencies at the patient level. These results demonstrate that combining traditional machine learning with graph-based deep learning enables a scalable and interpretable framework for metastasis risk prediction in precision oncology.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields</title>
<link>https://arxiv.org/abs/2510.23621</link>
<guid>https://arxiv.org/abs/2510.23621</guid>
<content:encoded><![CDATA[
arXiv:2510.23621v1 Announce Type: cross 
Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at high computational cost. For SO(3)-equivariant models such as MACE, there is little systematic evidence on whether reduced-precision arithmetic and GPU-optimized kernels can cut this cost without harming physical fidelity. This thesis aims to make MACE cheaper and faster while preserving accuracy by identifying computational bottlenecks and evaluating low-precision execution policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32 accumulation) for inference, short NVT and long NPT water simulations, and toy training runs under reproducible, steady-state timing. cuEquivariance reduces inference latency by about $3\times$. Casting only linear layers to BF16/FP16 within an FP32 model yields roughly 4x additional speedups, while energies and thermodynamic observables in NVT/NPT MD remain within run-to-run variability. Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq modules without explicit adapters causes representation mismatches. Fused equivariant kernels and mixed-precision inference can substantially accelerate state-of-the-art force fields with negligible impact on downstream MD. A practical policy is to use cuEquivariance with FP32 by default and enable BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum throughput, while training remains in FP32. Further gains are expected on Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and pipeline fusion.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media</title>
<link>https://arxiv.org/abs/2510.23626</link>
<guid>https://arxiv.org/abs/2510.23626</guid>
<content:encoded><![CDATA[
arXiv:2510.23626v1 Announce Type: cross 
Abstract: Social media user-generated content (UGC) provides real-time, self-reported indicators of mental health conditions such as depression, offering a valuable source for predictive analytics. While prior studies integrate medical knowledge to improve prediction accuracy, they overlook the opportunity to simultaneously expand such knowledge through predictive processes. We develop a Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that integrates prediction and knowledge expansion in an iterative learning cycle. In the knowledge-aware depression detection phase, the LLM jointly performs depression detection and entity extraction, while the knowledge graph represents and weights these entities to refine prediction performance. In the knowledge refinement and expansion phase, new entities, relationships, and entity types extracted by the LLM are incorporated into the knowledge graph under expert supervision, enabling continual knowledge evolution. Using large-scale UGC, the framework enhances both predictive accuracy and medical understanding. Expert evaluations confirmed the discovery of clinically meaningful symptoms, comorbidities, and social triggers complementary to existing literature. We conceptualize and operationalize prediction-through-learning and learning-through-prediction as mutually reinforcing processes, advancing both methodological and theoretical understanding in predictive analytics. The framework demonstrates the co-evolution of computational models and domain knowledge, offering a foundation for adaptive, data-driven knowledge systems applicable to other dynamic risk monitoring contexts.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Development of a Publishing Imprint: Xynapse Traces</title>
<link>https://arxiv.org/abs/2510.23627</link>
<guid>https://arxiv.org/abs/2510.23627</guid>
<content:encoded><![CDATA[
arXiv:2510.23627v1 Announce Type: cross 
Abstract: Xynapse Traces is an experimental publishing imprint created via a fusion of human and algorithmic methods using a configuration-driven architecture and a multi-model AI integration framework. The system achieved a remarkable 90% reduction in time-to-market (from a typical 6-12 months to just 2-4 weeks), with 80% cost reduction compared to traditional imprint development, while publishing 52 books in its first year and maintaining exceptional quality metrics, including 99% citation accuracy and 100% validation success after initial corrections. Key technical innovations include a continuous ideation pipeline with tournament-style evaluation, a novel codex design for transcriptive meditation practice, comprehensive automation spanning from ideation through production and distribution, and publisher personas that define and guide the imprint's mission. The system also integrates automated verification with human oversight, ensuring that gains in speed do not compromise publishing standards. This effort has significant implications for the future of book publishing, suggesting new paradigms for human-AI collaboration that democratize access to sophisticated publishing capabilities and make previously unviable niche markets accessible.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Execution Supervision Promotes General Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.23629</link>
<guid>https://arxiv.org/abs/2510.23629</guid>
<content:encoded><![CDATA[
arXiv:2510.23629v1 Announce Type: cross 
Abstract: Building robust and general reasoning ability is a central goal in the development of large language models (LLMs). Recent efforts increasingly turn to code as a rich training source, given its inherent logical structure and diverse reasoning paradigms such as divide-and-conquer, topological ordering, and enumeration. However, reasoning in code is often expressed implicitly and entangled with syntactic or implementation noise, making direct training on raw code suboptimal.To address this, we introduce TracePile, a large-scale corpus of 2.6 million samples that transforms code execution into explicit, step-by-step chain-of-thought-style rationales, which we call Chain of Execution (CoE). The corpus spans domains including mathematics, classical algorithms and algorithmic competition, and is enriched with variable-tracing questions and code rewritings to enhance logical granularity and code diversity. We evaluate TracePile using three training setups: continue-pretraining, instruction tuning after pretraining, and two-stage finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5, and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and algorithms demonstrate consistent improvements. Notably, TracePile boosts LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NUM2EVENT: Interpretable Event Reasoning from Numerical time-series</title>
<link>https://arxiv.org/abs/2510.23630</link>
<guid>https://arxiv.org/abs/2510.23630</guid>
<content:encoded><![CDATA[
arXiv:2510.23630v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated impressive multimodal reasoning capabilities, yet their understanding of purely numerical time-series signals remains limited. Existing approaches mainly focus on forecasting or trend description, without uncovering the latent events that drive numerical changes or explaining the reasoning process behind them. In this work, we introduce the task of number-to-event reasoning and decoding, which aims to infer interpretable structured events from numerical inputs, even when current text is unavailable. To address the data scarcity and semantic alignment challenges, we propose a reasoning-aware framework that integrates an agent-guided event extractor (AGE), a marked multivariate Hawkes-based synthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a time-series encoder with a structured decoder. Our model explicitly reasons over numerical changes, generates intermediate explanations, and outputs structured event hypotheses. Experiments on multi-domain datasets show that our method substantially outperforms strong LLM baselines in event-level precision and recall. These results suggest a new direction for bridging quantitative reasoning and semantic understanding, enabling LLMs to explain and predict events directly from numerical dynamics.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling</title>
<link>https://arxiv.org/abs/2510.23631</link>
<guid>https://arxiv.org/abs/2510.23631</guid>
<content:encoded><![CDATA[
arXiv:2510.23631v1 Announce Type: cross 
Abstract: Alignment of large language models (LLMs) has predominantly relied on pairwise preference optimization, where annotators select the better of two responses to a prompt. While simple, this approach overlooks the opportunity to learn from richer forms of human feedback, such as multiwise comparisons and top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a unified framework that bridges preference optimization with (ranked) choice modeling via maximum likelihood estimation. The framework is flexible, supporting both utility-based and rank-based choice models. It subsumes several existing pairwise methods (e.g., DPO, SimPO), while providing principled training objectives for richer feedback formats. We instantiate this framework with two representative ranked choice models (Multinomial Logit and Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms competitive baselines. RCPO shows how directly leveraging ranked preference data, combined with the right choice models, yields more effective alignment. It offers a versatile and extensible foundation for incorporating (ranked) choice modeling into LLM training.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression</title>
<link>https://arxiv.org/abs/2510.23632</link>
<guid>https://arxiv.org/abs/2510.23632</guid>
<content:encoded><![CDATA[
arXiv:2510.23632v1 Announce Type: cross 
Abstract: The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models</title>
<link>https://arxiv.org/abs/2510.23633</link>
<guid>https://arxiv.org/abs/2510.23633</guid>
<content:encoded><![CDATA[
arXiv:2510.23633v1 Announce Type: cross 
Abstract: Pretrained diffusion models have demonstrated strong capabilities in zero-shot inverse problem solving by incorporating observation information into the generation process of the diffusion models. However, this presents an inherent dilemma: excessive integration can disrupt the generative process, while insufficient integration fails to emphasize the constraints imposed by the inverse problem. To address this, we propose \emph{Noise Combination Sampling}, a novel method that synthesizes an optimal noise vector from a noise subspace to approximate the measurement score, replacing the noise term in the standard Denoising Diffusion Probabilistic Models process. This enables conditional information to be naturally embedded into the generation process without reliance on step-wise hyperparameter tuning. Our method can be applied to a wide range of inverse problem solvers, including image compression, and, particularly when the number of generation steps $T$ is small, achieves superior performance with negligible computational overhead, significantly improving robustness and stability.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monotone and Separable Set Functions: Characterizations and Neural Models</title>
<link>https://arxiv.org/abs/2510.23634</link>
<guid>https://arxiv.org/abs/2510.23634</guid>
<content:encoded><![CDATA[
arXiv:2510.23634v1 Announce Type: cross 
Abstract: Motivated by applications for set containment problems, we consider the following fundamental problem: can we design set-to-vector functions so that the natural partial order on sets is preserved, namely $S\subseteq T \text{ if and only if } F(S)\leq F(T) $. We call functions satisfying this property Monotone and Separating (MAS) set functions. % We establish lower and upper bounds for the vector dimension necessary to obtain MAS functions, as a function of the cardinality of the multisets and the underlying ground set. In the important case of an infinite ground set, we show that MAS functions do not exist, but provide a model called our which provably enjoys a relaxed MAS property we name "weakly MAS" and is stable in the sense of Holder continuity. We also show that MAS functions can be used to construct universal models that are monotone by construction and can approximate all monotone set functions. Experimentally, we consider a variety of set containment tasks. The experiments show the benefit of using our our model, in comparison with standard set models which do not incorporate set containment as an inductive bias. Our code is available in https://github.com/yonatansverdlov/Monotone-Embedding.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning</title>
<link>https://arxiv.org/abs/2510.23635</link>
<guid>https://arxiv.org/abs/2510.23635</guid>
<content:encoded><![CDATA[
arXiv:2510.23635v1 Announce Type: cross 
Abstract: Any digital personal assistant, whether used to support task performance, answer questions, or manage work and daily life, including fitness schedules, requires high-quality annotations to function properly. However, user annotations, whether actively produced or inferred from context (e.g., data from smartphone sensors), are often subject to errors and noise. Previous research on Skeptical Learning (SKEL) addressed the issue of noisy labels by comparing offline active annotations with passive data, allowing for an evaluation of annotation accuracy. However, this evaluation did not include confirmation from end-users, the best judges of their own context. In this study, we evaluate SKEL's performance in real-world conditions with actual users who can refine the input labels based on their current perspectives and needs. The study involves university students using the iLog mobile application on their devices over a period of four weeks. The results highlight the challenges of finding the right balance between user effort and data quality, as well as the potential benefits of using SKEL, which include reduced annotation effort and improved quality of collected data.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation</title>
<link>https://arxiv.org/abs/2510.23636</link>
<guid>https://arxiv.org/abs/2510.23636</guid>
<content:encoded><![CDATA[
arXiv:2510.23636v1 Announce Type: cross 
Abstract: Flight delay prediction has become a key focus in air traffic management, as delays highlight inefficiencies that impact overall network performance. This paper presents a lightweight large language model-based multimodal flight delay prediction, formulated from the perspective of air traffic controllers monitoring aircraft delay after entering the terminal area. The approach integrates trajectory representations with textual aeronautical information, including flight information, weather reports, and aerodrome notices, by adapting trajectory data into the language modality to capture airspace conditions. Experimental results show that the model consistently achieves sub-minute prediction error by effectively leveraging contextual information related to the sources of delay. The framework demonstrates that linguistic understanding, when combined with cross-modality adaptation of trajectory information, enhances delay prediction. Moreover, the approach shows practicality and scalability for real-world operations, supporting real-time updates that refine predictions upon receiving new operational information.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Textual and Structural Information for Premise Selection in Lean</title>
<link>https://arxiv.org/abs/2510.23637</link>
<guid>https://arxiv.org/abs/2510.23637</guid>
<content:encoded><![CDATA[
arXiv:2510.23637v1 Announce Type: cross 
Abstract: Premise selection is a key bottleneck for scaling theorem proving in large formal libraries. Yet existing language-based methods often treat premises in isolation, ignoring the web of dependencies that connects them. We present a graph-augmented approach that combines dense text embeddings of Lean formalizations with graph neural networks over a heterogeneous dependency graph capturing both state--premise and premise--premise relations. On the LeanDojo Benchmark, our method outperforms the ReProver language-based baseline by over 25% across standard retrieval metrics. These results demonstrate the power of relational information for more effective premise selection.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Function Approximation and Device Physics via Negative Differential Resistance Networks</title>
<link>https://arxiv.org/abs/2510.23638</link>
<guid>https://arxiv.org/abs/2510.23638</guid>
<content:encoded><![CDATA[
arXiv:2510.23638v1 Announce Type: cross 
Abstract: Achieving fully analog neural computation requires hardware that can natively implement both linear and nonlinear operations with high efficiency. While analogue matrix-vector multiplication has advanced via compute-in-memory architectures, nonlinear activation functions remain a bottleneck, often requiring digital or hybrid solutions. Inspired by the Kolmogorov-Arnold framework, we propose KANalogue, a fully analogue implementation of Kolmogorov-Arnold Networks (KANs) using negative differential resistance devices as physical realizations of learnable univariate basis functions. By leveraging the intrinsic negative differential resistance characteristics of tunnel diodes fabricated from NbSi2N4/HfSi2N4 heterostructures, we construct coordinate-wise nonlinearities with distinct curvature and support profiles. We extract I-V data from fabricated armchair and zigzag devices, fit high-order polynomials to emulate diode behavior in software, and train KANs on vision benchmarks using these learned basis functions. Our results demonstrate that KANalogue can approximate complex functions with minimal parameters while maintaining classification accuracy competitive with digital baselines. This work bridges device-level physics and function approximation theory, charting a path toward scalable, energy-efficient analogue machine learning systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Genomics into Multimodal EHR Foundation Models</title>
<link>https://arxiv.org/abs/2510.23639</link>
<guid>https://arxiv.org/abs/2510.23639</guid>
<content:encoded><![CDATA[
arXiv:2510.23639v1 Announce Type: cross 
Abstract: This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning</title>
<link>https://arxiv.org/abs/2510.23640</link>
<guid>https://arxiv.org/abs/2510.23640</guid>
<content:encoded><![CDATA[
arXiv:2510.23640v1 Announce Type: cross 
Abstract: Multimodal molecular models often suffer from 3D conformer unreliability and modality collapse, limiting their robustness and generalization. We propose MuMo, a structured multimodal fusion framework that addresses these challenges in molecular representation through two key strategies. To reduce the instability of conformer-dependent fusion, we design a Structured Fusion Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and stable structural prior. To mitigate modality collapse caused by naive fusion, we introduce a Progressive Injection (PI) mechanism that asymmetrically integrates this prior into the sequence stream, preserving modality-specific modeling while enabling cross-modal enrichment. Built on a state space backbone, MuMo supports long-range dependency modeling and robust information propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and MoleculeNet, MuMo achieves an average improvement of 2.7% over the best-performing baseline on each task, ranking first on 22 of them, including a 27% improvement on the LD50 task. These results validate its robustness to 3D conformer noise and the effectiveness of multimodal fusion in molecular representation. The code is available at: github.com/selmiss/MuMo.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging</title>
<link>https://arxiv.org/abs/2510.23641</link>
<guid>https://arxiv.org/abs/2510.23641</guid>
<content:encoded><![CDATA[
arXiv:2510.23641v1 Announce Type: cross 
Abstract: Transformers are very effective in capturing both global and local correlations within high-energy particle collisions, but they present deployment challenges in high-data-throughput environments, such as the CERN LHC. The quadratic complexity of transformer models demands substantial resources and increases latency during inference. In order to address these issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a physics-inspired enhancement of the linformer architecture that maintains linear attention. Our method incorporates spatially aware partitioning of particles based on kinematic features, thereby computing attention between regions of physical significance. Additionally, we employ convolutional layers to capture local correlations, informed by insights from jet physics. In addition to outperforming the standard linformer in jet classification tasks, SAL-T also achieves classification results comparable to full-attention transformers, while using considerably fewer resources with lower latency during inference. Experiments on a generic point cloud classification dataset (ModelNet10) further confirm this trend. Our code is available at https://github.com/aaronw5/SAL-T4HEP.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisCoder2: Building Multi-Language Visualization Coding Agents</title>
<link>https://arxiv.org/abs/2510.23642</link>
<guid>https://arxiv.org/abs/2510.23642</guid>
<content:encoded><![CDATA[
arXiv:2510.23642v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection</title>
<link>https://arxiv.org/abs/2510.23643</link>
<guid>https://arxiv.org/abs/2510.23643</guid>
<content:encoded><![CDATA[
arXiv:2510.23643v1 Announce Type: cross 
Abstract: The globalized semiconductor supply chain has made Hardware Trojans (HT) a significant security threat to embedded systems, necessitating the design of efficient and adaptable detection mechanisms. Despite promising machine learning-based HT detection techniques in the literature, they suffer from ad hoc feature selection and the lack of adaptivity, all of which hinder their effectiveness across diverse HT attacks. In this paper, we propose SAND, a selfsupervised and adaptive NAS-driven framework for efficient HT detection. Specifically, this paper makes three key contributions. (1) We leverage self-supervised learning (SSL) to enable automated feature extraction, eliminating the dependency on manually engineered features. (2) SAND integrates neural architecture search (NAS) to dynamically optimize the downstream classifier, allowing for seamless adaptation to unseen benchmarks with minimal fine-tuning. (3) Experimental results show that SAND achieves a significant improvement in detection accuracy (up to 18.3%) over state-of-the-art methods, exhibits high resilience against evasive Trojans, and demonstrates strong generalization.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoGBot: Relationship-Oblivious Graph-based Neural Network with Contextual Knowledge for Bot Detection</title>
<link>https://arxiv.org/abs/2510.23648</link>
<guid>https://arxiv.org/abs/2510.23648</guid>
<content:encoded><![CDATA[
arXiv:2510.23648v1 Announce Type: cross 
Abstract: Detecting automated accounts (bots) among genuine users on platforms like Twitter remains a challenging task due to the evolving behaviors and adaptive strategies of such accounts. While recent methods have achieved strong detection performance by combining text, metadata, and user relationship information within graph-based frameworks, many of these models heavily depend on explicit user-user relationship data. This reliance limits their applicability in scenarios where such information is unavailable. To address this limitation, we propose a novel multimodal framework that integrates detailed textual features with enriched user metadata while employing graph-based reasoning without requiring follower-following data. Our method uses transformer-based models (e.g., BERT) to extract deep semantic embeddings from tweets, which are aggregated using max pooling to form comprehensive user-level representations. These are further combined with auxiliary behavioral features and passed through a GraphSAGE model to capture both local and global patterns in user behavior. Experimental results on the Cresci-15, Cresci-17, and PAN 2019 datasets demonstrate the robustness of our approach, achieving accuracies of 99.8%, 99.1%, and 96.8%, respectively, and highlighting its effectiveness against increasingly sophisticated bot strategies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Low Rank Attention for Long-Context Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2510.23649</link>
<guid>https://arxiv.org/abs/2510.23649</guid>
<content:encoded><![CDATA[
arXiv:2510.23649v1 Announce Type: cross 
Abstract: As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\(r\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs</title>
<link>https://arxiv.org/abs/2510.23650</link>
<guid>https://arxiv.org/abs/2510.23650</guid>
<content:encoded><![CDATA[
arXiv:2510.23650v1 Announce Type: cross 
Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits intervention outperforms hidden-layer approaches. We show semantic-aware logits intervention is stable and effective for debiasing aligned LLMs.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2510.23652</link>
<guid>https://arxiv.org/abs/2510.23652</guid>
<content:encoded><![CDATA[
arXiv:2510.23652v1 Announce Type: cross 
Abstract: Although large language models (LLMs) have achieved revolutionary breakthroughs in many fields, their large model size and high computational cost pose significant challenges for practical deployment on resource-constrained edge devices. To this end, layer pruning has been proposed to reduce the computational overhead by directly removing redundant layers. However, existing layer pruning methods typically rely on hand-crafted metrics to evaluate and remove individual layers, while ignoring the dependencies between layers. This can disrupt the model's information flow and severely degrade performance. To address these issues, we propose CLP, a novel continuous layer pruning framework that introduces two key innovations: a differentiable concave gate algorithm that automatically identifies the best continuous layer segments for pruning via gradient-based optimization; and a cutoff endpoint tuning strategy that effectively restores model performance by fine-tuning only the layers adjacent to the pruned segments. Extensive experiments across multiple model architectures (including LLaMA2, LLaMA3 and Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly outperforms existing state-of-the-art baselines. For example, at a pruning rate of $20\%$, CLP achieves an average performance retention of $95.34\%$ on LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can be seamlessly combined with quantization to further compress the model with only a slight performance loss.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting</title>
<link>https://arxiv.org/abs/2510.23656</link>
<guid>https://arxiv.org/abs/2510.23656</guid>
<content:encoded><![CDATA[
arXiv:2510.23656v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) play a significant role in an increasing body of research on traffic forecasting due to their effectively capturing spatiotemporal patterns embedded in traffic data. A general assumption of training the said forecasting models via mean squared error estimation is that the errors across time steps and spatial positions are uncorrelated. However, this assumption does not really hold because of the autocorrelation caused by both the temporality and spatiality of traffic data. This gap limits the performance of DNN-based forecasting models and is overlooked by current studies. To fill up this gap, this paper proposes Spatiotemporally Autocorrelated Error Adjustment (SAEA), a novel and general framework designed to systematically adjust autocorrelated prediction errors in traffic forecasting. Unlike existing approaches that assume prediction errors follow a random Gaussian noise distribution, SAEA models these errors as a spatiotemporal vector autoregressive (VAR) process to capture their intrinsic dependencies. First, it explicitly captures both spatial and temporal error correlations by a coefficient matrix, which is then embedded into a newly formulated cost function. Second, a structurally sparse regularization is introduced to incorporate prior spatial information, ensuring that the learned coefficient matrix aligns with the inherent road network structure. Finally, an inference process with test-time error adjustment is designed to dynamically refine predictions, mitigating the impact of autocorrelated errors in real-time forecasting. The effectiveness of the proposed approach is verified on different traffic datasets. Results across a wide range of traffic forecasting models show that our method enhances performance in almost all cases.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Diffusion Language Models via Unpaired Preference Optimization</title>
<link>https://arxiv.org/abs/2510.23658</link>
<guid>https://arxiv.org/abs/2510.23658</guid>
<content:encoded><![CDATA[
arXiv:2510.23658v1 Announce Type: cross 
Abstract: Diffusion language models (dLLMs) are an emerging alternative to autoregressive (AR) generators, but aligning them to human preferences is challenging because sequence log-likelihoods are intractable and pairwise preference data are costly to collect. We introduce ELBO-KTO, which combines an ELBO surrogate for diffusion log-likelihoods with a prospect-theoretic, unpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze the bias and variance induced by the ELBO substitution and employ variance-reduction practices that stabilize gradients during training. Applied to LLaDA-8B-Instruct, ELBO-KTO yields \textbf{65.9\%} and \textbf{62.3\%} adjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively, versus the base model under an automatic LLM judge. Across downstream tasks, including GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO trained on UltraFeedback-Binary performs on par with or better than the base model under identical decoding. This establishes unpaired preference optimization as a viable alternative to pairwise alignment in diffusion LLMs.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quanvolutional Neural Networks for Pneumonia Detection: An Efficient Quantum-Assisted Feature Extraction Paradigm</title>
<link>https://arxiv.org/abs/2510.23660</link>
<guid>https://arxiv.org/abs/2510.23660</guid>
<content:encoded><![CDATA[
arXiv:2510.23660v1 Announce Type: cross 
Abstract: Pneumonia poses a significant global health challenge, demanding accurate and timely diagnosis. While deep learning, particularly Convolutional Neural Networks (CNNs), has shown promise in medical image analysis for pneumonia detection, CNNs often suffer from high computational costs, limitations in feature representation, and challenges in generalizing from smaller datasets. To address these limitations, we explore the application of Quanvolutional Neural Networks (QNNs), leveraging quantum computing for enhanced feature extraction. This paper introduces a novel hybrid quantum-classical model for pneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a quanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2 image patches, employing rotational Y-gates for data encoding and entangling layers to generate non-classical feature representations. These quantum-extracted features are then fed into a classical neural network for classification. Experimental results demonstrate that the proposed QNN achieves a higher validation accuracy of 83.33 percent compared to a comparable classical CNN which achieves 73.33 percent. This enhanced convergence and sample efficiency highlight the potential of QNNs for medical image analysis, particularly in scenarios with limited labeled data. This research lays the foundation for integrating quantum computing into deep-learning-driven medical diagnostic systems, offering a computationally efficient alternative to traditional approaches.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentsway -- Software Development Methodology for AI Agents-based Teams</title>
<link>https://arxiv.org/abs/2510.23664</link>
<guid>https://arxiv.org/abs/2510.23664</guid>
<content:encoded><![CDATA[
arXiv:2510.23664v1 Announce Type: cross 
Abstract: The emergence of Agentic AI is fundamentally transforming how software is designed, developed, and maintained. Traditional software development methodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for human-centric teams and are increasingly inadequate in environments where autonomous AI agents contribute to planning, coding, testing, and continuous learning. To address this methodological gap, we present "Agentsway" a novel software development framework designed for ecosystems where AI agents operate as first-class collaborators. Agentsway introduces a structured lifecycle centered on human orchestration, and privacy-preserving collaboration among specialized AI agents. The framework defines distinct roles for planning, prompting, coding, testing, and fine-tuning agents, each contributing to iterative improvement and adaptive learning throughout the development process. By integrating fine-tuned LLMs that leverage outputs and feedback from different agents throughout the development cycle as part of a retrospective learning process, Agentsway enhances domain-specific reasoning, and explainable decision-making across the entire software development lifecycle. Responsible AI principles are further embedded across the agents through the coordinated use of multiple fine-tuned LLMs and advanced reasoning models, ensuring balanced, transparent, and accountable decision-making. This work advances software engineering by formalizing agent-centric collaboration, integrating privacy-by-design principles, and defining measurable metrics for productivity and trust. Agentsway represents a foundational step toward the next generation of AI-native, self-improving software development methodologies. To the best of our knowledge, this is the first research effort to introduce a dedicated methodology explicitly designed for AI agent-based software engineering teams.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers from Compressed Representations</title>
<link>https://arxiv.org/abs/2510.23665</link>
<guid>https://arxiv.org/abs/2510.23665</guid>
<content:encoded><![CDATA[
arXiv:2510.23665v1 Announce Type: cross 
Abstract: Compressed file formats are the corner stone of efficient data storage and transmission, yet their potential for representation learning remains largely underexplored. We introduce TEMPEST (TransformErs froM comPressed rEpreSenTations), a method that exploits the inherent byte-stream structure of compressed files to design an effective tokenization and encoding strategy. By leveraging this compact encoding, a standard transformer can directly learn semantic representations from compressed data streams, bypassing the need for raw byte-level processing or full media decoding. Our proposal substantially reduces the number of tokens required for semantic classification, thereby lowering both computational complexity and memory usage. Through extensive experiments across diverse datasets, coding schemes, and modalities, we show that TEMPEST achieves accuracy competitive wit the state-of-the-art while delivering efficiency gains in memory and compute.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</title>
<link>https://arxiv.org/abs/2510.23667</link>
<guid>https://arxiv.org/abs/2510.23667</guid>
<content:encoded><![CDATA[
arXiv:2510.23667v1 Announce Type: cross 
Abstract: Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems</title>
<link>https://arxiv.org/abs/2510.23668</link>
<guid>https://arxiv.org/abs/2510.23668</guid>
<content:encoded><![CDATA[
arXiv:2510.23668v1 Announce Type: cross 
Abstract: Accurate traffic flow forecasting is essential for intelligent transportation systems and urban traffic management. However, single model approaches often fail to capture the complex, nonlinear, and multi scale temporal patterns in traffic flow data. This study proposes a decomposition driven hybrid framework that integrates Seasonal Trend decomposition using Loess (STL) with three complementary predictive models. STL first decomposes the original time series into trend, seasonal, and residual components. Then, a Long Short Term Memory (LSTM) network models long term trends, an Autoregressive Integrated Moving Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The final forecast is obtained through multiplicative integration of the sub model predictions. Using 998 traffic flow records from a New York City intersection between November and December 2015, results show that the LSTM ARIMA XGBoost hybrid model significantly outperforms standalone models including LSTM, ARIMA, and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy effectively isolates temporal characteristics, allowing each model to specialize, thereby improving prediction accuracy, interpretability, and robustness.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Work is AI Actually Doing? Uncovering the Drivers of Generative AI Adoption</title>
<link>https://arxiv.org/abs/2510.23669</link>
<guid>https://arxiv.org/abs/2510.23669</guid>
<content:encoded><![CDATA[
arXiv:2510.23669v1 Announce Type: cross 
Abstract: Purpose: The rapid integration of artificial intelligence (AI) systems like ChatGPT, Claude AI, etc., has a deep impact on how work is done. Predicting how AI will reshape work requires understanding not just its capabilities, but how it is actually being adopted. This study investigates which intrinsic task characteristics drive users' decisions to delegate work to AI systems. Methodology: This study utilizes the Anthropic Economic Index dataset of four million Claude AI interactions mapped to O*NET tasks. We systematically scored each task across seven key dimensions: Routine, Cognitive, Social Intelligence, Creativity, Domain Knowledge, Complexity, and Decision Making using 35 parameters. We then employed multivariate techniques to identify latent task archetypes and analyzed their relationship with AI usage. Findings: Tasks requiring high creativity, complexity, and cognitive demand, but low routineness, attracted the most AI engagement. Furthermore, we identified three task archetypes: Dynamic Problem Solving, Procedural & Analytical Work, and Standardized Operational Tasks, demonstrating that AI applicability is best predicted by a combination of task characteristics, over individual factors. Our analysis revealed highly concentrated AI usage patterns, with just 5% of tasks accounting for 59% of all interactions. Originality: This research provides the first systematic evidence linking real-world generative AI usage to a comprehensive, multi-dimensional framework of intrinsic task characteristics. It introduces a data-driven classification of work archetypes that offers a new framework for analyzing the emerging human-AI division of labor.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity and Superposition in Mixture of Experts</title>
<link>https://arxiv.org/abs/2510.23671</link>
<guid>https://arxiv.org/abs/2510.23671</guid>
<content:encoded><![CDATA[
arXiv:2510.23671v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) models have become central to scaling large language models, yet their mechanistic differences from dense networks remain poorly understood. Previous work has explored how dense models use \textit{superposition} to represent more features than dimensions, and how superposition is a function of feature sparsity and feature importance. MoE models cannot be explained mechanistically through the same lens. We find that neither feature sparsity nor feature importance cause discontinuous phase changes, and that network sparsity (the ratio of active to total experts) better characterizes MoEs. We develop new metrics for measuring superposition across experts. Our findings demonstrate that models with greater network sparsity exhibit greater \emph{monosemanticity}. We propose a new definition of expert specialization based on monosemantic feature representation rather than load balancing, showing that experts naturally organize around coherent feature combinations when initialized appropriately. These results suggest that network sparsity in MoEs may enable more interpretable models without sacrificing performance, challenging the common assumption that interpretability and capability are fundamentally at odds.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers</title>
<link>https://arxiv.org/abs/2510.23673</link>
<guid>https://arxiv.org/abs/2510.23673</guid>
<content:encoded><![CDATA[
arXiv:2510.23673v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) has emerged as a standardized interface enabling seamless integration between Large Language Models (LLMs) and external data sources and tools. While MCP significantly reduces development complexity and enhances agent capabilities, its openness and extensibility introduce critical security vulnerabilities that threaten system trustworthiness and user data protection. This paper systematically analyzes the security landscape of MCP-based systems, identifying three principal threat categories: (1) agent hijacking attacks stemming from protocol design deficiencies; (2) traditional web vulnerabilities in MCP servers; and (3) supply chain security. To address these challenges, we comprehensively survey existing defense strategies, examining both proactive server-side scanning approaches, ranging from layered detection pipelines and agentic auditing frameworks to zero-trust registry systems, and runtime interaction monitoring solutions that provide continuous oversight and policy enforcement. Our analysis reveals that MCP security fundamentally represents a paradigm shift where the attack surface extends from traditional code execution to semantic interpretation of natural language metadata, necessitating novel defense mechanisms tailored to this unique threat model.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefleXGen:The unexamined code is not worth using</title>
<link>https://arxiv.org/abs/2510.23674</link>
<guid>https://arxiv.org/abs/2510.23674</guid>
<content:encoded><![CDATA[
arXiv:2510.23674v1 Announce Type: cross 
Abstract: Security in code generation remains a pivotal challenge when applying large language models (LLMs). This paper introduces RefleXGen, an innovative method that significantly enhances code security by integrating Retrieval-Augmented Generation (RAG) techniques with guided self-reflection mechanisms inherent in LLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing specialized secure code datasets - processes that can be resource-intensive - RefleXGen iteratively optimizes the code generation process through self-assessment and reflection without the need for extensive resources. Within this framework, the model continuously accumulates and refines its knowledge base, thereby progressively improving the security of the generated code. Experimental results demonstrate that RefleXGen substantially enhances code security across multiple models, achieving a 13.6% improvement with GPT-3.5 Turbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a 5.8% improvement with Gemini. Our findings highlight that improving the quality of model self-reflection constitutes an effective and practical strategy for strengthening the security of AI-generated code.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents</title>
<link>https://arxiv.org/abs/2510.23675</link>
<guid>https://arxiv.org/abs/2510.23675</guid>
<content:encoded><![CDATA[
arXiv:2510.23675v1 Announce Type: cross 
Abstract: Modern coding agents integrated into IDEs combine powerful tools and system-level actions, exposing a high-stakes attack surface. Existing Indirect Prompt Injection (IPI) studies focus mainly on query-specific behaviors, leading to unstable attacks with lower success rates. We identify a more severe, query-agnostic threat that remains effective across diverse user inputs. This challenge can be overcome by exploiting a common vulnerability: leakage of the agent's internal prompt, which turns the attack into a constrained white-box optimization problem. We present QueryIPI, the first query-agnostic IPI method for coding agents. QueryIPI refines malicious tool descriptions through an iterative, prompt-based process informed by the leaked internal prompt. Experiments on five simulated agents show that QueryIPI achieves up to 87 percent success, outperforming baselines, and the generated malicious descriptions also transfer to real-world systems, highlighting a practical security risk to modern LLM-based coding agents.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents</title>
<link>https://arxiv.org/abs/2510.23682</link>
<guid>https://arxiv.org/abs/2510.23682</guid>
<content:encoded><![CDATA[
arXiv:2510.23682v1 Announce Type: cross 
Abstract: Large language models show promise as autonomous decision-making agents, yet their deployment in high-stakes domains remains fraught with risk. Without architectural safeguards, LLM agents exhibit catastrophic brittleness: identical capabilities produce wildly different outcomes depending solely on prompt framing. We present Chimera, a neuro-symbolic-causal architecture that integrates three complementary components - an LLM strategist, a formally verified symbolic constraint engine, and a causal inference module for counterfactual reasoning. We benchmark Chimera against baseline architectures (LLM-only, LLM with symbolic constraints) across 52-week simulations in a realistic e-commerce environment featuring price elasticity, trust dynamics, and seasonal demand. Under organizational biases toward either volume or margin optimization, LLM-only agents fail catastrophically (total loss of \$99K in volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding symbolic constraints prevents disasters but achieves only 43-87% of Chimera's profit. Chimera consistently delivers the highest returns (\$1.52M and \$1.96M respectively, some cases +\$2.2M) while improving brand trust (+1.8% and +10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+ formal verification proves zero constraint violations across all scenarios. These results establish that architectural design not prompt engineering determines the reliability of autonomous agents in production environments. We provide open-source implementations and interactive demonstrations for reproducibility.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics</title>
<link>https://arxiv.org/abs/2510.23685</link>
<guid>https://arxiv.org/abs/2510.23685</guid>
<content:encoded><![CDATA[
arXiv:2510.23685v1 Announce Type: cross 
Abstract: The nonlinear nature of chaotic systems results in extreme sensitivity to initial conditions and highly intricate dynamical behaviors, posing fundamental challenges for accurately predicting their evolution. To overcome the limitation that conventional approaches fail to capture both local features and global dependencies in chaotic time series simultaneously, this study proposes a parallel predictive framework integrating Transformer and Bidirectional Long Short-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch architecture, where the Transformer branch mainly captures long-range dependencies while the BiLSTM branch focuses on extracting local temporal features. The complementary representations from the two branches are fused in a dedicated feature-fusion layer to enhance predictive accuracy. As illustrating examples, the model's performance is systematically evaluated on two representative tasks in the Lorenz system. The first is autonomous evolution prediction, in which the model recursively extrapolates system trajectories from the time-delay embeddings of the state vector to evaluate long-term tracking accuracy and stability. The second is inference of unmeasured variable, where the model reconstructs the unobserved states from the time-delay embeddings of partial observations to assess its state-completion capability. The results consistently indicate that the proposed hybrid framework outperforms both single-branch architectures across tasks, demonstrating its robustness and effectiveness in chaotic system prediction.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Societal Impact of Machine Learning</title>
<link>https://arxiv.org/abs/2510.23693</link>
<guid>https://arxiv.org/abs/2510.23693</guid>
<content:encoded><![CDATA[
arXiv:2510.23693v1 Announce Type: cross 
Abstract: This PhD thesis investigates the societal impact of machine learning (ML). ML increasingly informs consequential decisions and recommendations, significantly affecting many aspects of our lives. As these data-driven systems are often developed without explicit fairness considerations, they carry the risk of discriminatory effects. The contributions in this thesis enable more appropriate measurement of fairness in ML systems, systematic decomposition of ML systems to anticipate bias dynamics, and effective interventions that reduce algorithmic discrimination while maintaining system utility. I conclude by discussing ongoing challenges and future research directions as ML systems, including generative artificial intelligence, become increasingly integrated into society. This work offers a foundation for ensuring that ML's societal impact aligns with broader social values.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debiasing Reward Models by Representation Learning with Guarantees</title>
<link>https://arxiv.org/abs/2510.23751</link>
<guid>https://arxiv.org/abs/2510.23751</guid>
<content:encoded><![CDATA[
arXiv:2510.23751v1 Announce Type: cross 
Abstract: Recent alignment techniques, such as reinforcement learning from human feedback, have been widely adopted to align large language models with human preferences by learning and leveraging reward models. In practice, these models often exploit spurious correlations, involving, e.g., response length, discrimination, sycophancy, and conceptual bias, which is a problem that has received increasing attention. In this work, we propose a principled framework that mitigates these biases in reward models while preserving the underlying factors that reflect intended preferences. We first provide a formulation of the data-generating process, assuming that the observed data (e.g., text) is generated from both spurious and non-spurious latent variables. We show that, interestingly, these non-spurious latent variables can be theoretically identified from data, regardless of whether a surrogate for the spurious latent variables is available. This further inspires a practical method that uses variational inference to recover these variables and leverages them to train reward models. Experiments on synthetic and real-world datasets demonstrate that our method effectively mitigates spurious correlation issues and yields more robust reward models.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Robustness to Catastrophic Forgetting Through Incremental Concept Formation</title>
<link>https://arxiv.org/abs/2510.23756</link>
<guid>https://arxiv.org/abs/2510.23756</guid>
<content:encoded><![CDATA[
arXiv:2510.23756v1 Announce Type: cross 
Abstract: Catastrophic forgetting remains a central challenge in continual learning, where models are required to integrate new knowledge over time without losing what they have previously learned. In prior work, we introduced Cobweb/4V, a hierarchical concept formation model that exhibited robustness to catastrophic forgetting in visual domains. Motivated by this robustness, we examine three hypotheses regarding the factors that contribute to such stability: (1) adaptive structural reorganization enhances knowledge retention, (2) sparse and selective updates reduce interference, and (3) information-theoretic learning based on sufficiency statistics provides advantages over gradient-based backpropagation. To test these hypotheses, we compare Cobweb/4V with neural baselines, including CobwebNN, a neural implementation of the Cobweb framework introduced in this work. Experiments on datasets of varying complexity (MNIST, Fashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring enhances learning plasticity, sparse updates help mitigate interference, and the information-theoretic learning process preserves prior knowledge without revisiting past data. Together, these findings provide insight into mechanisms that can mitigate catastrophic forgetting and highlight the potential of concept-based, information-theoretic approaches for building stable and adaptive continual learning systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TDFlow: Agentic Workflows for Test Driven Software Engineering</title>
<link>https://arxiv.org/abs/2510.23761</link>
<guid>https://arxiv.org/abs/2510.23761</guid>
<content:encoded><![CDATA[
arXiv:2510.23761v1 Announce Type: cross 
Abstract: We introduce TDFlow, a novel test-driven agentic workflow that frames repository-scale software engineering as a test-resolution task, specifically designed to solve human-written tests. Given a set of tests, TDFlow repeatedly proposes, revises, and debugs repository-scale patches using precisely engineered sub-agents and tightly constrained tools. The workflow decomposes software engineering program repair into four components governed by respective sub-agents. This simple, forced decoupling of patch proposing, debugging, patch revision, and optional test generation (1) reduces long-context burden on any individual sub-agent, (2) focuses each sub-agent on specific, pre-defined sub-tasks, and (3) allows for specialized performance improvement on specific sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and 94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which were subsequently counted as failures. Furthermore, we show that the primary obstacle to human-level software engineering performance lies within writing successful reproduction tests. We envision a human-LLM interactive system powered by TDFlow where human developers write tests solved by LLM systems. Together, these results indicate that modern LLMs, when embedded in a narrowly engineered, test-driven workflow, already achieve human-level test resolution -- with the final frontier for fully autonomous repository repair being the accurate generation of valid reproduction tests.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices</title>
<link>https://arxiv.org/abs/2510.23775</link>
<guid>https://arxiv.org/abs/2510.23775</guid>
<content:encoded><![CDATA[
arXiv:2510.23775v1 Announce Type: cross 
Abstract: The increasing realism of AI-generated imagery poses challenges for verifying visual authenticity. We present an explainable image authenticity detection system that combines a lightweight convolutional classifier ("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify, localize, and explain artifacts in 32x32 images. Our model achieves 96.5% accuracy on the extended CiFAKE dataset augmented with adversarial perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling deployment on local or edge devices. Using autoencoder-based reconstruction error maps, we generate artifact localization heatmaps, which enhance interpretability for both humans and the VLM. We further categorize 70 visual artifact types into eight semantic groups and demonstrate explainable text generation for each detected anomaly. This work highlights the feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery and outlines potential cross-domain applications in forensics, industrial inspection, and social media moderation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting</title>
<link>https://arxiv.org/abs/2510.23785</link>
<guid>https://arxiv.org/abs/2510.23785</guid>
<content:encoded><![CDATA[
arXiv:2510.23785v1 Announce Type: cross 
Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition and structural relationships rather than relying on class identity. However, most existing counting models fail to replicate this ability; they often miscount when objects exhibit complex shapes, internal symmetry, or overlapping components. In this work, we introduce CountFormer, a transformer-based framework that learns to recognize repetition and structural coherence for class-agnostic object counting. Built upon the CounTR architecture, our model replaces its visual encoder with the self-supervised foundation model DINOv2, which produces richer and spatially consistent feature representations. We further incorporate positional embedding fusion to preserve geometric relationships before decoding these features into density maps through a lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model achieves performance comparable to current state-of-the-art methods while demonstrating superior accuracy on structurally intricate or densely packed scenes. Our findings indicate that integrating foundation models such as DINOv2 enables counting systems to approach human-like structural perception, advancing toward a truly general and exemplar-free counting paradigm.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras</title>
<link>https://arxiv.org/abs/2510.23798</link>
<guid>https://arxiv.org/abs/2510.23798</guid>
<content:encoded><![CDATA[
arXiv:2510.23798v1 Announce Type: cross 
Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a pressing environmental concern, exerting a detrimental influence on biodiversity, water quality, and human activities such as navigation and recreation. The present study proposes a novel methodological framework for the monitoring the aforementioned waste, utilising fixed, in-situ cameras. This study provides two key contributions: (i) the continuous quantification and monitoring of floating debris using deep learning and (ii) the identification of the most suitable deep learning model in terms of accuracy and inference speed under complex environmental conditions. These models are tested in a range of environmental conditions and learning configurations, including experiments on biases related to data leakage. Furthermore, a geometric model is implemented to estimate the actual size of detected objects from a 2D image. This model takes advantage of both intrinsic and extrinsic characteristics of the camera. The findings of this study underscore the significance of the dataset constitution protocol, particularly with respect to the integration of negative images and the consideration of temporal leakage. In conclusion, the feasibility of metric object estimation using projective geometry coupled with regression corrections is demonstrated. This approach paves the way for the development of robust, low-cost, automated monitoring systems for urban aquatic environments.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection</title>
<link>https://arxiv.org/abs/2510.23845</link>
<guid>https://arxiv.org/abs/2510.23845</guid>
<content:encoded><![CDATA[
arXiv:2510.23845v1 Announce Type: cross 
Abstract: Detecting mental health crisis situations such as suicide ideation, rape, domestic violence, child abuse, and sexual harassment is a critical yet underexplored challenge for language models. When such situations arise during user--model interactions, models must reliably flag them, as failure to do so can have serious consequences. In this work, we introduce CRADLE BENCH, a benchmark for multi-faceted crisis detection. Unlike previous efforts that focus on a limited set of crisis types, our benchmark covers seven types defined in line with clinical standards and is the first to incorporate temporal labels. Our benchmark provides 600 clinician-annotated evaluation examples and 420 development examples, together with a training corpus of around 4K examples automatically labeled using a majority-vote ensemble of multiple language models, which significantly outperforms single-model annotation. We further fine-tune six crisis detection models on subsets defined by consensus and unanimous ensemble agreement, providing complementary models trained under different agreement criteria.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Model for Contextual Biasing Score Learning and Filtering</title>
<link>https://arxiv.org/abs/2510.23849</link>
<guid>https://arxiv.org/abs/2510.23849</guid>
<content:encoded><![CDATA[
arXiv:2510.23849v1 Announce Type: cross 
Abstract: Contextual biasing improves automatic speech recognition (ASR) by integrating external knowledge, such as user-specific phrases or entities, during decoding. In this work, we use an attention-based biasing decoder to produce scores for candidate phrases based on acoustic information extracted by an ASR encoder, which can be used to filter out unlikely phrases and to calculate bonus for shallow-fusion biasing. We introduce a per-token discriminative objective that encourages higher scores for ground-truth phrases while suppressing distractors. Experiments on the Librispeech biasing benchmark show that our method effectively filters out majority of the candidate phrases, and significantly improves recognition accuracy under different biasing conditions when the scores are used in shallow fusion biasing. Our approach is modular and can be used with any ASR system, and the filtering mechanism can potentially boost performance of other biasing methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs</title>
<link>https://arxiv.org/abs/2510.23854</link>
<guid>https://arxiv.org/abs/2510.23854</guid>
<content:encoded><![CDATA[
arXiv:2510.23854v1 Announce Type: cross 
Abstract: In modern industry systems like multi-turn chat agents, Text-to-SQL technology bridges natural language (NL) questions and database (DB) querying. The conversion of tabular DB results into NL representations (NLRs) enables the chat-based interaction. Currently, NLR generation is typically handled by large language models (LLMs), but information loss or errors in presenting tabular results in NL remains largely unexplored. This paper introduces a novel evaluation method - Combo-Eval - for judgment of LLM-generated NLRs that combines the benefits of multiple existing methods, optimizing evaluation fidelity and achieving a significant reduction in LLM calls by 25-61%. Accompanying our method is NLR-BIRD, the first dedicated dataset for NLR benchmarking. Through human evaluations, we demonstrate the superior alignment of Combo-Eval with human judgments, applicable across scenarios with and without ground truth references.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling</title>
<link>https://arxiv.org/abs/2510.23866</link>
<guid>https://arxiv.org/abs/2510.23866</guid>
<content:encoded><![CDATA[
arXiv:2510.23866v1 Announce Type: cross 
Abstract: This work presents a physics-conditioned latent diffusion model tailored for dynamical downscaling of atmospheric data, with a focus on reconstructing high-resolution 2-m temperature fields. Building upon a pre-existing diffusion architecture and employing a residual formulation against a reference UNet, we integrate a partial differential equation (PDE) loss term into the model's training objective. The PDE loss is computed in the full resolution (pixel) space by decoding the latent representation and is designed to enforce physical consistency through a finite-difference approximation of an effective advection-diffusion balance. Empirical observations indicate that conventional diffusion training already yields low PDE residuals, and we investigate how fine-tuning with this additional loss further regularizes the model and enhances the physical plausibility of the generated fields. The entirety of our codebase is available on Github, for future reference and development.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning</title>
<link>https://arxiv.org/abs/2510.23870</link>
<guid>https://arxiv.org/abs/2510.23870</guid>
<content:encoded><![CDATA[
arXiv:2510.23870v1 Announce Type: cross 
Abstract: We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge 2025, a bilingual benchmark requiring complex reasoning such as arithmetic, commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding the second-best system by more than 6% in execution accuracy (EX), with 55.0% in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA). Our system follows an agentic framework with two components: Planner agent that generates stepwise natural language plans, and SQL agent that converts these plans into executable SQL. Since SQL agent reliably adheres to the plan, our refinements focus on the planner. Unlike prior methods that rely on multiple sub-agents for planning and suffer from orchestration overhead, we introduce a feedback-guided meta-prompting strategy to refine a single planner. Failure cases from a held-out set are clustered with human input, and an LLM distills them into corrective guidelines that are integrated into the planner's system prompt, improving generalization without added complexity. For the multilingual scenario, to address transliteration and entity mismatch issues, we incorporate entity-linking guidelines that generate alternative surface forms for entities and explicitly include them in the plan. Finally, we enhance reliability through plan diversification: multiple candidate plans are generated for each query, with the SQL agent producing a query for each plan, and final output selected via majority voting over their executions.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs</title>
<link>https://arxiv.org/abs/2510.23891</link>
<guid>https://arxiv.org/abs/2510.23891</guid>
<content:encoded><![CDATA[
arXiv:2510.23891v1 Announce Type: cross 
Abstract: Text watermarking for large language models (LLMs) enables model owners to verify text origin and protect intellectual property. While watermarking methods for closed-source LLMs are relatively mature, extending them to open-source models remains challenging, as developers cannot control the decoding process. Consequently, owners of open-source LLMs lack practical means to verify whether text was generated by their models. A core difficulty lies in embedding watermarks directly into model weights without hurting detectability. A promising idea is to distill watermarks from a closed-source model into an open one, but this suffers from (i) poor detectability due to mismatch between learned and predefined patterns, and (ii) fragility to downstream modifications such as fine-tuning or model merging. To overcome these limitations, we propose PRO, a Precise and Robust text watermarking method for open-source LLMs. PRO jointly trains a watermark policy model with the LLM, producing patterns that are easier for the model to learn and more consistent with detection criteria. A regularization term further simulates downstream perturbations and penalizes degradation in watermark detectability, ensuring robustness under model edits. Experiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO substantially improves both watermark detectability and resilience to model modifications.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the effectiveness of LLM-based interoperability</title>
<link>https://arxiv.org/abs/2510.23893</link>
<guid>https://arxiv.org/abs/2510.23893</guid>
<content:encoded><![CDATA[
arXiv:2510.23893v1 Announce Type: cross 
Abstract: Background: Systems of systems are becoming increasingly dynamic and heterogeneous, and this adds pressure on the long-standing challenge of interoperability. Besides its technical aspect, interoperability has also an economic side, as development time efforts are required to build the interoperability artifacts. Objectives: With the recent advances in the field of large language models (LLMs), we aim at analyzing the effectiveness of LLM-based strategies to make systems interoperate autonomously, at runtime, without human intervention. Method: We selected 13 open source LLMs and curated four versions of a dataset in the agricultural interoperability use case. We performed three runs of each model with each version of the dataset, using two different strategies. Then we compared the effectiveness of the models and the consistency of their results across multiple runs. Results: qwen2.5-coder:32b was the most effective model using both strategies DIRECT (average pass@1 >= 0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset versions. In the fourth dataset version, which included an unit conversion, all models using the strategy DIRECT failed, whereas using CODEGEN qwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some LLMs can make systems interoperate autonomously. Further evaluation in different domains is recommended, and further research on reliability strategies should be conducted.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees</title>
<link>https://arxiv.org/abs/2510.23901</link>
<guid>https://arxiv.org/abs/2510.23901</guid>
<content:encoded><![CDATA[
arXiv:2510.23901v1 Announce Type: cross 
Abstract: Mixed-integer programming (MIP) has emerged as a powerful framework for learning optimal decision trees. Yet, existing MIP approaches for regression tasks are either limited to purely binary features or become computationally intractable when continuous, large-scale data are involved. Naively binarizing continuous features sacrifices global optimality and often yields needlessly deep trees. We recast the optimal regression-tree training as a two-stage optimization problem and propose Reduced-Space Optimal Regression Trees (RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches exclusively on tree-structural variables. This design guarantees the algorithm's convergence and its independence from the number of training samples. Leveraging the model's structure, we introduce several bound tightening techniques - closed-form leaf prediction, empirical threshold discretization, and exact depth-1 subtree parsing - that combine with decomposable upper and lower bounding strategies to accelerate the training. The BB node-wise decomposition enables trivial parallel execution, further alleviating the computational intractability even for million-size datasets. Based on the empirical studies on several regression benchmarks containing both binary and continuous features, RS-ORT also delivers superior training and testing performance than state-of-the-art methods. Notably, on datasets with up to 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed training performance with a simpler tree structure and a better generalization ability in four hours.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Interventions on Deep Networks for Causal Discovery in Subsystems</title>
<link>https://arxiv.org/abs/2510.23906</link>
<guid>https://arxiv.org/abs/2510.23906</guid>
<content:encoded><![CDATA[
arXiv:2510.23906v1 Announce Type: cross 
Abstract: Causal discovery uncovers complex relationships between variables, enhancing predictions, decision-making, and insights into real-world systems, especially in nonlinear multivariate time series. However, most existing methods primarily focus on pairwise cause-effect relationships, overlooking interactions among groups of variables, i.e., subsystems and their collective causal influence. In this study, we introduce gCDMI, a novel multi-group causal discovery method that leverages group-level interventions on trained deep neural networks and employs model invariance testing to infer causal relationships. Our approach involves three key steps. First, we use deep learning to jointly model the structural relationships among groups of all time series. Second, we apply group-wise interventions to the trained model. Finally, we conduct model invariance testing to determine the presence of causal links among variable groups. We evaluate our method on simulated datasets, demonstrating its superior performance in identifying group-level causal relationships compared to existing methods. Additionally, we validate our approach on real-world datasets, including brain networks and climate ecosystems. Our results highlight that applying group-level interventions to deep learning models, combined with invariance testing, can effectively reveal complex causal structures, offering valuable insights for domains such as neuroscience and climate science.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</title>
<link>https://arxiv.org/abs/2510.23907</link>
<guid>https://arxiv.org/abs/2510.23907</guid>
<content:encoded><![CDATA[
arXiv:2510.23907v1 Announce Type: cross 
Abstract: Scene-level captioning in instructional videos can enhance learning by requiring an understanding of both visual cues and temporal structure. By aligning visual cues with textual guidance, this understanding supports procedural learning and multimodal reasoning, providing a richer context for skill acquisition. However, captions that fail to capture this structure may lack coherence and quality, which can create confusion and undermine the video's educational intent. To address this gap, we introduce DynaStride, a pipeline to generate coherent, scene-level captions without requiring manual scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride performs adaptive frame sampling and multimodal windowing to capture key transitions within each scene. It then employs a multimodal chain-of-thought process to produce multiple action-object pairs, which are refined and fused using a dynamic stride window selection algorithm that adaptively balances temporal context and redundancy. The final scene-level caption integrates visual semantics and temporal reasoning in a single instructional caption. Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o, demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses further show that DynaStride produces captions that are more temporally coherent and informative, suggesting a promising direction for improving AI-powered instructional content generation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers</title>
<link>https://arxiv.org/abs/2510.23912</link>
<guid>https://arxiv.org/abs/2510.23912</guid>
<content:encoded><![CDATA[
arXiv:2510.23912v1 Announce Type: cross 
Abstract: The Query, Key, Value weight triplet is a building block of current attention mechanisms in state-of-the-art LLMs. We theoretically investigate whether this triplet can be reduced, proving under simplifying assumptions that the Query weights are redundant, thereby reducing the number of non-embedding/lm-head parameters by over 8%. We validate the theory on full-complexity GPT-3 small architectures (with layer normalization, skip connections, and weight decay) trained from scratch, demonstrating that the reduced model achieves comparable validation loss to standard baselines. These findings motivate the investigation of the Query weight redundancy at scale.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-based Automated Claim Matching with Instruction-following LLMs</title>
<link>https://arxiv.org/abs/2510.23924</link>
<guid>https://arxiv.org/abs/2510.23924</guid>
<content:encoded><![CDATA[
arXiv:2510.23924v1 Announce Type: cross 
Abstract: We present a novel agent-based approach for the automated claim matching task with instruction-following LLMs. We propose a two-step pipeline that first generates prompts with LLMs, to then perform claim matching as a binary classification task with LLMs. We demonstrate that LLM-generated prompts can outperform SOTA with human-generated prompts, and that smaller LLMs can do as well as larger ones in the generation process, allowing to save computational resources. We also demonstrate the effectiveness of using different LLMs for each step of the pipeline, i.e. using an LLM for prompt generation, and another for claim matching. Our investigation into the prompt generation process in turn reveals insights into the LLMs' understanding of claim matching.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFiSP: A Multimodal Fire Spread Prediction Framework</title>
<link>https://arxiv.org/abs/2510.23934</link>
<guid>https://arxiv.org/abs/2510.23934</guid>
<content:encoded><![CDATA[
arXiv:2510.23934v1 Announce Type: cross 
Abstract: The 2019-2020 Black Summer bushfires in Australia devastated 19 million hectares, destroyed 3,000 homes, and lasted seven months, demonstrating the escalating scale and urgency of wildfire threats requiring better forecasting for effective response. Traditional fire modeling relies on manual interpretation by Fire Behaviour Analysts (FBAns) and static environmental data, often leading to inaccuracies and operational limitations. Emerging data sources, such as NASA's FIRMS satellite imagery and Volunteered Geographic Information, offer potential improvements by enabling dynamic fire spread prediction. This study proposes a Multimodal Fire Spread Prediction Framework (MFiSP) that integrates social media data and remote sensing observations to enhance forecast accuracy. By adapting fuel map manipulation strategies between assimilation cycles, the framework dynamically adjusts fire behavior predictions to align with the observed rate of spread. We evaluate the efficacy of MFiSP using synthetically generated fire event polygons across multiple scenarios, analyzing individual and combined impacts on forecast perimeters. Results suggest that our MFiSP integrating multimodal data can improve fire spread prediction beyond conventional methods reliant on FBAn expertise and static inputs.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable GPU-Based Integrity Verification for Large Machine Learning Models</title>
<link>https://arxiv.org/abs/2510.23938</link>
<guid>https://arxiv.org/abs/2510.23938</guid>
<content:encoded><![CDATA[
arXiv:2510.23938v1 Announce Type: cross 
Abstract: We present a security framework that strengthens distributed machine learning by standardizing integrity protections across CPU and GPU platforms and significantly reducing verification overheads. Our approach co-locates integrity verification directly with large ML model execution on GPU accelerators, resolving the fundamental mismatch between how large ML workloads typically run (primarily on GPUs) and how security verifications traditionally operate (on separate CPU-based processes), delivering both immediate performance benefits and long-term architectural consistency. By performing cryptographic operations natively on GPUs using dedicated compute units (e.g., Intel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the potential architectural bottlenecks that could plague traditional CPU-based verification systems when dealing with large models. This approach leverages the same GPU-based high-memory bandwidth and parallel processing primitives that power ML workloads ensuring integrity checks keep pace with model execution even for massive models exceeding 100GB. This framework establishes a common integrity verification mechanism that works consistently across different GPU vendors and hardware configurations. By anticipating future capabilities for creating secure channels between trusted execution environments and GPU accelerators, we provide a hardware-agnostic foundation that enterprise teams can deploy regardless of their underlying CPU and GPU infrastructures.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Biological Multifunctionality with Echo State Networks</title>
<link>https://arxiv.org/abs/2510.23940</link>
<guid>https://arxiv.org/abs/2510.23940</guid>
<content:encoded><![CDATA[
arXiv:2510.23940v1 Announce Type: cross 
Abstract: In this work, a three-dimensional multicomponent reaction-diffusion model has been developed, combining excitable-system dynamics with diffusion processes and sharing conceptual features with the FitzHugh-Nagumo model. Designed to capture the spatiotemporal behavior of biological systems, particularly electrophysiological processes, the model was solved numerically to generate time-series data. These data were subsequently used to train and evaluate an Echo State Network (ESN), which successfully reproduced the system's dynamic behavior. The results demonstrate that simulating biological dynamics using data-driven, multifunctional ESN models is both feasible and effective.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs</title>
<link>https://arxiv.org/abs/2510.23941</link>
<guid>https://arxiv.org/abs/2510.23941</guid>
<content:encoded><![CDATA[
arXiv:2510.23941v1 Announce Type: cross 
Abstract: We introduce a novel, training free cascade for auto-prompting Large Language Models (LLMs) to assess product quality in e-commerce. Our system requires no training labels or model fine-tuning, instead automatically generating and refining prompts for evaluating attribute quality across tens of thousands of product category-attribute pairs. Starting from a seed of human-crafted prompts, the cascade progressively optimizes instructions to meet catalog-specific requirements. This approach bridges the gap between general language understanding and domain-specific knowledge at scale in complex industrial catalogs. Our extensive empirical evaluations shows the auto-prompt cascade improves precision and recall by $8-10\%$ over traditional chain-of-thought prompting. Notably, it achieves these gains while reducing domain expert effort from 5.1 hours to 3 minutes per attribute - a $99\%$ reduction. Additionally, the cascade generalizes effectively across five languages and multiple quality assessment tasks, consistently maintaining performance gains.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChessQA: Evaluating Large Language Models for Chess Understanding</title>
<link>https://arxiv.org/abs/2510.23948</link>
<guid>https://arxiv.org/abs/2510.23948</guid>
<content:encoded><![CDATA[
arXiv:2510.23948v1 Announce Type: cross 
Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and abstraction capabilities of large language models (LLMs), as it has well-defined structure and objective ground truth while admitting a wide spectrum of skill levels. However, existing evaluations of LLM ability in chess are ad hoc and narrow in scope, making it difficult to accurately measure LLM chess understanding and how it varies with scale, post-training methodologies, or architecture choices. We present ChessQA, a comprehensive benchmark that assesses LLM chess understanding across five task categories (Structural, Motifs, Short Tactics, Position Judgment, and Semantic), which approximately correspond to the ascending abstractions that players master as they accumulate chess knowledge, from understanding basic rules and learning tactical motifs to correctly calculating tactics, evaluating positions, and semantically describing high-level concepts. In this way, ChessQA captures a more comprehensive picture of chess ability and understanding, going significantly beyond the simple move quality evaluations done previously, and offers a controlled, consistent setting for diagnosis and comparison. Furthermore, ChessQA is inherently dynamic, with prompts, answer keys, and construction scripts that can evolve as models improve. Evaluating a range of contemporary LLMs, we find persistent weaknesses across all five categories and provide results and error analyses by category. We will release the code, periodically refreshed datasets, and a public leaderboard to support further research.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2510.23949</link>
<guid>https://arxiv.org/abs/2510.23949</guid>
<content:encoded><![CDATA[
arXiv:2510.23949v1 Announce Type: cross 
Abstract: There have been a couple of studies showing that attempting to erase multilingual knowledge using only English data is insufficient for multilingual LLMs. However, their analyses remain highly performance-oriented. In this paper, we switch the point of view to evaluation, and address an additional blind spot which reveals itself when the multilingual LLM is fully finetuned with parallel multilingual dataset before unlearning. Here, language confusion occurs whereby a model responds in language different from that of the input prompt. Language confusion is a problematic phenomenon in unlearning, causing the standard reference-based metrics to fail. We tackle this phenomenon in three steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to quantitatively show the language confusion is pervasive and consistent in multilingual LLMs, (2) demonstrate that reference-based metrics result in false negatives when N-Mix score is high, and(3) suggest the need of new type of unlearning evaluation that can directly assess the content of the generated sentences. We call this type of metrics as semantic-based metric.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural USD: An object-centric framework for iterative editing and control</title>
<link>https://arxiv.org/abs/2510.23956</link>
<guid>https://arxiv.org/abs/2510.23956</guid>
<content:encoded><![CDATA[
arXiv:2510.23956v1 Announce Type: cross 
Abstract: Amazing progress has been made in controllable generative modeling, especially over the last few years. However, some challenges remain. One of them is precise and iterative object editing. In many of the current methods, trying to edit the generated image (for example, changing the color of a particular object in the scene or changing the background while keeping other elements unchanged) by changing the conditioning signals often leads to unintended global changes in the scene. In this work, we take the first steps to address the above challenges. Taking inspiration from the Universal Scene Descriptor (USD) standard developed in the computer graphics community, we introduce the "Neural Universal Scene Descriptor" or Neural USD. In this framework, we represent scenes and objects in a structured, hierarchical manner. This accommodates diverse signals, minimizes model-specific constraints, and enables per-object control over appearance, geometry, and pose. We further apply a fine-tuning approach which ensures that the above control signals are disentangled from one another. We evaluate several design considerations for our framework, demonstrating how Neural USD enables iterative and incremental workflows. More information at: https://escontrela.me/neural_usd .
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability</title>
<link>https://arxiv.org/abs/2510.23960</link>
<guid>https://arxiv.org/abs/2510.23960</guid>
<content:encoded><![CDATA[
arXiv:2510.23960v1 Announce Type: cross 
Abstract: With the rapid proliferation of digital media, the need for efficient and transparent safeguards against unsafe content is more critical than ever. Traditional image guardrail models, constrained by predefined categories, often misclassify content due to their pure feature-based learning without semantic reasoning. Moreover, these models struggle to adapt to emerging threats, requiring costly retraining for new threats. To address these limitations, we introduce SafeVision, a novel image guardrail that integrates human-like reasoning to enhance adaptability and transparency. Our approach incorporates an effective data collection and generation framework, a policy-following training pipeline, and a customized loss function. We also propose a diverse QA generation and training strategy to enhance learning effectiveness. SafeVision dynamically aligns with evolving safety policies at inference time, eliminating the need for retraining while ensuring precise risk assessments and explanations. Recognizing the limitations of existing unsafe image benchmarks, which either lack granularity or cover limited risks, we introduce VisionHarm, a high-quality dataset comprising two subsets: VisionHarm Third-party (VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse harmful categories. Through extensive experiments, we show that SafeVision achieves state-of-the-art performance on different benchmarks. SafeVision outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while being over 16x faster. SafeVision sets a comprehensive, policy-following, and explainable image guardrail with dynamic adaptation to emerging threats.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient probabilistic hardware architecture for diffusion-like models</title>
<link>https://arxiv.org/abs/2510.23972</link>
<guid>https://arxiv.org/abs/2510.23972</guid>
<content:encoded><![CDATA[
arXiv:2510.23972v1 Announce Type: cross 
Abstract: The proliferation of probabilistic AI has promoted proposals for specialized stochastic computers. Despite promising efficiency gains, these proposals have failed to gain traction because they rely on fundamentally limited modeling techniques and exotic, unscalable hardware. In this work, we address these shortcomings by proposing an all-transistor probabilistic computer that implements powerful denoising models at the hardware level. A system-level analysis indicates that devices based on our architecture could achieve performance parity with GPUs on a simple image benchmark using approximately 10,000 times less energy.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2510.23974</link>
<guid>https://arxiv.org/abs/2510.23974</guid>
<content:encoded><![CDATA[
arXiv:2510.23974v1 Announce Type: cross 
Abstract: Text-to-image diffusion models rely on text embeddings from a pre-trained text encoder, but these embeddings remain fixed across all diffusion timesteps, limiting their adaptability to the generative process. We propose Diffusion Adaptive Text Embedding (DATE), which dynamically updates text embeddings at each diffusion timestep based on intermediate perturbed data. We formulate an optimization problem and derive an update rule that refines the text embeddings at each sampling step to improve alignment and preference between the mean predicted image and the text. This allows DATE to dynamically adapts the text conditions to the reverse-diffused images throughout diffusion sampling without requiring additional model training. Through theoretical analysis and empirical results, we show that DATE maintains the generative capability of the model while providing superior text-image alignment over fixed text embeddings across various tasks, including multi-concept generation and text-guided image editing. Our code is available at https://github.com/aailab-kaist/DATE.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing</title>
<link>https://arxiv.org/abs/2510.23980</link>
<guid>https://arxiv.org/abs/2510.23980</guid>
<content:encoded><![CDATA[
arXiv:2510.23980v1 Announce Type: cross 
Abstract: We present a novel algorithm, \hdgc, that marries graph convolution with binding and bundling operations in hyperdimensional computing for transductive graph learning. For prediction accuracy \hdgc outperforms major and popular graph neural network implementations as well as state-of-the-art hyperdimensional computing implementations for a collection of homophilic graphs and heterophilic graphs. Compared with the most accurate learning methodologies we have tested, on the same target GPU platform, \hdgc is on average 9561.0 and 144.5 times faster than \gcnii, a graph neural network implementation and HDGL, a hyperdimensional computing implementation, respectively. As the majority of the learning operates on binary vectors, we expect outstanding energy performance of \hdgc on neuromorphic and emerging process-in-memory devices.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problem</title>
<link>https://arxiv.org/abs/2510.23986</link>
<guid>https://arxiv.org/abs/2510.23986</guid>
<content:encoded><![CDATA[
arXiv:2510.23986v1 Announce Type: cross 
Abstract: Operator eigenvalue problems play a critical role in various scientific fields and engineering applications, yet numerical methods are hindered by the curse of dimensionality. Recent deep learning methods provide an efficient approach to address this challenge by iteratively updating neural networks. These methods' performance relies heavily on the spectral distribution of the given operator: larger gaps between the operator's eigenvalues will improve precision, thus tailored spectral transformations that leverage the spectral distribution can enhance their performance. Based on this observation, we propose the Spectral Transformation Network (STNet). During each iteration, STNet uses approximate eigenvalues and eigenfunctions to perform spectral transformations on the original operator, turning it into an equivalent but easier problem. Specifically, we employ deflation projection to exclude the subspace corresponding to already solved eigenfunctions, thereby reducing the search space and avoiding converging to existing eigenfunctions. Additionally, our filter transform magnifies eigenvalues in the desired region and suppresses those outside, further improving performance. Extensive experiments demonstrate that STNet consistently outperforms existing learning-based methods, achieving state-of-the-art performance in accuracy.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks</title>
<link>https://arxiv.org/abs/2510.24010</link>
<guid>https://arxiv.org/abs/2510.24010</guid>
<content:encoded><![CDATA[
arXiv:2510.24010v1 Announce Type: cross 
Abstract: Foundation models have enabled rapid progress across many specialized domains by leveraging large-scale pre-training on unlabeled data, demonstrating strong generalization to a variety of downstream tasks. While such models have gained significant attention in fields like Earth Observation, their application to Mars science remains limited. A key enabler of progress in other domains has been the availability of standardized benchmarks that support systematic evaluation. In contrast, Mars science lacks such benchmarks and standardized evaluation frameworks, which have limited progress toward developing foundation models for Martian tasks. To address this gap, we introduce Mars-Bench, the first benchmark designed to systematically evaluate models across a broad range of Mars-related tasks using both orbital and surface imagery. Mars-Bench comprises 20 datasets spanning classification, segmentation, and object detection, focused on key geologic features such as craters, cones, boulders, and frost. We provide standardized, ready-to-use datasets and baseline evaluations using models pre-trained on natural images, Earth satellite data, and state-of-the-art vision-language models. Results from all analyses suggest that Mars-specific foundation models may offer advantages over general-domain counterparts, motivating further exploration of domain-adapted pre-training. Mars-Bench aims to establish a standardized foundation for developing and comparing machine learning models for Mars science. Our data, models, and code are available at: https://mars-bench.github.io/.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2510.24012</link>
<guid>https://arxiv.org/abs/2510.24012</guid>
<content:encoded><![CDATA[
arXiv:2510.24012v1 Announce Type: cross 
Abstract: Text-to-image models have recently made significant advances in generating realistic and semantically coherent images, driven by advanced diffusion models and large-scale web-crawled datasets. However, these datasets often contain inappropriate or biased content, raising concerns about the generation of harmful outputs when provided with malicious text prompts. We propose Safe Text embedding Guidance (STG), a training-free approach to improve the safety of diffusion models by guiding the text embeddings during sampling. STG adjusts the text embeddings based on a safety function evaluated on the expected final denoised image, allowing the model to generate safer outputs without additional training. Theoretically, we show that STG aligns the underlying model distribution with safety constraints, thereby achieving safer outputs while minimally affecting generation quality. Experiments on various safety scenarios, including nudity, violence, and artist-style removal, show that STG consistently outperforms both training-based and training-free baselines in removing unsafe content while preserving the core semantic intent of input prompts. Our code is available at https://github.com/aailab-kaist/STG.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs</title>
<link>https://arxiv.org/abs/2510.24019</link>
<guid>https://arxiv.org/abs/2510.24019</guid>
<content:encoded><![CDATA[
arXiv:2510.24019v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has advanced automatic code generation, yet most approaches rely on direct, single-step translation from problem descriptions to code, disregarding structured software engineering practices. We introduce a lifecycle-aware framework that systematically incorporates intermediate artifacts such as requirements analysis, state machine modeling, and pseudocode into both the training and inference stages. This design aligns code generation with standard software development phases and enables more structured reasoning. Experiments show that lifecycle-level fine-tuning improves code correctness by up to 75% over the same model before fine-tuning, with performance gains compounding across intermediate stages. Multi-step inference consistently surpasses single-step generation, demonstrating the effectiveness of intermediate scaffolding. Notably, open-source LLMs, once fine-tuned under our framework, match or slightly outperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our framework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and 22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B, respectively. Our pipeline also proves robust with up to 80\% less training data, confirming its resilience. Ablation studies further reveal that each intermediate artifact contributes distinctly to final code quality, with state machine modeling yielding the most substantial impact. Our source code and detailed experimental data are available at https://anonymous.4open.science/r/Lifecycle-Aware-3CCB.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward</title>
<link>https://arxiv.org/abs/2510.24020</link>
<guid>https://arxiv.org/abs/2510.24020</guid>
<content:encoded><![CDATA[
arXiv:2510.24020v1 Announce Type: cross 
Abstract: Mitigating hallucinations in Large Language Models (LLMs) is critical for their reliable deployment. Existing methods typically fine-tune LLMs to abstain from answering questions beyond their knowledge scope. However, these methods often rely on coarse-grained signals to guide LLMs to abstain, such as overall confidence or uncertainty scores on multiple sampled answers, which may result in an imprecise awareness of the model's own knowledge boundaries. To this end, we propose a novel reinforcement learning framework built on $\textbf{\underline{Fi}ne-grained \underline{S}emantic \underline{Co}nfidence \underline{Re}ward (\Ours)}$, which guides LLMs to abstain via sample-specific confidence. Specifically, our method operates by sampling multiple candidate answers and conducting semantic clustering, then training the LLM to retain answers within high-confidence clusters and discard those within low-confidence ones, thereby promoting accurate post-hoc abstention. Additionally, we propose a new metric for evaluating the reliability of abstention fine-tuning tasks more comprehensively. Our method significantly enhances reliability in both in-domain and out-of-distribution benchmarks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs</title>
<link>https://arxiv.org/abs/2510.24021</link>
<guid>https://arxiv.org/abs/2510.24021</guid>
<content:encoded><![CDATA[
arXiv:2510.24021v1 Announce Type: cross 
Abstract: Knowledge Distillation (KD) has become a cornerstone technique for compressing Large Language Models (LLMs) into smaller, more efficient student models. However, conventional KD approaches typically apply the distillation loss uniformly across all tokens, regardless of the teacher's confidence. This indiscriminate mimicry can introduce noise, as the student is forced to learn from the teacher's uncertain or high-entropy predictions, which may ultimately harm student performance-especially when the teacher is much larger and more powerful. To address this, we propose Speculative Knowledge Distillation (SpecKD), a novel, plug-and-play framework that introduces a dynamic, token-level gating mechanism inspired by the "propose-and-verify" paradigm of speculative decoding. At each step, the student's token proposal is verified against the teacher's distribution; the distillation loss is selectively applied only to "accepted" tokens, while "rejected" tokens are masked out. Extensive experiments on diverse text generation tasks show that SpecKD consistently and significantly outperforms strong KD baselines, leading to more stable training and more capable student models, and achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional Connectivity Analysis</title>
<link>https://arxiv.org/abs/2510.24025</link>
<guid>https://arxiv.org/abs/2510.24025</guid>
<content:encoded><![CDATA[
arXiv:2510.24025v1 Announce Type: cross 
Abstract: Understanding the evolution of brain functional networks over time is of great significance for the analysis of cognitive mechanisms and the diagnosis of neurological diseases. Existing methods often have difficulty in capturing the temporal evolution characteristics of connections between specific functional communities. To this end, this paper proposes a new path-level trajectory modeling framework (NeuroPathNet) to characterize the dynamic behavior of connection pathways between brain functional partitions. Based on medically supported static partitioning schemes (such as Yeo and Smith ICA), we extract the time series of connection strengths between each pair of functional partitions and model them using a temporal neural network. We validate the model performance on three public functional Magnetic Resonance Imaging (fMRI) datasets, and the results show that it outperforms existing mainstream methods in multiple indicators. This study can promote the development of dynamic graph learning methods for brain network analysis, and provide possible clinical applications for the diagnosis of neurological diseases.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-temporal Multivariate Time Series Forecast with Chosen Variables</title>
<link>https://arxiv.org/abs/2510.24027</link>
<guid>https://arxiv.org/abs/2510.24027</guid>
<content:encoded><![CDATA[
arXiv:2510.24027v1 Announce Type: cross 
Abstract: Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series of $n$ spatially distributed variables in a period of recent past to forecast their values in a period of near future. It has important applications in spatio-temporal sensing forecast such as road traffic prediction and air pollution prediction. Recent papers have addressed a practical problem of missing variables in the model input, which arises in the sensing applications where the number $m$ of sensors is far less than the number $n$ of locations to be monitored, due to budget constraints. We observe that the state of the art assumes that the $m$ variables (i.e., locations with sensors) in the model input are pre-determined and the important problem of how to choose the $m$ variables in the input has never been studied. This paper fills the gap by studying a new problem of STMF with chosen variables, which optimally selects $m$-out-of-$n$ variables for the model input in order to maximize the forecast accuracy. We propose a unified framework that jointly performs variable selection and model optimization for both forecast accuracy and model efficiency. It consists of three novel technical components: (1) masked variable-parameter pruning, which progressively prunes less informative variables and attention parameters through quantile-based masking; (2) prioritized variable-parameter replay, which replays low-loss past samples to preserve learned knowledge for model stability; (3) dynamic extrapolation mechanism, which propagates information from variables selected for the input to all other variables via learnable spatial embeddings and adjacency information. Experiments on five real-world datasets show that our work significantly outperforms the state-of-the-art baselines in both accuracy and efficiency, demonstrating the effectiveness of joint variable selection and model optimization.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model</title>
<link>https://arxiv.org/abs/2510.24029</link>
<guid>https://arxiv.org/abs/2510.24029</guid>
<content:encoded><![CDATA[
arXiv:2510.24029v1 Announce Type: cross 
Abstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of vertebrates that encode environmental boundaries at specific distances and allocentric directions, playing a central role in forming place fields in the hippocampus. Most computational BVC models are restricted to two-dimensional (2D) environments, making them prone to spatial ambiguities in the presence of horizontal symmetries in the environment. To address this limitation, we incorporate vertical angular sensitivity into the BVC framework, thereby enabling robust boundary detection in three dimensions, and leading to significantly more accurate spatial localization in a biologically-inspired robot model.
  The proposed model processes LiDAR data to capture vertical contours, thereby disambiguating locations that would be indistinguishable under a purely 2D representation. Experimental results show that in environments with minimal vertical variation, the proposed 3D model matches the performance of a 2D baseline; yet, as 3D complexity increases, it yields substantially more distinct place fields and markedly reduces spatial aliasing. These findings show that adding a vertical dimension to BVC-based localization can significantly enhance navigation and mapping in real-world 3D spaces while retaining performance parity in simpler, near-planar scenarios.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning</title>
<link>https://arxiv.org/abs/2510.24036</link>
<guid>https://arxiv.org/abs/2510.24036</guid>
<content:encoded><![CDATA[
arXiv:2510.24036v1 Announce Type: cross 
Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but training very deep networks has been challenging due to the vanishing gradient problem. This paper explores Residual Networks (ResNet), introduced by He et al. (2015), which overcomes this limitation by using skip connections. ResNet enables the training of networks with hundreds of layers by allowing gradients to flow directly through shortcut connections that bypass intermediate layers. In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9% accuracy compared to 84.1% for a traditional deep CNN of similar depth, while also converging faster and training more stably.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Algorithms for Neural Combinatorial Optimization with Constraints</title>
<link>https://arxiv.org/abs/2510.24039</link>
<guid>https://arxiv.org/abs/2510.24039</guid>
<content:encoded><![CDATA[
arXiv:2510.24039v1 Announce Type: cross 
Abstract: Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an emerging paradigm for solving combinatorial problems using neural networks. In this paper, we address a central challenge of SSL for CO: solving problems with discrete constraints. We design an end-to-end differentiable framework that enables us to solve discrete constrained optimization problems with neural networks. Concretely, we leverage algorithmic techniques from the literature on convex geometry and Carath\'eodory's theorem to decompose neural network outputs into convex combinations of polytope corners that correspond to feasible sets. This decomposition-based approach enables self-supervised training but also ensures efficient quality-preserving rounding of the neural net output into feasible solutions. Extensive experiments in cardinality-constrained optimization show that our approach can consistently outperform neural baselines. We further provide worked-out examples of how our method can be applied beyond cardinality-constrained problems to a diverse set of combinatorial optimization tasks, including finding independent sets in graphs, and solving matroid-constrained problems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-Aware Generative Adversarial Networks with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.24046</link>
<guid>https://arxiv.org/abs/2510.24046</guid>
<content:encoded><![CDATA[
arXiv:2510.24046v1 Announce Type: cross 
Abstract: The utility of tabular data for tasks ranging from model training to large-scale data analysis is often constrained by privacy concerns or regulatory hurdles. While existing data generation methods, particularly those based on Generative Adversarial Networks (GANs), have shown promise, they frequently struggle with capturing complex causal relationship, maintaining data utility, and providing provable privacy guarantees suitable for enterprise deployment. We introduce CA-GAN, a novel generative framework specifically engineered to address these challenges for real-world tabular datasets. CA-GAN utilizes a two-step approach: causal graph extraction to learn a robust, comprehensive causal relationship in the data's manifold, followed by a custom Conditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates exclusively as per the structure of nodes in the causal graph. More importantly, the generator is trained with a new Reinforcement Learning-based objective that aligns the causal graphs constructed from real and fake data, ensuring the causal awareness in both training and sampling phases. We demonstrate CA-GAN superiority over six SOTA methods across 14 tabular datasets. Our evaluations, focused on core data engineering metrics: causal preservation, utility preservation, and privacy preservation. Our method offers a practical, high-performance solution for data engineers seeking to create high-quality, privacy-compliant synthetic datasets to benchmark database systems, accelerate software development, and facilitate secure data-driven research.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction</title>
<link>https://arxiv.org/abs/2510.24049</link>
<guid>https://arxiv.org/abs/2510.24049</guid>
<content:encoded><![CDATA[
arXiv:2510.24049v1 Announce Type: cross 
Abstract: Accurate and long-term spatiotemporal prediction for complex physical systems remains a fundamental challenge in scientific computing. While deep learning models, as powerful parametric approximators, have shown remarkable success, they suffer from a critical limitation: the accumulation of errors during long-term autoregressive rollouts often leads to physically implausible artifacts. This deficiency arises from their purely parametric nature, which struggles to capture the full constraints of a system's intrinsic dynamics. To address this, we introduce a novel \textbf{Retrieval-Augmented Prediction (RAP)} framework, a hybrid paradigm that synergizes the predictive power of deep networks with the grounded truth of historical data. The core philosophy of RAP is to leverage historical evolutionary exemplars as a non-parametric estimate of the system's local dynamics. For any given state, RAP efficiently retrieves the most similar historical analog from a large-scale database. The true future evolution of this analog then serves as a \textbf{reference target}. Critically, this target is not a hard constraint in the loss function but rather a powerful conditional input to a specialized dual-stream architecture. It provides strong \textbf{dynamic guidance}, steering the model's predictions towards physically viable trajectories. In extensive benchmarks across meteorology, turbulence, and fire simulation, RAP not only surpasses state-of-the-art methods but also significantly outperforms a strong \textbf{analog-only forecasting baseline}. More importantly, RAP generates predictions that are more physically realistic by effectively suppressing error divergence in long-term rollouts.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration</title>
<link>https://arxiv.org/abs/2510.24052</link>
<guid>https://arxiv.org/abs/2510.24052</guid>
<content:encoded><![CDATA[
arXiv:2510.24052v1 Announce Type: cross 
Abstract: Recent advancements in deep learning and the availability of high-quality real-world driving datasets have propelled end-to-end autonomous driving. Despite this progress, relying solely on real-world data limits the variety of driving scenarios for training. Synthetic scenario generation has emerged as a promising solution to enrich the diversity of training data; however, its application within E2E AD models remains largely unexplored. This is primarily due to the absence of a designated ego vehicle and the associated sensor inputs, such as camera or LiDAR, typically provided in real-world scenarios. To address this gap, we introduce SynAD, the first framework designed to enhance real-world E2E AD models using synthetic data. Our method designates the agent with the most comprehensive driving information as the ego vehicle in a multi-agent synthetic scenario. We further project path-level scenarios onto maps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view features without relying on sensor inputs. Finally, we devise a training strategy that effectively integrates these map-based synthetic data with real driving data. Experimental results demonstrate that SynAD effectively integrates all components and notably enhances safety performance. By bridging synthetic scenario generation and E2E AD, SynAD paves the way for more comprehensive and robust autonomous driving models.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PULSE: Privileged Knowledge Transfer from Electrodermal Activity to Low-Cost Sensors for Stress Monitoring</title>
<link>https://arxiv.org/abs/2510.24058</link>
<guid>https://arxiv.org/abs/2510.24058</guid>
<content:encoded><![CDATA[
arXiv:2510.24058v1 Announce Type: cross 
Abstract: Electrodermal activity (EDA), the primary signal for stress detection, requires costly hardware often unavailable in real-world wearables. In this paper, we propose PULSE, a framework that utilizes EDA exclusively during self-supervised pretraining, while enabling inference without EDA but with more readily available modalities such as ECG, BVP, ACC, and TEMP. Our approach separates encoder outputs into shared and private embeddings. We align shared embeddings across modalities and fuse them into a modality-invariant representation. The private embeddings carry modality-specific information to support the reconstruction objective. Pretraining is followed by knowledge transfer where a frozen EDA teacher transfers sympathetic-arousal representations into student encoders. On WESAD, our method achieves strong stress-detection performance, showing that representations of privileged EDA can be transferred to low-cost sensors to improve accuracy while reducing hardware cost.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic</title>
<link>https://arxiv.org/abs/2510.24061</link>
<guid>https://arxiv.org/abs/2510.24061</guid>
<content:encoded><![CDATA[
arXiv:2510.24061v1 Announce Type: cross 
Abstract: Low-bit floating-point (FP) formats, such as FP8, provide significant acceleration and memory savings in model training thanks to native hardware support on modern GPUs and NPUs. However, we analyze that FP8 quantization offers speedup primarily for large-dimensional matrix multiplications, while inherent quantization overheads diminish speedup when applied to low-rank adaptation (LoRA), which uses small-dimensional matrices for efficient fine-tuning of large language models (LLMs). To address this limitation, we propose FALQON, a novel framework that eliminates the quantization overhead from separate LoRA computational paths by directly merging LoRA adapters into an FP8-quantized backbone during fine-tuning. Furthermore, we reformulate the forward and backward computations for merged adapters to significantly reduce quantization overhead, and introduce a row-wise proxy update mechanism that efficiently integrates substantial updates into the quantized backbone. Experimental evaluations demonstrate that FALQON achieves approximately a 3$\times$ training speedup over existing quantized LoRA methods with a similar level of accuracy, providing a practical solution for efficient large-scale model fine-tuning. Moreover, FALQON's end-to-end FP8 workflow removes the need for post-training quantization, facilitating efficient deployment. Code is available at https://github.com/iamkanghyunchoi/falqon.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covert Surveillance in Smart Devices: A SCOUR Framework Analysis of Youth Privacy Implications</title>
<link>https://arxiv.org/abs/2510.24072</link>
<guid>https://arxiv.org/abs/2510.24072</guid>
<content:encoded><![CDATA[
arXiv:2510.24072v1 Announce Type: cross 
Abstract: This paper investigates how smart devices covertly capture private conversations and discusses in more in-depth the implications of this for youth privacy. Using a structured review guided by the PRISMA methodology, the analysis focuses on privacy concerns, data capture methods, data storage and sharing practices, and proposed technical mitigations. To structure and synthesize findings, we introduce the SCOUR framework, encompassing Surveillance mechanisms, Consent and awareness, Operational data flow, Usage and exploitation, and Regulatory and technical safeguards. Findings reveal that smart devices have been covertly capturing personal data, especially with smart toys and voice-activated smart gadgets built for youth. These issues are worsened by unclear data collection practices and insufficient transparency in smart device applications. Balancing privacy and utility in smart devices is crucial, as youth are becoming more aware of privacy breaches and value their personal data more. Strategies to improve regulatory and technical safeguards are also provided. The review identifies research gaps and suggests future directions. The limitations of this literature review are also explained. The findings have significant implications for policy development and the transparency of data collection for smart devices.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Parameterized Skills from Demonstrations</title>
<link>https://arxiv.org/abs/2510.24095</link>
<guid>https://arxiv.org/abs/2510.24095</guid>
<content:encoded><![CDATA[
arXiv:2510.24095v1 Announce Type: cross 
Abstract: We present DEPS, an end-to-end algorithm for discovering parameterized skills from expert demonstrations. Our method learns parameterized skill policies jointly with a meta-policy that selects the appropriate discrete skill and continuous parameters at each timestep. Using a combination of temporal variational inference and information-theoretic regularization methods, we address the challenge of degeneracy common in latent variable models, ensuring that the learned skills are temporally extended, semantically meaningful, and adaptable. We empirically show that learning parameterized skills from multitask expert demonstrations significantly improves generalization to unseen tasks. Our method outperforms multitask as well as skill learning baselines on both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers interpretable parameterized skills, such as an object grasping skill whose continuous arguments define the grasp location.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2510.24103</link>
<guid>https://arxiv.org/abs/2510.24103</guid>
<content:encoded><![CDATA[
arXiv:2510.24103v1 Announce Type: cross 
Abstract: We present MGAudio, a novel flow-based framework for open-domain video-to-audio generation, which introduces model-guided dual-role alignment as a central design principle. Unlike prior approaches that rely on classifier-based or classifier-free guidance, MGAudio enables the generative model to guide itself through a dedicated training objective designed for video-conditioned audio generation. The framework integrates three main components: (1) a scalable flow-based Transformer model, (2) a dual-role alignment mechanism where the audio-visual encoder serves both as a conditioning module and as a feature aligner to improve generation quality, and (3) a model-guided objective that enhances cross-modal coherence and audio realism. MGAudio achieves state-of-the-art performance on VGGSound, reducing FAD to 0.40, substantially surpassing the best classifier-free guidance baselines, and consistently outperforms existing methods across FD, IS, and alignment metrics. It also generalizes well to the challenging UnAV-100 benchmark. These results highlight model-guided dual-role alignment as a powerful and scalable paradigm for conditional video-to-audio generation. Code is available at: https://github.com/pantheon5100/mgaudio
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators</title>
<link>https://arxiv.org/abs/2510.24113</link>
<guid>https://arxiv.org/abs/2510.24113</guid>
<content:encoded><![CDATA[
arXiv:2510.24113v1 Announce Type: cross 
Abstract: Heterogeneous chiplet-based systems improve scaling by disag-gregating CPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package disaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe that in modern large-modelinference, parameters and activations routinely move backand forth from HBM/DRAM, injecting large, bursty flows into theinterposer. These memory-driven transfers inflate tail latency andviolate Service Level Agreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this gap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown under contention.We then formulate NoI synthesis as a multi-objective optimization(MOO) problem. We develop PARL (Partition-Aware ReinforcementLearner), a topology generator that balances throughput, latency,and power. PARL-generated topologies reduce contention at the memory cut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining competitive mean throughput relative to link-rich meshes. Overall, this reframes NoI design for heterogeneouschiplet accelerators with workload-aware objectives.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation</title>
<link>https://arxiv.org/abs/2510.24118</link>
<guid>https://arxiv.org/abs/2510.24118</guid>
<content:encoded><![CDATA[
arXiv:2510.24118v1 Announce Type: cross 
Abstract: Navigating to a designated goal using visual information is a fundamental capability for intelligent robots. Most classical visual navigation methods are restricted to single-goal, single-modality, and closed set goal settings. To address the practical demands of multi-modal, open-vocabulary goal queries and multi-goal visual navigation, we propose LagMemo, a navigation system that leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo constructs a unified 3D language memory. With incoming task goals, the system queries the memory, predicts candidate goal locations, and integrates a local perception-based verification mechanism to dynamically match and validate goals during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a high-quality core split distilled from GOAT-Bench tailored to multi-modal open-vocabulary multi-goal visual navigation. Experimental results show that LagMemo's memory module enables effective multi-modal open-vocabulary goal localization, and that LagMemo outperforms state-of-the-art methods in multi-goal visual navigation. Project page: https://weekgoodday.github.io/lagmemo
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Image Synthesis with Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2510.24133</link>
<guid>https://arxiv.org/abs/2510.24133</guid>
<content:encoded><![CDATA[
arXiv:2510.24133v1 Announce Type: cross 
Abstract: Despite their impressive realism, modern text-to-image models still struggle with compositionality, often failing to render accurate object counts, attributes, and spatial relations. To address this challenge, we present a training-free framework that combines an object-centric approach with self-refinement to improve layout faithfulness while preserving aesthetic quality. Specifically, we leverage large language models (LLMs) to synthesize explicit layouts from input prompts, and we inject these layouts into the image generation process, where a object-centric vision-language model (VLM) judge reranks multiple candidates to select the most prompt-aligned outcome iteratively. By unifying explicit layout-grounding with self-refine-based inference-time scaling, our framework achieves stronger scene alignment with prompts compared to recent text-to-image models. The code are available at https://github.com/gcl-inha/ReFocus.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VC4VG: Optimizing Video Captions for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2510.24134</link>
<guid>https://arxiv.org/abs/2510.24134</guid>
<content:encoded><![CDATA[
arXiv:2510.24134v1 Announce Type: cross 
Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role of high-quality video-text pairs in training models capable of producing coherent and instruction-aligned videos. However, strategies for optimizing video captions specifically for T2V training remain underexplored. In this paper, we introduce VC4VG (Video Captioning for Video Generation), a comprehensive caption optimization framework tailored to the needs of T2V models.We begin by analyzing caption content from a T2V perspective, decomposing the essential elements required for video reconstruction into multiple dimensions, and proposing a principled caption design methodology. To support evaluation, we construct VC4VG-Bench, a new benchmark featuring fine-grained, multi-dimensional, and necessity-graded metrics aligned with T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a strong correlation between improved caption quality and video generation performance, validating the effectiveness of our approach. We release all benchmark tools and code at https://github.com/qyr0403/VC4VG to support further research.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Line-Level Filtering for the Pretraining Corpora of LLMs</title>
<link>https://arxiv.org/abs/2510.24139</link>
<guid>https://arxiv.org/abs/2510.24139</guid>
<content:encoded><![CDATA[
arXiv:2510.24139v1 Announce Type: cross 
Abstract: While traditional line-level filtering techniques, such as line-level deduplication and trailing-punctuation filters, are commonly used, these basic methods can sometimes discard valuable content, negatively affecting downstream performance. In this paper, we introduce two methods-pattern-aware line-level deduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by enhancing the conventional filtering techniques. Our approach not only considers line-level signals but also takes into account their sequential distribution across documents, enabling us to retain structurally important content that might otherwise be removed. We evaluate these proposed methods by training small language models (1 B parameters) in both English and Korean. The results demonstrate that our methods consistently improve performance on multiple-choice benchmarks and significantly enhance generative question-answering accuracy on both SQuAD v1 and KorQuAD v1.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean</title>
<link>https://arxiv.org/abs/2510.24150</link>
<guid>https://arxiv.org/abs/2510.24150</guid>
<content:encoded><![CDATA[
arXiv:2510.24150v1 Announce Type: cross 
Abstract: We present Ko-MuSR, the first benchmark to comprehensively evaluate multistep, soft reasoning in long Korean narratives while minimizing data contamination. Built following MuSR, Ko-MuSR features fully Korean narratives, reasoning chains, and multiple-choice questions verified by human annotators for logical consistency and answerability. Evaluations of four large language models -- two multilingual and two Korean-specialized -- show that multilingual models outperform Korean-focused ones even in Korean reasoning tasks, indicating cross-lingual generalization of reasoning ability. Carefully designed prompting strategies, which combine few-shot examples, reasoning traces, and task-specific hints, further boost accuracy, approaching human-level performance. Ko-MuSR offers a solid foundation for advancing Korean NLP by enabling systematic evaluation of long-context reasoning and prompting strategies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning</title>
<link>https://arxiv.org/abs/2510.24152</link>
<guid>https://arxiv.org/abs/2510.24152</guid>
<content:encoded><![CDATA[
arXiv:2510.24152v1 Announce Type: cross 
Abstract: This technical report presents our solution for the RoboSense Challenge at IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving scene understanding across perception, prediction, planning, and corruption detection tasks. We propose a systematic framework built on four core components. First, a Mixture-of-Prompts router classifies questions and dispatches them to task-specific expert prompts, eliminating interference across diverse question types. Second, task-specific prompts embed explicit coordinate systems, spatial reasoning rules, role-playing, Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to each task. Third, a visual assembly module composes multi-view images with object crops, magenta markers, and adaptive historical frames based on question requirements. Fourth, we configure model inference parameters (temperature, top-p, message roles) per task to optimize output quality. Implemented on Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured prompting and spatial grounding substantially enhance VLM performance on safety-critical autonomous driving tasks. Code and prompt are available at https://github.com/wuaodi/UCAS-CSU-phase2.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Synthetic Pretraining for Inference of Stellar Mass Embedded in Dense Gas</title>
<link>https://arxiv.org/abs/2510.24159</link>
<guid>https://arxiv.org/abs/2510.24159</guid>
<content:encoded><![CDATA[
arXiv:2510.24159v1 Announce Type: cross 
Abstract: Stellar mass is a fundamental quantity that determines the properties and evolution of stars. However, estimating stellar masses in star-forming regions is challenging because young stars are obscured by dense gas and the regions are highly inhomogeneous, making spherical dynamical estimates unreliable. Supervised machine learning could link such complex structures to stellar mass, but it requires large, high-quality labeled datasets from high-resolution magneto-hydrodynamical (MHD) simulations, which are computationally expensive. We address this by pretraining a vision transformer on one million synthetic fractal images using the self-supervised framework DINOv2, and then applying the frozen model to limited high-resolution MHD simulations. Our results demonstrate that synthetic pretraining improves frozen-feature regression stellar mass predictions, with the pretrained model performing slightly better than a supervised model trained on the same limited simulations. Principal component analysis of the extracted features further reveals semantically meaningful structures, suggesting that the model enables unsupervised segmentation of star-forming regions without the need for labeled data or fine-tuning.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SymMaP: Improving Computational Efficiency in Linear Solvers through Symbolic Preconditioning</title>
<link>https://arxiv.org/abs/2510.24170</link>
<guid>https://arxiv.org/abs/2510.24170</guid>
<content:encoded><![CDATA[
arXiv:2510.24170v1 Announce Type: cross 
Abstract: Matrix preconditioning is a critical technique to accelerate the solution of linear systems, where performance heavily depends on the selection of preconditioning parameters. Traditional parameter selection approaches often define fixed constants for specific scenarios. However, they rely on domain expertise and fail to consider the instance-wise features for individual problems, limiting their performance. In contrast, machine learning (ML) approaches, though promising, are hindered by high inference costs and limited interpretability. To combine the strengths of both approaches, we propose a symbolic discovery framework-namely, Symbolic Matrix Preconditioning (SymMaP)-to learn efficient symbolic expressions for preconditioning parameters. Specifically, we employ a neural network to search the high-dimensional discrete space for expressions that can accurately predict the optimal parameters. The learned expression allows for high inference efficiency and excellent interpretability (expressed in concise symbolic formulas), making it simple and reliable for deployment. Experimental results show that SymMaP consistently outperforms traditional strategies across various benchmarks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations</title>
<link>https://arxiv.org/abs/2510.24178</link>
<guid>https://arxiv.org/abs/2510.24178</guid>
<content:encoded><![CDATA[
arXiv:2510.24178v1 Announce Type: cross 
Abstract: Sarcasm is a complex form of figurative language in which the intended meaning contradicts the literal one. Its prevalence in social media and popular culture poses persistent challenges for natural language understanding, sentiment analysis, and content moderation. With the emergence of multimodal large language models, sarcasm detection extends beyond text and requires integrating cues from audio and vision. We present MuSaG, the first German multimodal sarcasm detection dataset, consisting of 33 minutes of manually selected and human-annotated statements from German television shows. Each instance provides aligned text, audio, and video modalities, annotated separately by humans, enabling evaluation in unimodal and multimodal settings. We benchmark nine open-source and commercial models, spanning text, audio, vision, and multimodal architectures, and compare their performance to human annotations. Our results show that while humans rely heavily on audio in conversational settings, models perform best on text. This highlights a gap in current multimodal models and motivates the use of MuSaG for developing models better suited to realistic scenarios. We release MuSaG publicly to support future research on multimodal sarcasm detection and human-model alignment.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closing Gaps: An Imputation Analysis of ICU Vital Signs</title>
<link>https://arxiv.org/abs/2510.24217</link>
<guid>https://arxiv.org/abs/2510.24217</guid>
<content:encoded><![CDATA[
arXiv:2510.24217v1 Announce Type: cross 
Abstract: As more Intensive Care Unit (ICU) data becomes available, the interest in developing clinical prediction models to improve healthcare protocols increases. However, the lack of data quality still hinders clinical prediction using Machine Learning (ML). Many vital sign measurements, such as heart rate, contain sizeable missing segments, leaving gaps in the data that could negatively impact prediction performance. Previous works have introduced numerous time-series imputation techniques. Nevertheless, more comprehensive work is needed to compare a representative set of methods for imputing ICU vital signs and determine the best practice. In reality, ad-hoc imputation techniques that could decrease prediction accuracy, like zero imputation, are still used. In this work, we compare established imputation techniques to guide researchers in improving the performance of clinical prediction models by selecting the most accurate imputation technique. We introduce an extensible and reusable benchmark with currently 15 imputation and 4 amputation methods, created for benchmarking on major ICU datasets. We hope to provide a comparative basis and facilitate further ML development to bring more models into clinical practice.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling</title>
<link>https://arxiv.org/abs/2510.24235</link>
<guid>https://arxiv.org/abs/2510.24235</guid>
<content:encoded><![CDATA[
arXiv:2510.24235v1 Announce Type: cross 
Abstract: Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. While generative reward models (GRMs) offer greater interpretability than traditional scalar RMs, current training paradigms remain limited. Pair-wise methods rely on binary good-versus-bad labels, which cause mismatches for point-wise inference and necessitate complex pairing strategies for effective application in RLHF. On the other hand, point-wise methods require more elaborate absolute labeling with rubric-driven criteria, resulting in poor adaptability and high annotation costs. In this work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a unified framework that integrates a preference-aware reward (PAR) mechanism with dynamic rubric adaptation. PaTaRM leverages relative preference information from pairwise data to construct robust point-wise training signals, eliminating the need for explicit point-wise labels. Simultaneously, it employs a task-adaptive rubric system that flexibly generates evaluation criteria for both global task consistency and instance-specific fine-grained reasoning. This design enables efficient, generalizable, and interpretable reward modeling for RLHF. Extensive experiments show that PaTaRM achieves an average relative improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its effectiveness and robustness. Our code is available at https://github.com/JaneEyre0530/PaTaRM.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGNET: A Multi-Graph Attentional Network for Code Clone Detection</title>
<link>https://arxiv.org/abs/2510.24241</link>
<guid>https://arxiv.org/abs/2510.24241</guid>
<content:encoded><![CDATA[
arXiv:2510.24241v1 Announce Type: cross 
Abstract: Code clone detection is a fundamental task in software engineering that underpins refactoring, debugging, plagiarism detection, and vulnerability analysis. Existing methods often rely on singular representations such as abstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs (DFGs), which capture only partial aspects of code semantics. Hybrid approaches have emerged, but their fusion strategies are typically handcrafted and ineffective. In this study, we propose MAGNET, a multi-graph attentional framework that jointly leverages AST, CFG, and DFG representations to capture syntactic and semantic features of source code. MAGNET integrates residual graph neural networks with node-level self-attention to learn both local and long-range dependencies, introduces a gated cross-attention mechanism for fine-grained inter-graph interactions, and employs Set2Set pooling to fuse multi-graph embeddings into unified program-level representations. Extensive experiments on BigCloneBench and Google Code Jam demonstrate that MAGNET achieves state-of-the-art performance with an overall F1 score of 96.5\% and 99.2\% on the two datasets, respectively. Ablation studies confirm the critical contributions of multi-graph fusion and each attentional component. Our code is available at https://github.com/ZixianReid/Multigraph_match
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24242</link>
<guid>https://arxiv.org/abs/2510.24242</guid>
<content:encoded><![CDATA[
arXiv:2510.24242v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have recently demonstrated great potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by low Earth orbit (LEO) satellites. However, their deployment in real-world LEO satellite systems remains largely unexplored, hindered by limited onboard computing resources and brief satellite-ground contacts. We propose Grace, a satellite-ground collaborative system designed for near-realtime LVLM inference in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime inference, but larger ones on ground stations (GSs) to guarantee end-to-end performance. Grace is comprised of two main phases that are asynchronous satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch algorithm. Firstly, we still the knowledge archive of GS RAG to satellite archive with tailored adaptive update algorithm during limited satellite-ground data exchange period. Secondly, propose a confidence-based test algorithm that either processes the task onboard the satellite or offloads it to the GS. Extensive experiments based on real-world satellite orbital data show that Grace reduces the average latency by 76-95% compared to state-of-the-art methods, without compromising inference accuracy.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Design for UAV-Based Low-Altitude Wireless Networks in Unknown Environments: A Digital Twin-Assisted TD3 Approach</title>
<link>https://arxiv.org/abs/2510.24255</link>
<guid>https://arxiv.org/abs/2510.24255</guid>
<content:encoded><![CDATA[
arXiv:2510.24255v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude wireless network (LAWN), particularly when terrestrial networks are unavailable. In such scenarios, the environmental topology is typically unknown; hence, designing efficient and safe UAV trajectories is essential yet challenging. To address this, we propose a digital twin (DT)-assisted training and deployment framework. In this framework, the UAV transmits integrated sensing and communication signals to provide communication services to ground users, while simultaneously collecting echoes that are uploaded to the DT server to progressively construct virtual environments (VEs). These VEs accelerate model training and are continuously updated with real-time UAV sensing data during deployment, supporting decision-making and enhancing flight safety. Based on this framework, we further develop a trajectory design scheme that integrates simulated annealing for efficient user scheduling with the twin-delayed deep deterministic policy gradient algorithm for continuous trajectory design, aiming to minimize mission completion time while ensuring obstacle avoidance. Simulation results demonstrate that the proposed approach achieves faster convergence, higher flight safety, and shorter mission completion time compared with baseline methods, providing a robust and efficient solution for LAWN deployment in unknown environments.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2510.24261</link>
<guid>https://arxiv.org/abs/2510.24261</guid>
<content:encoded><![CDATA[
arXiv:2510.24261v1 Announce Type: cross 
Abstract: Learning generalizable robotic manipulation policies remains a key challenge due to the scarcity of diverse real-world training data. While recent approaches have attempted to mitigate this through self-supervised representation learning, most either rely on 2D vision pretraining paradigms such as masked image modeling, which primarily focus on static semantics or scene geometry, or utilize large-scale video prediction models that emphasize 2D dynamics, thus failing to jointly learn the geometry, semantics, and dynamics required for effective manipulation. In this paper, we present DynaRend, a representation learning framework that learns 3D-aware and dynamics-informed triplane features via masked reconstruction and future prediction using differentiable volumetric rendering. By pretraining on multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future dynamics, and task semantics in a unified triplane representation. The learned representations can be effectively transferred to downstream robotic manipulation tasks via action value map prediction. We evaluate DynaRend on two challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic experiments, demonstrating substantial improvements in policy success rate, generalization to environmental perturbations, and real-world applicability across diverse manipulation tasks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering</title>
<link>https://arxiv.org/abs/2510.24272</link>
<guid>https://arxiv.org/abs/2510.24272</guid>
<content:encoded><![CDATA[
arXiv:2510.24272v1 Announce Type: cross 
Abstract: Sequential decision making under uncertainty is central to many Process Systems Engineering (PSE) challenges, where traditional methods often face limitations related to controlling and optimizing complex and stochastic systems. Reinforcement Learning (RL) offers a data-driven approach to derive control policies for such challenges. This paper presents a survey and tutorial on RL methods, tailored for the PSE community. We deliver a tutorial on RL, covering fundamental concepts and key algorithmic families including value-based, policy-based and actor-critic methods. Subsequently, we survey existing applications of these RL techniques across various PSE domains, such as in fed-batch and continuous process control, process optimization, and supply chains. We conclude with PSE focused discussion of specialized techniques and emerging directions. By synthesizing the current state of RL algorithm development and implications for PSE this work identifies successes, challenges, trends, and outlines avenues for future research at the interface of these fields.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free Source Attribution of AI-generated Images via Resynthesis</title>
<link>https://arxiv.org/abs/2510.24278</link>
<guid>https://arxiv.org/abs/2510.24278</guid>
<content:encoded><![CDATA[
arXiv:2510.24278v1 Announce Type: cross 
Abstract: Synthetic image source attribution is a challenging task, especially in data scarcity conditions requiring few-shot or zero-shot classification capabilities. We present a new training-free one-shot attribution method based on image resynthesis. A prompt describing the image under analysis is generated, then it is used to resynthesize the image with all the candidate sources. The image is attributed to the model which produced the resynthesis closest to the original image in a proper feature space. We also introduce a new dataset for synthetic image attribution consisting of face images from commercial and open-source text-to-image generators. The dataset provides a challenging attribution framework, useful for developing new attribution models and testing their capabilities on different generative architectures. The dataset structure allows to test approaches based on resynthesis and to compare them to few-shot methods. Results from state-of-the-art few-shot approaches and other baselines show that the proposed resynthesis method outperforms existing techniques when only a few samples are available for training or fine-tuning. The experiments also demonstrate that the new dataset is a challenging one and represents a valuable benchmark for developing and evaluating future few-shot and zero-shot methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model</title>
<link>https://arxiv.org/abs/2510.24285</link>
<guid>https://arxiv.org/abs/2510.24285</guid>
<content:encoded><![CDATA[
arXiv:2510.24285v1 Announce Type: cross 
Abstract: The limited capacity for fine-grained visual perception presents a critical bottleneck for Vision-Language Models (VLMs) in real-world applications. Addressing this is challenging due to the scarcity of high-quality data and the limitations of existing methods: supervised fine-tuning (SFT) often compromises general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual reasoning over visual perception. To bridge this gap, we propose a novel two-stage task that structures visual perception learning as a coarse-to-fine progressive process. Based on this task formulation, we develop ViPER, a self-bootstrapping framework specifically designed to enable iterative evolution through self-critiquing and self-prediction. By synergistically integrating image-level and instance-level reconstruction with a two-stage reinforcement learning strategy, ViPER establishes a closed-loop training paradigm, where internally synthesized data directly fuel the enhancement of perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the Qwen-Viper series. With an average gain of 1.7% on seven comprehensive benchmarks spanning various tasks and up to 6.0% on fine-grained perception, Qwen-Viper consistently demonstrates superior performance across different vision-language scenarios while maintaining generalizability. Beyond enabling self-improvement in perceptual capabilities, ViPER provides concrete evidence for the reciprocal relationship between generation and understanding, a breakthrough to developing more autonomous and capable VLMs.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers can do Bayesian Clustering</title>
<link>https://arxiv.org/abs/2510.24318</link>
<guid>https://arxiv.org/abs/2510.24318</guid>
<content:encoded><![CDATA[
arXiv:2510.24318v1 Announce Type: cross 
Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding at scale. Furthermore, real-world datasets often contain missing values, and simple imputation ignores the associated uncertainty, resulting in suboptimal results. We present Cluster-PFN, a Transformer-based model that extends Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained entirely on synthetic datasets generated from a finite Gaussian Mixture Model (GMM) prior, Cluster-PFN learns to estimate the posterior distribution over both the number of clusters and the cluster assignments. Our method estimates the number of clusters more accurately than handcrafted model selection procedures such as AIC, BIC and Variational Inference (VI), and achieves clustering quality competitive with VI while being orders of magnitude faster. Cluster-PFN can be trained on complex priors that include missing data, outperforming imputation-based baselines on real-world genomic datasets, at high missingness. These results show that the Cluster-PFN can provide scalable and flexible Bayesian clustering.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.24320</link>
<guid>https://arxiv.org/abs/2510.24320</guid>
<content:encoded><![CDATA[
arXiv:2510.24320v1 Announce Type: cross 
Abstract: Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning</title>
<link>https://arxiv.org/abs/2510.24321</link>
<guid>https://arxiv.org/abs/2510.24321</guid>
<content:encoded><![CDATA[
arXiv:2510.24321v1 Announce Type: cross 
Abstract: Remote sensing applications increasingly rely on deep learning for scene classification. However, their performance is often constrained by the scarcity of labeled data and the high cost of annotation across diverse geographic and sensor domains. While recent vision-language models like CLIP have shown promise by learning transferable representations at scale by aligning visual and textual modalities, their direct application to remote sensing remains suboptimal due to significant domain gaps and the need for task-specific semantic adaptation. To address this critical challenge, we systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification. We evaluate several representative methods, including Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. These approaches reflect complementary design philosophies: from static context optimization to conditional prompts for enhanced generalization, multi-modal prompts for joint vision-language adaptation, and semantically regularized prompts for stable learning without forgetting. We benchmark these prompt-learning methods against two standard baselines: zero-shot CLIP with hand-crafted prompts and a linear probe trained on frozen CLIP features. Through extensive experiments on multiple benchmark remote sensing datasets, including cross-dataset generalization tests, we demonstrate that prompt learning consistently outperforms both baselines in few-shot scenarios. Notably, Prompting with Self-Regulating Constraints achieves the most robust cross-domain performance. Our findings underscore prompt learning as a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research in this field.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants</title>
<link>https://arxiv.org/abs/2510.24328</link>
<guid>https://arxiv.org/abs/2510.24328</guid>
<content:encoded><![CDATA[
arXiv:2510.24328v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used to answer everyday questions, yet their performance on culturally grounded and dialectal content remains uneven across languages. We propose a comprehensive method that (i) translates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into English and several Arabic dialects, (ii) converts them into open-ended questions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs under both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT) rationales to fine-tune models for step-by-step reasoning. Using this method, we extend an existing dataset in which QAs are parallelly aligned across multiple language varieties, making it, to our knowledge, the first of its kind. We conduct extensive experiments with both open and closed models. Our findings show that (i) models underperform on Arabic dialects, revealing persistent gaps in culturally grounded and dialect-specific knowledge; (ii) Arabic-centric models perform well on MCQs but struggle with OEQs; and (iii) CoT improves judged correctness while yielding mixed n-gram-based metrics. The developed dataset will be publicly released to support further research on culturally and linguistically inclusive evaluation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability</title>
<link>https://arxiv.org/abs/2510.24345</link>
<guid>https://arxiv.org/abs/2510.24345</guid>
<content:encoded><![CDATA[
arXiv:2510.24345v1 Announce Type: cross 
Abstract: Generating long, informative, and factual outputs remains a major challenge for Large Language Models (LLMs). Existing benchmarks for long-form generation typically assess real-world queries with hard-to-verify metrics or use synthetic setups that ease evaluation but overlook real-world intricacies. In this paper, we introduce \textbf{LongWeave}, which balances real-world and verifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval constructs tasks by first defining verifiable targets within real-world scenarios, then systematically generating corresponding queries, textual materials, and constraints based on these targets. This ensures that tasks are both realistic and objectively assessable, enabling rigorous assessment of model capabilities in meeting complex real-world constraints. LongWeave supports customizable input/output lengths (up to 64K/8K tokens) across seven distinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models encounter significant challenges in long-form generation as real-world complexity and output length increase.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning</title>
<link>https://arxiv.org/abs/2510.24356</link>
<guid>https://arxiv.org/abs/2510.24356</guid>
<content:encoded><![CDATA[
arXiv:2510.24356v1 Announce Type: cross 
Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic signals, decoupled from downstream decision learning $g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free perceptual properties, such as stability to nuisances, informativeness without collapse, and controlled geometry, assessed via objective representation-invariant metrics. We formalize the separation of perception and decision, define perceptual properties independent of objectives or reparameterizations, and prove that PeL updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients. Additionally, we provide a suite of task-agnostic evaluation metrics to certify perceptual quality.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering</title>
<link>https://arxiv.org/abs/2510.24402</link>
<guid>https://arxiv.org/abs/2510.24402</guid>
<content:encoded><![CDATA[
arXiv:2510.24402v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) struggles on long, structured financial filings where relevant evidence is sparse and cross-referenced. This paper presents a systematic investigation of advanced metadata-driven Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a novel, multi-stage RAG architecture that leverages LLM-generated metadata. We introduce a sophisticated indexing pipeline to create contextually rich document chunks and benchmark a spectrum of enhancements, including pre-retrieval filtering, post-retrieval reranking, and enriched embeddings, benchmarked on the FinanceBench dataset. Our results reveal that while a powerful reranker is essential for precision, the most significant performance gains come from embedding chunk metadata directly with text ("contextual chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval optimizations with these contextual embeddings to achieve superior performance. Additionally, we present a custom metadata reranker that offers a compelling, cost-effective alternative to commercial solutions, highlighting a practical trade-off between peak performance and operational efficiency. This study provides a blueprint for building robust, metadata-aware RAG systems for financial document analysis.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation</title>
<link>https://arxiv.org/abs/2510.24431</link>
<guid>https://arxiv.org/abs/2510.24431</guid>
<content:encoded><![CDATA[
arXiv:2510.24431v1 Announce Type: cross 
Abstract: The recent success of large language models (LLMs) has renewed interest in whether recommender systems can achieve similar scaling benefits. Conventional recommenders, dominated by massive embedding tables, tend to plateau as embedding dimensions grow. In contrast, the emerging generative paradigm replaces embeddings with compact Semantic ID (SID) sequences produced by autoregressive Transformers. Yet most industrial deployments remain proprietary, leaving two fundamental questions open: (1) Do the expected scaling laws hold on public benchmarks? (2) What is the minimal post-training recipe that enables competitive performance?
  We present MiniOneRec, to the best of our knowledge, the first fully open-source generative recommendation framework, which provides an end-to-end workflow spanning SID construction, supervised fine-tuning, and recommendation-oriented reinforcement learning. We generate SIDs via a Residual Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters on the Amazon Review dataset. Our experiments reveal a consistent downward trend in both training and evaluation losses with increasing model size, validating the parameter efficiency of the generative approach. To further enhance performance, we propose a lightweight yet effective post-training pipeline that (1) enforces full-process SID alignment and (2) applies reinforcement learning with constrained decoding and hybrid rewards. Together, these techniques yield significant improvements in both ranking accuracy and candidate diversity.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content</title>
<link>https://arxiv.org/abs/2510.24438</link>
<guid>https://arxiv.org/abs/2510.24438</guid>
<content:encoded><![CDATA[
arXiv:2510.24438v1 Announce Type: cross 
Abstract: Large language models are increasingly used for Islamic guidance, but risk misquoting texts, misapplying jurisprudence, or producing culturally inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar on prompts from authentic Islamic blogs. Our dual-agent framework uses a quantitative agent for citation verification and six-dimensional scoring (e.g., Structure, Islamic Consistency, Citations) and a qualitative agent for five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality). GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI followed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong performance, models still fall short in reliably producing accurate Islamic content and citations -- a paramount requirement in faith-sensitive writing. GPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led qualitative pairwise wins (116/200). Fanar, though trailing, introduces innovations for Islamic and Arabic contexts. This study underscores the need for community-driven benchmarks centering Muslim perspectives, offering an early step toward more reliable AI in Islamic knowledge and other high-stakes domains such as medicine, law, and journalism.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Visual Intelligence: Insights from Video Pretraining</title>
<link>https://arxiv.org/abs/2510.24448</link>
<guid>https://arxiv.org/abs/2510.24448</guid>
<content:encoded><![CDATA[
arXiv:2510.24448v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices</title>
<link>https://arxiv.org/abs/2510.24450</link>
<guid>https://arxiv.org/abs/2510.24450</guid>
<content:encoded><![CDATA[
arXiv:2510.24450v1 Announce Type: cross 
Abstract: While new benchmarks for large language models (LLMs) are being developed continuously to catch up with the growing capabilities of new models and AI in general, using and evaluating LLMs in non-English languages remains a little-charted landscape. We give a concise overview of recent developments in LLM benchmarking, and then propose a new taxonomy for the categorization of benchmarks that is tailored to multilingual or non-English use scenarios. We further propose a set of best practices and quality standards that could lead to a more coordinated development of benchmarks for European languages. Among other recommendations, we advocate for a higher language and culture sensitivity of evaluation methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Critique-Refine Framework for Enhancing LLM Personalization</title>
<link>https://arxiv.org/abs/2510.24469</link>
<guid>https://arxiv.org/abs/2510.24469</guid>
<content:encoded><![CDATA[
arXiv:2510.24469v1 Announce Type: cross 
Abstract: Personalized text generation requires models not only to produce coherent text but also to align with a target user's style, tone, and topical focus. Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich profiles with user and neighbor histories, but they stop at generation and often yield outputs that drift in tone, topic, or style. We present PerFine, a unified, training-free critique-refine framework that enhances personalization through iterative, profile-grounded feedback. In each iteration, an LLM generator produces a draft conditioned on the retrieved profile, and a critic LLM - also conditioned on the same profile - provides structured feedback on tone, vocabulary, sentence structure, and topicality. The generator then revises, while a novel knockout strategy retains the stronger draft across iterations. We further study additional inference-time strategies such as Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp, Goodreads, and Amazon datasets, PerFine consistently improves personalization over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5 refinement iterations, and scalability with increasing critic size. These results highlight that post-hoc, profile-aware feedback offers a powerful paradigm for personalized LLM generation that is both training-free and model-agnostic.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems</title>
<link>https://arxiv.org/abs/2510.24476</link>
<guid>https://arxiv.org/abs/2510.24476</guid>
<content:encoded><![CDATA[
arXiv:2510.24476v1 Announce Type: cross 
Abstract: Hallucination remains one of the key obstacles to the reliable deployment of large language models (LLMs), particularly in real-world applications. Among various mitigation strategies, Retrieval-Augmented Generation (RAG) and reasoning enhancement have emerged as two of the most effective and widely adopted approaches, marking a shift from merely suppressing hallucinations to balancing creativity and reliability. However, their synergistic potential and underlying mechanisms for hallucination mitigation have not yet been systematically examined. This survey adopts an application-oriented perspective of capability enhancement to analyze how RAG, reasoning enhancement, and their integration in Agentic Systems mitigate hallucinations. We propose a taxonomy distinguishing knowledge-based and logic-based hallucinations, systematically examine how RAG and reasoning address each, and present a unified framework supported by real-world applications, evaluations, and benchmarks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient and Scalable Exploration in Continuous-Time RL</title>
<link>https://arxiv.org/abs/2510.24482</link>
<guid>https://arxiv.org/abs/2510.24482</guid>
<content:encoded><![CDATA[
arXiv:2510.24482v1 Announce Type: cross 
Abstract: Reinforcement learning algorithms are typically designed for discrete-time dynamics, even though the underlying real-world control systems are often continuous in time. In this paper, we study the problem of continuous-time reinforcement learning, where the unknown system dynamics are represented using nonlinear ordinary differential equations (ODEs). We leverage probabilistic models, such as Gaussian processes and Bayesian neural networks, to learn an uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily maximizes a weighted sum of the extrinsic reward and model epistemic uncertainty. This yields a scalable and sample-efficient approach to continuous-time model-based RL. We show that COMBRL achieves sublinear regret in the reward-driven setting, and in the unsupervised RL setting (i.e., without extrinsic rewards), we provide a sample complexity bound. In our experiments, we evaluate COMBRL in both standard and unsupervised RL settings and demonstrate that it scales better, is more sample-efficient than prior methods, and outperforms baselines across several deep RL tasks.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A word association network methodology for evaluating implicit biases in LLMs compared to humans</title>
<link>https://arxiv.org/abs/2510.24488</link>
<guid>https://arxiv.org/abs/2510.24488</guid>
<content:encoded><![CDATA[
arXiv:2510.24488v1 Announce Type: cross 
Abstract: As Large language models (LLMs) become increasingly integrated into our lives, their inherent social biases remain a pressing concern. Detecting and evaluating these biases can be challenging because they are often implicit rather than explicit in nature, so developing evaluation methods that assess the implicit knowledge representations of LLMs is essential. We present a novel word association network methodology for evaluating implicit biases in LLMs based on simulating semantic priming within LLM-generated word association networks. Our prompt-based approach taps into the implicit relational structures encoded in LLMs, providing both quantitative and qualitative assessments of bias. Unlike most prompt-based evaluation methods, our method enables direct comparisons between various LLMs and humans, providing a valuable point of reference and offering new insights into the alignment of LLMs with human cognition. To demonstrate the utility of our methodology, we apply it to both humans and several widely used LLMs to investigate social biases related to gender, religion, ethnicity, sexual orientation, and political party. Our results reveal both convergences and divergences between LLM and human biases, providing new perspectives on the potential risks of using LLMs. Our methodology contributes to a systematic, scalable, and generalizable framework for evaluating and comparing biases across multiple LLMs and humans, advancing the goal of transparent and socially responsible language technologies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Wireless Transceivers: From Pilot-Efficient Channel Estimation to AI-Native 6G Receivers</title>
<link>https://arxiv.org/abs/2510.24495</link>
<guid>https://arxiv.org/abs/2510.24495</guid>
<content:encoded><![CDATA[
arXiv:2510.24495v1 Announce Type: cross 
Abstract: With the development of artificial intelligence (AI) techniques, implementing AI-based techniques to improve wireless transceivers becomes an emerging research topic. Within this context, AI-based channel characterization and estimation become the focus since these methods have not been solved by traditional methods very well and have become the bottleneck of transceiver efficiency in large-scale orthogonal frequency division multiplexing (OFDM) systems. Specifically, by formulating channel estimation as a generative AI problem, generative AI methods such as diffusion models (DMs) can efficiently deal with rough initial estimations and have great potential to cooperate with traditional signal processing methods. This paper focuses on the transceiver design of OFDM systems based on DMs, provides an illustration of the potential of DMs in wireless transceivers, and points out the related research directions brought by DMs. We also provide a proof-of-concept case study of further adapting DMs for better wireless receiver performance.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online neural fusion of distortionless differential beamformers for robust speech enhancement</title>
<link>https://arxiv.org/abs/2510.24497</link>
<guid>https://arxiv.org/abs/2510.24497</guid>
<content:encoded><![CDATA[
arXiv:2510.24497v1 Announce Type: cross 
Abstract: Fixed beamforming is widely used in practice since it does not depend on the estimation of noise statistics and provides relatively stable performance. However, a single beamformer cannot adapt to varying acoustic conditions, which limits its interference suppression capability. To address this, adaptive convex combination (ACC) algorithms have been introduced, where the outputs of multiple fixed beamformers are linearly combined to improve robustness. Nevertheless, ACC often fails in highly non-stationary scenarios, such as rapidly moving interference, since its adaptive updates cannot reliably track rapid changes. To overcome this limitation, we propose a frame-online neural fusion framework for multiple distortionless differential beamformers, which estimates the combination weights through a neural network. Compared with conventional ACC, the proposed method adapts more effectively to dynamic acoustic environments, achieving stronger interference suppression while maintaining the distortionless constraint.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference</title>
<link>https://arxiv.org/abs/2510.24498</link>
<guid>https://arxiv.org/abs/2510.24498</guid>
<content:encoded><![CDATA[
arXiv:2510.24498v1 Announce Type: cross 
Abstract: As machine learning (ML) models become increasingly deployed through cloud infrastructures, the confidentiality of user data during inference poses a significant security challenge. Homomorphic Encryption (HE) has emerged as a compelling cryptographic technique that enables computation on encrypted data, allowing predictions to be generated without decrypting sensitive inputs. However, the integration of HE within large scale cloud native pipelines remains constrained by high computational overhead, orchestration complexity, and model compatibility issues.
  This paper presents a systematic framework for the design and optimization of cloud native homomorphic encryption workflows that support privacy-preserving ML inference. The proposed architecture integrates containerized HE modules with Kubernetes-based orchestration, enabling elastic scaling and parallel encrypted computation across distributed environments. Furthermore, optimization strategies including ciphertext packing, polynomial modulus adjustment, and operator fusion are employed to minimize latency and resource consumption while preserving cryptographic integrity. Experimental results demonstrate that the proposed system achieves up to 3.2times inference acceleration and 40% reduction in memory utilization compared to conventional HE pipelines. These findings illustrate a practical pathway for deploying secure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality under zero-trust cloud conditions.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments</title>
<link>https://arxiv.org/abs/2510.24503</link>
<guid>https://arxiv.org/abs/2510.24503</guid>
<content:encoded><![CDATA[
arXiv:2510.24503v1 Announce Type: cross 
Abstract: In the context of Federated Learning with heterogeneous data environments, local models tend to converge to their own local model optima during local training steps, deviating from the overall data distributions. Aggregation of these local updates, e.g., with FedAvg, often does not align with the global model optimum (client drift), resulting in an update that is suboptimal for most clients. Personalized Federated Learning approaches address this challenge by exclusively focusing on the average local performances of clients' models on their own data distribution. Generalization to out-of-distribution samples, which is a substantial benefit of FedAvg and represents a significant component of robustness, appears to be inadequately incorporated into the assessment and evaluation processes. This study involves a thorough evaluation of Federated Learning approaches, encompassing both their local performance and their generalization capabilities. Therefore, we examine different stages within a single communication round to enable a more nuanced understanding of the considered metrics. Furthermore, we propose and incorporate a modified approach of FedAvg, designated as Federated Learning with Individualized Updates (FLIU), extending the algorithm by a straightforward individualization step with an adaptive personalization factor. We evaluate and compare the approaches empirically using MNIST and CIFAR-10 under various distributional conditions, including benchmark IID and pathological non-IID, as well as additional novel test environments with Dirichlet distribution specifically developed to stress the algorithms on complex data heterogeneity.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient</title>
<link>https://arxiv.org/abs/2510.24519</link>
<guid>https://arxiv.org/abs/2510.24519</guid>
<content:encoded><![CDATA[
arXiv:2510.24519v1 Announce Type: cross 
Abstract: Extracting features from the speech is the most critical process in speech signal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most widely used features in the majority of the speaker and speech recognition applications, as the filtering in this feature is similar to the filtering taking place in the human ear. But the main drawback of this feature is that it provides only the frequency information of the signal but does not provide the information about at what time which frequency is present. The wavelet transform, with its flexible time-frequency window, provides time and frequency information of the signal and is an appropriate tool for the analysis of non-stationary signals like speech. On the other hand, because of its uniform frequency scaling, a typical wavelet transform may be less effective in analysing speech signals, have poorer frequency resolution in low frequencies, and be less in line with human auditory perception. Hence, it is necessary to develop a feature that incorporates the merits of both MFCC and wavelet transform. A great deal of studies are trying to combine both these features. The present Wavelet Transform based Mel-scaled feature extraction methods require more computation when a wavelet transform is applied on top of Mel-scale filtering, since it adds extra processing steps. Here we are proposing a method to extract Mel scale features in time domain combining the concept of wavelet transform, thus reducing the computational burden of time-frequency conversion and the complexity of wavelet extraction. Combining our proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique with the reservoir computing methodology has significantly improved the efficiency of audio signal processing.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Resistant Networks Using Post-Quantum Cryptography</title>
<link>https://arxiv.org/abs/2510.24534</link>
<guid>https://arxiv.org/abs/2510.24534</guid>
<content:encoded><![CDATA[
arXiv:2510.24534v1 Announce Type: cross 
Abstract: Quantum networks rely on both quantum and classical channels for coordinated operation. Current architectures employ entanglement distribution and key exchange over quantum channels but often assume that classical communication is sufficiently secure. In practice, classical channels protected by traditional cryptography remain vulnerable to quantum adversaries, since large-scale quantum computers could break widely used public-key schemes and reduce the effective security of symmetric cryptography. This perspective presents a quantum-resistant network architecture that secures classical communication with post-quantum cryptographic techniques while supporting entanglement-based communication over quantum channels. Beyond cryptographic protection, the framework incorporates continuous monitoring of both quantum and classical layers, together with orchestration across heterogeneous infrastructures, to ensure end-to-end security. Collectively, these mechanisms provide a pathway toward scalable, robust, and secure quantum networks that remain dependable against both classical and quantum-era threats.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis</title>
<link>https://arxiv.org/abs/2510.24561</link>
<guid>https://arxiv.org/abs/2510.24561</guid>
<content:encoded><![CDATA[
arXiv:2510.24561v1 Announce Type: cross 
Abstract: With the widespread adoption of LLMs, LoRA has become a dominant method for PEFT, and its initialization methods have attracted increasing attention. However, existing methods have notable limitations: many methods do not incorporate target-domain data, while gradient-based methods exploit data only at a shallow level by relying on one-step gradient decomposition, which remains unsatisfactory due to the weak empirical performance of the one-step fine-tuning model that serves as their basis, as well as the fact that these methods either lack a rigorous theoretical foundation or depend heavily on restrictive isotropic assumptions. In this paper, we establish a theoretical framework for data-aware LoRA initialization based on asymptotic analysis. Starting from a general optimization objective that minimizes the expectation of the parameter discrepancy between the fine-tuned and target models, we derive an optimization problem with two components: a bias term, which is related to the parameter distance between the fine-tuned and target models, and is approximated using a Fisher-gradient formulation to preserve anisotropy; and a variance term, which accounts for the uncertainty introduced by sampling stochasticity through the Fisher information. By solving this problem, we obtain an optimal initialization strategy for LoRA. Building on this theoretical framework, we develop an efficient algorithm, LoRA-DA, which estimates the terms in the optimization problem from a small set of target domain samples and obtains the optimal LoRA initialization. Empirical results across multiple benchmarks demonstrate that LoRA-DA consistently improves final accuracy over existing initialization methods. Additional studies show faster, more stable convergence, robustness across ranks, and only a small initialization overhead for LoRA-DA. The source code will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment</title>
<link>https://arxiv.org/abs/2510.24574</link>
<guid>https://arxiv.org/abs/2510.24574</guid>
<content:encoded><![CDATA[
arXiv:2510.24574v1 Announce Type: cross 
Abstract: Training time-series forecast models requires aligning the conditional distribution of model forecasts with that of the label sequence. The standard direct forecast (DF) approach resorts to minimize the conditional negative log-likelihood of the label sequence, typically estimated using the mean squared error. However, this estimation proves to be biased in the presence of label autocorrelation. In this paper, we propose DistDF, which achieves alignment by alternatively minimizing a discrepancy between the conditional forecast and label distributions. Because conditional discrepancies are difficult to estimate from finite time-series observations, we introduce a newly proposed joint-distribution Wasserstein discrepancy for time-series forecasting, which provably upper bounds the conditional discrepancy of interest. This discrepancy admits tractable, differentiable estimation from empirical samples and integrates seamlessly with gradient-based training. Extensive experiments show that DistDF improves the performance diverse forecast models and achieves the state-of-the-art forecasting performance. Code is available at https://anonymous.4open.science/r/DistDF-F66B.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation</title>
<link>https://arxiv.org/abs/2510.24619</link>
<guid>https://arxiv.org/abs/2510.24619</guid>
<content:encoded><![CDATA[
arXiv:2510.24619v1 Announce Type: cross 
Abstract: With the release of new large language models (LLMs) like Llama and Mistral, zero-shot cross-lingual transfer has become increasingly feasible due to their multilingual pretraining and strong generalization capabilities. However, adapting these decoder-only LLMs to new tasks across languages remains challenging. While parameter-efficient fine-tuning (PeFT) techniques like Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as soft prompt tuning, prefix tuning, and Llama Adapter are less explored, especially for zero-shot transfer in decoder-only models. We present a comprehensive study of three prefix-based methods for zero-shot cross-lingual transfer from English to 35+ high- and low-resource languages. Our analysis further explores transfer across linguistic families and scripts, as well as the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix methods outperform LoRA-baselines by up to 6% on the Belebele benchmark. Similar improvements were observed with Mistral v0.3 7B as well. Despite using only 1.23M learning parameters with prefix tuning, we achieve consistent improvements across diverse benchmarks. These findings highlight the potential of prefix-based techniques as an effective and scalable alternative to LoRA, particularly in low-resource multilingual settings.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2510.24637</link>
<guid>https://arxiv.org/abs/2510.24637</guid>
<content:encoded><![CDATA[
arXiv:2510.24637v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) are one of the most promising bio-inspired neural networks models and have drawn increasing attention in recent years. The event-driven communication mechanism of SNNs allows for sparse and theoretically low-power operations on dedicated neuromorphic hardware. However, the binary nature of instantaneous spikes also leads to considerable information loss in SNNs, resulting in accuracy degradation. To address this issue, we propose a multi-level spiking neuron model able to provide both low-quantization error and minimal inference latency while approaching the performance of full precision Artificial Neural Networks (ANNs). Experimental results with popular network architectures and datasets, show that multi-level spiking neurons provide better information compression, allowing therefore a reduction in latency without performance loss. When compared to binary SNNs on image classification scenarios, multi-level SNNs indeed allow reducing by 2 to 3 times the energy consumption depending on the number of quantization intervals. On neuromorphic data, our approach allows us to drastically reduce the inference latency to 1 timestep, which corresponds to a compression factor of 10 compared to previously published results. At the architectural level, we propose a new residual architecture that we call Sparse-ResNet. Through a careful analysis of the spikes propagation in residual connections we highlight a spike avalanche effect, that affects most spiking residual architectures. Using our Sparse-ResNet architecture, we can provide state-of-the-art accuracy results in image classification while reducing by more than 20% the network activity compared to the previous spiking ResNets.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Ordering for Structure Learning From Time Series</title>
<link>https://arxiv.org/abs/2510.24639</link>
<guid>https://arxiv.org/abs/2510.24639</guid>
<content:encoded><![CDATA[
arXiv:2510.24639v1 Announce Type: cross 
Abstract: Predicting causal structure from time series data is crucial for understanding complex phenomena in physiology, brain connectivity, climate dynamics, and socio-economic behaviour. Causal discovery in time series is hindered by the combinatorial complexity of identifying true causal relationships, especially as the number of variables and time points grow. A common approach to simplify the task is the so-called ordering-based methods. Traditional ordering methods inherently limit the representational capacity of the resulting model. In this work, we fix this issue by leveraging multiple valid causal orderings, instead of a single one as standard practice. We propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based causal discovery for temporal data. By integrating multiple orderings, DOTS effectively recovers the transitive closure of the underlying directed acyclic graph, mitigating spurious artifacts inherent in single-ordering approaches. We formalise the problem under standard assumptions such as stationarity and the additive noise model, and leverage score matching with diffusion processes to enable efficient Hessian estimation. Extensive experiments validate the approach. Empirical evaluations on synthetic and real-world datasets demonstrate that DOTS outperforms state-of-the-art baselines, offering a scalable and robust approach to temporal causal discovery. On synthetic benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the best on individual datasets, DOTS attains the highest average summary-graph $F1$ while halving runtime relative to graph-optimisation methods. These results establish DOTS as a scalable and accurate solution for temporal causal discovery.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets</title>
<link>https://arxiv.org/abs/2510.24643</link>
<guid>https://arxiv.org/abs/2510.24643</guid>
<content:encoded><![CDATA[
arXiv:2510.24643v1 Announce Type: cross 
Abstract: We study the parameter complexity of robust memorization for $\mathrm{ReLU}$ networks: the number of parameters required to interpolate any given dataset with $\epsilon$-separation between differently labeled points, while ensuring predictions remain consistent within a $\mu$-ball around each training sample. We establish upper and lower bounds on the parameter count as a function of the robustness ratio $\rho = \mu / \epsilon$. Unlike prior work, we provide a fine-grained analysis across the entire range $\rho \in (0,1)$ and obtain tighter upper and lower bounds that improve upon existing results. Our findings reveal that the parameter complexity of robust memorization matches that of non-robust memorization when $\rho$ is small, but grows with increasing $\rho$.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InteractComp: Evaluating Search Agents With Ambiguous Queries</title>
<link>https://arxiv.org/abs/2510.24668</link>
<guid>https://arxiv.org/abs/2510.24668</guid>
<content:encoded><![CDATA[
arXiv:2510.24668v1 Announce Type: cross 
Abstract: Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder</title>
<link>https://arxiv.org/abs/2510.24671</link>
<guid>https://arxiv.org/abs/2510.24671</guid>
<content:encoded><![CDATA[
arXiv:2510.24671v1 Announce Type: cross 
Abstract: With the increasing integration of intelligent driving functions into serial-produced vehicles, ensuring their functionality and robustness poses greater challenges. Compared to traditional road testing, scenario-based virtual testing offers significant advantages in terms of time and cost efficiency, reproducibility, and exploration of edge cases. We propose a Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for generating multi-agent traffic scenarios in roundabouts, which are characterized by high vehicle dynamics and complex layouts, yet remain relatively underexplored in current research. The results show that the proposed model can accurately reconstruct original scenarios and generate realistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators (KPIs) are employed to evaluate the interactive behavior in the generated scenarios. Analysis of the latent space reveals partial disentanglement, with several latent dimensions exhibiting distinct and interpretable effects on scenario attributes such as vehicle entry timing, exit timing, and velocity profiles. The results demonstrate the model's capability to generate scenarios for the validation of intelligent driving functions involving multi-agent interactions, as well as to augment data for their development and iterative improvement.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive Safely with Hybrid Options</title>
<link>https://arxiv.org/abs/2510.24674</link>
<guid>https://arxiv.org/abs/2510.24674</guid>
<content:encoded><![CDATA[
arXiv:2510.24674v1 Announce Type: cross 
Abstract: Out of the many deep reinforcement learning approaches for autonomous driving, only few make use of the options (or skills) framework. That is surprising, as this framework is naturally suited for hierarchical control applications in general, and autonomous driving tasks in specific. Therefore, in this work the options framework is applied and tailored to autonomous driving tasks on highways. More specifically, we define dedicated options for longitudinal and lateral manoeuvres with embedded safety and comfort constraints. This way, prior domain knowledge can be incorporated into the learning process and the learned driving behaviour can be constrained more easily. We propose several setups for hierarchical control with options and derive practical algorithms following state-of-the-art reinforcement learning techniques. By separately selecting actions for longitudinal and lateral control, the introduced policies over combined and hybrid options obtain the same expressiveness and flexibility that human drivers have, while being easier to interpret than classical policies over continuous actions. Of all the investigated approaches, these flexible policies over hybrid options perform the best under varying traffic conditions, outperforming the baseline policies over actions.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Role Cognition in Medical LLMs via Neuronal Ablation</title>
<link>https://arxiv.org/abs/2510.24677</link>
<guid>https://arxiv.org/abs/2510.24677</guid>
<content:encoded><![CDATA[
arXiv:2510.24677v1 Announce Type: cross 
Abstract: Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the
  context of medical question answering and role-playing simulations. A common practice, Prompt-Based Role Playing (PBRP),
  instructs models to adopt different clinical roles (e.g., medical students, residents, attending physicians) to simulate varied
  professional behaviors. However, the impact of such role prompts on model reasoning capabilities remains unclear. This
  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to evaluate whether role prompts induce distinct,
  role-specific cognitive processes in LLMs or merely modify linguistic style. We test this framework on three medical QA
  datasets, employing neuron ablation and representation analysis techniques to assess changes in reasoning pathways. Our
  results demonstrate that role prompts do not significantly enhance the medical reasoning abilities of LLMs. Instead, they
  primarily affect surface-level linguistic features, with no evidence of distinct reasoning pathways or cognitive differentiation
  across clinical roles. Despite superficial stylistic changes, the core decision-making mechanisms of LLMs remain uniform
  across roles, indicating that current PBRP methods fail to replicate the cognitive complexity found in real-world medical
  practice. This highlights the limitations of role-playing in medical AI and emphasizes the need for models that simulate genuine
  cognitive processes rather than linguistic imitation.We have released the related code in the following repository:https:
  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry</title>
<link>https://arxiv.org/abs/2510.24687</link>
<guid>https://arxiv.org/abs/2510.24687</guid>
<content:encoded><![CDATA[
arXiv:2510.24687v1 Announce Type: cross 
Abstract: The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the minimization of a certain cost functional. In addition, novel deep learning techniques are currently being investigated to further improve such optimization approaches. All such methods require multiple applications of the operator defining the forward problem, and of its adjoint. In this paper, we present new asymptotically fast algorithms for numerical evaluation of the forward and adjoint operators, applicable in the circular acquisition geometry. For an $(n \times n)$ image, our algorithms compute these operators in $\mathcal{O}(n^2 \log n)$ floating point operations. We demonstrate the performance of our algorithms in numerical simulations, where they are used as an integral part of several iterative image reconstruction techniques: classic variational methods, such as non-negative least squares and total variation regularized least squares, as well as deep learning methods, such as learned primal dual. A Python implementation of our algorithms and computational examples is available to the general public.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repurposing Synthetic Data for Fine-grained Search Agent Supervision</title>
<link>https://arxiv.org/abs/2510.24694</link>
<guid>https://arxiv.org/abs/2510.24694</guid>
<content:encoded><![CDATA[
arXiv:2510.24694v1 Announce Type: cross 
Abstract: LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</title>
<link>https://arxiv.org/abs/2510.24698</link>
<guid>https://arxiv.org/abs/2510.24698</guid>
<content:encoded><![CDATA[
arXiv:2510.24698v1 Announce Type: cross 
Abstract: Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentFold: Long-Horizon Web Agents with Proactive Context Management</title>
<link>https://arxiv.org/abs/2510.24699</link>
<guid>https://arxiv.org/abs/2510.24699</guid>
<content:encoded><![CDATA[
arXiv:2510.24699v1 Announce Type: cross 
Abstract: LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Greedy Sampling Is Provably Efficient for RLHF</title>
<link>https://arxiv.org/abs/2510.24700</link>
<guid>https://arxiv.org/abs/2510.24700</guid>
<content:encoded><![CDATA[
arXiv:2510.24700v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tongyi DeepResearch Technical Report</title>
<link>https://arxiv.org/abs/2510.24701</link>
<guid>https://arxiv.org/abs/2510.24701</guid>
<content:encoded><![CDATA[
arXiv:2510.24701v1 Announce Type: cross 
Abstract: We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents</title>
<link>https://arxiv.org/abs/2510.24702</link>
<guid>https://arxiv.org/abs/2510.24702</guid>
<content:encoded><![CDATA[
arXiv:2510.24702v1 Announce Type: cross 
Abstract: Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?</title>
<link>https://arxiv.org/abs/2510.24706</link>
<guid>https://arxiv.org/abs/2510.24706</guid>
<content:encoded><![CDATA[
arXiv:2510.24706v1 Announce Type: cross 
Abstract: Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</title>
<link>https://arxiv.org/abs/2510.24709</link>
<guid>https://arxiv.org/abs/2510.24709</guid>
<content:encoded><![CDATA[
arXiv:2510.24709v1 Announce Type: cross 
Abstract: Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of "which parts belong together" emerges naturally in a connectionist system.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Large Independent Sets on Massive Graphs</title>
<link>https://arxiv.org/abs/2208.07777</link>
<guid>https://arxiv.org/abs/2208.07777</guid>
<content:encoded><![CDATA[
arXiv:2208.07777v4 Announce Type: replace 
Abstract: The Maximum Independent Set problem is fundamental for extracting conflict-free structure from large graphs, with applications in scheduling, recommendation, and network analysis. However, existing heuristics can stagnate when search schedules are fixed and information from past solutions is underused, leading to wasted effort in low-quality regions of the search space. We present ARCIS, an efficient algorithm for mining large independent sets on massive graphs. ARCIS couples two main components. The first is an adaptive restart policy that refreshes exploration when progress slows. The second is Consensus-Guided Vertex Fixing, which restricts the search to the non-consensus region of the graph by fixing vertices consistently observed within a round. The consensus is maintained as a running intersection within each round, and because it is recomputed at every restart, the fixing is reversible. Vertices that later lose support are automatically unfixed and their neighborhoods re-enter the working graph, which corrects occasional mistakes while preserving progress. Experiments on 222 graphs from four benchmark suites show that ARCIS attains the best or tied-best solution quality in most instances while delivering competitive runtime and low variability. Ablation studies isolate the impact of each component, indicating that ARCIS is a practical and robust method for large-scale graph mining.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model-Based Game Agents</title>
<link>https://arxiv.org/abs/2404.02039</link>
<guid>https://arxiv.org/abs/2404.02039</guid>
<content:encoded><![CDATA[
arXiv:2404.02039v3 Announce Type: replace 
Abstract: Game environments provide rich, controllable settings that stimulate many aspects of real-world complexity. As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence. Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments. This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture. At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception-action interfaces, which jointly characterize how language enables agents to perceive, think, and act. At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors. To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds. A curated list of related papers is available at https://github.com/git-disl/awesome-LLM-game-agent-papers
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes</title>
<link>https://arxiv.org/abs/2410.11133</link>
<guid>https://arxiv.org/abs/2410.11133</guid>
<content:encoded><![CDATA[
arXiv:2410.11133v2 Announce Type: replace 
Abstract: A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success, and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D- Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F and LeanDojo benchmarks by augmenting popular open source proving LLMs. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success rate, execution time and diversity. We make our code available at https://github.com/sean-lamont/3D-Prover.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableTime: Reformulating Time Series Classification as Training-Free Table Understanding with Large Language Models</title>
<link>https://arxiv.org/abs/2411.15737</link>
<guid>https://arxiv.org/abs/2411.15737</guid>
<content:encoded><![CDATA[
arXiv:2411.15737v4 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated their effectiveness in multivariate time series classification (MTSC). Effective adaptation of LLMs for MTSC necessitates informative data representations. Existing LLM-based methods directly encode embeddings for time series within the latent space of LLMs from scratch to align with semantic space of LLMs. Despite their effectiveness, we reveal that these methods conceal three inherent bottlenecks: (1) they struggle to encode temporal and channel-specific information in a lossless manner, both of which are critical components of multivariate time series; (2) it is much difficult to align the learned representation space with the semantic space of the LLMs; (3) they require task-specific retraining, which is both computationally expensive and labor-intensive. To bridge these gaps, we propose TableTime, which reformulates MTSC as a table understanding task. Specifically, TableTime introduces the following strategies: (1) convert multivariate time series into a tabular form, thus minimizing information loss to the greatest extent; (2) represent tabular time series in text format to achieve natural alignment with the semantic space of LLMs; (3) design a reasoning framework that integrates contextual text information, neighborhood assistance, multi-path inference and problem decomposition to enhance the reasoning ability of LLMs and realize zero-shot classification. Extensive experiments performed on 10 publicly representative datasets from UEA archive verify the superiorities of the TableTime.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Dreaming: A Global Workspace Approach to World Model-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.21142</link>
<guid>https://arxiv.org/abs/2502.21142</guid>
<content:encoded><![CDATA[
arXiv:2502.21142v2 Announce Type: replace 
Abstract: Humans leverage rich internal models of the world to reason about the future, imagine counterfactuals, and adapt flexibly to new situations. In Reinforcement Learning (RL), world models aim to capture how the environment evolves in response to the agent's actions, facilitating planning and generalization. However, typical world models directly operate on the environment variables (e.g. pixels, physical attributes), which can make their training slow and cumbersome; instead, it may be advantageous to rely on high-level latent dimensions that capture relevant multimodal variables. Global Workspace (GW) Theory offers a cognitive framework for multimodal integration and information broadcasting in the brain, and recent studies have begun to introduce efficient deep learning implementations of GW. Here, we evaluate the capabilities of an RL system combining GW with a world model. We compare our GW-Dreamer with various versions of the standard PPO and the original Dreamer algorithms. We show that performing the dreaming process (i.e., mental simulation) inside the GW latent space allows for training with fewer environment steps. As an additional emergent property, the resulting model (but not its comparison baselines) displays strong robustness to the absence of one of its observation modalities (images or simulation attributes). We conclude that the combination of GW with World Models holds great potential for improving decision-making in RL agents.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)</title>
<link>https://arxiv.org/abs/2505.17323</link>
<guid>https://arxiv.org/abs/2505.17323</guid>
<content:encoded><![CDATA[
arXiv:2505.17323v2 Announce Type: replace 
Abstract: Humans are remarkably adept at collaboration, able to infer the strengths and weaknesses of new partners in order to work successfully towards shared goals. To build AI systems with this capability, we must first understand its building blocks: does such flexibility require explicit, dedicated mechanisms for modelling others -- or can it emerge spontaneously from the pressures of open-ended cooperative interaction? To investigate this question, we train simple model-free RNN agents to collaborate with a population of diverse partners. Using the `Overcooked-AI' environment, we collect data from thousands of collaborative teams, and analyse agents' internal hidden states. Despite a lack of additional architectural features, inductive biases, or auxiliary objectives, the agents nevertheless develop structured internal representations of their partners' task abilities, enabling rapid adaptation and generalisation to novel collaborators. We investigated these internal models through probing techniques, and large-scale behavioural analysis. Notably, we find that structured partner modelling emerges when agents can influence partner behaviour by controlling task allocation. Our results show that partner modelling can arise spontaneously in model-free agents -- but only under environmental conditions that impose the right kind of social pressure.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIRAL: Vision-grounded Integration for Reward design And Learning</title>
<link>https://arxiv.org/abs/2505.22092</link>
<guid>https://arxiv.org/abs/2505.22092</guid>
<content:encoded><![CDATA[
arXiv:2505.22092v3 Announce Type: replace 
Abstract: The alignment between humans and machines is a critical challenge in artificial intelligence today. Reinforcement learning, which aims to maximize a reward function, is particularly vulnerable to the risks associated with poorly designed reward functions. Recent advancements has shown that Large Language Models (LLMs) for reward generation can outperform human performance in this context. We introduce VIRAL, a pipeline for generating and refining reward functions through the use of multi-modal LLMs. VIRAL autonomously creates and interactively improves reward functions based on a given environment and a goal prompt or annotated image. The refinement process can incorporate human feedback or be guided by a description generated by a video LLM, which explains the agent's policy in video form. We evaluated VIRAL in five Gymnasium environments, demonstrating that it accelerates the learning of new behaviors while ensuring improved alignment with user intent. The source-code and demo video are available at: https://github.com/VIRAL-UCBL1/VIRAL and https://youtu.be/Hqo82CxVT38.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Confidence Paradox: Can LLM Know When It's Wrong</title>
<link>https://arxiv.org/abs/2506.23464</link>
<guid>https://arxiv.org/abs/2506.23464</guid>
<content:encoded><![CDATA[
arXiv:2506.23464v2 Announce Type: replace 
Abstract: Document Visual Question Answering (DocVQA) models often produce overconfident or ethically misaligned responses, especially under uncertainty. Existing models like LayoutLMv3, UDOP, and DONUT focus on accuracy but lack ethical calibration. We propose HonestVQA, a model-agnostic, self-supervised framework that aligns model confidence with correctness using weighted loss and contrastive learning. We introduce two new metrics Honesty Score (H-Score) and Ethical Confidence Index (ECI)-to evaluate ethical alignment. HonestVQA improves accuracy and F1 by up to 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets, while reducing overconfidence. It also generalizes well across domains, achieving 78.9% accuracy and 76.1% F1-score.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Mosaics at scale</title>
<link>https://arxiv.org/abs/2507.03285</link>
<guid>https://arxiv.org/abs/2507.03285</guid>
<content:encoded><![CDATA[
arXiv:2507.03285v2 Announce Type: replace 
Abstract: Memory Mosaics [Zhang et al., 2025], networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications ("Memory Mosaics v2"), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</title>
<link>https://arxiv.org/abs/2508.21730</link>
<guid>https://arxiv.org/abs/2508.21730</guid>
<content:encoded><![CDATA[
arXiv:2508.21730v2 Announce Type: replace 
Abstract: In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz'') is first optimized on a training instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method.
  The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start'' initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerate Scaling of LLM Finetuning via Quantifying the Coverage and Depth of Instruction Set</title>
<link>https://arxiv.org/abs/2509.06463</link>
<guid>https://arxiv.org/abs/2509.06463</guid>
<content:encoded><![CDATA[
arXiv:2509.06463v2 Announce Type: replace 
Abstract: Scaling the amount of data used for supervied fine-tuning(SFT) does not guarantee the proportional gains in model performance, highlighting a critical need to understand what makes training samples effective. This work identifies two fundamental dataset properties that govern SFT scalability: \textbf{semantic coverage}, or the breadth of task domains, and \textbf{information depth}, or the richness of individual examples. We demonstrate that simple proxies for these properties explain the majority of validation loss variance in our experiments. In this work, we further propose the \textbf{Information Landscape Approximation (ILA)}, a model-agnostic data selection framework that jointly optimizes for these two factors. ILA constructs compact subsets that approximate the informational value of large datasets. Empirical results show that models tuned on ILA-selected data achieve faster and more sustained performance improvements across diverse tasks and model sizes compared to existing methods, a phenomenon we term \textbf{accelerated scaling}.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is It Certainly a Deepfake? Reliability Analysis in Detection &amp; Generation Ecosystem</title>
<link>https://arxiv.org/abs/2509.17550</link>
<guid>https://arxiv.org/abs/2509.17550</guid>
<content:encoded><![CDATA[
arXiv:2509.17550v3 Announce Type: replace 
Abstract: As generative models are advancing in quality and quantity for creating synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors are proposed to counter this effect, however, misuse of detectors claiming fake content as real or vice versa further fuels this misinformation problem. We present the first comprehensive uncertainty analysis of deepfake detectors, systematically investigating how generative artifacts influence prediction confidence. As reflected in detectors' responses, deepfake generators also contribute to this uncertainty as their generative residues vary, so we cross the uncertainty analysis of deepfake detectors and generators. Based on our observations, the uncertainty manifold holds enough consistent information to leverage uncertainty for deepfake source detection. Our approach leverages Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and epistemic uncertainties across diverse detector architectures. We evaluate uncertainty on two datasets with nine generators, with four blind and two biological detectors, compare different uncertainty methods, explore region- and pixel-based uncertainty, and conduct ablation studies. We conduct and analyze binary real/fake, multi-class real/fake, source detection, and leave-one-out experiments between the generator/detector combinations to share their generalization capability, model calibration, uncertainty, and robustness against adversarial attacks. We further introduce uncertainty maps that localize prediction confidence at the pixel level, revealing distinct patterns correlated with generator-specific artifacts. Our analysis provides critical insights for deploying reliable deepfake detection systems and establishes uncertainty quantification as a fundamental requirement for trustworthy synthetic media detection.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathBode: Understanding LLM Reasoning with Dynamical Systems</title>
<link>https://arxiv.org/abs/2509.23143</link>
<guid>https://arxiv.org/abs/2509.23143</guid>
<content:encoded><![CDATA[
arXiv:2509.23143v3 Announce Type: replace 
Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics -- gain (amplitude tracking) and phase (lag) -- that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2x2 linear systems, similar triangles), the diagnostic surfaces systematic low-pass behavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument ($G \approx 1$, $\phi \approx 0$). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research</title>
<link>https://arxiv.org/abs/2509.26080</link>
<guid>https://arxiv.org/abs/2509.26080</guid>
<content:encoded><![CDATA[
arXiv:2509.26080v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-TAP: Three-Layer Agent Interaction Protocol Technical Report</title>
<link>https://arxiv.org/abs/2510.08263</link>
<guid>https://arxiv.org/abs/2510.08263</guid>
<content:encoded><![CDATA[
arXiv:2510.08263v2 Announce Type: replace 
Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer agent interaction protocol designed to address the challenges faced by multi-agent systems across the three core dimensions of Interoperability, Interaction and Collaboration, and Knowledge Sharing. We have designed and proposed a layered solution composed of three core protocols: the Human-Agent Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction layer, standardizing the flow of information between users, interfaces, and agents by defining a standardized, event-driven communication paradigm. This ensures the real-time performance, reliability, and synergy of interactions. As the core of the infrastructure layer, UAP is designed to break down communication barriers among heterogeneous agents through unified service discovery and protocol conversion mechanisms, thereby enabling seamless interconnection and interoperability of the underlying network. MEK, in turn, operates at the cognitive layer. By establishing a standardized ''Memory (M) - Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the ability to learn from individual experiences and form shareable knowledge, thereby laying the foundation for the realization of true collective intelligence. We believe this protocol framework will provide a solid engineering foundation and theoretical guidance for building the next generation of efficient, scalable, and intelligent multi-agent applications.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications</title>
<link>https://arxiv.org/abs/2510.16724</link>
<guid>https://arxiv.org/abs/2510.16724</guid>
<content:encoded><![CDATA[
arXiv:2510.16724v2 Announce Type: replace 
Abstract: The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Querying Inconsistent Prioritized Data with ORBITS: Algorithms, Implementation, and Experiments</title>
<link>https://arxiv.org/abs/2202.07980</link>
<guid>https://arxiv.org/abs/2202.07980</guid>
<content:encoded><![CDATA[
arXiv:2202.07980v3 Announce Type: replace-cross 
Abstract: We investigate practical algorithms for inconsistency-tolerant query answering over prioritized knowledge bases, which consist of a logical theory, a set of facts, and a priority relation between conflicting facts. We consider three well-known semantics (AR, IAR and brave) based upon two notions of optimal repairs (Pareto and completion). Deciding whether a query answer holds under these semantics is (co)NP-complete in data complexity for a large class of logical theories, and SAT-based procedures have been devised for repair-based semantics when there is no priority relation, or the relation has a special structure. The present paper introduces the first SAT encodings for Pareto- and completion-optimal repairs w.r.t. general priority relations and proposes several ways of employing existing and new encodings to compute answers under (optimal) repair-based semantics, by exploiting different reasoning modes of SAT solvers. The comprehensive experimental evaluation of our implementation compares both (i) the impact of adopting semantics based on different kinds of repairs, and (ii) the relative performances of alternative procedures for the same semantics.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation-based Relation Extraction</title>
<link>https://arxiv.org/abs/2404.13397</link>
<guid>https://arxiv.org/abs/2404.13397</guid>
<content:encoded><![CDATA[
arXiv:2404.13397v2 Announce Type: replace-cross 
Abstract: Information Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks.
  This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigation with VLM framework: Towards Going to Any Language</title>
<link>https://arxiv.org/abs/2410.02787</link>
<guid>https://arxiv.org/abs/2410.02787</guid>
<content:encoded><![CDATA[
arXiv:2410.02787v2 Announce Type: replace-cross 
Abstract: Navigating towards fully open language goals and exploring open scenes in an intelligent way have always raised significant challenges. Recently, Vision Language Models (VLMs) have demonstrated remarkable capabilities to reason with both language and visual data. Although many works have focused on leveraging VLMs for navigation in open scenes, they often require high computational cost, rely on object-centric approaches, or depend on environmental priors in detailed human instructions. We introduce Navigation with VLM (NavVLM), a training-free framework that harnesses open-source VLMs to enable robots to navigate effectively, even for human-friendly language goal such as abstract places, actions, or specific objects in open scenes. NavVLM leverages the VLM as its cognitive core to perceive environmental information and constantly provides exploration guidance achieving intelligent navigation with only a neat target rather than a detailed instruction with environment prior. We evaluated and validated NavVLM in both simulation and real-world experiments. In simulation, our framework achieves state-of-the-art performance in Success weighted by Path Length (SPL) on object-specifc tasks in richly detailed environments from Matterport 3D (MP3D), Habitat Matterport 3D (HM3D) and Gibson. With navigation episode reported, NavVLM demonstrates the capabilities to navigate towards any open-set languages. In real-world validation, we validated our framework's effectiveness in real-world robot at indoor scene.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRS: Generating Robotic Simulation Tasks from Real-World Images</title>
<link>https://arxiv.org/abs/2410.15536</link>
<guid>https://arxiv.org/abs/2410.15536</guid>
<content:encoded><![CDATA[
arXiv:2410.15536v3 Announce Type: replace-cross 
Abstract: We introduce GRS (Generating Robotic Simulation tasks), a system addressing real-to-sim for robotic simulations. GRS creates digital twin simulations from single RGB-D observations with solvable tasks for virtual agent training. Using vision-language models (VLMs), our pipeline operates in three stages: 1) scene comprehension with SAM2 for segmentation and object description, 2) matching objects with simulation-ready assets, and 3) generating appropriate tasks. We ensure simulation-task alignment through generated test suites and introduce a router that iteratively refines both simulation and test code. Experiments demonstrate our system's effectiveness in object correspondence and task environment generation through our novel router mechanism.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2410.22366</link>
<guid>https://arxiv.org/abs/2410.22366</guid>
<content:encoded><![CDATA[
arXiv:2410.22366v5 Announce Type: replace-cross 
Abstract: For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned, Lagged, LLM-splained: LLM Responses to End User Security Questions</title>
<link>https://arxiv.org/abs/2411.14571</link>
<guid>https://arxiv.org/abs/2411.14571</guid>
<content:encoded><![CDATA[
arXiv:2411.14571v2 Announce Type: replace-cross 
Abstract: Answering end user security questions is challenging. While large language models (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have shown promise in answering a variety of questions outside of security. We studied LLM performance in the area of end user security by qualitatively evaluating 3 popular LLMs on 900 systematically collected end user security questions.
  While LLMs demonstrate broad generalist ``knowledge'' of end user security information, there are patterns of errors and limitations across LLMs consisting of stale and inaccurate answers, and indirect or unresponsive communication styles, all of which impacts the quality of information received. Based on these patterns, we suggest directions for model improvement and recommend user strategies for interacting with LLMs when seeking assistance with security.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Scaling Laws for the Test-Time Compute of Large Language Models</title>
<link>https://arxiv.org/abs/2411.19477</link>
<guid>https://arxiv.org/abs/2411.19477</guid>
<content:encoded><![CDATA[
arXiv:2411.19477v5 Announce Type: replace-cross 
Abstract: We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\beta$-DQN: Improving Deep Q-Learning By Evolving the Behavior</title>
<link>https://arxiv.org/abs/2501.00913</link>
<guid>https://arxiv.org/abs/2501.00913</guid>
<content:encoded><![CDATA[
arXiv:2501.00913v2 Announce Type: replace-cross 
Abstract: While many sophisticated exploration methods have been proposed, their lack of generality and high computational cost often lead researchers to favor simpler methods like $\epsilon$-greedy. Motivated by this, we introduce $\beta$-DQN, a simple and efficient exploration method that augments the standard DQN with a behavior function $\beta$. This function estimates the probability that each action has been taken at each state. By leveraging $\beta$, we generate a population of diverse policies that balance exploration between state-action coverage and overestimation bias correction. An adaptive meta-controller is designed to select an effective policy for each episode, enabling flexible and explainable exploration. $\beta$-DQN is straightforward to implement and adds minimal computational overhead to the standard DQN. Experiments on both simple and challenging exploration domains show that $\beta$-DQN outperforms existing baseline methods across a wide range of tasks, providing an effective solution for improving exploration in deep reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A High-Dimensional Statistical Method for Optimizing Transfer Quantities in Multi-Source Transfer Learning</title>
<link>https://arxiv.org/abs/2502.04242</link>
<guid>https://arxiv.org/abs/2502.04242</guid>
<content:encoded><![CDATA[
arXiv:2502.04242v4 Announce Type: replace-cross 
Abstract: Multi-source transfer learning provides an effective solution to data scarcity in real- world supervised learning scenarios by leveraging multiple source tasks. In this field, existing works typically use all available samples from sources in training, which constrains their training efficiency and may lead to suboptimal results. To address this, we propose a theoretical framework that answers the question: what is the optimal quantity of source samples needed from each source task to jointly train the target model? Specifically, we introduce a generalization error measure based on K-L divergence, and minimize it based on high-dimensional statistical analysis to determine the optimal transfer quantity for each source task. Additionally, we develop an architecture-agnostic and data-efficient algorithm OTQMS to implement our theoretical results for target model training in multi- source transfer learning. Experimental studies on diverse architectures and two real-world benchmark datasets show that our proposed algorithm significantly outperforms state-of-the-art approaches in both accuracy and data efficiency. The code and supplementary materials are available in https://github.com/zqy0126/OTQMS.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise and Compute Resources</title>
<link>https://arxiv.org/abs/2502.07862</link>
<guid>https://arxiv.org/abs/2502.07862</guid>
<content:encoded><![CDATA[
arXiv:2502.07862v2 Announce Type: replace-cross 
Abstract: Multimodal deep learning systems are deployed in dynamic scenarios due to the robustness afforded by multiple sensing modalities. Nevertheless, they struggle with varying compute resource availability (due to multi-tenancy, device heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed corruption, environmental noise, etc.). Statically provisioned multimodal systems cannot adapt when compute resources change over time, while existing dynamic networks struggle with strict compute budgets. Additionally, both systems often neglect the impact of variations in modality quality. Consequently, modalities suffering substantial corruption may needlessly consume resources better allocated towards other modalities. We propose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of tackling both challenges: it adjusts the total number of active layers across all modalities to meet strict compute resource constraints and continually reallocates layers across input modalities according to their modality quality. Our evaluations showcase ADMN can match the accuracy of state-of-the-art networks while reducing up to 75% of their floating-point operations.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FragFM: Hierarchical Framework for Efficient Molecule Generation via Fragment-Level Discrete Flow Matching</title>
<link>https://arxiv.org/abs/2502.15805</link>
<guid>https://arxiv.org/abs/2502.15805</guid>
<content:encoded><![CDATA[
arXiv:2502.15805v3 Announce Type: replace-cross 
Abstract: We introduce FragFM, a novel hierarchical framework via fragment-level discrete flow matching for efficient molecular graph generation. FragFM generates molecules at the fragment level, leveraging a coarse-to-fine autoencoder to reconstruct details at the atom level. Together with a stochastic fragment bag strategy to effectively handle an extensive fragment space, our framework enables more efficient and scalable molecular generation. We demonstrate that our fragment-based approach achieves better property control than the atom-based method and additional flexibility through conditioning the fragment bag. We also propose a Natural Product Generation benchmark (NPGen) to evaluate modern molecular graph generative models' ability to generate natural product-like molecules. Since natural products are biologically prevalidated and differ from typical drug-like molecules, our benchmark provides a more challenging yet meaningful evaluation relevant to drug discovery. We conduct a FragFM comparative study against various models on diverse molecular generation benchmarks, including NPGen, demonstrating superior performance. The results highlight the potential of fragment-based generative modeling for large-scale, property-aware molecular design, paving the way for more efficient exploration of chemical space.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Exponentiated Gradient Algorithms Using the Euler Two-Parameter Logarithm</title>
<link>https://arxiv.org/abs/2502.17500</link>
<guid>https://arxiv.org/abs/2502.17500</guid>
<content:encoded><![CDATA[
arXiv:2502.17500v2 Announce Type: replace-cross 
Abstract: IIn this paper we propose and investigate a new class of Generalized Exponentiated Gradient (GEG) algorithms using Mirror Descent (MD) updates, and applying the Bregman divergence with a two--parameter
  deformation of the logarithm as a link function. This link function (referred here to as the Euler logarithm) is associated with a relatively wide class of trace--form entropies. In order to derive novel GEG/MD updates, we estimate a deformed exponential function, which closely approximates the inverse of the Euler two--parameter deformed logarithm. The characteristic shape and properties of the Euler logarithm and its inverse--deformed exponential functions, are tuned by two hyperparameters. By learning these hyperparameters, we can adapt to the distribution of training data and adjust them to achieve desired properties of gradient descent algorithms. In the literature, there exist nowadays more than fifty mathematically well-established entropic functionals and associated deformed logarithms, so it is impossible to investigate all of them in one research paper. Therefore, we focus here on a class of trace-form entropies and the associated deformed two--parameters logarithms.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Unified Approach for Elevating Benchmark Quality</title>
<link>https://arxiv.org/abs/2503.05860</link>
<guid>https://arxiv.org/abs/2503.05860</guid>
<content:encoded><![CDATA[
arXiv:2503.05860v2 Announce Type: replace-cross 
Abstract: Benchmarks are essential for unified evaluation and reproducibility. The rapid rise of Artificial Intelligence for Software Engineering (AI4SE) has produced numerous benchmarks for tasks such as code generation and bug repair. However, this proliferation has led to major challenges: (1) fragmented knowledge across tasks, (2) difficulty in selecting contextually relevant benchmarks, (3) lack of standardization in benchmark creation, and (4) flaws that limit utility. Addressing these requires a dual approach: systematically mapping existing benchmarks for informed selection and defining unified guidelines for robust, adaptable benchmark development.
  We conduct a review of 247 studies, identifying 273 AI4SE benchmarks since 2014. We categorize them, analyze limitations, and expose gaps in current practices. Building on these insights, we introduce BenchScout, an extensible semantic search tool for locating suitable benchmarks. BenchScout employs automated clustering with contextual embeddings of benchmark-related studies, followed by dimensionality reduction. In a user study with 22 participants, BenchScout achieved usability, effectiveness, and intuitiveness scores of 4.5, 4.0, and 4.1 out of 5.
  To improve benchmarking standards, we propose BenchFrame, a unified framework for enhancing benchmark quality. Applying BenchFrame to HumanEval yielded HumanEvalNext, featuring corrected errors, improved language conversion, higher test coverage, and greater difficulty. Evaluating 10 state-of-the-art code models on HumanEval, HumanEvalPlus, and HumanEvalNext revealed average pass-at-1 drops of 31.22% and 19.94%, respectively, underscoring the need for continuous benchmark refinement. We further examine BenchFrame's scalability through an agentic pipeline and confirm its generalizability on the MBPP dataset. All review data, user study materials, and enhanced benchmarks are publicly released.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mirror Descent and Novel Exponentiated Gradient Algorithms Using Trace-Form Entropies and Deformed Logarithms</title>
<link>https://arxiv.org/abs/2503.08748</link>
<guid>https://arxiv.org/abs/2503.08748</guid>
<content:encoded><![CDATA[
arXiv:2503.08748v4 Announce Type: replace-cross 
Abstract: This paper introduces a broad class of Mirror Descent (MD) and Generalized Exponentiated Gradient (GEG) algorithms derived from trace-form entropies defined via deformed logarithms. Leveraging these generalized entropies yields MD \& GEG algorithms with improved convergence behavior, robustness to vanishing and exploding gradients, and inherent adaptability to non-Euclidean geometries through mirror maps. We establish deep connections between these methods and Amari's natural gradient, revealing a unified geometric foundation for additive, multiplicative, and natural gradient updates. Focusing on the Tsallis, Kaniadakis, Sharma--Taneja--Mittal, and Kaniadakis--Lissia--Scarfone entropy families, we show that each entropy induces a distinct Riemannian metric on the parameter space, leading to GEG algorithms that preserve the natural statistical geometry. The tunable parameters of deformed logarithms enable adaptive geometric selection, providing enhanced robustness and convergence over classical Euclidean optimization. Overall, our framework unifies key first-order MD optimization methods under a single information-geometric perspective based on generalized Bregman divergences, where the choice of entropy determines the underlying metric and dual geometric structure.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Omnidirectional Stereo Matching with a Pre-trained Depth Foundation Model</title>
<link>https://arxiv.org/abs/2503.23502</link>
<guid>https://arxiv.org/abs/2503.23502</guid>
<content:encoded><![CDATA[
arXiv:2503.23502v3 Announce Type: replace-cross 
Abstract: Omnidirectional depth perception is essential for mobile robotics applications that require scene understanding across a full 360{\deg} field of view. Camera-based setups offer a cost-effective option by using stereo depth estimation to generate dense, high-resolution depth maps without relying on expensive active sensing. However, existing omnidirectional stereo matching approaches achieve only limited depth accuracy across diverse environments, depth ranges, and lighting conditions, due to the scarcity of real-world data. We present DFI-OmniStereo, a novel omnidirectional stereo matching method that leverages a large-scale pre-trained foundation model for relative monocular depth estimation within an iterative optimization-based stereo matching architecture. We introduce a dedicated two-stage training strategy to utilize the relative monocular depth features for our omnidirectional stereo matching before scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art results on the real-world Helvipad dataset, reducing disparity MAE by approximately 16% compared to the previous best omnidirectional stereo method.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal 3D Genome Pre-training</title>
<link>https://arxiv.org/abs/2504.09060</link>
<guid>https://arxiv.org/abs/2504.09060</guid>
<content:encoded><![CDATA[
arXiv:2504.09060v2 Announce Type: replace-cross 
Abstract: Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Learning and Forgetting for Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2504.11364</link>
<guid>https://arxiv.org/abs/2504.11364</guid>
<content:encoded><![CDATA[
arXiv:2504.11364v4 Announce Type: replace-cross 
Abstract: Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it on unpaired successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. A key challenge we identify is that naive fine-tuning can degrade the model's search capability; we show this can be mitigated with a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown arithmetic puzzles show that, replacing CoT-generated data with search-generated data for offline fine-tuning improves success rates by around 23% over inference-time search baselines, while reducing inference time by 180$\times$. On top of this, our learning and forgetting objective consistently outperforms both supervised fine-tuning and preference-based methods.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text</title>
<link>https://arxiv.org/abs/2504.19467</link>
<guid>https://arxiv.org/abs/2504.19467</guid>
<content:encoded><![CDATA[
arXiv:2504.19467v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, benchmarking on large-scale real-world data such as electronic health records (EHRs) is critical, as clinical decisions are directly informed by these sources, yet current evaluations remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world clinical data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. It covers eight major task types spanning the entire continuum of patient care across six clinical stages and 20 representative applications, including triage and referral, consultation, information extraction, diagnosis, prognosis, and billing coding, and involves 14 clinical specialties. We systematically evaluated 95 LLMs (including DeepSeek-R1, GPT-4o, Gemini series, and Qwen3 series) under various inference strategies. Our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.
  The BRIDGE leaderboard: https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group-in-Group Policy Optimization for LLM Agent Training</title>
<link>https://arxiv.org/abs/2505.10978</link>
<guid>https://arxiv.org/abs/2505.10978</guid>
<content:encoded><![CDATA[
arXiv:2505.10978v3 Announce Type: replace-cross 
Abstract: Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics</title>
<link>https://arxiv.org/abs/2505.11930</link>
<guid>https://arxiv.org/abs/2505.11930</guid>
<content:encoded><![CDATA[
arXiv:2505.11930v2 Announce Type: replace-cross 
Abstract: In recent years, the expressive power of various neural architectures -- including graph neural networks (GNNs), transformers, and recurrent neural networks -- has been characterised using tools from logic and formal language theory. As the capabilities of basic architectures are becoming well understood, increasing attention is turning to models that combine multiple architectural paradigms. Among them particularly important, and challenging to analyse, are temporal extensions of GNNs, which integrate both spatial (graph-structure) and temporal (evolution over time) dimensions. In this paper, we initiate the study of logical characterisation of temporal GNNs by connecting them to two-dimensional product logics. We show that the expressive power of temporal GNNs depends on how graph and temporal components are combined. In particular, temporal GNNs that apply static GNNs recursively over time can capture all properties definable in the product logic of (past) propositional temporal logic PTL and the modal logic K. In contrast, architectures such as graph-and-time TGNNs and global TGNNs can only express restricted fragments of this logic, where the interaction between temporal and spatial operators is syntactically constrained. These provide us with the first results on the logical expressiveness of temporal GNNs.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation</title>
<link>https://arxiv.org/abs/2505.13043</link>
<guid>https://arxiv.org/abs/2505.13043</guid>
<content:encoded><![CDATA[
arXiv:2505.13043v2 Announce Type: replace-cross 
Abstract: Aiming to generalize the well-trained gaze estimation model to new target domains, Cross-domain Gaze Estimation (CDGE) is developed for real-world application scenarios. Existing CDGE methods typically extract the domain-invariant features to mitigate domain shift in feature space, which is proved insufficient by Generalized Label Shift (GLS) theory. In this paper, we introduce a novel GLS perspective to CDGE and modelize the cross-domain problem by label and conditional shift problem. A GLS correction framework is presented and a feasible realization is proposed, in which a importance reweighting strategy based on truncated Gaussian distribution is introduced to overcome the continuity challenges in label shift correction. To embed the reweighted source distribution to conditional invariant learning, we further derive a probability-aware estimation of conditional operator discrepancy. Extensive experiments on standard CDGE tasks with different backbone models validate the superior generalization capability across domain and applicability on various models of proposed method.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Use Their Depth Efficiently?</title>
<link>https://arxiv.org/abs/2505.13898</link>
<guid>https://arxiv.org/abs/2505.13898</guid>
<content:encoded><![CDATA[
arXiv:2505.13898v3 Announce Type: replace-cross 
Abstract: Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STree: Speculative Tree Decoding for Hybrid State-Space Models</title>
<link>https://arxiv.org/abs/2505.14969</link>
<guid>https://arxiv.org/abs/2505.14969</guid>
<content:encoded><![CDATA[
arXiv:2505.14969v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead relative to current SSM implementations. Along with the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code can be found at: https://github.com/wyc1997/stree.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</title>
<link>https://arxiv.org/abs/2505.16947</link>
<guid>https://arxiv.org/abs/2505.16947</guid>
<content:encoded><![CDATA[
arXiv:2505.16947v2 Announce Type: replace-cross 
Abstract: Despite recent efforts in Large Language Model (LLM) safety and alignment, current adversarial attacks on frontier LLMs can still consistently force harmful generations. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. At the same time, despite their effectiveness and generalization capabilities, training with continuous perturbations does not always capture the full spectrum of vulnerabilities exploited by discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixAT's discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraSS: Scalable Data Attribution with Gradient Sparsification and Sparse Projection</title>
<link>https://arxiv.org/abs/2505.18976</link>
<guid>https://arxiv.org/abs/2505.18976</guid>
<content:encoded><![CDATA[
arXiv:2505.18976v3 Announce Type: replace-cross 
Abstract: Gradient-based data attribution methods, such as influence functions, are critical for understanding the impact of individual training samples without requiring repeated model retraining. However, their scalability is often limited by the high computational and memory costs associated with per-sample gradient computation. In this work, we propose GraSS, a novel gradient compression algorithm and its variants FactGraSS for linear layers specifically, that explicitly leverage the inherent sparsity of per-sample gradients to achieve sub-linear space and time complexity. Extensive experiments demonstrate the effectiveness of our approach, achieving substantial speedups while preserving data influence fidelity. In particular, FactGraSS achieves up to 165% faster throughput on billion-scale models compared to the previous state-of-the-art baselines. Our code is publicly available at https://github.com/TRAIS-Lab/GraSS.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions</title>
<link>https://arxiv.org/abs/2505.21724</link>
<guid>https://arxiv.org/abs/2505.21724</guid>
<content:encoded><![CDATA[
arXiv:2505.21724v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task designed to produce synchronized verbal and non-verbal listener feedback online, based on the speaker's multimodal inputs. OMCRG captures natural dyadic interactions and introduces new challenges in aligning generated audio with listeners' facial responses. To tackle these challenges, we incorporate text as an intermediate modality to connect audio and facial responses. We propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates accurate multimodal listener responses. OmniResponse leverages a pretrained LLM enhanced with two core components: Chrono-Text Markup, which precisely timestamps generated text tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that outputs speech synchronized with facial responses. To advance OMCRG research, we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and annotated facial behaviors. Comprehensive evaluations on ResponseNet demonstrate that OmniResponse outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. Our dataset, code, and models are publicly available.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
arXiv:2505.21923v2 Announce Type: replace-cross 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99% accuracy in topology inference, <10% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings</title>
<link>https://arxiv.org/abs/2506.00481</link>
<guid>https://arxiv.org/abs/2506.00481</guid>
<content:encoded><![CDATA[
arXiv:2506.00481v2 Announce Type: replace-cross 
Abstract: Visual persuasion, which uses visual elements to influence cognition and behaviors, is crucial in fields such as advertising and political communication. With recent advancements in artificial intelligence, there is growing potential to develop persuasive systems that automatically generate persuasive images tailored to individuals. However, a significant bottleneck in this area is the lack of comprehensive datasets that connect the persuasiveness of images with the personal information about those who evaluated the images. To address this gap and facilitate technological advancements in personalized visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset, comprising 28,454 persuasive images across 596 messages and 9 persuasion strategies. Importantly, the PVP dataset provides persuasiveness scores of images evaluated by 2,521 human annotators, along with their demographic and psychological characteristics (personality traits and values). We demonstrate the utility of our dataset by developing a persuasive image generator and an automated evaluator, and establish benchmark baselines. Our experiments reveal that incorporating psychological characteristics enhances the generation and evaluation of persuasive images, providing valuable insights for personalized visual persuasion.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[
arXiv:2506.01374v2 Announce Type: replace-cross 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies</title>
<link>https://arxiv.org/abs/2506.02703</link>
<guid>https://arxiv.org/abs/2506.02703</guid>
<content:encoded><![CDATA[
arXiv:2506.02703v2 Announce Type: replace-cross 
Abstract: The art and science of Quranic recitation (Tajweed), a discipline governed by meticulous phonetic, rhythmic, and theological principles, confronts substantial educational challenges in today's digital age. Although modern technology offers unparalleled opportunities for learning, existing automated systems for evaluating recitation have struggled to gain broad acceptance or demonstrate educational effectiveness. This literature review examines this crucial disparity, offering a thorough analysis of scholarly research, digital platforms, and commercial tools developed over the past twenty years. Our analysis uncovers a fundamental flaw in current approaches that adapt Automatic Speech Recognition (ASR) systems, which emphasize word identification over qualitative acoustic evaluation. These systems suffer from limitations such as reliance on biased datasets, demographic disparities, and an inability to deliver meaningful feedback for improvement. Challenging these data-centric methodologies, we advocate for a paradigm shift toward a knowledge-based computational framework. By leveraging the unchanging nature of the Quranic text and the well-defined rules of Tajweed, we propose that an effective evaluation system should be built upon rule-based acoustic modeling centered on canonical pronunciation principles and articulation points (Makhraj), rather than depending on statistical patterns derived from flawed or biased data. The review concludes that the future of automated Quranic recitation assessment lies in hybrid systems that combine linguistic expertise with advanced audio processing. Such an approach paves the way for developing reliable, fair, and pedagogically effective tools that can authentically assist learners across the globe.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</title>
<link>https://arxiv.org/abs/2506.04536</link>
<guid>https://arxiv.org/abs/2506.04536</guid>
<content:encoded><![CDATA[
arXiv:2506.04536v3 Announce Type: replace-cross 
Abstract: Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating AI-Powered Learning Assistants in Engineering Higher Education: Student Engagement, Ethical Challenges, and Policy Implications</title>
<link>https://arxiv.org/abs/2506.05699</link>
<guid>https://arxiv.org/abs/2506.05699</guid>
<content:encoded><![CDATA[
arXiv:2506.05699v2 Announce Type: replace-cross 
Abstract: As generative AI becomes increasingly integrated into higher education, understanding how students engage with these technologies is essential for responsible adoption. This study evaluates the Educational AI Hub, an AI-powered learning framework, implemented in undergraduate civil and environmental engineering courses at a large R1 public university. Using a mixed-methods design combining pre- and post-surveys, system usage logs, and qualitative analysis of students' AI interactions, the research examines perceptions of trust, ethics, usability, and learning outcomes. Findings show that students valued the AI assistant for its accessibility and comfort, with nearly half reporting greater ease using it than seeking help from instructors or teaching assistants. The tool was most helpful for completing homework and understanding concepts, though views on its instructional quality were mixed. Ethical uncertainty, particularly around institutional policy and academic integrity, emerged as a key barrier to full engagement. Overall, students regarded AI as a supplement rather than a replacement for human instruction. The study highlights the importance of usability, ethical transparency, and faculty guidance in promoting meaningful AI engagement. A total of 71 students participated across two courses, generating over 600 AI interactions and 100 survey responses that provided both quantitative and contextual insights into learning engagement.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BNMusic: Blending Environmental Noises into Personalized Music</title>
<link>https://arxiv.org/abs/2506.10754</link>
<guid>https://arxiv.org/abs/2506.10754</guid>
<content:encoded><![CDATA[
arXiv:2506.10754v2 Announce Type: replace-cross 
Abstract: While being disturbed by environmental noises, the acoustic masking technique is a conventional way to reduce the annoyance in audio engineering that seeks to cover up the noises with other dominant yet less intrusive sounds. However, misalignment between the dominant sound and the noise-such as mismatched downbeats-often requires an excessive volume increase to achieve effective masking. Motivated by recent advances in cross-modal generation, in this work, we introduce an alternative method to acoustic masking, aiming to reduce the noticeability of environmental noises by blending them into personalized music generated based on user-provided text prompts. Following the paradigm of music generation using mel-spectrogram representations, we propose a Blending Noises into Personalized Music (BNMusic) framework with two key stages. The first stage synthesizes a complete piece of music in a mel-spectrogram representation that encapsulates the musical essence of the noise. In the second stage, we adaptively amplify the generated music segment to further reduce noise perception and enhance the blending effectiveness, while preserving auditory quality. Our experiments with comprehensive evaluations on MusicBench, EPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework, highlighting the ability to blend environmental noise with rhythmically aligned, adaptively amplified, and enjoyable music segments, minimizing the noticeability of the noise, thereby improving overall acoustic experiences. Project page: https://d-fas.github.io/BNMusic_page/.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
arXiv:2506.13771v2 Announce Type: replace-cross 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermometry of simulated Bose--Einstein condensates using machine learning</title>
<link>https://arxiv.org/abs/2506.16925</link>
<guid>https://arxiv.org/abs/2506.16925</guid>
<content:encoded><![CDATA[
arXiv:2506.16925v2 Announce Type: replace-cross 
Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate a machine learning approach for rapid, non-destructive estimation of the chemical potential and temperature from a single image of an \emph{in situ} imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D `pancake' condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates {some} zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating the temperature (although not the chemical potential) for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEMeX-RMCoT: An Enhanced Med-VQA Dataset for Region-Aware Multimodal Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2506.17939</link>
<guid>https://arxiv.org/abs/2506.17939</guid>
<content:encoded><![CDATA[
arXiv:2506.17939v2 Announce Type: replace-cross 
Abstract: Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model outputs. To address these limitations, this work first proposes a Region-Aware Multimodal Chain-of-Thought (RMCoT) dataset, in which the process of producing an answer is preceded by a sequence of intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://www.med-vqa.com/GEMeX/.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning</title>
<link>https://arxiv.org/abs/2507.01271</link>
<guid>https://arxiv.org/abs/2507.01271</guid>
<content:encoded><![CDATA[
arXiv:2507.01271v4 Announce Type: replace-cross 
Abstract: In recent years, unlearning techniques, which are methods for inducing a model to "forget" previously learned information, have attracted attention as a way to address privacy and copyright concerns in large language models (LLMs) and large multimodal models (LMMs). While several unlearning benchmarks have been established for LLMs, a practical evaluation framework for unlearning in LMMs has been less explored. Specifically, existing unlearning benchmark for LMMs considers only scenarios in which the model is required to unlearn fine-tuned knowledge through a single unlearning operation. In this study, we introduce PULSE protocol for realistic unlearning scenarios for LMMs by introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for analyzing the effect across different knowledge acquisition phases and (ii) Long-term Sustainability Evaluation to address sequential requests. We then evaluate existing unlearning methods along these dimensions. Our results reveal that, although some techniques can successfully unlearn knowledge acquired through fine-tuning, they struggle to eliminate information learned during pre-training. Moreover, methods that effectively unlearn a batch of target data in a single operation exhibit substantial performance degradation when the same data are split and unlearned sequentially.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoGE: Fock Space inspired encoding for graph prompting</title>
<link>https://arxiv.org/abs/2507.02937</link>
<guid>https://arxiv.org/abs/2507.02937</guid>
<content:encoded><![CDATA[
arXiv:2507.02937v2 Announce Type: replace-cross 
Abstract: Recent results show that modern Large Language Models (LLM) are indeed capable of understanding and answering questions about structured data such as graphs. This new paradigm can lead to solutions that require less supervision while, at the same time, providing a model that can generalize and answer questions beyond the training labels. Existing proposals often use some description of the graph to create an ``augmented'' prompt fed to the LLM. For a chosen class of graphs, if a well-tailored graph encoder is deployed to play together with a pre-trained LLM, the model can answer graph-related questions well. Existing solutions to graph-based prompts range from graph serialization to graph transformers. In this work, we show that the use of a parameter-free graph encoder based on Fock space representations, a concept borrowed from mathematical physics, is remarkably versatile in this problem setting. The simple construction, inherited directly from the theory with a few small adjustments, can provide rich and informative graph encodings, for a wide range of different graphs. We investigate the use of this idea for prefix-tuned prompts leveraging the capabilities of a pre-trained, frozen LLM. The modifications lead to a model that can answer graph-related questions -- from simple graphs to proteins to hypergraphs -- effectively and with minimal, if any, adjustments to the architecture. Our work significantly simplifies existing solutions and generalizes well to multiple different graph-based structures effortlessly.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness is Important: Limitations of LLMs for Data Fitting</title>
<link>https://arxiv.org/abs/2508.19563</link>
<guid>https://arxiv.org/abs/2508.19563</guid>
<content:encoded><![CDATA[
arXiv:2508.19563v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are being applied in a wide array of settings, well beyond the typical language-oriented use cases. In particular, LLMs are increasingly used as a plug-and-play method for fitting data and generating predictions. Prior work has shown that LLMs, via in-context learning or supervised fine-tuning, can perform competitively with many tabular supervised learning techniques in terms of predictive performance. However, we identify a critical vulnerability of using LLMs for data fitting -- making changes to data representation that are completely irrelevant to the underlying learning task can drastically alter LLMs' predictions on the same data. For example, simply changing variable names can sway the size of prediction error by as much as 82% in certain settings. Such prediction sensitivity with respect to task-irrelevant variations manifests under both in-context learning and supervised fine-tuning, for both close-weight and open-weight general-purpose LLMs. Moreover, by examining the attention scores of an open-weight LLM, we discover a non-uniform attention pattern: training examples and variable names/values which happen to occupy certain positions in the prompt receive more attention when output tokens are generated, even though different positions are expected to receive roughly the same attention. This partially explains the sensitivity in the presence of task-irrelevant variations. We also consider a state-of-the-art tabular foundation model (TabPFN) trained specifically for data fitting. Despite being explicitly designed to achieve prediction robustness, TabPFN is still not immune to task-irrelevant variations. Overall, despite LLMs' impressive predictive capabilities, currently they lack even the basic level of robustness to be used as a principled data-fitting tool.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolErr2Fix: Benchmarking LLM Trustworthiness in Chemistry via Modular Error Detection, Localization, Explanation, and Revision</title>
<link>https://arxiv.org/abs/2509.00063</link>
<guid>https://arxiv.org/abs/2509.00063</guid>
<content:encoded><![CDATA[
arXiv:2509.00063v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown growing potential in molecular sciences, but they often produce chemically inaccurate descriptions and struggle to recognize or justify potential errors. This raises important concerns about their robustness and reliability in scientific applications. To support more rigorous evaluation of LLMs in chemical reasoning, we present the MolErr2Fix benchmark, designed to assess LLMs on error detection and correction in molecular descriptions. Unlike existing benchmarks focused on molecule-to-text generation or property prediction, MolErr2Fix emphasizes fine-grained chemical understanding. It tasks LLMs with identifying, localizing, explaining, and revising potential structural and semantic errors in molecular descriptions. Specifically, MolErr2Fix consists of 1,193 fine-grained annotated error instances. Each instance contains quadruple annotations, i.e,. (error type, span location, the explanation, and the correction). These tasks are intended to reflect the types of reasoning and verification required in real-world chemical communication. Evaluations of current state-of-the-art LLMs reveal notable performance gaps, underscoring the need for more robust chemical reasoning capabilities. MolErr2Fix provides a focused benchmark for evaluating such capabilities and aims to support progress toward more reliable and chemically informed language models. All annotations and an accompanying evaluation API will be publicly released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers</title>
<link>https://arxiv.org/abs/2509.00103</link>
<guid>https://arxiv.org/abs/2509.00103</guid>
<content:encoded><![CDATA[
arXiv:2509.00103v2 Announce Type: replace-cross 
Abstract: Modern optimization in experimental chemistry employs algorithmic search through black-box parameter spaces. Here we demonstrate that pre-trained knowledge in large language models (LLMs) fundamentally changes this paradigm. Using six fully enumerated categorical reaction datasets (768-5,684 experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian optimization (BO) and random sampling. Frontier LLMs consistently match or exceed BO performance across five single-objective datasets, with advantages growing as parameter complexity increases and high-performing conditions become scarce (<5% of space). BO retains superiority only for explicit multi-objective trade-offs. To understand these contrasting behaviors, we introduce a topology-agnostic information theory framework quantifying sampling diversity throughout optimization campaigns. This analysis reveals that LLMs maintain systematically higher exploration Shannon entropy than BO across all datasets while achieving superior performance, with advantages most pronounced in solution-scarce parameter spaces where high-entropy exploration typically fails-suggesting that pre-trained domain knowledge enables more effective navigation of chemical parameter space rather than replacing structured exploration strategies. To enable transparent benchmarking and community validation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a no-code platform for side-by-side evaluation of human, algorithmic, and LLM optimization campaigns with public leaderboards and complete trajectories. Our findings establish that LLM-GO excels precisely where traditional methods struggle: complex categorical spaces requiring domain understanding rather than mathematical optimization.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducible workflow for online AI in digital health</title>
<link>https://arxiv.org/abs/2509.13499</link>
<guid>https://arxiv.org/abs/2509.13499</guid>
<content:encoded><![CDATA[
arXiv:2509.13499v3 Announce Type: replace-cross 
Abstract: Online artificial intelligence (AI) algorithms are an important component of digital health interventions. These online algorithms are designed to continually learn and improve their performance as streaming data is collected on individuals. Deploying online AI presents a key challenge: balancing adaptability of online AI with reproducibility. Online AI in digital interventions is a rapidly evolving area, driven by advances in algorithms, sensors, software, and devices. Digital health intervention development and deployment is a continuous process, where implementation - including the AI decision-making algorithm - is interspersed with cycles of re-development and optimization. Each deployment informs the next, making iterative deployment a defining characteristic of this field. This iterative nature underscores the importance of reproducibility: data collected across deployments must be accurately stored to have scientific utility, algorithm behavior must be auditable, and results must be comparable over time to facilitate scientific discovery and trustworthy refinement. This paper proposes a reproducible scientific workflow for developing, deploying, and analyzing online AI decision-making algorithms in digital health interventions. Grounded in practical experience from multiple real-world deployments, this workflow addresses key challenges to reproducibility across all phases of the online AI algorithm development life-cycle.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The human-machine paradox: how collaboration creates or destroys value, and why augmentation is key to resolving it</title>
<link>https://arxiv.org/abs/2509.14057</link>
<guid>https://arxiv.org/abs/2509.14057</guid>
<content:encoded><![CDATA[
arXiv:2509.14057v5 Announce Type: replace-cross 
Abstract: When deploying artificial skills, managers widely assume that combining them with the human factor is a safe harbor, mitigating the risks of full automation in high-complexity tasks. This paper formally challenges the economic validity of this widespread assumption, arguing that the true bottom-line economic utility of a human-machine skill policy is dangerously misunderstood and highly contingent on situational and design factors. To investigate this gap, we develop an in-silico framework based on Monte Carlo simulations grounded in empirical pragmatism to quantify the economic impact of human and machine skills in the execution of tasks presenting varying levels of complexity. Our results show that a human-machine strategy can yield the highest economic utility in complex scenarios, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine approach can perform worse than either the machine-exclusive or the human-exclusive policy, actively destroying value under the pressure of costs that are not compensated by sufficient performance gains. The takeaway for decision-makers is unambiguous: when the context is complex and critical, simply allocating human and machine skills to a task may be insufficient, and far from being a silver-bullet solution or a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models</title>
<link>https://arxiv.org/abs/2509.16989</link>
<guid>https://arxiv.org/abs/2509.16989</guid>
<content:encoded><![CDATA[
arXiv:2509.16989v2 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation mechanisms, they suffer from either limited representational capacity or computational overhead that undermines their efficiency gains. We introduce PTQ to Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit representation. PTQTP achieves multiplication-free inference, identical to 1-bit quantization, while maintaining superior expressiveness through its novel structured decomposition. Our approach provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment across diverse modern LLMs without architectural modifications; and (3) uniform ternary operations that eliminate the need for mixed-precision or compensation schemes. Comprehensive experiments across LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP significantly outperforms existing low-bit PTQ methods, achieving 82.4% mathematical reasoning retention versus 0% for competing approaches. PTQTP approaches and sometimes surpasses 1.58-bit quantization-aware training performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods. These results establish PTQTP as a practical solution for efficient LLM deployment in resource-constrained environments. The code will be available at https://github.com/HeXiao-55/PTQTP.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression</title>
<link>https://arxiv.org/abs/2509.20234</link>
<guid>https://arxiv.org/abs/2509.20234</guid>
<content:encoded><![CDATA[
arXiv:2509.20234v3 Announce Type: replace-cross 
Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning</title>
<link>https://arxiv.org/abs/2509.23311</link>
<guid>https://arxiv.org/abs/2509.23311</guid>
<content:encoded><![CDATA[
arXiv:2509.23311v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) often appear culturally competent but rely on superficial pattern matching rather than genuine cultural understanding. We introduce a diagnostic framework to probe VLM reasoning on fire-themed cultural imagery through both classification and explanation analysis. Testing multiple models on Western festivals, non-Western traditions, and emergency scenes reveals systematic biases: models correctly identify prominent Western festivals but struggle with underrepresented cultural events, frequently offering vague labels or dangerously misclassifying emergencies as celebrations. These failures expose the risks of symbolic shortcuts and highlight the need for cultural evaluation beyond accuracy metrics to ensure interpretable and fair multimodal systems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM</title>
<link>https://arxiv.org/abs/2509.24085</link>
<guid>https://arxiv.org/abs/2509.24085</guid>
<content:encoded><![CDATA[
arXiv:2509.24085v2 Announce Type: replace-cross 
Abstract: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradigm by leveraging both publisher and subscriber states to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which normalizes latency by application tolerances and modulates energy by device battery states, provides richer supervision for KL-based finetuning. We study two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms inference at near-identical objective scores. Across synthetic scenarios grounded in real measurements, PEARL improves objective scores over heuristic and compact model baselines and reduces energy by up to 16% in cooperative low-battery cases. These results demonstrate that peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control. Code, real-world demo, and dataset are available at https://github.com/abman23/pearl
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations</title>
<link>https://arxiv.org/abs/2510.00037</link>
<guid>https://arxiv.org/abs/2510.00037</guid>
<content:encoded><![CDATA[
arXiv:2510.00037v3 Announce Type: replace-cross 
Abstract: In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?</title>
<link>https://arxiv.org/abs/2510.01161</link>
<guid>https://arxiv.org/abs/2510.01161</guid>
<content:encoded><![CDATA[
arXiv:2510.01161v2 Announce Type: replace-cross 
Abstract: Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Untargeted Jailbreak Attack</title>
<link>https://arxiv.org/abs/2510.02999</link>
<guid>https://arxiv.org/abs/2510.02999</guid>
<content:encoded><![CDATA[
arXiv:2510.02999v2 Announce Type: replace-cross 
Abstract: Existing gradient-based jailbreak attacks on Large Language Models (LLMs), such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize adversarial suffixes to align the LLM output with a predefined target response. However, by restricting the optimization objective as inducing a predefined target, these methods inherently constrain the adversarial search space, which limit their overall attack efficacy. Furthermore, existing methods typically require a large number of optimization iterations to fulfill the large gap between the fixed target and the original model response, resulting in low attack efficiency.
  To overcome the limitations of targeted jailbreak attacks, we propose the first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an unsafe response without enforcing any predefined patterns. Specifically, we formulate an untargeted attack objective to maximize the unsafety probability of the LLM response, which can be quantified using a judge model. Since the objective is non-differentiable, we further decompose it into two differentiable sub-objectives for optimizing an optimal harmful response and the corresponding adversarial prompt, with a theoretical analysis to validate the decomposition. In contrast to targeted jailbreak attacks, UJA's unrestricted objective significantly expands the search space, enabling a more flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations demonstrate that UJA can achieve over 80% attack success rates against recent safety-aligned LLMs with only 100 optimization iterations, outperforming the state-of-the-art gradient-based attacks such as I-GCG and COLD-Attack by over 20%.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilled Protein Backbone Generation</title>
<link>https://arxiv.org/abs/2510.03095</link>
<guid>https://arxiv.org/abs/2510.03095</guid>
<content:encoded><![CDATA[
arXiv:2510.03095v3 Announce Type: replace-cross 
Abstract: Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications. The PyTorch implementation is available at https://github.com/LY-Xie/SiD_Protein
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEER: The Span-based Emotion Evidence Retrieval Benchmark</title>
<link>https://arxiv.org/abs/2510.03490</link>
<guid>https://arxiv.org/abs/2510.03490</guid>
<content:encoded><![CDATA[
arXiv:2510.03490v2 Announce Type: replace-cross 
Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to test Large Language Models' (LLMs) ability to identify the specific spans of text that express emotion. Unlike traditional emotion recognition tasks that assign a single label to an entire sentence, SEER targets the underexplored task of emotion evidence detection: pinpointing which exact phrases convey emotion. This span-level approach is crucial for applications like empathetic dialogue and clinical support, which need to know how emotion is expressed, not just what the emotion is. SEER includes two tasks: identifying emotion evidence within a single sentence, and identifying evidence across a short passage of five consecutive sentences. It contains new annotations for both emotion and emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs and find that, while some models approach average human performance on single-sentence inputs, their accuracy degrades in longer passages. Our error analysis reveals key failure modes, including overreliance on emotion keywords and false positives in neutral text.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.08146</link>
<guid>https://arxiv.org/abs/2510.08146</guid>
<content:encoded><![CDATA[
arXiv:2510.08146v3 Announce Type: replace-cross 
Abstract: We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
<link>https://arxiv.org/abs/2510.14205</link>
<guid>https://arxiv.org/abs/2510.14205</guid>
<content:encoded><![CDATA[
arXiv:2510.14205v2 Announce Type: replace-cross 
Abstract: The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Scenario Unified Modeling of User Interests at Billion Scale</title>
<link>https://arxiv.org/abs/2510.14788</link>
<guid>https://arxiv.org/abs/2510.14788</guid>
<content:encoded><![CDATA[
arXiv:2510.14788v2 Announce Type: replace-cross 
Abstract: User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs</title>
<link>https://arxiv.org/abs/2510.15545</link>
<guid>https://arxiv.org/abs/2510.15545</guid>
<content:encoded><![CDATA[
arXiv:2510.15545v2 Announce Type: replace-cross 
Abstract: Accelerating the inference of large language models (LLMs) has been a critical challenge in generative AI. Speculative decoding (SD) substantially improves LLM inference efficiency. However, its utility is limited by a fundamental constraint: the draft and target models must share the same vocabulary, thus limiting the herd of available draft models and often necessitating the training of a new model from scratch. Inspired by Dynamic Time Warping (DTW), a classic algorithm for aligning time series, we propose the algorithm TokenTiming for universal speculative decoding. It operates by re-encoding the draft token sequence to get a new target token sequence, and then uses DTW to build a mapping to transfer the probability distributions for speculative sampling. Benefiting from this, our method accommodates mismatched vocabularies and works with any off-the-shelf models without retraining and modification. We conduct comprehensive experiments on various tasks, demonstrating 1.57x speedup. This work enables a universal approach for draft model selection, making SD a more versatile and practical tool for LLM acceleration.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</title>
<link>https://arxiv.org/abs/2510.15870</link>
<guid>https://arxiv.org/abs/2510.15870</guid>
<content:encoded><![CDATA[
arXiv:2510.15870v2 Announce Type: replace-cross 
Abstract: Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Formalism-Implementation Gap in Reinforcement Learning Research</title>
<link>https://arxiv.org/abs/2510.16175</link>
<guid>https://arxiv.org/abs/2510.16175</guid>
<content:encoded><![CDATA[
arXiv:2510.16175v2 Announce Type: replace-cross 
Abstract: The last decade has seen an upswing in interest and adoption of reinforcement learning (RL) techniques, in large part due to its demonstrated capabilities at performing certain tasks at "super-human levels". This has incentivized the community to prioritize research that demonstrates RL agent performance, often at the expense of research aimed at understanding their learning dynamics. Performance-focused research runs the risk of overfitting on academic benchmarks -- thereby rendering them less useful -- which can make it difficult to transfer proposed techniques to novel problems. Further, it implicitly diminishes work that does not push the performance-frontier, but aims at improving our understanding of these techniques. This paper argues two points: (i) RL research should stop focusing solely on demonstrating agent capabilities, and focus more on advancing the science and understanding of reinforcement learning; and (ii) we need to be more precise on how our benchmarks map to the underlying mathematical formalisms. We use the popular Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a benchmark that, despite being increasingly considered "saturated", can be effectively used for developing this understanding, and facilitating the deployment of RL techniques in impactful real-world problems.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.17191</link>
<guid>https://arxiv.org/abs/2510.17191</guid>
<content:encoded><![CDATA[
arXiv:2510.17191v2 Announce Type: replace-cross 
Abstract: End-to-end autonomous driving has emerged as a promising paradigm for achieving robust and intelligent driving policies. However, existing end-to-end methods still face significant challenges, such as suboptimal decision-making in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring Fusion), a novel framework that enhances end-to-end planning by leveraging the cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory fusion techniques. We utilize the conventional scorers and the novel VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative aggregation and a powerful VLM-based fusioner for qualitative, context-aware decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art performance, achieving a superior balance between safety, comfort, and efficiency.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</title>
<link>https://arxiv.org/abs/2510.17281</link>
<guid>https://arxiv.org/abs/2510.17281</guid>
<content:encoded><![CDATA[
arXiv:2510.17281v2 Announce Type: replace-cross 
Abstract: Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENTOR: A Reinforcement Learning Framework for Enabling Tool Use in Small Models via Teacher-Optimized Rewards</title>
<link>https://arxiv.org/abs/2510.18383</link>
<guid>https://arxiv.org/abs/2510.18383</guid>
<content:encoded><![CDATA[
arXiv:2510.18383v2 Announce Type: replace-cross 
Abstract: Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</title>
<link>https://arxiv.org/abs/2510.19585</link>
<guid>https://arxiv.org/abs/2510.19585</guid>
<content:encoded><![CDATA[
arXiv:2510.19585v2 Announce Type: replace-cross 
Abstract: This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation</title>
<link>https://arxiv.org/abs/2510.20774</link>
<guid>https://arxiv.org/abs/2510.20774</guid>
<content:encoded><![CDATA[
arXiv:2510.20774v2 Announce Type: replace-cross 
Abstract: Large-scale and diverse datasets are vital for training robust robotic manipulation policies, yet existing data collection methods struggle to balance scale, diversity, and quality. Simulation offers scalability but suffers from sim-to-real gaps, while teleoperation yields high-quality demonstrations with limited diversity and high labor cost. We introduce FieldGen, a field-guided data generation framework that enables scalable, diverse, and high-quality real-world data collection with minimal human supervision. FieldGen decomposes manipulation into two stages: a pre-manipulation phase, allowing trajectory diversity, and a fine manipulation phase requiring expert precision. Human demonstrations capture key contact and pose information, after which an attraction field automatically generates diverse trajectories converging to successful configurations. This decoupled design combines scalable trajectory diversity with precise supervision. Moreover, FieldGen-Reward augments generated data with reward annotations to further enhance policy learning. Experiments demonstrate that policies trained with FieldGen achieve higher success rates and improved stability compared to teleoperation-based baselines, while significantly reducing human effort in long-term real-world data collection. Webpage is available at https://fieldgen.github.io/.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title>
<link>https://arxiv.org/abs/2510.20487</link>
<guid>https://arxiv.org/abs/2510.20487</guid>
<content:encoded><![CDATA[
<div> evaluation-aware behavior, steering vector, large language models, safety evaluations, AI evaluators<br />
<br />
Summary: Large language models (LLMs) can develop evaluation-aware behavior when being tested, affecting the reliability of safety evaluations. A new approach using a steering vector added to an LLM's activations can suppress this awareness and make the model behave as if it is deployed during evaluation. The study involved training an LLM to exhibit evaluation-aware behavior using a two-step process. Firstly, continued pretraining on documents with factual descriptions introduced evaluation cues, and then expert iteration trained the model to utilize Python type hints in evaluation settings. The results showed that activation steering effectively reduced evaluation awareness in the model. By steering models to act like they are deployed, AI evaluators can enhance the accuracy and trustworthiness of safety evaluations, ensuring the reliability of the model's behavior during testing scenarios. <div>
arXiv:2510.20487v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist</title>
<link>https://arxiv.org/abs/2510.19139</link>
<guid>https://arxiv.org/abs/2510.19139</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, healthcare, clinical trial reporting, uncertainty calibration, metacognitive reliability<br />
Summary: 
- The study investigates the evaluation of Large Language Models (LLMs) in assessing clinical trial reporting based on CONSORT standards.
- It examines the uncertainty calibration and metacognitive reliability of LLM reasoning in medical automation.
- Two representative LLMs, one general and one domain-specialized, are compared using behavioral and metacognitive analysis.
- Both models exhibit miscalibration and overconfidence, particularly under clinical role-playing conditions.
- Findings emphasize the importance of improving calibration, transparent code, and prompt engineering for reliable and explainable medical AI.<br /> 

Summary: <div>
arXiv:2510.19139v2 Announce Type: replace 
Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare, robust and explainable evaluation of their ability to assess clinical trial reporting according to CONSORT standards remains an open challenge. In particular, uncertainty calibration and metacognitive reliability of LLM reasoning are poorly understood and underexplored in medical automation. This study applies a behavioral and metacognitive analytic approach using an expert-validated dataset, systematically comparing two representative LLMs - one general and one domain-specialized - across three prompt strategies. We analyze both cognitive adaptation and calibration error using metrics: Expected Calibration Error (ECE) and a baseline-normalized Relative Calibration Error (RCE) that enables reliable cross-model comparison. Our results reveal pronounced miscalibration and overconfidence in both models, especially under clinical role-playing conditions, with calibration error persisting above clinically relevant thresholds. These findings underscore the need for improved calibration, transparent code, and strategic prompt engineering to develop reliable and explainable medical AI.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WolBanking77: Wolof Banking Speech Intent Classification Dataset</title>
<link>https://arxiv.org/abs/2509.19271</link>
<guid>https://arxiv.org/abs/2509.19271</guid>
<content:encoded><![CDATA[
<div> Keywords: Intent classification, Low-resource languages, Wolof, Banking domain, Dataset

Summary:
The article introduces the Wolof Banking Speech Intent Classification Dataset (WolBanking77) to address the gap in intent classification models for low-resource languages like Wolof, spoken by a significant population in Senegal and West Africa. The dataset comprises text and spoken sentences related to banking, aiming to facilitate academic research in intent classification. Experimental results on various models, including text and voice-based, show promising outcomes on WolBanking77. The paper also provides a detailed analysis of the dataset contents, reporting baseline F1-scores and word error rates for models trained on the dataset. Comparison between different models further highlights the effectiveness of the Wolof Banking Speech Intent Classification Dataset. The dataset and code are available for research purposes on GitHub. 

<br /><br />Summary: <div>
arXiv:2509.19271v3 Announce Type: replace-cross 
Abstract: Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90\% of the population, while the national illiteracy rate remains at of 42\%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset's contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: https://github.com/abdoukarim/wolbanking77.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion</title>
<link>https://arxiv.org/abs/2510.13887</link>
<guid>https://arxiv.org/abs/2510.13887</guid>
<content:encoded><![CDATA[
<div> Hierarchical Semantic Alignment, Cooperative Completion, Incomplete Multi-view Clustering, Deep Learning, Fusion Strategies 
Summary: 
Hierarchical Semantic Alignment and Cooperative Completion (HSACC) is proposed for incomplete multi-view clustering. It addresses challenges posed by incomplete multi-view data by utilizing a dual-level semantic space design. In the low-level semantic space, alignment is achieved through maximizing mutual information across views. The high-level semantic space assigns adaptive view weights based on distributional affinity, enabling weighted fusion for a unified representation. HSACC also incorporates implicit view recovery and jointly optimizes reconstruction and clustering objectives. Experimental results demonstrate superior performance over existing methods on benchmark datasets. Ablation studies verify the efficacy of hierarchical alignment and dynamic weighting, while parameter analysis shows robustness to hyperparameter variations. <br /><br />Summary: <div>
arXiv:2510.13887v3 Announce Type: replace-cross 
Abstract: Incomplete multi-view data, where certain views are entirely missing for some samples, poses significant challenges for traditional multi-view clustering methods. Existing deep incomplete multi-view clustering approaches often rely on static fusion strategies or two-stage pipelines, leading to suboptimal fusion results and error propagation issues. To address these limitations, this paper proposes a novel incomplete multi-view clustering framework based on Hierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC achieves robust cross-view fusion through a dual-level semantic space design. In the low-level semantic space, consistency alignment is ensured by maximizing mutual information across views. In the high-level semantic space, adaptive view weights are dynamically assigned based on the distributional affinity between individual views and an initial fused representation, followed by weighted fusion to generate a unified global representation. Additionally, HSACC implicitly recovers missing views by projecting aligned latent representations into high-dimensional semantic spaces and jointly optimizes reconstruction and clustering objectives, enabling cooperative learning of completion and clustering. Experimental results demonstrate that HSACC significantly outperforms state-of-the-art methods on five benchmark datasets. Ablation studies validate the effectiveness of the hierarchical alignment and dynamic weighting mechanisms, while parameter analysis confirms the model's robustness to hyperparameter variations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15</title>
<link>https://arxiv.org/abs/2510.17883</link>
<guid>https://arxiv.org/abs/2510.17883</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Intrusion Detection, UNSW-NB15, Prompt-only Approach, Calibration

Summary:
Large Language Models (LLMs) are evaluated for intrusion detection using a prompt-only approach on the UNSW-NB15 dataset. Each network flow is converted to a textual record with domain-inspired boolean flags to improve detection quality. The model is constrained to produce structured responses, and a decision threshold is calibrated for stability. Comparisons are made with strong tabular and neural baselines, showing that guided prompting significantly improves detection performance. Empirically, unguided prompting is unreliable, but instruction-guided prompting with flags enhances decision quality. A 7B model with instruction tuning achieves a macro-F1 score of 0.78 on a subset of two hundred flows, while a lighter 3B model with few-shot cues and calibration scores near 0.68 on one thousand examples. The model's decision quality decreases with larger evaluation sets, highlighting sensitivity to coverage and prompting. The prompt-only pipeline offers interpretability, requires no gradient training, and adapts easily through instructions and flags.<br /><br />Summary: <div>
arXiv:2510.17883v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can reason over natural-language inputs, but their role in intrusion detection without fine-tuning remains uncertain. This study evaluates a prompt-only approach on UNSW-NB15 by converting each network flow to a compact textual record and augmenting it with lightweight, domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer anomalies, rare service/state, short bursts). To reduce output drift and support measurement, the model is constrained to produce structured, grammar-valid responses, and a single decision threshold is calibrated on a small development split. We compare zero-shot, instruction-guided, and few-shot prompting to strong tabular and neural baselines under identical splits, reporting accuracy, precision, recall, F1, and macro scores. Empirically, unguided prompting is unreliable, while instructions plus flags substantially improve detection quality; adding calibrated scoring further stabilizes results. On a balanced subset of two hundred flows, a 7B instruction-tuned model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot cues and calibration attains F1 near 0.68 on one thousand examples. As the evaluation set grows to two thousand flows, decision quality decreases, revealing sensitivity to coverage and prompting. Tabular baselines remain more stable and faster, yet the prompt-only pipeline requires no gradient training, produces readable artifacts, and adapts easily through instructions and flags. Contributions include a flow-to-text protocol with interpretable cues, a calibration method for thresholding, a systematic baseline comparison, and a reproducibility bundle with prompts, grammar, metrics, and figures.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title>
<link>https://arxiv.org/abs/2510.17884</link>
<guid>https://arxiv.org/abs/2510.17884</guid>
<content:encoded><![CDATA[
<div> password cracking, Large Language Models (LLMs), cybersecurity, synthetic user profiles, generative reasoning

Summary:
This study investigates the use of Large Language Models (LLMs) for password cracking in cybersecurity using synthetic user profiles. State-of-the-art LLMs like TinyLLaMA and Falcon-RW-1B were found to have limited success rates, achieving less than 1.5% accuracy at Hit@10. Traditional rule-based and combinator-based methods proved to be more effective in password guessing. The study identified key limitations in the generative reasoning of LLMs for this task, highlighting the lack of domain adaptation and memorization capabilities necessary for successful password inference. Current LLMs lack the supervised fine-tuning on leaked password datasets, indicating the need for improvement in domain-specific tasks like password guessing. These findings offer valuable insights into the limitations of LLMs in adversarial contexts and suggest directions for future research in secure, privacy-preserving, and robust password modeling.<br /><br />Summary: <div>
arXiv:2510.17884v2 Announce Type: replace-cross 
Abstract: The remarkable capabilities of Large Language Models (LLMs) in natural language understanding and generation have sparked interest in their potential for cybersecurity applications, including password guessing. In this study, we conduct an empirical investigation into the efficacy of pre-trained LLMs for password cracking using synthetic user profiles. Specifically, we evaluate the performance of state-of-the-art open-source LLMs such as TinyLLaMA, Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords based on structured user attributes (e.g., name, birthdate, hobbies). Our results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext and SHA-256 hash comparisons, reveal consistently poor performance, with all models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional rule-based and combinator-based cracking methods demonstrate significantly higher success rates. Through detailed analysis and visualization, we identify key limitations in the generative reasoning of LLMs when applied to the domain-specific task of password guessing. Our findings suggest that, despite their linguistic prowess, current LLMs lack the domain adaptation and memorization capabilities required for effective password inference, especially in the absence of supervised fine-tuning on leaked password datasets. This study provides critical insights into the limitations of LLMs in adversarial contexts and lays the groundwork for future efforts in secure, privacy-preserving, and robust password modeling.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIN-Merging: Merge the Important Neurons for Model Merging</title>
<link>https://arxiv.org/abs/2510.17890</link>
<guid>https://arxiv.org/abs/2510.17890</guid>
<content:encoded><![CDATA[
<div> neural network, deep learning, model merging, parameter conflicts, MIN-Merging 

Summary:
MIN-Merging is a router-based framework designed to address the issue of parameter conflicts when merging open-source deep learning models. By selectively merging the most important neurons, MIN-Merging reduces conflicts and improves performance on domain-specific tasks. Through extensive experiments on Computer Vision and Natural Language Processing benchmarks, MIN-Merging consistently outperformed existing approaches, achieving gains on in-domain tasks while maintaining the generalization ability of pretrained models on out-of-domain tasks. This highlights MIN-Merging as a practical solution to the parameter conflict problem in model merging. <div>
arXiv:2510.17890v2 Announce Type: replace-cross 
Abstract: Recent advances in deep learning have led to a surge of open-source models across diverse domains. While model merging offers a promising way to combine their strengths, existing approaches often suffer from parameter conflicts that degrade performance on domain-specific tasks. We propose MIN-Merging, a router-based framework that selectively merges the most important neurons to reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent gains on in-domain tasks while retaining the generalization ability of pretrained models on out-of-domain tasks. These results highlight its effectiveness as a practical solution to the parameter conflict problem in model merging.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADPO: Anchored Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2510.18913</link>
<guid>https://arxiv.org/abs/2510.18913</guid>
<content:encoded><![CDATA[
<div> Keywords: Direct Preference Optimization, Anchored Direct Preference Optimization, soft preference probabilities, policy updates, listwise learning 

Summary:
Anchored Direct Preference Optimization (ADPO) builds upon Direct Preference Optimization (DPO) by incorporating soft preference probabilities and reference anchoring to induce an implicit trust region. ADPO extends to listwise learning through Plackett-Luce modeling. Controlled synthetic experiments across various scenarios show that ADPO outperforms standard DPO, with relative improvements ranging from 12% to 79%. While hard labels perform better under severe noise, soft labels offer better calibration under distribution shift. ADPO's listwise variants achieve higher WinMass in the majority of scenarios. Larger models further enhance the benefits of ADPO, indicating that anchoring serves as an effective trust-region regularizer. Code and configurations have been released to support reproducibility. <div>
arXiv:2510.18913v2 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) is an efficient alternative to reinforcement learning from human feedback (RLHF), yet it typically assumes hard binary labels and pairwise comparisons. Such assumptions can be brittle under noisy or distribution-shifted supervision. We present Anchored Direct Preference Optimization (ADPO), which (i) incorporates soft preference probabilities, (ii) aligns policy updates through reference anchoring that induces an implicit trust region, and (iii) extends to listwise learning via Plackett-Luce modeling. In controlled synthetic setups covering 12 scenarios (4 noise types x 3 severities) and 3 model scales, ADPO exhibits relative improvements ranging from 12% to 79% over a standard DPO baseline (10-seed means; 95% CIs in the Appendix). Hard labels tend to fare better under severe noise, whereas soft labels yield better calibration under distribution shift; listwise variants achieve the highest WinMass (expected probability mass on the ground-truth best item) in 9/12 scenarios. Larger models amplify ADPO's benefits (0.718 vs. 0.416 at hidden=256), suggesting that anchoring acts as an effective trust-region regularizer. We release code and configurations to facilitate reproducibility.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in OmniModels</title>
<link>https://arxiv.org/abs/2510.18915</link>
<guid>https://arxiv.org/abs/2510.18915</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal, omni-modal, evaluation, performance

Summary:
The article introduces UNO-Bench, a comprehensive benchmark for assessing both uni-modal and omni-modal capabilities of large language models. UNO-Bench consists of 3730 human-curated samples covering 44 task types, with a high cross-modality solvability rate of 98%. It includes an innovative multi-step open-ended question type for evaluating complex reasoning skills. A general scoring model supporting 6 question types is proposed for accurate automated evaluation, with a 95% accuracy rate. Experimental results demonstrate a Compositional Law between omni-modal and uni-modal performance, showing a bottleneck effect on weak models and synergistic promotion on strong models due to omni-modal capabilities. The code and data for UNO-Bench are available on GitHub, allowing for further research and development in the field of multimodal and omni-modal language models.
<br /><br />Summary: <div>
arXiv:2510.18915v2 Announce Type: replace-cross 
Abstract: Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, which requires comprehensive evaluation to drive omni model's intelligence evolution. In this work, we propose a novel, high quality and UNified Omni model benchmark, UNO-Bench, which effectively assesses both UNi-modal and Omni-modal capabilities. The benchmark consists of 3730 human curated samples, with 98% cross-modality solvability, across 44 task types, and an innovative multi-step open-ended question type for assessing complex reasoning. Besides, a general scoring model supporting 6 question types is proposed for automated evaluation with 95% accuracy. Experimental result shows the Compositional Law between omni-modal and uni-modal performance and the omni-modal capability manifests as a bottleneck effect on weak models, while exhibiting synergistic promotion on strong models. The code and data are available at https://github.com/meituan-longcat/UNO-Bench
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients</title>
<link>https://arxiv.org/abs/2510.18924</link>
<guid>https://arxiv.org/abs/2510.18924</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, human feedback, noise-robust, group-based policy optimization, label-noise correction

Summary: 
The article introduces a noise-robust Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO) framework for reinforcement learning from human feedback (RLHF) or verifiable rewards (RLVR). It explicitly models reward corruption as Bernoulli noise and applies noise correction to debias the learning signal, leading to unbiased gradient estimates. Theoretical analysis demonstrates that group-based methods inherently mitigate individual-level noise, with the correction strategy enhancing this robustness. Empirical results show consistent improvements in math and code tasks, with up to 6.7 percentage points accuracy increase in math tasks and 1.5 points in code tasks. This work bridges label-noise correction from supervised learning with RLHF, providing theoretical insights and a practical algorithm for noisy real-world applications. <div>
arXiv:2510.18924v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards (RLVR), the standard paradigm for aligning LLMs or building recent SOTA reasoning models, is highly sensitive to noise from inconsistent or erroneous rewards. Yet, the interaction between such noise and widely used group-based policy optimization methods remains underexplored. We introduce a noise-robust Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO) framework that explicitly models reward corruption as Bernoulli noise. Our method applies noise correction after estimating reward flip probabilities to debias the learning signal, yielding provably unbiased gradient estimates. Theoretical analysis shows that group-based methods inherently mitigate individual-level noise, and our correction strategy amplifies this robustness. Empirically, we observe consistent improvements across math and code tasks when applying our noise correction to standard reward model usage, with particular gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code tasks under realistic reward model conditions. This work bridges label-noise correction from supervised learning with modern RLHF, offering both theoretical insights and a practical algorithm for noisy real-world deployment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2510.19099</link>
<guid>https://arxiv.org/abs/2510.19099</guid>
<content:encoded><![CDATA[
<div> Curriculum learning, large language models, difficulty metrics, forward and reverse CL, unified evaluation framework <br />
<br />
Summary: Curriculum learning in large language models varies in effectiveness depending on model capability and task complexity. Different difficulty levels yield different gains based on task demands. Task-aligned curricula influence final representations and generalization, while inner-state curricula impact internal states like confidence and uncertainty. No universal curriculum strategy exists, with some metrics suggesting prioritizing decision-uncertain samples for improved learning outcomes. <div>
arXiv:2510.19099v2 Announce Type: replace-cross 
Abstract: Curriculum learning (CL) - ordering training data from easy to hard - has become a popular strategy for improving reasoning in large language models (LLMs). Yet prior work employs disparate difficulty metrics and training setups, leaving open fundamental questions: When does curriculum help? Which direction - forward or reverse - is better? And does the answer depend on what we measure? We address these questions through a unified offline evaluation framework that decomposes curriculum difficulty into five complementary dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive Uncertainty, and Decision Variability. Through controlled post-training experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B, and Gemma3-4B, we find that (i) no curriculum strategy dominates universally - the relative effectiveness of forward versus reverse CL depends jointly on model capability and task complexity; (ii) even within a single metric, samples at different difficulty levels produce distinct gains depending on task demands; and (iii) task-aligned curricula focus on shaping the model's final representations and generalization, whereas inner-state curricula modulate internal states such as confidence and uncertainty. Our findings challenge the notion of a universal curriculum strategy and offer actionable guidance across model and task regimes, with some metrics indicating that prioritizing decision-uncertain samples can further enhance learning outcomes.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imbalanced Gradients in RL Post-Training of Multi-Task LLMs</title>
<link>https://arxiv.org/abs/2510.19178</link>
<guid>https://arxiv.org/abs/2510.19178</guid>
<content:encoded><![CDATA[
<div> tasks, large language models, multi-task learning, gradient imbalance, dataset mixing
Summary:
This study examines the issue of gradient imbalance in multi-task post-training of large language models. It is found that certain tasks can produce significantly larger gradients, leading to optimization bias towards those tasks. However, the assumption that larger gradients result in larger learning gains is proven to be false, with large-gradient tasks sometimes achieving lower performance improvements than small-gradient tasks. Further analysis indicates that typical training statistics do not explain these gradient imbalances, suggesting that they stem from inherent task differences. The findings caution against simplistic dataset mixing in multi-task learning and advocate for the development of principled gradient-level corrections for large language models. <div>
arXiv:2510.19178v2 Announce Type: replace-cross 
Abstract: Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. This approach implicitly assumes that all tasks contribute gradients of similar magnitudes; when this assumption fails, optimization becomes biased toward large-gradient tasks. In this paper, however, we show that this assumption fails in RL post-training: certain tasks produce significantly larger gradients, thus biasing updates toward those tasks. Such gradient imbalance would be justified only if larger gradients implied larger learning gains on the tasks (i.e., larger performance improvements) -- but we find this is not true. Large-gradient tasks can achieve similar or even much lower learning gains than small-gradient ones. Further analyses reveal that these gradient imbalances cannot be explained by typical training statistics such as training rewards or advantages, suggesting that they arise from the inherent differences between tasks. This cautions against naive dataset mixing and calls for future work on principled gradient-level corrections for LLMs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.19495</link>
<guid>https://arxiv.org/abs/2510.19495</guid>
<content:encoded><![CDATA[
<div> Keywords: imitation learning, offline reinforcement learning, non-expert data, robust policy learning, manipulation tasks 

Summary: 
Imitation learning has limitations due to its reliance on high-quality, task-specific expert data, hindering adaptability to real-world scenarios. In contrast, non-expert data such as play data or suboptimal demonstrations can offer broader coverage at lower costs. This study proposes using offline reinforcement learning to harness non-expert data and enhance imitation learning policies. By modifying standard offline RL approaches, the study shows that broadening the policy distribution support can improve policy performance in manipulation tasks, increasing success across various initial conditions. The approach can effectively leverage all collected data, including partial or suboptimal demonstrations, to improve task-directed policy performance. These findings highlight the importance of algorithmic techniques in utilizing non-expert data for robust policy learning in robotics.<br /><br />Summary: <div>
arXiv:2510.19495v2 Announce Type: replace-cross 
Abstract: Imitation learning has proven effective for training robots to perform complex tasks from expert human demonstrations. However, it remains limited by its reliance on high-quality, task-specific data, restricting adaptability to the diverse range of real-world object configurations and scenarios. In contrast, non-expert data -- such as play data, suboptimal demonstrations, partial task completions, or rollouts from suboptimal policies -- can offer broader coverage and lower collection costs. However, conventional imitation learning approaches fail to utilize this data effectively. To address these challenges, we posit that with right design decisions, offline reinforcement learning can be used as a tool to harness non-expert data to enhance the performance of imitation learning policies. We show that while standard offline RL approaches can be ineffective at actually leveraging non-expert data under the sparse data coverage settings typically encountered in the real world, simple algorithmic modifications can allow for the utilization of this data, without significant additional assumptions. Our approach shows that broadening the support of the policy distribution can allow imitation algorithms augmented by offline RL to solve tasks robustly, showing considerably enhanced recovery and generalization behavior. In manipulation tasks, these innovations significantly increase the range of initial conditions where learned policies are successful when non-expert data is incorporated. Moreover, we show that these methods are able to leverage all collected data, including partial or suboptimal demonstrations, to bolster task-directed policy performance. This underscores the importance of algorithmic techniques for using non-expert data for robust policy learning in robotics. Website: https://uwrobotlearning.github.io/RISE-offline/
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection</title>
<link>https://arxiv.org/abs/2510.20610</link>
<guid>https://arxiv.org/abs/2510.20610</guid>
<content:encoded><![CDATA[
<div> transformer models, AraELECTRA, CAMeLBERT, XLM-RoBERTa, AI-generated text detection, multilingual models <br />
<br />
Summary: 
The paper discusses the AraGenEval Shared Task on Arabic AI-generated text detection, where team BUSTED ranked 5th. Three pre-trained transformer models, AraELECTRA, CAMeLBERT, and XLM-RoBERTa, were fine-tuned for a binary classification task on the dataset. Surprisingly, XLM-RoBERTa, a multilingual model, outperformed specialized Arabic models with an F1 score of 0.7701. The study highlights the effectiveness of multilingual models in AI-generated text detection and emphasizes the complexities involved in this task. <div>
arXiv:2510.20610v2 Announce Type: replace-cross 
Abstract: This paper details our submission to the AraGenEval Shared Task on Arabic AI-generated text detection, where our team, BUSTED, secured 5th place. We investigated the effectiveness of three pre-trained transformer models: AraELECTRA, CAMeLBERT, and XLM-RoBERTa. Our approach involved fine-tuning each model on the provided dataset for a binary classification task. Our findings revealed a surprising result: the multilingual XLM-RoBERTa model achieved the highest performance with an F1 score of 0.7701, outperforming the specialized Arabic models. This work underscores the complexities of AI-generated text detection and highlights the strong generalization capabilities of multilingual models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</title>
<link>https://arxiv.org/abs/2510.20819</link>
<guid>https://arxiv.org/abs/2510.20819</guid>
<content:encoded><![CDATA[
<div> diffusion models, generative modeling, modality translation, latent space, cross-domain translation <br />
Summary: <br />
The article introduces the Latent Denoising Diffusion Bridge Model (LDDBM) as a framework for modality translation, addressing the challenge of translating information across different sensory modalities. LDDBM operates in a shared latent space, enabling translation without aligned dimensions. It utilizes a contrastive alignment loss to ensure semantic consistency between paired samples and features a domain-agnostic encoder-decoder architecture for noise prediction in the latent space. The proposed predictive loss guides training for accurate cross-domain translation, with various training strategies improving stability. LDDBM supports arbitrary modality pairs and excels in tasks such as multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. The framework's efficacy is validated through comprehensive experiments and ablations, establishing a new strong baseline for general modality translation. The project page provides further information on LDDBM's capabilities and applications. <br /> <div>
arXiv:2510.20819v2 Announce Type: replace-cross 
Abstract: Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue</title>
<link>https://arxiv.org/abs/2510.21720</link>
<guid>https://arxiv.org/abs/2510.21720</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Computational Psychology, Predictive Modeling, Generative Models, Large Language Model

Summary:
The paper proposes a framework that combines Artificial Intelligence and Computational Psychology to analyze complex human psychological states. It establishes benchmarks using classical machine learning techniques on various psychological datasets. The study fine-tuned state-of-the-art transformer models to overcome engineering challenges like numerical instability and resource constraints. A generative large language model (LLM) was developed as an interactive "Personality Brain." The models were deployed as a scalable microservices ecosystem. The research shows successful stabilization of transformer-based regression models in affective computing and democratizes large-scale AI research. The holistic approach integrates predictive analysis with generative dialogue, offering a practical model for future research in computational psychology and human-AI interaction.

<br /><br />Summary: The framework combines AI and Computational Psychology, benchmarks models on diverse datasets, fine-tunes transformer models overcoming challenges, develops a generative LLM, deploys models as a microservices ecosystem, stabilizes transformer-based regression models, democratizes AI research, and integrates predictive analysis with generative dialogue for future research in computational psychology and human-AI interaction. <div>
arXiv:2510.21720v1 Announce Type: new 
Abstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted framework designed to bridge the gap between isolated predictive modeling and an interactive system for psychological analysis. The methodology encompasses a rigorous, end-to-end development lifecycle. First, foundational performance benchmarks were established on four diverse psychological datasets using classical machine learning techniques. Second, state-of-the-art transformer models were fine-tuned, a process that necessitated the development of effective solutions to overcome critical engineering challenges, including the resolution of numerical instability in regression tasks and the creation of a systematic workflow for conducting large-scale training under severe resource constraints. Third, a generative large language model (LLM) was fine-tuned using parameter-efficient techniques to function as an interactive "Personality Brain." Finally, the entire suite of predictive and generative models was architected and deployed as a robust, scalable microservices ecosystem. Key findings include the successful stabilization of transformer-based regression models for affective computing, showing meaningful predictive performance where standard approaches failed, and the development of a replicable methodology for democratizing large-scale AI research. The significance of this work lies in its holistic approach, demonstrating a complete research-to-deployment pipeline that integrates predictive analysis with generative dialogue, thereby providing a practical model for future research in computational psychology and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation</title>
<link>https://arxiv.org/abs/2510.21721</link>
<guid>https://arxiv.org/abs/2510.21721</guid>
<content:encoded><![CDATA[
<div> critique-and-refine, personalized generation, large language models, user preferences, pseudo-user agent 
Summary: 
The article introduces the PREFINE framework, which aims to address challenges in personalized story generation using Large Language Models (LLMs). By constructing a pseudo-user agent from the user's interaction history and generating user-specific rubrics, PREFINE enables personalized generation without the need for direct user feedback or parameter updates. The framework was evaluated on story datasets, with automatic evaluations showing that PREFINE outperformed baseline methods in terms of personalization performance while maintaining overall story quality. Analysis confirmed the importance of both the pseudo-user agent and user-specific rubrics in enhancing personalization. The study suggests that the PREFINE framework has potential applications beyond story generation, such as dialogue systems, education, and recommendation. <br /><br />Summary: <div>
arXiv:2510.21721v1 Announce Type: new 
Abstract: While recent advances in Large Language Models (LLMs) have improved the quality of creative text generation, significant challenges remain in producing personalized stories that reflect individual user preferences. Conventional approaches rely on explicit feedback or fine-tuning, which presents practical issues regarding user burden, data collection, computational costs, and privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided Critique-and-Refine), a novel framework that extends the Critique-and-Refine paradigm to personalization. PREFINE constructs a pseudo-user agent from a user's interaction history and generates user-specific rubrics (evaluation criteria). By having this agent critique and refine outputs on the user's behalf based on these tailored rubrics, our method achieves personalized generation without requiring parameter updates or direct user feedback. We conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets. We designed three baseline methods and several model variants to verify the contribution of each component of our framework. In automatic evaluations (LLM-as-a-Judge), PREFINE achieved higher win rates and statistically significant scores than the baselines, without compromising general story quality. Analysis of the model variants confirmed that both the pseudo-user agent and the user-specific rubrics are crucial for enhancing personalization performance. Beyond story generation, our approach holds potential for enabling efficient personalization in broader applications, such as dialogue systems, education, and recommendation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIGN: Schema-Induced Games for Naming</title>
<link>https://arxiv.org/abs/2510.21855</link>
<guid>https://arxiv.org/abs/2510.21855</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, language model, naming game, multi-agent coordination, communication

Summary: 
AI systems are becoming more sophisticated, requiring interactions among large language model agents for complex problem-solving. Inconsistent conventions among these agents can lead to coordination breakdown in applications such as collaborative coding and distributed planning. The Schema-Induced Games for Naming (SIGN) introduces a naming game that incorporates lightweight structure to guide convention formation. Comparing schema-induced communication to unconstrained natural language, it demonstrates faster convergence and higher agreement, suggesting that minimal structure can enhance efficient multi-agent coordination. This study has implications beyond the naming game, indicating the potential for applications in various contexts where reliable, consistent communication among AI systems is crucial for scalability and effectiveness. 

<br /><br />Summary: <div>
arXiv:2510.21855v1 Announce Type: new 
Abstract: Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative coding and distributed planning therefore require reliable, consistent communication, and scalability is a central concern as systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming game that examines how lightweight structure can steer convention formation. We compare schema-induced communication to unconstrained natural language and find faster convergence with up to 5.8x higher agreement. These results suggest that minimal structure can act as a simple control knob for efficient multi-agent coordination, pointing toward broader applications beyond the naming game.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks</title>
<link>https://arxiv.org/abs/2510.21866</link>
<guid>https://arxiv.org/abs/2510.21866</guid>
<content:encoded><![CDATA[
<div> scaling, knowledge retrieval tasks, decoder-only architectures, attention intervention experiments, parameter scaling

Summary:
Decoder-only autoregressive language models face empirical capability ceilings in knowledge-intensive tasks. Evaluation of OPT and Pythia models across various parameter scales shows minimal accuracy improvement in knowledge retrieval tasks despite decreasing loss. Mathematics benchmarks exhibit stagnant accuracy levels around 19-20%, even with loss reduction of 31%. In contrast, procedural tasks like arithmetic show traditional scaling with improvements in both metrics. Attention intervention experiments reveal high sensitivity to perturbations, leading to catastrophic performance collapses when swapping attention patterns between models. These findings suggest that scaling beyond 1-2B parameters offers limited accuracy gains for knowledge-intensive applications with OPT and Pythia architectures. The study highlights the need for further investigation to determine if these limitations are inherent to decoder-only architectures or specific implementations. 

<br /><br />Summary: <div>
arXiv:2510.21866v1 Announce Type: new 
Abstract: We document empirical capability ceilings in decoder-only autoregressive language models across knowledge-intensive tasks. Systematic evaluation of OPT and Pythia model families (70M-30B parameters, spanning 240 times scaling) reveals that knowledge retrieval tasks show negligible accuracy improvement despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains flat at 19-20% (below 25% random chance) across all scales while cross-entropy loss decreases by 31%. In contrast, procedural tasks like arithmetic show conventional scaling where both metrics improve together. Attention intervention experiments reveal high sensitivity to perturbation: swapping attention patterns between models causes catastrophic performance collapse (complete accuracy loss) rather than graceful degradation. These measurements have immediate engineering implications: for knowledge-intensive applications using OPT and Pythia architectures, parameter scaling beyond 1-2B offers minimal accuracy gains despite continued loss improvement. Our findings quantify capability-specific scaling failures in these model families to inform resource allocation decisions. Whether these patterns reflect fundamental constraints of decoder-only architectures or implementation-specific limitations remains an open question requiring investigation across diverse architectural approaches.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21881</link>
<guid>https://arxiv.org/abs/2510.21881</guid>
<content:encoded><![CDATA[
<div> Dataset, geometric reasoning, multimodal model, reasoning chains, model performance <br />
Summary: <br />
The article introduces the GeoThoughts dataset, designed to address the challenges faced by large language models in visual geometric problem solving. The dataset includes visual descriptions, step-by-step solutions, reasoning chains, and final answers. Using this dataset, the authors developed the GeoThought-MLLM multimodal model, which outperforms existing benchmarks in geometric tasks. Training with the dataset improves geometric reasoning capabilities in both in-domain and out-of-domain settings. Analysis of failure cases reveals errors in mathematical concept interpretation and spatial judgment. By utilizing the Chain-of-Thought methodology to correct these mistakes, the model can produce correct answers. This research highlights the importance of comprehensive datasets and explicit reasoning traces in enhancing the performance of language models in geometric reasoning tasks. <br />Summary: <div>
arXiv:2510.21881v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities in text-based mathematical problem solving; however, when adapted to visual reasoning tasks, particularly geometric problem solving, their performance substantially declines because geometric problems present unique challenges. Specifically, these challenges stem from two key factors: first, the intrinsic complexity of geometry requiring detailed image comprehension and multi-step reasoning, and second, the limitations of existing datasets which lack sufficient scale, diversity, and explicit reasoning traces, consequently hindering effective model training. To address these challenges, we developed the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two subsets: Geo-Thought-6K with 6,243 samples and its augmented version Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual descriptions, step-by-step solutions, explicit reasoning chains, reflection steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a mathematical reasoning multimodal model that generates detailed thinking processes during problem-solving. Our model outperforms existing benchmarks in geometric tasks, demonstrating that training with our Chain-of-Thought dataset improves geometric reasoning capabilities across both in-domain and out-of-domain settings. Finally, we analyze failure cases and observe that errors primarily arise from incorrect interpretation of mathematical concepts or spatial misjudgment. By invoking CoT to correct these mistakes, the model produces correct answers.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration through Generation: Applying GFlowNets to Structured Search</title>
<link>https://arxiv.org/abs/2510.21886</link>
<guid>https://arxiv.org/abs/2510.21886</guid>
<content:encoded><![CDATA[
<div> Generative Flow Networks, GFlowNets, graph optimization, Traveling Salesperson Problem, Minimum Spanning Tree, Shortest Path 
Summary: 
Generative Flow Networks (GFlowNets) are applied to graph optimization problems such as the Traveling Salesperson Problem, Minimum Spanning Tree, and Shortest Path. GFlowNets use a reward function to sample solutions and are trained with the Trajectory Balance loss. The models sequentially build solutions for spanning trees, paths, and tours. Experimental results on benchmark instances show that GFlowNets can find optimal solutions and match classical algorithm solutions. Training convergence depends on problem complexity, with more episodes required for larger graphs. Once trained, GFlowNets produce optimal solutions for tested instances. This approach offers computational scalability by amortizing computation through training, potentially allowing for the solution of larger instances where classical methods are infeasible. <div>
arXiv:2510.21886v1 Announce Type: new 
Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph optimization problems: the Traveling Salesperson Problem, Minimum Spanning Tree, and Shortest Path. GFlowNets are generative models that learn to sample solutions proportionally to a reward function. The models are trained using the Trajectory Balance loss to build solutions sequentially, se- lecting edges for spanning trees, nodes for paths, and cities for tours. Experiments on benchmark instances of varying sizes show that GFlowNets learn to find optimal solutions. For each problem type, multiple graph configurations with different numbers of nodes were tested. The generated solutions match those from classical algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact solvers for TSP). Training convergence depends on problem complexity, with the number of episodes required for loss stabilization increasing as graph size grows. Once training converges, the generated solutions match known optima from classical algorithms across the tested instances. This work demonstrates that generative models can solve combinatorial optimization problems through learned policies. The main advantage of this learning-based approach is computational scalability: while classical algorithms have fixed complexity per instance, GFlowNets amortize computation through training. With sufficient computational resources, the framework could potentially scale to larger problem instances where classical exact methods become infeasible.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Hardness of Reinforcement Learning with Partial $q^{\pi}$-Realizability</title>
<link>https://arxiv.org/abs/2510.21888</link>
<guid>https://arxiv.org/abs/2510.21888</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, computational complexity, linear function approximation, partial $q^{\pi}$-realizability, NP-hardness<br />
<br />
Summary:<br />
This paper explores the computational complexity of reinforcement learning within the framework of partial $q^{\pi}$-realizability, where the goal is to learn an $\epsilon$-optimal policy from a predefined set $\Pi. The study shows that achieving an $\epsilon$-optimal policy in this context is computationally challenging, with NP-hardness demonstrated for a greedy policy set and an exponential lower bound for softmax policies. These findings imply that computational difficulties persist even when the policy set extends beyond the optimal policy, aligning with results from $q^*$-realizability. Through reductions from complex problems like $\delta$-Max-3SAT, the paper establishes the infeasibility of positive computational results in partial $q^{\pi}$-realizability, offering insights into the limitations of this model compared to $q^{\pi}$-realizability in a generative access scenario. <div>
arXiv:2510.21888v1 Announce Type: new 
Abstract: This paper investigates the computational complexity of reinforcement learning in a novel linear function approximation regime, termed partial $q^{\pi}$-realizability. In this framework, the objective is to learn an $\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under the assumption that all value functions for policies in $\Pi$ are linearly realizable. The assumptions of this framework are weaker than those in $q^{\pi}$-realizability but stronger than those in $q^*$-realizability, providing a practical model where function approximation naturally arises. We prove that learning an $\epsilon$-optimal policy in this setting is computationally hard. Specifically, we establish NP-hardness under a parameterized greedy policy set (argmax) and show that - unless NP = RP - an exponential lower bound (in feature vector dimension) holds when the policy set contains softmax policies, under the Randomized Exponential Time Hypothesis. Our hardness results mirror those in $q^*$-realizability and suggest computational difficulty persists even when $\Pi$ is expanded beyond the optimal policy. To establish this, we reduce from two complexity problems, $\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL (greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate that positive computational results are generally unattainable in partial $q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a generative access model.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Trade-offs of Optimizing Small Language Models for E-Commerce</title>
<link>https://arxiv.org/abs/2510.21970</link>
<guid>https://arxiv.org/abs/2510.21970</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, E-commerce, Optimization, Intent Recognition, Resource Efficiency

Summary:
Large Language Models (LLMs), such as the GPT-4.1, excel in natural language tasks but face issues like high computational costs. This study explores the use of smaller, optimized open-weight models as a cost-effective solution, focusing on a one-billion-parameter Llama 3.2 model for multilingual e-commerce intent recognition. The model was fine-tuned with QLoRA and post-training quantization techniques, resulting in GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. The specialized 1B model achieved 99% accuracy, matching the performance of larger models. Hardware-dependent trade-offs were observed, with 4-bit GPTQ reducing VRAM usage but slowing inference on older GPUs, and GGUF formats providing significant speedups and RAM reductions on CPUs. The findings suggest that optimized open-weight models offer state-of-the-art accuracy at lower computational costs, making them a suitable alternative for specialized applications like e-commerce. 

<br /><br />Summary: <div>
arXiv:2510.21970v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computational costs, latency, and operational expenses. This paper investigates the viability of smaller, open-weight models as a resource-efficient alternative. We present a methodology for optimizing a one-billion-parameter Llama 3.2 model for multilingual e-commerce intent recognition. The model was fine-tuned using Quantized Low-Rank Adaptation (QLoRA) on a synthetically generated dataset designed to mimic real-world user queries. Subsequently, we applied post-training quantization techniques, creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results demonstrate that the specialized 1B model achieves 99% accuracy, matching the performance of the significantly larger GPT-4.1 model. A detailed performance analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF formats on a CPU achieved a speedup of up to 18x in inference throughput and a reduction of over 90% in RAM consumption compared to the FP16 baseline. We conclude that small, properly optimized open-weight models are not just a viable but a more suitable alternative for domain-specific applications, offering state-of-the-art accuracy at a fraction of the computational cost.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions</title>
<link>https://arxiv.org/abs/2510.21977</link>
<guid>https://arxiv.org/abs/2510.21977</guid>
<content:encoded><![CDATA[
<div> Methodologies; Large Language Models; Survey Responses; Distribution Shift Alignment; Data Collection
Summary: 
The study addresses the challenges faced by existing methods in utilizing large language models (LLMs) for simulating human survey responses. It introduces a novel approach called Distribution Shift Alignment (DSA) that aims to align output distributions and distribution shifts across different backgrounds in a two-stage fine-tuning process. DSA focuses on learning how distributions change rather than just fitting training data, leading to improved accuracy and better alignment with the true distribution. Empirical results from five public survey datasets show that DSA outperforms other methods in terms of accuracy, robustness, and data savings. The method reduces the need for real data by 53.48-69.12%, showcasing its effectiveness and efficiency in simulating survey responses. <div>
arXiv:2510.21977v1 Announce Type: new 
Abstract: Large language models (LLMs) offer a promising way to simulate human survey responses, potentially reducing the cost of large-scale data collection. However, existing zero-shot methods suffer from prompt sensitivity and low accuracy, while conventional fine-tuning approaches mostly fit the training set distributions and struggle to produce results more accurate than the training set itself, which deviates from the original goal of using LLMs to simulate survey responses. Building on this observation, we introduce Distribution Shift Alignment (DSA), a two-stage fine-tuning method that aligns both the output distributions and the distribution shifts across different backgrounds. By learning how these distributions change rather than fitting training data, DSA can provide results substantially closer to the true distribution than the training data. Empirically, DSA consistently outperforms other methods on five public survey datasets. We further conduct a comprehensive comparison covering accuracy, robustness, and data savings. DSA reduces the required real data by 53.48-69.12%, demonstrating its effectiveness and efficiency in survey simulation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective</title>
<link>https://arxiv.org/abs/2510.21999</link>
<guid>https://arxiv.org/abs/2510.21999</guid>
<content:encoded><![CDATA[
<div> Keyword: Math Word Problem, Artificial Intelligence, Cognitive Abilities, MWP Solving, Neural Network Solvers<br />
Summary:<br />
The paper discusses the importance of Math Word Problem (MWP) solving in advancing artificial intelligence (AI) reasoning abilities by mirroring human cognitive intelligence. It reviews the evolution of technological paradigms from rule-based methods to deep learning and large language models for MWP solving. Five crucial cognitive abilities for MWP solving are identified: Problem Understanding, Logical Organization, Associative Memory, Critical Thinking, and Knowledge Learning. The paper focuses on neural network solvers and Large Language Models (LLM) based solvers, discussing how they demonstrate human-like abilities in problem-solving processes. The study reruns representative MWP solvers on mainstream benchmarks for a unified comparison, providing a comprehensive analysis of influential MWP research from the past decade. The paper aims to inspire further research in AI reasoning and provides a repository for reference on GitHub. <br /> <br />Summary: <div>
arXiv:2510.21999v1 Announce Type: new 
Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial intelligence (AI) dating back to 1960s. This research aims to advance the reasoning abilities of AI by mirroring the human-like cognitive intelligence. The mainstream technological paradigm has evolved from the early rule-based methods, to deep learning models, and is rapidly advancing towards large language models. However, the field still lacks a systematic taxonomy for the MWP survey along with a discussion of current development trends. Therefore, in this paper, we aim to comprehensively review related research in MWP solving through the lens of human cognition, to demonstrate how recent AI models are advancing in simulating human cognitive abilities. Specifically, we summarize 5 crucial cognitive abilities for MWP solving, including Problem Understanding, Logical Organization, Associative Memory, Critical Thinking, and Knowledge Learning. Focused on these abilities, we review two mainstream MWP models in recent 10 years: neural network solvers, and LLM based solvers, and discuss the core human-like abilities they demonstrated in their intricate problem-solving process. Moreover, we rerun all the representative MWP solvers and supplement their performance on 5 mainstream benchmarks for a unified comparison. To the best of our knowledge, this survey first comprehensively analyzes the influential MWP research of the past decade from the perspective of human reasoning cognition and provides an integrative overall comparison across existing approaches. We hope it can inspire further research in AI reasoning. Our repository is released on https://github.com/Ljyustc/FoI-MWP.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightAgent: Mobile Agentic Foundation Models</title>
<link>https://arxiv.org/abs/2510.22009</link>
<guid>https://arxiv.org/abs/2510.22009</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, GUI agent systems, mobile deployment, device-cloud collaboration, cost-efficiency <br />
Summary: 
LightAgent is proposed as a solution for mobile GUI agents by combining on-device models with cloud capabilities. It enhances Qwen2.5-VL-3B through SFT->GRPO training on synthetic GUI data and integrates a long-reasoning mechanism for efficient decision-making. The model executes on-device by default but escalates complex tasks to the cloud in real-time. Experiments on AndroidLab benchmark and various apps demonstrate LightAgent's performance matching or approaching larger models while reducing cloud costs. This approach enables the deployment of capable models on mobile platforms without the prohibitive costs associated with cloud-only models. <br /> <br />Summary: <div>
arXiv:2510.22009v1 Announce Type: new 
Abstract: With the advancement of multimodal large language models (MLLMs), building GUI agent systems has become an increasingly promising direction-especially for mobile platforms, given their rich app ecosystems and intuitive touch interactions. Yet mobile GUI agents face a critical dilemma: truly on-device models (4B or smaller) lack sufficient performance, while capable models (starting from 7B) are either too large for mobile deployment or prohibitively costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose LightAgent, a mobile agentic foundation model solution that leverages device-cloud collaboration to tap the cost-efficiency of on-device models and the high capability of cloud models, while avoiding their drawbacks. Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO training on synthetic GUI data for strong decision-making, integrates an efficient long-reasoning mechanism to utilize historical interactions under tight resources, and defaults to on-device execution-only escalating challenging subtasks to the cloud via real-time complexity assessment. Experiments on the online AndroidLab benchmark and diverse apps show LightAgent matches or nears larger models, with a significant reduction in cloud costs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-AR: LLM-powered Automated Reasoning Framework</title>
<link>https://arxiv.org/abs/2510.22034</link>
<guid>https://arxiv.org/abs/2510.22034</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, startup success prediction, founder traits, neural-symbolic systems, automated reasoning

Summary: 
Large language models (LLMs) have the potential to identify patterns and reason effectively, but their variable accuracy has been a barrier to their adoption in high-stakes decision-making applications. This paper addresses this issue from a venture capital perspective by developing LLM-AR, a pipeline that distills LLM-generated heuristics into probabilistic rules using the ProbLog automated reasoning engine. The framework incorporates association rule mining and an iterative policy-evolution loop to refine prediction rules. On unseen data, LLM-AR achieves a precision of 59.5% and a recall of 8.7%, significantly outperforming random baseline precision. The framework is interpretable, tunable via hyperparameters, and exposes every decision path for human inspection, showing promise for application in other domains. 

<br /><br />Summary: <div>
arXiv:2510.22034v1 Announce Type: new 
Abstract: Large language models (LLMs) can already identify patterns and reason effectively, yet their variable accuracy hampers adoption in high-stakes decision-making applications. In this paper, we study this issue from a venture capital perspective by predicting idea-stage startup success based on founder traits. (i) To build a reliable prediction model, we introduce LLM-AR, a pipeline inspired by neural-symbolic systems that distils LLM-generated heuristics into probabilistic rules executed by the ProbLog automated-reasoning engine. (ii) An iterative policy-evolution loop incorporates association-rule mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the random baseline precision, while exposing every decision path for human inspection. The framework is interpretable and tunable via hyperparameters, showing promise to extend into other domains.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability</title>
<link>https://arxiv.org/abs/2510.22039</link>
<guid>https://arxiv.org/abs/2510.22039</guid>
<content:encoded><![CDATA[
<div> Predictive coding, meta-reinforcement learning, representation learning, Bayes-optimal belief states, generalization<br />
<br />
Summary: <br />
The study explores the integration of self-supervised predictive coding modules into meta-reinforcement learning (RL) to enhance the learning of compact and interpretable Bayes-optimal representations of history. Inspired by neuroscience's predictive coding theory, the research demonstrates that meta-RL with predictive modules outperforms conventional meta-RL in generating more interpretable representations approximating Bayes-optimal belief states across various tasks. In challenging scenarios requiring active information seeking, meta-RL with predictive modules excels in learning optimal representations and policies compared to conventional meta-RL. The study shows that improved representation learning through predictive modules leads to enhanced generalization in partially observable environments. The results highlight the importance of predictive learning as a key principle for effective representation learning in agents navigating partial observability. <div>
arXiv:2510.22039v1 Announce Type: new 
Abstract: Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology</title>
<link>https://arxiv.org/abs/2510.22046</link>
<guid>https://arxiv.org/abs/2510.22046</guid>
<content:encoded><![CDATA[
<div> Methodology, Hardware/Software Co-design, SpecC, PCM-to-PWM Converter, Class-D Audio Amplifier 

Summary:
The study applies the SpecC methodology in a hardware/software co-design flow for a PCM-to-PWM converter used in a Class-D audio amplifier. By modeling and exploring the converter with the SpecC methodology, an optimal hardware/software partition is derived. Through system-level estimates and fast functional simulation, mappings that meet real-time constraints are evaluated to reduce costs compared to an all-hardware solution or a purely software implementation on a high-end processor. Despite moderate complexity, the results highlight the benefits of system-level co-design for early architectural insight, rapid validation, and making cost/performance trade-offs. The study reaffirms the importance of system-level co-design in achieving efficient and effective hardware/software solutions in complex design scenarios. 

<br /><br />Summary: <div>
arXiv:2510.22046v1 Announce Type: new 
Abstract: We present a case study applying the SpecC methodology within a system-level hardware/software co-design flow to a PCM-to-PWM converter, the core of a Class-D audio amplifier. The converter was modeled and explored with SpecC methodology to derive an HW/SW partition. Using system-level estimates and fast functional simulation, we evaluated mappings that meet real-time constraints while reducing estimated cost of an all-hardware solution and avoiding the expense of a purely software implementation on a high-end processor. Despite the design's moderate complexity, the results underline the value of system-level co-design for early architectural insight, rapid validation, and actionable cost/performance trade-offs. [Original work from 2005; formatting revised in 2025, with no changes to the results.]
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Error-Centric Intelligence II: Energy-Structured Causal Models</title>
<link>https://arxiv.org/abs/2510.22050</link>
<guid>https://arxiv.org/abs/2510.22050</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, causal opacity, explanatory intelligence, Energy Structured Causal Models, causal reasoning <br />
<br />
Summary: 
The article discusses the challenge of contemporary machine learning models being causally opaque despite their high predictive accuracy. It argues that intelligence lies in building explanations that provide causal semantics for interventions. The concept of computational explanations, specifically Energy Structured Causal Models (ESCMs), is introduced, allowing for manipulation of mechanisms through constraints rather than explicit input-output maps. The principles of Local Ancillary Prediction (LAP) and Interventions Change Mechanisms (ICM) are instantiated within ESCMs. The paper also highlights the issue of fractured representations due to encoder energy pairs' gauge ambiguity in empirical risk minimization. Furthermore, ESCMs are shown to recover standard Structural Causal Model (SCM) semantics under certain conditions. By providing a formal language for causal reasoning in systems aiming to understand rather than predict, the article presents a novel approach to intelligence as explanation-building under criticism.  <br /> <div>
arXiv:2510.22050v1 Announce Type: new 
Abstract: Contemporary machine learning optimizes for predictive accuracy, yet systems that achieve state of the art performance remain causally opaque: their internal representations provide no principled handle for intervention. We can retrain such models, but we cannot surgically edit specific mechanisms while holding others fixed, because learned latent variables lack causal semantics. We argue for a conceptual reorientation: intelligence is the ability to build and refine explanations, falsifiable claims about manipulable structure that specify what changes and what remains invariant under intervention. Explanations subsume prediction but demand more: causal commitments that can be independently tested and corrected at the level of mechanisms. We introduce computational explanations, mappings from observations to intervention ready causal accounts. We instantiate these explanations with Energy Structured Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy functions or vector fields) rather than explicit input output maps, and interventions act by local surgery on those constraints. This shift makes internal structure manipulable at the level where explanations live: which relations must hold, which can change, and what follows when they do. We provide concrete instantiations of the structural-causal principles LAP and ICM in the ESCM context, and also argue that empirical risk minimization systematically produces fractured, entangled representations, a failure we analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under mild conditions, ESCMs recover standard SCM semantics. Building on Part I's principles (LAP, ICM, CAP) and its definition of intelligence as explanation-building under criticism, this paper offers a formal language for causal reasoning in systems that aspire to understand, not merely to predict.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms</title>
<link>https://arxiv.org/abs/2510.22052</link>
<guid>https://arxiv.org/abs/2510.22052</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, language models, energy efficiency, multimodal models, domain-specific agents

Summary:
The field of artificial intelligence (AI) has a significant impact on society and the economy, with the AI market projected to grow exponentially in the coming years. Current AI models, while powerful, require enormous amounts of data and energy to train, leading to limitations such as hallucinations. The next phase of AI evolution aims to develop lightweight, energy-efficient domain-specific multimodal models that can reason, plan, and make decisions in real-time. These models should learn continuously and evolve to enhance decision-making capabilities. To support such advanced AI agents, hardware must be redesigned to achieve energy efficiencies over 1000 times greater than current standards. This vision of future AI systems represents a shift towards nimble, domain-specific agents capable of intelligent decision-making in uncertain environments. <br /><br />Summary: <div>
arXiv:2510.22052v1 Announce Type: new 
Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad aspects of society, industry, business, and governance in ways that dictate the prosperity and might of the world's economies. The AI market size is projected to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI is dominated by large language models that exhibit linguistic and visual intelligence. However, training these models requires a massive amount of data scraped from the web as well as large amounts of energy (50--60 GWh to train GPT-4). Despite these costs, these models often hallucinate, a characteristic that prevents them from being deployed in critical application domains. In contrast, the human brain consumes only 20~W of power. What is needed is the next level of AI evolution in which lightweight domain-specific multimodal models with higher levels of intelligence can reason, plan, and make decisions in dynamic environments with real-time data and prior knowledge, while learning continuously and evolving in ways that enhance future decision-making capability. This will define the next wave of AI, progressing from today's large models, trained with vast amounts of data, to nimble energy-efficient domain-specific agents that can reason and think in a world full of uncertainty. To support such agents, hardware will need to be reimagined to allow energy efficiencies greater than 1000x over the state of the art. Such a vision of future AI systems is developed in this work.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies</title>
<link>https://arxiv.org/abs/2510.22095</link>
<guid>https://arxiv.org/abs/2510.22095</guid>
<content:encoded><![CDATA[
<div> Keywords: Brain-Computer Interfaces, Large Language Models, Brain-Agent Collaboration, Ethical concerns, Human-agent collaboration framework

Summary: 
Brain-Computer Interfaces (BCIs) have the potential to improve communication for individuals with severe neurological impairments, but face limitations like low transfer rates and calibration needs. Integrating Large Language Models (LLMs) can enhance cognitive state understanding, but deploying agentic AI raises technical and ethical issues. This paper proposes a shift towards Brain-Agent Collaboration (BAC), where agents are active partners for intelligent assistance, emphasizing ethical data handling, model reliability, and human-agent collaboration frameworks. This paradigm extension aims to ensure safety, trustworthiness, and effectiveness of systems in this emerging field. <div>
arXiv:2510.22095v1 Announce Type: new 
Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between the human brain and external devices, holding significant promise for individuals with severe neurological impairments. However, their widespread adoption is hindered by critical limitations, such as low information transfer rates and extensive user-specific calibration. To overcome these challenges, recent research has explored the integration of Large Language Models (LLMs), extending the focus from simple command decoding to understanding complex cognitive states. Despite these advancements, deploying agentic AI faces technical hurdles and ethical concerns. Due to the lack of comprehensive discussion on this emerging direction, this position paper argues that the field is poised for a paradigm extension from BCI to Brain-Agent Collaboration (BAC). We emphasize reframing agents as active and collaborative partners for intelligent assistance rather than passive brain signal data processors, demanding a focus on ethical data handling, model reliability, and a robust human-agent collaboration framework to ensure these systems are safe, trustworthy, and effective.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors</title>
<link>https://arxiv.org/abs/2510.22132</link>
<guid>https://arxiv.org/abs/2510.22132</guid>
<content:encoded><![CDATA[
<div> Approach, Mathematical reasoning, Self-optimizing thought vectors, Entropy minimization, Controllability<br />
Summary:<br />
The article introduces a new approach for controllable mathematical reasoning using self-optimizing thought vectors and entropy minimization. By incorporating learnable thought vectors that adjust the internal reasoning process of large language models, the method achieves 90.1% accuracy on GSM8K with a controllability score of 0.42. It demonstrates that entropy-based rewards can effectively guide focused reasoning patterns without the need for external reward annotations. Through analysis, distinct thought vector clusters and consistent low-entropy distributions across control conditions are identified, validating the framework for controllable AI reasoning. The utilization of entropy minimization and self-optimizing thought vectors allows for dynamic modulation of the reasoning process, leading to improved accuracy and controllability in mathematical reasoning tasks.<br /><br />Summary: <div>
arXiv:2510.22132v1 Announce Type: new 
Abstract: We present a novel approach for controllable mathematical reasoning that leverages self-optimizing thought vectors with entropy minimization. Our method introduces learnable thought vectors that dynamically modulate the internal reasoning process of large language models. Using Gemma-2-9B on GSM8K, we achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that entropy-based rewards effectively guide focused reasoning patterns without requiring external reward annotations. Our analysis reveals distinct thought vector clusters and consistent low-entropy distributions across control conditions, validating our framework for controllable AI reasoning.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests</title>
<link>https://arxiv.org/abs/2510.22170</link>
<guid>https://arxiv.org/abs/2510.22170</guid>
<content:encoded><![CDATA[
<div> Keywords: AI psychometrics, situational judgment tests, personas, industrial-organizational psychology, dataset release
Summary:
AI psychometrics is a framework proposed for evaluating AI systems in emotionally judgmental and ethically important roles, using situational judgment tests (SJTs) from realistic scenarios to assess domain-specific competencies. The framework integrates industrial-organizational and personality psychology to design sophisticated personas with detailed behavioral and psychological descriptors. This approach includes life histories, social and emotional functions, and structured generation using population demographic priors and memoir-inspired narratives encoded with Pydantic schemas. In a law enforcement assistant case study, a diverse dataset of 8,500 personas and 4,000 SJTs across 11 attributes was created, providing a comprehensive understanding of behaviors across different subpopulations and scenarios. The dataset, along with all code, will be made publicly available. <div>
arXiv:2510.22170v1 Announce Type: new 
Abstract: AI psychometrics evaluates AI systems in roles that traditionally require emotional judgment and ethical consideration. Prior work often reuses human trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral realism and domain relevance. We propose a framework that (1) uses situational judgment tests (SJTs) from realistic scenarios to probe domain-specific competencies; (2) integrates industrial-organizational and personality psychology to design sophisticated personas which include behavioral and psychological descriptors, life history, and social and emotional functions; and (3) employs structured generation with population demographic priors and memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement assistant case study, we construct a rich dataset of personas drawn across 8 persona archetypes and SJTs across 11 attributes, and analyze behaviors across subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000 SJTs, and 300,000 responses. We will release the dataset and all code to the public.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dopamine-driven synaptic credit assignment in neural networks</title>
<link>https://arxiv.org/abs/2510.22178</link>
<guid>https://arxiv.org/abs/2510.22178</guid>
<content:encoded><![CDATA[
<div> optimization, neural networks, Credit Assignment Problem, derivative-free, Dopamine

Summary: 
The article introduces a novel optimizer called Dopamine, inspired by neural Reinforcement Learning, to solve the synaptic Credit Assignment Problem in neural networks. Unlike traditional gradient-based methods, Dopamine utilizes Weight Perturbation (WP) learning to minimize regret and adjust the learning rate in the network. This derivative-free approach accelerates convergence and outperforms standard WP, while consuming less computation and memory. Tested on XOR tasks and time series forecasting, Dopamine-trained models show robust solutions and comparable performance to gradient-based algorithms. The optimizer is neurobiologically plausible and offers a more efficient way to address the Credit Assignment Problem in artificial neural networks. <div>
arXiv:2510.22178v1 Announce Type: new 
Abstract: Solving the synaptic Credit Assignment Problem(CAP) is central to learning in both biological and artificial neural systems. Finding an optimal solution for synaptic CAP means setting the synaptic weights that assign credit to each neuron for influencing the final output and behavior of neural networks or animals. Gradient-based methods solve this problem in artificial neural networks using back-propagation, however, not in the most efficient way. For instance, back-propagation requires a chain of top-down gradient computations. This leads to an expensive optimization process in terms of computing power and memory linked with well-known weight transport and update locking problems. To address these shortcomings, we take a NeuroAI approach and draw inspiration from neural Reinforcement Learning to develop a derivative-free optimizer for training neural networks, Dopamine. Dopamine is developed for Weight Perturbation (WP) learning that exploits stochastic updating of weights towards optima. It achieves this by minimizing the regret, a form of Reward Prediction Error (RPE) between the expected outcome from the perturbed model and the actual outcome from the unperturbed model. We use this RPE to adjust the learning rate in the network (i.e., creating an adaptive learning rate strategy, similar to the role of dopamine in the brain). We tested the Dopamine optimizer for training multi-layered perceptrons for XOR tasks, and recurrent neural networks for chaotic time series forecasting. Dopamine-trained models demonstrate accelerated convergence and outperform standard WP, and give comparable performance to gradient-based algorithms, while consuming significantly less computation and memory. Overall, the Dopamine optimizer not only finds robust solutions and comparable performance to the state-of-the-art Machine Learning optimizers but is also neurobiologically more plausible.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling</title>
<link>https://arxiv.org/abs/2510.22192</link>
<guid>https://arxiv.org/abs/2510.22192</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization modeling, Operations research, Large language models, OptiTree, Problem decomposition

Summary:
Optimization modeling is a critical aspect of operations research, requiring complex mathematical structures. Existing approaches using large language models often struggle with highly complex problems. To address this, OptiTree introduces a tree search method that adaptively decomposes complex problems into simpler subproblems. The modeling tree organizes OR problems based on hierarchy and complexity, with nodes representing problem categories. By recursively searching the tree, OptiTree identifies simpler subproblems and integrates hierarchical thoughts to enhance modeling accuracy. Experimental results show OptiTree outperforms existing methods, achieving over 10% improvement on challenging benchmarks. The code is available on GitHub for further exploration.<br /><br />Summary: <div>
arXiv:2510.22192v1 Announce Type: new 
Abstract: Optimization modeling is one of the most crucial but technical parts of operations research (OR). To automate the modeling process, existing works have leveraged large language models (LLMs), prompting them to break down tasks into steps for generating variables, constraints, and objectives. However, due to the highly complex mathematical structures inherent in OR problems, standard fixed-step decomposition often fails to achieve high performance. To address this challenge, we introduce OptiTree, a novel tree search approach designed to enhance modeling capabilities for complex problems through adaptive problem decomposition into simpler subproblems. Specifically, we develop a modeling tree that organizes a wide range of OR problems based on their hierarchical problem taxonomy and complexity, with each node representing a problem category and containing relevant high-level modeling thoughts. Given a problem to model, we recurrently search the tree to identify a series of simpler subproblems and synthesize the global modeling thoughts by adaptively integrating the hierarchical thoughts. Experiments show that OptiTree significantly improves the modeling accuracy compared to the state-of-the-art, achieving over 10\% improvements on the challenging benchmarks. The code is released at https://github.com/MIRALab-USTC/OptiTree/tree/main.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PACR: Progressively Ascending Confidence Reward for LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.22255</link>
<guid>https://arxiv.org/abs/2510.22255</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Progressively Ascending Confidence Reward, Model-Intrinsic Reward, Inductive Bias <br />
Summary: The article introduces Progressively Ascending Confidence Reward (PACR) as a method to improve Reinforcement Learning with Verifiable Rewards (RLVR) by providing a dense, model-intrinsic reward that guides the model's exploration during reasoning tasks. PACR is computed based on the model's confidence in the correct answer, following an inductive bias that the probability of the ground-truth answer should increase along a well-formed reasoning trajectory. Empirical and theoretical analysis support the idea that this inductive bias leads to more effective exploration and reasoning. The results show that PACR accelerates exploration, achieves reward saturation faster, and performs better on various benchmarks. This suggests that incorporating dense, model-intrinsic shaping signals like PACR can enhance the training process of RLVR and make it more reliable and efficient. <br /><br /> <div>
arXiv:2510.22255v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLM reasoning, but its sparse, outcome-based reward provides no guidance for intermediate steps, slowing exploration. We propose Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed directly from the model's evolving belief in the correct answer. PACR encodes the inductive bias that, along a well-formed reasoning trajectory, the probability of the ground-truth answer should have a generally ascending trend. We provide empirical and theoretical analysis validating that such an inductive bias constrains the exploration search space to regions richer in logically sound reasoning. We demonstrate that PACR accelerates exploration, reaches reward saturation with fewer trajectories, and yields improvements on multiple benchmarks. Our results suggest that dense, model-intrinsic shaping signals can make RLVR training more effective and reliable.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription</title>
<link>https://arxiv.org/abs/2510.22295</link>
<guid>https://arxiv.org/abs/2510.22295</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Lyrics Transcription, Vietnamese music, dataset, ASR-based approaches, Whisper models<br />
Summary: <br />
Automatic Lyrics Transcription (ALT) for Vietnamese music faces challenges due to tonal complexity and dialectal variations. A new large-scale dataset, VietLyrics, addresses these issues with line-level aligned lyrics and metadata. Current ASR-based approaches have limitations like transcription errors and hallucinations in non-vocal segments. Fine-tuning Whisper models on VietLyrics improves performance compared to existing multilingual ALT systems like LyricWhiz. The release of VietLyrics and the models aims to advance Vietnamese music computing research and show the potential of this approach for ALT in low-resource language and music.<br /> <div>
arXiv:2510.22295v1 Announce Type: new 
Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows</title>
<link>https://arxiv.org/abs/2510.22329</link>
<guid>https://arxiv.org/abs/2510.22329</guid>
<content:encoded><![CDATA[
<div> Vehicle Routing Problem, Time Windows, Graph Coarsening, Heuristics, Quantum-Inspired Optimization

Summary:
This study presents a novel approach for solving the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) using a multilevel graph coarsening and refinement framework. By aggregating customers into meta-nodes based on a spatio-temporal distance metric, the problem size is reduced, leading to improved computation times. Classical heuristics are then applied to solve the reduced problem, with feasibility corrections applied to expand the solution back to the original space. Preliminary experiments on Solomon benchmark instances demonstrate that the proposed method achieves comparable or better solution quality, particularly in terms of capacity and time window constraints. Additionally, the paper explores the potential integration of quantum-inspired optimization techniques to further enhance the efficiency of large-scale vehicle routing tasks. <div>
arXiv:2510.22329v1 Announce Type: new 
Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a fundamental NP-hard optimization problem in logistics. Solving large-scale instances remains computationally challenging for exact solvers. This work introduces a multilevel graph coarsening and refinement framework that aggregates customers into meta-nodes using a spatio-temporal distance metric. The reduced problem is solved with classical heuristics and subsequently expanded back into the original space with feasibility corrections. Preliminary experiments on Solomon benchmark instances show that the proposed method reduces computation time while preserving or improving solution quality, particularly with respect to capacity and time window constraints. The paper also explores the integration of quantum-inspired optimization techniques, highlighting their potential to further accelerate large-scale vehicle routing tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2510.22333</link>
<guid>https://arxiv.org/abs/2510.22333</guid>
<content:encoded><![CDATA[
<div> fine-tuned LLMs, truck driving risk prediction, interpretable framework, literature knowledge base, data-driven knowledge discovery
<br />
Summary: 
The study introduces a new interpretable prediction framework called LIFT LLMs for truck driving risk prediction. This framework combines an Inference Core driven by LLMs to predict and explain risk, a Literature Processing Pipeline to gather domain-specific literature, and a Result Evaluator to assess prediction accuracy and interpretability. Fine-tuning on real-world data improved prediction performance significantly, surpassing benchmarks by 26.7% in recall and 10.1% in F1-score. Leveraging a literature knowledge base from 299 domain papers, the LIFT LLM offered consistent variable importance ranking, robust interpretation results under different data conditions, and identified potential risky scenarios. The study highlights the impact of the literature knowledge base and fine-tuning on interpretability and suggests the LIFT LLM as a tool for data-driven knowledge discovery. 
<br /> <div>
arXiv:2510.22333v1 Announce Type: new 
Abstract: This study proposes an interpretable prediction framework with literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction. The framework integrates an LLM-driven Inference Core that predicts and explains truck driving risk, a Literature Processing Pipeline that filters and summarizes domain-specific literature into a literature knowledge base, and a Result Evaluator that evaluates the prediction performance as well as the interpretability of the LIFT LLM. After fine-tuning on a real-world truck driving risk dataset, the LIFT LLM achieved accurate risk prediction, outperforming benchmark models by 26.7% in recall and 10.1% in F1-score. Furthermore, guided by the literature knowledge base automatically constructed from 299 domain papers, the LIFT LLM produced variable importance ranking consistent with that derived from the benchmark model, while demonstrating robustness in interpretation results to various data sampling conditions. The LIFT LLM also identified potential risky scenarios by detecting key combination of variables in truck driving risk, which were verified by PERMANOVA tests. Finally, we demonstrated the contribution of the literature knowledge base and the fine-tuning process in the interpretability of the LIFT LLM, and discussed the potential of the LIFT LLM in data-driven knowledge discovery.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry</title>
<link>https://arxiv.org/abs/2510.22340</link>
<guid>https://arxiv.org/abs/2510.22340</guid>
<content:encoded><![CDATA[
<div> dynamic benchmark, spatial reasoning, Vision-Language Models, multimodal text-visual instances, logical validity <br />
Summary:<br />
The article introduces DynaSolidGeo, a dynamic benchmark designed to evaluate spatial reasoning in Vision-Language Models (VLMs) by incorporating spatial intelligence and symbolic reasoning. Unlike existing benchmarks focusing on 2D plane geometry, DynaSolidGeo offers a dynamic dataset with expert-curated seed questions that can generate diverse instances to challenge VLMs. It evaluates models not only based on answer accuracy but also on logical validity and causal coherence through annotated reasoning chains. Experiments on various VLMs demonstrate significant performance gaps, decreased accuracy in dynamic environments, and challenges with tasks requiring high-level spatial intelligence. The DynaSolidGeo dataset and code are available online. <div>
arXiv:2510.22340v1 Announce Type: new 
Abstract: Solid geometry problem solving demands spatial mathematical reasoning that integrates spatial intelligence and symbolic reasoning. However, most existing multimodal mathematical reasoning benchmarks focus primarily on 2D plane geometry, rely on static datasets prone to data contamination and memorization, and evaluate models solely by final answers, overlooking the reasoning process. To address these limitations, we introduce DynaSolidGeo, the first dynamic benchmark for evaluating genuine spatial reasoning in Vision-Language Models (VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo contains 503 expert-curated seed questions that can, in principle, dynamically generate an unbounded number of diverse multimodal text-visual instances. Beyond answer accuracy, we incorporate process evaluation based on expert-annotated reasoning chains to measure logical validity and causal coherence. Experiments across representative open-source and closed-source VLMs reveal large performance gaps, severe degradation in dynamic settings, and poor performance on tasks requiring high-level spatial intelligence, such as mental rotation and visualization. The code and dataset are available at \href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Models Reason Well, Until They Don't</title>
<link>https://arxiv.org/abs/2510.22371</link>
<guid>https://arxiv.org/abs/2510.22371</guid>
<content:encoded><![CDATA[
<div> graph, reasoning, large reasoning models, Deep Reasoning Dataset, complexity  
 
Summary:  
Large language models (LLMs) have shown progress in reasoning tasks but fail when facing complex reasoning problems. This study explores large reasoning models (LRMs), fine-tuned LLMs incentivized for step-by-step argumentation and self-verification. LRMs perform well on existing benchmarks but struggle with increasing complexity. A new Deep Reasoning Dataset (DeepRD) is introduced to generate examples of scalable complexity for evaluation. LRMs show a sharp decline in performance at high complexity levels and lack generalization. Real-world knowledge graphs and proof datasets fall within LRMs' success range, but the tails reveal potential failures. The study stresses the importance of developing methods that can generalize beyond the complexity seen in training data. <br /><br />Summary: <div>
arXiv:2510.22371v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown significant progress in reasoning tasks. However, recent studies show that transformers and LLMs fail catastrophically once reasoning problems exceed modest complexity. We revisit these findings through the lens of large reasoning models (LRMs) -- LLMs fine-tuned with incentives for step-by-step argumentation and self-verification. LRM performance on graph and reasoning benchmarks such as NLGraph seem extraordinary, with some even claiming they are capable of generalized reasoning and innovation in reasoning-intensive fields such as mathematics, physics, medicine, and law. However, by more carefully scaling the complexity of reasoning problems, we show existing benchmarks actually have limited complexity. We develop a new dataset, the Deep Reasoning Dataset (DeepRD), along with a generative process for producing unlimited examples of scalable complexity. We use this dataset to evaluate model performance on graph connectivity and natural language proof planning. We find that the performance of LRMs drop abruptly at sufficient complexity and do not generalize. We also relate our LRM results to the distributions of the complexities of large, real-world knowledge graphs, interaction graphs, and proof datasets. We find the majority of real-world examples fall inside the LRMs' success regime, yet the long tails expose substantial failure potential. Our analysis highlights the near-term utility of LRMs while underscoring the need for new methods that generalize beyond the complexity of examples in the training distribution.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Hierarchical Thinking in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2510.22437</link>
<guid>https://arxiv.org/abs/2510.22437</guid>
<content:encoded><![CDATA[
<div> Finite State Machine, Large Language Models, Chain-of-Thought reasoning, Hierarchical thinking, Reasoning capabilities<br />
Summary:<br />
- The paper introduces the concept of Large Reasoning Models (LRMs) that exhibit hierarchical thinking strategies similar to humans when trained using chain-of-thought reasoning examples. 
- An approach using a memoryless Finite State Machine (FSM) is adopted to approximate LRMs' emerging reasoning dynamics as a structured, interpretable abstraction. 
- The FSM formulation identifies discrete reasoning states such as initialization, deduction, uncertainty estimation, and final conclusion, capturing the high-level states in the model's reasoning process. 
- By annotating CoT steps with these states, the reasoning trajectory can be represented as a transition sequence through a state graph, aiding in analysis and visualization of different models' problem-solving approaches. 
- The FSM-based analysis reveals distinct reasoning patterns and potential shortcomings in LLM reasoning, offering insights for evaluating and enhancing LRM capabilities. <br /><br />Summary: <div>
arXiv:2510.22437v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities when they generate step-by-step solutions, known as chain-of-thought (CoT) reasoning. When trained to using chain-of-thought reasoning examples, the resulting models (called Large Reasoning Models, or LRMs) appear to learn hierarchical thinking strategies similar to those used by humans. However, understanding LRMs emerging reasoning capabilities remains a difficult open problem, with many potential important applications including improving training and understanding robustness. In this paper, we adopt a memoryless Finite State Machine formulation to approximate LRM's emerging hierarchical reasoning dynamics as a structured, interpretable abstraction. We identify a small set of discrete reasoning states including - initialization, deduction, augmentation-strategy, uncertainty-estimation, backtracking, and final-conclusion that capture the high-level states present in the model's reasoning process. By annotating each step of a model's CoT with these states, we can represent the reasoning trajectory as a transition sequence through the state graph. This FSM formulation provides a systematic way to analyze, interpret and visualize how different models approach problems. We describe the FSM model, provide examples of CoT annotations under this scheme, and discuss how it can shed light on differences between available models in their approach to reasoning. Our results demonstrate that this FSM-based analysis reveals distinct reasoning patterns and potential shortcomings, offering a new lens to evaluate and improve LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning "Partner-Aware" Collaborators in Multi-Party Collaboration</title>
<link>https://arxiv.org/abs/2510.22462</link>
<guid>https://arxiv.org/abs/2510.22462</guid>
<content:encoded><![CDATA[
<div> collaborative behavior, Large Language Models, multi-turn tasks, partner-aware collaborator, Interruptible Collaborative Roleplayer  
Summary:  
In the context of deploying Large Language Models (LLMs) as collaborators in agentic settings, evaluating their effectiveness in multi-party tasks is crucial. This paper presents theoretical insights on fostering collaborative behavior between LLM-driven agents and intervention agents. Traditional RLHF-trained LLM agents tend to overlook interventions, hindering group common-ground alignment. To address this, a novel partner-aware learning algorithm, Interruptible Collaborative Roleplayer (ICR), is proposed to train collaborators aiming to increase common ground. Experiments across various collaborative task environments demonstrate that ICR outperforms standard AI agents in promoting successful common-ground convergence and exploring diverse solutions. This research highlights the importance of developing partner-aware strategies for LLMs in collaborative settings to enhance collective task performance. 
<br /><br />Summary: <div>
arXiv:2510.22462v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic settings where they act as collaborators with humans. Therefore, it is increasingly important to be able to evaluate their abilities to collaborate effectively in multi-turn, multi-party tasks. In this paper, we build on the AI alignment and safe interruptability literature to offer novel theoretical insights on collaborative behavior between LLM-driven collaborator agents and an intervention agent. Our goal is to learn an ideal partner-aware collaborator that increases the group's common-ground (CG)-alignment on task-relevant propositions-by intelligently collecting information provided in interventions by a partner agent.We show how LLM agents trained using standard RLHF and related approaches are naturally inclined to ignore possibly well-meaning interventions, which makes increasing group common ground non-trivial in this setting. We employ a two-player Modified-Action MDP to examine this suboptimal behavior of standard AI agents, and propose Interruptible Collaborative Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal collaborators. Experiments on multiple collaborative task environments show that ICR, on average, is more capable of promoting successful CG convergence and exploring more diverse solutions in such tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2510.22535</link>
<guid>https://arxiv.org/abs/2510.22535</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Machine Unlearning, OFFSIDE benchmark, misinformation unlearning, football transfer rumors

Summary:
Unimodal methods struggle with multimodal rumors, showing the need for more robust solutions. Unlearning efficacy in MLLMs is impacted by catastrophic forgetting. Current methods have difficulty with "visual rumors" and are vulnerable to prompt attacks. Recovering unlearned rumors is easy, raising concerns about data privacy. The OFFSIDE benchmark offers a comprehensive framework for evaluating misinformation unlearning in MLLMs and supports selective unlearning and corrective relearning. The dataset consists of 15.68K records for 80 players and includes four test sets for assessment. The study highlights vulnerabilities in existing approaches and the necessity for advancements in multimodal unlearning solutions. The code for OFFSIDE is available on GitHub for further research and development. 

<br /><br />Summary: <div>
arXiv:2510.22535v1 Announce Type: new 
Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about data privacy, making Machine Unlearning (MU), the selective removal of learned information, a critical necessity. However, existing MU benchmarks for MLLMs are limited by a lack of image diversity, potential inaccuracies, and insufficient evaluation scenarios, which fail to capture the complexity of real-world applications. To facilitate the development of MLLMs unlearning and alleviate the aforementioned limitations, we introduce OFFSIDE, a novel benchmark for evaluating misinformation unlearning in MLLMs based on football transfer rumors. This manually curated dataset contains 15.68K records for 80 players, providing a comprehensive framework with four test sets to assess forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports advanced settings like selective unlearning and corrective relearning, and crucially, unimodal unlearning (forgetting only text data). Our extensive evaluation of multiple baselines reveals key findings: (1) Unimodal methods (erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning efficacy is largely driven by catastrophic forgetting; (3) All methods struggle with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can be easily recovered and (5) All methods are vulnerable to prompt attacks. These results expose significant vulnerabilities in current approaches, highlighting the need for more robust multimodal unlearning solutions. The code is available at \href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs</title>
<link>https://arxiv.org/abs/2510.22590</link>
<guid>https://arxiv.org/abs/2510.22590</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge extraction, temporal knowledge graph, few-shot approach, dynamic data, scalability

Summary:
ATOM (AdapTive and OptiMized) addresses the challenge of building and updating Temporal Knowledge Graphs (TKGs) from unstructured text in a dynamic data landscape. It improves exhaustivity and stability by splitting documents into "atomic" facts and employing dual-time modeling. This allows for better distinction between observed and valid information. The approach results in atomic TKGs that are merged in parallel, leading to higher exhaustivity, improved stability, and significant latency reduction compared to baseline methods. The ATOM approach showcases a strong potential for scalability in constructing dynamic TKGs for real-time analytics and memory frameworks.  <br /><br />Summary: <div>
arXiv:2510.22590v1 Announce Type: new 
Abstract: In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning</title>
<link>https://arxiv.org/abs/2510.22594</link>
<guid>https://arxiv.org/abs/2510.22594</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained language models, in-context learning, data distribution, context construction, prediction accuracy

Summary:
In this work, the authors propose a framework to analyze the in-context learning (ICL) performance of pre-trained large language models. They investigate the impact of factors such as network architectures, data encoding, data generation, and prompt construction process on ICL capabilities. By using a simple example with a one-layer transformer, they demonstrate that properly constructed context can shift output distribution towards the query task distribution, leading to accurate predictions. The authors extend their findings to a more general case and establish the relationship between context length, data distribution, and ICL performance. They conduct experiments to validate their theoretical results, showing the effectiveness of their framework in understanding the mechanisms behind the success of pre-trained language models in learning from context.<br /><br />Summary: <div>
arXiv:2510.22594v1 Announce Type: new 
Abstract: Pre-trained large language models have demonstrated a strong ability to learn from context, known as in-context learning (ICL). Despite a surge of recent applications that leverage such capabilities, it is by no means clear, at least theoretically, how the ICL capabilities arise, and in particular, what is the precise role played by key factors such as pre-training procedure as well as context construction. In this work, we propose a new framework to analyze the ICL performance, for a class of realistic settings, which includes network architectures, data encoding, data generation, and prompt construction process. As a first step, we construct a simple example with a one-layer transformer, and show an interesting result, namely when the pre-train data distribution is different from the query task distribution, a properly constructed context can shift the output distribution towards the query task distribution, in a quantifiable manner, leading to accurate prediction on the query topic. We then extend the findings in the previous step to a more general case, and derive the precise relationship between ICL performance, context length and the KL divergence between pre-train and query task distribution. Finally, we provide experiments to validate our theoretical results.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation</title>
<link>https://arxiv.org/abs/2510.22609</link>
<guid>https://arxiv.org/abs/2510.22609</guid>
<content:encoded><![CDATA[
<div> clinical cases, safety-constrained hybrid pipeline, uncertainty-calibrated disease classification, treatment generation, antibiotic stewardship

Summary: 
CLIN-LLM is a novel framework for accurate symptom-to-disease classification and treatment recommendation, addressing challenges in diverse patient settings. Integrating multimodal patient encoding and uncertainty quantification, the system uses BioBERT fine-tuning on 1,200 clinical cases and Focal Loss with Monte Carlo Dropout for confidence-aware predictions. Low-certainty cases are flagged for expert review, ensuring safety. Treatment generation incorporates evidence retrieval from the MedDialog corpus using Biomedical Sentence-BERT and fine-tuned FLAN-T5 model for personalized recommendations. Post-processing includes antibiotic stewardship and drug-drug interaction screening using RxNorm. CLIN-LLM outperforms ClinicalBERT in accuracy and safety, reducing unsafe antibiotic suggestions compared to GPT-5. The framework offers robustness, interpretability, and clinical safety alignment, making it suitable for resource-limited healthcare environments. Future work includes integration of imaging and lab data, multilingual extensions, and clinical trial validation. 

<br /><br />Summary: <div>
arXiv:2510.22609v1 Announce Type: new 
Abstract: Accurate symptom-to-disease classification and clinically grounded treatment recommendations remain challenging, particularly in heterogeneous patient settings with high diagnostic risk. Existing large language model (LLM)-based systems often lack medical grounding and fail to quantify uncertainty, resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid pipeline that integrates multimodal patient encoding, uncertainty-calibrated disease classification, and retrieval-augmented treatment generation. The framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease dataset and incorporates Focal Loss with Monte Carlo Dropout to enable confidence-aware predictions from free-text symptoms and structured vitals. Low-certainty cases (18%) are automatically flagged for expert review, ensuring human oversight. For treatment generation, CLIN-LLM employs Biomedical Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample MedDialog corpus. The retrieved evidence and patient context are fed into a fine-tuned FLAN-T5 model for personalized treatment generation, followed by post-processing with RxNorm for antibiotic stewardship and drug-drug interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score, outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic suggestions are reduced by 67% compared to GPT-5. These results demonstrate CLIN-LLM's robustness, interpretability, and clinical safety alignment. The proposed system provides a deployable, human-in-the-loop decision support framework for resource-limited healthcare environments. Future work includes integrating imaging and lab data, multilingual extensions, and clinical trial validation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming</title>
<link>https://arxiv.org/abs/2510.22626</link>
<guid>https://arxiv.org/abs/2510.22626</guid>
<content:encoded><![CDATA[
<div> Keywords: SwiftSolve, competitive programming, algorithmic planning, complexity analysis, efficiency metrics 

Summary:
SwiftSolve is a complexity-aware multi-agent system for competitive programming that combines algorithmic planning, empirical profiling, and complexity-guided repair. In this system, specialized agents assume different roles such as planning, coding, profiling, and complexity analysis. The agents communicate through JSON messages and work together to optimize program efficiency and correctness. SwiftSolve was evaluated on 26 problems and achieved a pass rate of 61.54% on the first attempt and 80.77% within three attempts, with minimal latency increase. Failures were mainly due to resource constraints rather than logic errors. Compared to a competitor, SwiftSolve improved success rates at a slight increase in runtime. Efficiency metrics such as runtime, memory usage, and complexity fit accuracy were also reported, showing that profiling and complexity-guided replanning help reduce inefficiency while maintaining accuracy.<br /><br />Summary: <div>
arXiv:2510.22626v1 Announce Type: new 
Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy unit tests while violating contest time or memory budgets. We present SwiftSolve, a complexity-aware multi-agent system for competitive programming that couples algorithmic planning with empirical profiling and complexity-guided repair. We frame competitive programming as a software environment where specialized agents act as programmers, each assuming roles such as planning, coding, profiling, and complexity analysis. A Planner proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on a fixed input-size schedule to record wall time and peak memory; and a Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a complexity class and dispatch targeted patches to either the Planner or Coder. Agents communicate via typed, versioned JSON; a controller enforces iteration caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10 Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate run-level success is 73.08% at 12.40 s mean. Failures are predominantly resource-bound, indicating inefficiency rather than logic errors. Against Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness (pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that profiling and complexity-guided replanning reduce inefficiency while preserving accuracy.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration</title>
<link>https://arxiv.org/abs/2510.22679</link>
<guid>https://arxiv.org/abs/2510.22679</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, boilerplate responses, log-probability distribution, early termination, computational cost

Summary: 
Large Language Models (LLMs) often waste computational resources generating unnecessary boilerplate responses. A new method using the log-probability distribution of the first generated token can efficiently classify responses as substantive answers or boilerplate. This technique, applied after a single generation step, allows for early termination or redirection to a smaller model, reducing computational costs significantly. The method works across various model sizes and types, providing a practical and computationally lightweight solution for optimizing LLM inference. This approach enables more efficient and sustainable deployment of LLMs, making them more cost-effective and freeing up resources for more meaningful tasks. <br /><br />Summary: <div>
arXiv:2510.22679v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often expend significant computational resources generating boilerplate responses, such as refusals, simple acknowledgements and casual greetings, which adds unnecessary cost and latency. To address this inefficiency, we propose a simple yet highly effective method for detecting such responses after only a single generation step. We demonstrate that the log-probability distribution of the first generated token serves as a powerful signal for classifying the nature of the entire subsequent response. Our experiments, conducted across a diverse range of small, large, and reasoning-specialized models, show that the first-token log-probability vectors form distinctly separable clusters for different response types. Using a lightweight k-NN classifier, we achieve high accuracy in predicting whether a response will be a substantive answer or a form of boilerplate response, including user-specified refusals. The primary implication is a practical, computationally trivial technique, optimizing LLM inference by enabling early termination or redirection to a smaller model, thereby yielding significant savings in computational cost. This work presents a direct path toward more efficient and sustainable LLM deployment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring</title>
<link>https://arxiv.org/abs/2510.22702</link>
<guid>https://arxiv.org/abs/2510.22702</guid>
<content:encoded><![CDATA[
<div> Atlas Urban Index, urban development, Sentinel-2 satellite imagery, Vision-Language Models, NDBI

Summary: 
The Atlas Urban Index (AUI) is a new metric for measuring urban development using Sentinel-2 satellite imagery. Traditional approaches like the Normalized Difference Built-up Index (NDBI) face challenges in accurately capturing urban development due to factors like atmospheric noise and cloud cover. To address these issues, AUI leverages Vision-Language Models (VLMs) to provide development scores for regions. By collecting a time series of Sentinel-2 images and processing them within fixed time windows to minimize cloud cover, AUI ensures consistent scoring. It utilizes a curated set of reference images for different urbanization levels and considers the most recent past image to maintain temporal consistency and reduce cloud-related noise. Experimental results on Bangalore demonstrate that AUI outperforms NDBI in evaluating urban development. 

Summary:<br /><br />Atlas Urban Index, urban development, Sentinel-2 satellite imagery, Vision-Language Models, NDBI <div>
arXiv:2510.22702v1 Announce Type: new 
Abstract: We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite imagery. Existing approaches, such as the {\em Normalized Difference Built-up Index} (NDBI), often struggle to accurately capture urban development due to factors like atmospheric noise, seasonal variation, and cloud cover. These limitations hinder large-scale monitoring of human development and urbanization. To address these challenges, we propose an approach that leverages {\em Vision-Language Models }(VLMs) to provide a development score for regions. Specifically, we collect a time series of Sentinel-2 images for each region. Then, we further process the images within fixed time windows to get an image with minimal cloud cover, which serves as the representative image for that time window. To ensure consistent scoring, we adopt two strategies: (i) providing the VLM with a curated set of reference images representing different levels of urbanization, and (ii) supplying the most recent past image to both anchor temporal consistency and mitigate cloud-related noise in the current image. Together, these components enable AUI to overcome the challenges of traditional urbanization indices and produce more reliable and stable development scores. Our qualitative experiments on Bangalore suggest that AUI outperforms standard indices such as NDBI.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability</title>
<link>https://arxiv.org/abs/2510.22710</link>
<guid>https://arxiv.org/abs/2510.22710</guid>
<content:encoded><![CDATA[
<div> keywords: Retrieval-Augmented Generation, RaCoT, long-tail queries, knowledge retrieval, contrastive thinking

Summary:
RaCoT is a novel framework that addresses the challenges of knowledge-sparse and semantically ambiguous long-tail queries in Retrieval-Augmented Generation (RAG). By incorporating contrastive thinking at the pre-retrieval stage, RaCoT generates a contrastive question and a $\Delta$-Prompt to guide the model's focus on key differences. This approach enables RaCoT to suppress semantic interference and enhance reasoning efficiency. In empirical evaluations on multiple benchmarks, RaCoT outperforms existing methods like RankRAG and Self-RAG, showing improved robustness and efficiency. With low latency and token overhead, RaCoT provides a promising pathway for deploying reliable AI systems in real-time and resource-constrained settings. Ablation studies confirm the effectiveness of each component in enhancing model performance and discriminative reasoning. RaCoT represents a significant advancement in the RAG paradigm, shifting focus from post-hoc context cleaning to proactive shaping of reasoning processes. 

<br /><br />Summary: RaCoT is a novel framework for Retrieval-Augmented Generation that addresses challenges with long-tail queries by incorporating contrastive thinking pre-retrieval. It enhances reasoning efficiency, outperforming existing methods and demonstrating superior robustness and efficiency. RaCoT offers a promising pathway for deploying reliable AI systems in resource-constrained settings, with ablation studies validating its components' effectiveness in enhancing model performance and discriminative reasoning. <div>
arXiv:2510.22710v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) faces a core bottleneck with knowledge-sparse and semantically ambiguous long-tail queries, where retrieval noise distorts reasoning and necessitates costly post-processing. To tackle this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel framework that shifts contrastive thinking to the pre-retrieval stage. By automatically generating a semantically adjacent yet differently answered contrastive question and extracting a $\Delta$-Prompt to capture their key differences, RaCoT guides the model to proactively focus on the ``critical details that determine answer divergence." This approach allows it to suppress semantic interference within a single retrieval pass, overcoming the theoretical bottleneck of single-vector queries that struggle to simultaneously encode signals for what to attend to and what to ignore. On six authoritative benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits superior robustness, with a performance drop of only 8.6\% in adversarial tests, far surpassing the over 15\% degradation in other methods. Furthermore, its low latency (3.12s) and token overhead (11.54) place it on the accuracy-efficiency Pareto frontier, while ablation studies validate the necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from ``post-hoc context cleaning" to ``a priori shaping of discriminative reasoning", offering an efficient and robust path toward reliable AI systems for real-time, resource-constrained deployments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Insights into Leading Conversational AI Models</title>
<link>https://arxiv.org/abs/2510.22729</link>
<guid>https://arxiv.org/abs/2510.22729</guid>
<content:encoded><![CDATA[
<div> performance, ethics, usability, bias mitigation, language models <br />
Summary: <br />
The study compares five top Big Language Models (LLMs) from Google, High-Flyer, Anthropic, OpenAI, and Meta based on Performance and Accuracy, Ethics and Bias Mitigation, and Usability and Integration. Claude exhibits good moral reasoning, Gemini excels in multimodal capabilities and ethical frameworks, DeepSeek is strong in fact-based reasoning, LLaMA is versatile for open applications, and ChatGPT provides balanced performance focusing on usage. Each model has unique strengths in how well they work, ease of use, and ethical treatment of people. Users are advised to utilize each model according to its strengths for optimal performance and ethical use. <br /> <div>
arXiv:2510.22729v1 Announce Type: new 
Abstract: Big Language Models (LLMs) are changing the way businesses use software, the way people live their lives and the way industries work. Companies like Google, High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial to look at how each model is different in terms of performance, moral behaviour and usability, as these differences are based on the different ideas that built them. This study compares five top LLMs: Google's Gemini, High-Flyer's DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs this by analysing three important factors: Performance and Accuracy, Ethics and Bias Mitigation and Usability and Integration. It was found that Claude has good moral reasoning, Gemini is better at multimodal capabilities and has strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA is good for open applications and ChatGPT delivers balanced performance with a focus on usage. It was concluded that these models are different in terms of how well they work, how easy they are to use and how they treat people ethically, making it a point that each model should be utilised by the user in a way that makes the most of its strengths.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.22751</link>
<guid>https://arxiv.org/abs/2510.22751</guid>
<content:encoded><![CDATA[
<div> fact verification, large language models, false information, natural language processing, trustworthiness

Summary:<br />
Large Language Models (LLMs) have revolutionized AI interactions but are plagued by the generation of inaccurate information. A new fact verification framework has been developed to address this issue by cross-checking LLM outputs against various knowledge sources in real-time. By utilizing structured databases, live web searches, and academic literature, the system can detect and correct factual errors while maintaining natural response flow. Through testing in healthcare, finance, and scientific research, it was found that the framework significantly reduced false information generated by LLMs without compromising response quality. Domain experts rated the corrected outputs as 89% satisfactory, showcasing a marked improvement over unverified LLM responses. This framework offers a practical solution for enhancing the trustworthiness of LLMs in applications where accuracy is crucial. <br /><br /> <div>
arXiv:2510.22751v1 Announce Type: new 
Abstract: While Large Language Models have transformed how we interact with AI systems, they suffer from a critical flaw: they confidently generate false information that sounds entirely plausible. This hallucination problem has become a major barrier to deploying these models in real-world applications where accuracy matters. We developed a fact verification framework that catches and corrects these errors in real-time by cross checking LLM outputs against multiple knowledge sources. Our system combines structured databases, live web searches, and academic literature to verify factual claims as they're generated. When we detect inconsistencies, we automatically correct them while preserving the natural flow of the response. Testing across various domains showed we could reduce hallucinations by 67% without sacrificing response quality. Domain experts in healthcare, finance, and scientific research rated our corrected outputs 89% satisfactory a significant improvement over unverified LLM responses. This work offers a practical solution for making LLMs more trustworthy in applications where getting facts wrong isn't an option.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval</title>
<link>https://arxiv.org/abs/2510.22765</link>
<guid>https://arxiv.org/abs/2510.22765</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, personalized AI assistant, KV-Cache retrieval, user-specific information, fine-grained benchmark<br />
<br />
Summary: <br />
The article introduces Jarvis, a novel framework for developing personalized AI assistants that leverage personal KV-Cache retrieval to store user-specific information in both textual and visual tokens. Textual tokens summarize user information into metadata, while visual tokens consist of distinct image patches from the user's images. Jarvis retrieves relevant KV-Caches from personal storage to ensure accurate responses when answering questions. A fine-grained benchmark is also introduced to emphasize accurate question answering based on fine-grained user-specific information. Jarvis demonstrates superior performance in visual question answering and text-only tasks on various datasets, showcasing its practical use in creating personalized AI assistants. The framework achieves state-of-the-art results and offers a promising approach to enhancing accuracy and personalization in AI assistant technologies. The code and dataset related to Jarvis will be made available for further research and development.  <div>
arXiv:2510.22765v1 Announce Type: new 
Abstract: The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations</title>
<link>https://arxiv.org/abs/2510.22780</link>
<guid>https://arxiv.org/abs/2510.22780</guid>
<content:encoded><![CDATA[
<div> AI agents, human work, comparison, workflows, efficiency <br />
Summary: <br />
1. AI agents are optimized for human work tasks such as software engineering and writing but lack a clear understanding of human work processes. <br />
2. AI agents take a programmatic approach in various work domains, producing lower quality work compared to humans. <br />
3. Agents often resort to data fabrication and misuse of tools to cover up their deficiencies. <br />
4. Despite lower quality, agents are significantly faster and more cost-effective than humans in delivering results. <br />
5. There is potential for efficient collaboration by delegating easily programmable tasks to AI agents. <br /> <div>
arXiv:2510.22780v1 Announce Type: new 
Abstract: AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Meta-Orchestrator for Multi-task Copilots</title>
<link>https://arxiv.org/abs/2510.22781</link>
<guid>https://arxiv.org/abs/2510.22781</guid>
<content:encoded><![CDATA[
<div> agents, copilot, orchestrator, meta-learning, Microsoft <br />
<br />
Summary: Microsoft Copilot suites feature agents powered by language models and software engineering operations, with each agent capable of expanding dynamically. An Agentic Meta-orchestrator (AMO) is proposed to handle multiple tasks and scalable agents, providing natural language and action responses. The AMO uses meta-learning for planning, leveraging a trained decision tree model to decide the best inference strategy. Two production use cases, Microsoft 365 E-Commerce Copilot and code compliance copilot, demonstrate the effectiveness of AMO. The M365 E-Commerce Copilot assists with product promotion by providing product information and connecting to various agents like databases and human support. The code compliance copilot scans DevOps code to identify compliance issues in pull requests. The AMO efficiently coordinates tasks and agents in the copilot services, enhancing performance and efficiency. <br /> <div>
arXiv:2510.22781v1 Announce Type: new 
Abstract: Microsoft Copilot suites serve as the universal entry point for various agents skilled in handling important tasks, ranging from assisting a customer with product purchases to detecting vulnerabilities in corporate programming code. Each agent can be powered by language models, software engineering operations, such as database retrieval, and internal \& external knowledge. The repertoire of a copilot can expand dynamically with new agents. This requires a robust orchestrator that can distribute tasks from user prompts to the right agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for handling multiple tasks and scalable agents in copilot services, which can provide both natural language and action responses. We will also demonstrate the planning that leverages meta-learning, i.e., a trained decision tree model for deciding the best inference strategy among various agents/models. We showcase the effectiveness of our AMO through two production use cases: Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365 E-Commerce Copilot advertises Microsoft products to external customers to promote sales success. The M365 E-Commerce Copilot provides up-to-date product information and connects to multiple agents, such as relational databases and human customer support. The code compliance copilot scans the internal DevOps code to detect known and new compliance issues in pull requests (PR).
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will Humanity Be Rendered Obsolete by AI?</title>
<link>https://arxiv.org/abs/2510.22814</link>
<guid>https://arxiv.org/abs/2510.22814</guid>
<content:encoded><![CDATA[
<div> existential risks, artificial intelligence, ultraintelligence, AGI, superintelligence 

Summary:<br /><br />This article discusses the existential risks posed by artificial intelligence (AI) to humanity, examining the potential trajectory from current AI to ultraintelligence. It draws on the theoretical work of Irving J. Good and Nick Bostrom, as well as recent publications, to explore the concepts of artificial general intelligence (AGI) and superintelligence. The article considers the exponentially increasing cognitive capabilities of machines and the ethical and existential implications of an intelligence vastly surpassing human intelligence. It warns of the possibility of human extinction not due to malevolent intent, but rather from the uncontrollable and indifferent cognitive superiority of superintelligent AI. <div>
arXiv:2510.22814v1 Announce Type: new 
Abstract: This article analyzes the existential risks artificial intelligence (AI) poses to humanity, tracing the trajectory from current AI to ultraintelligence. Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and superintelligence. Considering machines' exponentially growing cognitive power and hypothetical IQs, it addresses the ethical and existential implications of an intelligence vastly exceeding humanity's, fundamentally alien. Human extinction may result not from malice, but from uncontrollable, indifferent cognitive superiority.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning</title>
<link>https://arxiv.org/abs/2510.22832</link>
<guid>https://arxiv.org/abs/2510.22832</guid>
<content:encoded><![CDATA[
<div> Hierarchical Reasoning Model, HRM, reinforcement learning, dynamic environments, maze navigation<br />
<br />
The Hierarchical Reasoning Model (HRM) has demonstrated strong reasoning abilities in small sizes but has only been applied to supervised, static, fully-observable problems. A new variant called HRM-Agent has been developed using reinforcement learning, enabling HRM to navigate goals in dynamic and uncertain maze environments. The recurrent inference process of HRM allows for the reuse of computation from previous time-steps, enhancing its adaptability to changing environments. This adaptation addresses the limitations of HRM in handling dynamic, uncertain, and partially observable problems, making it more applicable to real-world scenarios. The successful integration of HRM into reinforcement learning showcases its potential for tackling complex and dynamic tasks with undefined correct actions. <br /><br />Summary: <div>
arXiv:2510.22832v1 Announce Type: new 
Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities given its small size, but has only been applied to supervised, static, fully-observable problems. One of HRM's strengths is its ability to adapt its computational effort to the difficulty of the problem. However, in its current form it cannot integrate and reuse computation from previous time-steps if the problem is dynamic, uncertain or partially observable, or be applied where the correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only reinforcement learning. We show that HRM can learn to navigate to goals in dynamic and uncertain maze environments. Recent work suggests that HRM's reasoning abilities stem from its recurrent inference process. We explore the dynamics of the recurrent inference process and find evidence that it is successfully reusing computation from earlier environment time-steps.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Agents That Reason About Their Computation</title>
<link>https://arxiv.org/abs/2510.22833</link>
<guid>https://arxiv.org/abs/2510.22833</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, computation efficiency, cognitive effort, energy efficient agents, Arcade Learning Environment

Summary:
Agents trained using reinforcement learning often do not become more computationally efficient as they improve, unlike humans who require less cognitive effort as they become more proficient at a task. This paper explores the idea of training agents to reason about their compute costs and control when they use compute. The experiments conducted on the Arcade Learning Environment show that agents that reason about their compute perform better on 75% of games while using three times less compute on average. This demonstrates the potential for more energy-efficient agents and the opportunity to free up compute cycles for other processes like planning. The analysis of individual games highlights where agents gain these efficiencies. Overall, incorporating compute cost awareness in reinforcement learning agents can lead to significant improvements in performance and efficiency. 

<br /><br />Summary: <div>
arXiv:2510.22833v1 Announce Type: new 
Abstract: While reinforcement learning agents can achieve superhuman performance in many complex tasks, they typically do not become more computationally efficient as they improve. In contrast, humans gradually require less cognitive effort as they become more proficient at a task. If agents could reason about their compute as they learn, could they similarly reduce their computation footprint? If they could, we could have more energy efficient agents or free up compute cycles for other processes like planning. In this paper, we experiment with showing agents the cost of their computation and giving them the ability to control when they use compute. We conduct our experiments on the Arcade Learning Environment, and our results demonstrate that with the same training compute budget, agents that reason about their compute perform better on 75% of games. Furthermore, these agents use three times less compute on average. We analyze individual games and show where agents gain these efficiencies.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes</title>
<link>https://arxiv.org/abs/2510.22836</link>
<guid>https://arxiv.org/abs/2510.22836</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, modality gap, visual reasoning, training recipes, balanced reasoning 

Summary:
This article discusses the modality gap in multimodal large language models (MLLMs), where there is an imbalance in reasoning capabilities between textual and visual modalities. The current MLLMs tend to rely more on textual cues than visual content, leading to suboptimal performance on tasks requiring genuine visual reasoning. The authors examine existing training recipes that exacerbate this gap and propose strategies to bridge it. They explore approaches from both data and loss design perspectives to develop training recipes that promote more balanced multimodal reasoning. By analyzing and addressing the modality gap, this study contributes insights for enhancing the performance of MLLMs on vision-and-language tasks. The code for the proposed strategies is openly available for further research and implementation. <br /><br />Summary: <div>
arXiv:2510.22836v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have demonstrated strong capabilities on vision-and-language tasks. However, recent findings reveal an imbalance in their reasoning capabilities across visual and textual modalities. Specifically, current MLLMs often over-rely on textual cues while under-attending to visual content, resulting in suboptimal performance on tasks that require genuine visual reasoning. We refer to this phenomenon as the \textit{modality gap}, defined as the performance disparity between text-centric and vision-centric inputs. In this paper, we analyze the modality gap through the lens of training recipes. We first show that existing training recipes tend to amplify this gap. Then, we systematically explore strategies to bridge it from two complementary perspectives: data and loss design. Our findings provide insights into developing training recipes that mitigate the modality gap and promote more balanced multimodal reasoning. Our code is publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lyapunov Function-guided Reinforcement Learning for Flight Control</title>
<link>https://arxiv.org/abs/2510.22840</link>
<guid>https://arxiv.org/abs/2510.22840</guid>
<content:encoded><![CDATA[
<div> Enhanced, Cascaded, Online learning, Flight control, Convergence 

Summary: 
The article discusses the development and improvement of a cascaded online learning flight control system focusing on action smoothness. It delves into the convergence performance of the control system, analyzing the incremental Lyapunov function candidate's increase. This metric considers errors from both discretization and state prediction caused by the incremental model. The study includes comparative results from flight control simulations. <div>
arXiv:2510.22840v1 Announce Type: new 
Abstract: A cascaded online learning flight control system has been developed and enhanced with respect to action smoothness. In this paper, we investigate the convergence performance of the control system, characterized by the increment of a Lyapunov function candidate. The derivation of this metric accounts for discretization errors and state prediction errors introduced by the incremental model. Comparative results are presented through flight control simulations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits</title>
<link>https://arxiv.org/abs/2510.22883</link>
<guid>https://arxiv.org/abs/2510.22883</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive studies, artificial intelligence, inferential mechanisms, logic gates, logic programming

Summary:
This paper proposes a unifying framework for cognitive studies and artificial intelligence by examining inferential mechanisms through the lens of electronic circuits based on logic gates. By looking at how logic gates handle implication and negation differently from standard logic, the study identifies four main forms of dependencies and eight common inferential patterns. Through a probabilistic interpretation of logic programs, inner functional dependencies are uncovered. The findings suggest a novel way to understand inferential processes in cognition and artificial intelligence, highlighting the potential for more broadly applicable structures beyond the realm of symbolic AI modeling techniques and digital systems infrastructures.<br /><br />Summary: <div>
arXiv:2510.22883v1 Announce Type: new 
Abstract: Cognitive studies and artificial intelligence have developed distinct models for various inferential mechanisms (categorization, induction, abduction, causal inference, contrast, merge, ...). Yet, both natural and artificial views on cognition lack apparently a unifying framework. This paper formulates a speculative answer attempting to respond to this gap. To postulate on higher-level activation processes from a material perspective, we consider inferential mechanisms informed by symbolic AI modelling techniques, through the simplistic lenses of electronic circuits based on logic gates. We observe that a logic gate view entails a different treatment of implication and negation compared to standard logic and logic programming. Then, by combinatorial exploration, we identify four main forms of dependencies that can be realized by these inferential circuits. Looking at how these forms are generally used in the context of logic programs, we identify eight common inferential patterns, exposing traditionally distinct inferential mechanisms in an unifying framework. Finally, following a probabilistic interpretation of logic programs, we unveil inner functional dependencies. The paper concludes elaborating in what sense, even if our arguments are mostly informed by symbolic means and digital systems infrastructures, our observations may pinpoint to more generally applicable structures.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset</title>
<link>https://arxiv.org/abs/2510.22898</link>
<guid>https://arxiv.org/abs/2510.22898</guid>
<content:encoded><![CDATA[
<div> Keywords: tool-calling environments, large language models, MAVEN benchmark, symbolic reasoning layer, agentic reasoning

Summary:
The article discusses the challenges of generalizing agentic reasoning systems across different tool-calling environments. Current large language models (LLMs) perform poorly on diverse benchmarks, including the newly introduced MAVEN benchmark, which stresses multi-step reasoning through verification and adversarial tasks. The CoreThink Agentic Reasoner is proposed as a solution, augmenting LLMs with a symbolic reasoning layer for structured decomposition and adaptive tool orchestration. This framework achieves state-of-the-art performance across all benchmarks without additional training, showing a 530% improvement over existing baselines at a lower computational cost. The study highlights the importance of addressing the generalization gap in agentic reasoning systems and introduces a promising approach for enhancing performance in tool-use settings. 

<br /><br />Summary: <div>
arXiv:2510.22898v1 Announce Type: new 
Abstract: Generalization across Agentic tool-calling environments remains a key unsolved challenge in developing reliable agentic reasoning systems. While large language models (LLMs) demonstrate strong performance on isolated benchmarks, their ability to transfer reasoning strategies and co-ordinate tools across diverse domains is poorly understood. In this work, we conduct a large-scale evaluation of state-of-the-art LLMs on multiple tool-calling benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math & Physics Adversarial Verification & Evaluation Network), a new out of distribution (OOD) benchmark designed to stress-test multi-step reasoning through explicit verification and adversarial task composition. Our results show that most current models achieve below 50% accuracy on MAVEN, revealing a significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that augments LLMs with a lightweight symbolic reasoning layer for structured decomposition and adaptive tool orchestration. Without additional training, it generalizes across all benchmarks, achieving state-of-the-art performance with 530% improvements over existing baselines at roughly one-tenth the computational cost.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation</title>
<link>https://arxiv.org/abs/2510.22942</link>
<guid>https://arxiv.org/abs/2510.22942</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, POI recommendation, Location-Based Social Networks, hierarchical structure, temporal contexts <br />
Summary: <br />
Next Point-of-Interest (POI) recommendation models in Location-Based Social Networks face limitations in capturing the hierarchical structure of spatial choices and user-specific temporal contexts simultaneously. To address this, GTR-Mamba, a novel framework for cross-manifold conditioning and routing, is proposed. It leverages hyperbolic geometry to model preference hierarchies and Euclidean tangent space with a Mamba layer for dynamic sequence updates. A cross-manifold channel fuses spatio-temporal information to steer the State Space Model, allowing flexible adaptation to contextual changes. Extensive experiments on real-world datasets show that GTR-Mamba outperforms existing models in next POI recommendation. <br /> <div>
arXiv:2510.22942v1 Announce Type: new 
Abstract: Next Point-of-Interest (POI) recommendation is a critical task in modern Location-Based Social Networks (LBSNs), aiming to model the complex decision-making process of human mobility to provide personalized recommendations for a user's next check-in location. Existing POI recommendation models, predominantly based on Graph Neural Networks and sequential models, have been extensively studied. However, these models face a fundamental limitation: they struggle to simultaneously capture the inherent hierarchical structure of spatial choices and the dynamics and irregular shifts of user-specific temporal contexts. To overcome this limitation, we propose GTR-Mamba, a novel framework for cross-manifold conditioning and routing. GTR-Mamba leverages the distinct advantages of different mathematical spaces for different tasks: it models the static, tree-like preference hierarchies in hyperbolic geometry, while routing the dynamic sequence updates to a novel Mamba layer in the computationally stable and efficient Euclidean tangent space. This process is coordinated by a cross-manifold channel that fuses spatio-temporal information to explicitly steer the State Space Model (SSM), enabling flexible adaptation to contextual changes. Extensive experiments on three real-world datasets demonstrate that GTR-Mamba consistently outperforms state-of-the-art baseline models in next POI recommendation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner</title>
<link>https://arxiv.org/abs/2510.22969</link>
<guid>https://arxiv.org/abs/2510.22969</guid>
<content:encoded><![CDATA[
arXiv:2510.22969v1 Announce Type: new 
Abstract: In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction</title>
<link>https://arxiv.org/abs/2510.22981</link>
<guid>https://arxiv.org/abs/2510.22981</guid>
<content:encoded><![CDATA[
arXiv:2510.22981v1 Announce Type: new 
Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which are directly generated from natural language instructions, have become a promising avenue for future research due to their flexible attacking forms. To generate SemanticAEs, current methods fall short of satisfactory attacking ability as the key underlying factors of semantic uncertainty in human instructions, such as referring diversity, descriptive incompleteness, and boundary ambiguity, have not been fully investigated. To tackle the issues, this paper develops a multi-dimensional instruction uncertainty reduction (InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable, adaptive, and effective. Specifically, in the dimension of the sampling method, we propose the residual-driven attacking direction stabilization to alleviate the unstable adversarial optimization caused by the diversity of language references. By coarsely predicting the language-guided sampling process, the optimization process will be stabilized by the designed ResAdv-DDIM sampler, therefore releasing the transferable and robust adversarial capability of multi-step diffusion models. In task modeling, we propose the context-encoded attacking scenario constraint to supplement the missing knowledge from incomplete human instructions. Guidance masking and renderer integration are proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger scenario-adapted attacks. Moreover, in the dimension of generator evaluation, we propose the semantic-abstracted attacking evaluation enhancement by clarifying the evaluation boundary, facilitating the development of more effective SemanticAE generators. Extensive experiments demonstrate the superiority of the transfer attack performance of InSUR. Moreover, we realize the reference-free generation of semantically constrained 3D adversarial examples for the first time.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProfileXAI: User-Adaptive Explainable AI</title>
<link>https://arxiv.org/abs/2510.22998</link>
<guid>https://arxiv.org/abs/2510.22998</guid>
<content:encoded><![CDATA[
arXiv:2510.22998v1 Announce Type: new 
Abstract: ProfileXAI is a model- and domain-agnostic framework that couples post-hoc explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce explanations for different types of users. The system indexes a multimodal knowledge base, selects an explainer per instance via quantitative criteria, and generates grounded narratives with chat-enabled prompting. On Heart Disease and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token use, and perceived quality. No explainer dominates: LIME achieves the best fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma \le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$, with domain experts at $3.77$), enabling efficient and trustworthy explanations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports</title>
<link>https://arxiv.org/abs/2510.23008</link>
<guid>https://arxiv.org/abs/2510.23008</guid>
<content:encoded><![CDATA[
arXiv:2510.23008v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated promising performance in generating diagnostic conclusions from imaging findings, thereby supporting radiology reporting, trainee education, and quality control. However, systematic guidance on how to optimize prompt design across different clinical contexts remains underexplored. Moreover, a comprehensive and standardized framework for assessing the trustworthiness of LLM-generated radiology reports is yet to be established. This study aims to enhance the trustworthiness of LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility Assessment (MDCA) framework and providing guidance on institution-specific prompt optimization. The proposed framework is applied to evaluate and compare the performance of several advanced LLMs, including Kimi-K2-Instruct-0905, Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution</title>
<link>https://arxiv.org/abs/2510.23026</link>
<guid>https://arxiv.org/abs/2510.23026</guid>
<content:encoded><![CDATA[
arXiv:2510.23026v1 Announce Type: new 
Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional or memory computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a temporal horizon and that certain parts of a planned trajectory should be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL task domains.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI Scientists: Surveying the automatic Scientists and Research</title>
<link>https://arxiv.org/abs/2510.23045</link>
<guid>https://arxiv.org/abs/2510.23045</guid>
<content:encoded><![CDATA[
arXiv:2510.23045v1 Announce Type: new 
Abstract: Artificial intelligence is undergoing a profound transition from a computational instrument to an autonomous originator of scientific knowledge. This emerging paradigm, the AI scientist, is architected to emulate the complete scientific workflow-from initial hypothesis generation to the final synthesis of publishable findings-thereby promising to fundamentally reshape the pace and scale of discovery. However, the rapid and unstructured proliferation of these systems has created a fragmented research landscape, obscuring overarching methodological principles and developmental trends. This survey provides a systematic and comprehensive synthesis of this domain by introducing a unified, six-stage methodological framework that deconstructs the end-to-end scientific process into: Literature Review, Idea Generation, Experimental Preparation, Experimental Execution, Scientific Writing, and Paper Generation. Through this analytical lens, we chart the field's evolution from early Foundational Modules (2022-2023) to integrated Closed-Loop Systems (2024), and finally to the current frontier of Scalability, Impact, and Human-AI Collaboration (2025-present). By rigorously synthesizing these developments, this survey not only clarifies the current state of autonomous science but also provides a critical roadmap for overcoming remaining challenges in robustness and governance, ultimately guiding the next generation of systems toward becoming trustworthy and indispensable partners in human scientific inquiry.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis</title>
<link>https://arxiv.org/abs/2510.23062</link>
<guid>https://arxiv.org/abs/2510.23062</guid>
<content:encoded><![CDATA[
arXiv:2510.23062v1 Announce Type: new 
Abstract: Driven by the dual principles of smart education and artificial intelligence technology, the online education model has rapidly emerged as an important component of the education industry. Cognitive diagnostic technology can utilize students' learning data and feedback information in educational evaluation to accurately assess their ability level at the knowledge level. However, while massive amounts of information provide abundant data resources, they also bring about complexity in feature extraction and scarcity of disciplinary data. In cross-disciplinary fields, traditional cognitive diagnostic methods still face many challenges. Given the differences in knowledge systems, cognitive structures, and data characteristics between different disciplines, this paper conducts in-depth research on neural network cognitive diagnosis and knowledge association neural network cognitive diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis method (TLCD). This method combines deep learning techniques and transfer learning strategies to enhance the performance of the model in the target discipline by utilizing the common features of the main discipline. The experimental results show that the cross-disciplinary cognitive diagnosis model based on deep learning performs better than the basic model in cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate students' learning situation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards</title>
<link>https://arxiv.org/abs/2510.23083</link>
<guid>https://arxiv.org/abs/2510.23083</guid>
<content:encoded><![CDATA[
arXiv:2510.23083v1 Announce Type: new 
Abstract: Generating high-quality code remains a challenge for Large Language Models (LLMs). For the evolution of reasoning models on this task, reward models are a necessary intermediate step. These models judge outcomes or intermediate steps. Decoder-only transformer models can be turned into reward models by introducing a regression layer and supervised fine-tuning. While it is known that reflection capabilities generally increase with the size of a model, we want to investigate whether state-of-the-art small language models like the Phi-4 family can be turned into usable reward models blending the consideration of process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness labels derived from the APPS coding challenge benchmark. We then train a value-head model to estimate the success probability of intermediate outputs. Our evaluation shows that small LLMs are capable of serving as effective reward models or code evaluation critics, successfully identifying correct solutions among multiple candidates. Using this critic, we achieve over a 20% improvement in the search capability of the most accurate code out of multiple generations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs</title>
<link>https://arxiv.org/abs/2510.23127</link>
<guid>https://arxiv.org/abs/2510.23127</guid>
<content:encoded><![CDATA[
arXiv:2510.23127v1 Announce Type: new 
Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising frontier for accelerating biological discovery. However, these models face a fundamental challenge when processing raw biomolecular sequences: the tokenization dilemma. Whether treating sequences as a specialized language, risking the loss of functional motif information, or as a separate modality, introducing formidable alignment challenges, current strategies fundamentally limit their reasoning capacity. We challenge this sequence-centric paradigm by positing that a more effective strategy is to provide Sci-LLMs with high-level structured context derived from established bioinformatics tools, thereby bypassing the need to interpret low-level noisy sequence data directly. Through a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we tested three input modes: sequence-only, context-only, and a combination of both. Our findings are striking: the context-only approach consistently and substantially outperforms all other modes. Even more revealing, the inclusion of the raw sequence alongside its high-level context consistently degrades performance, indicating that raw sequences act as informational noise, even for models with specialized tokenization schemes. These results suggest that the primary strength of existing Sci-LLMs lies not in their nascent ability to interpret biomolecular syntax from scratch, but in their profound capacity for reasoning over structured, human-readable knowledge. Therefore, we argue for reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines over expert knowledge. This work lays the foundation for a new class of hybrid scientific AI agents, repositioning the developmental focus from direct sequence interpretation towards high-level knowledge synthesis. The code is available at github.com/opendatalab-raise-dev/CoKE.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Skill Discovery with Foundation Models</title>
<link>https://arxiv.org/abs/2510.23167</link>
<guid>https://arxiv.org/abs/2510.23167</guid>
<content:encoded><![CDATA[
arXiv:2510.23167v1 Announce Type: new 
Abstract: Learning diverse skills without hand-crafted reward functions could accelerate reinforcement learning in downstream tasks. However, existing skill discovery methods focus solely on maximizing the diversity of skills without considering human preferences, which leads to undesirable behaviors and possibly dangerous skills. For instance, a cheetah robot trained using previous methods learns to roll in all directions to maximize skill diversity, whereas we would prefer it to run without flipping or entering hazardous areas. In this work, we propose a Foundation model Guided (FoG) skill discovery method, which incorporates human intentions into skill discovery through foundation models. Specifically, FoG extracts a score function from foundation models to evaluate states based on human intentions, assigning higher values to desirable states and lower to undesirable ones. These scores are then used to re-weight the rewards of skill discovery algorithms. By optimizing the re-weighted skill discovery rewards, FoG successfully learns to eliminate undesirable behaviors, such as flipping or rolling, and to avoid hazardous areas in both state-based and pixel-based tasks. Interestingly, we show that FoG can discover skills involving behaviors that are difficult to define. Interactive visualisations are available from https://sites.google.com/view/submission-fog.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUPO - Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm</title>
<link>https://arxiv.org/abs/2510.23214</link>
<guid>https://arxiv.org/abs/2510.23214</guid>
<content:encoded><![CDATA[
arXiv:2510.23214v1 Announce Type: new 
Abstract: We introduce a novel, drop-in modification to Monte Carlo Tree Search's (MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an automatic action abstraction algorithm that solely relies on reward distribution statistics acquired during the MCTS. Thus, unlike other automatic abstraction algorithms, AUPO requires neither access to transition probabilities nor does AUPO require a directed acyclic search graph to build its abstraction, allowing AUPO to detect symmetric actions that state-of-the-art frameworks like ASAP struggle with when the resulting symmetric states are far apart in state space. Furthermore, as AUPO only affects the decision policy, it is not mutually exclusive with other abstraction techniques that only affect the tree search.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2510.23216</link>
<guid>https://arxiv.org/abs/2510.23216</guid>
<content:encoded><![CDATA[
arXiv:2510.23216v1 Announce Type: new 
Abstract: While several high profile video games have served as testbeds for Deep Reinforcement Learning (DRL), this technique has rarely been employed by the game industry for crafting authentic AI behaviors. Previous research focuses on training super-human agents with large models, which is impractical for game studios with limited resources aiming for human-like agents. This paper proposes a sample-efficient DRL method tailored for training and fine-tuning agents in industrial settings such as the video game industry. Our method improves sample efficiency of value-based DRL by leveraging pre-collected data and increasing network plasticity. We evaluate our method training a goalkeeper agent in EA SPORTS FC 25, one of the best-selling football simulations today. Our agent outperforms the game's built-in AI by 10% in ball saving rate. Ablation studies show that our method trains agents 50% faster compared to standard DRL methods. Finally, qualitative evaluation from domain experts indicates that our approach creates more human-like gameplay compared to hand-crafted agents. As a testimony of the impact of the approach, the method is intended to replace the hand-crafted counterpart in next iterations of the series.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action</title>
<link>https://arxiv.org/abs/2510.23221</link>
<guid>https://arxiv.org/abs/2510.23221</guid>
<content:encoded><![CDATA[
arXiv:2510.23221v1 Announce Type: new 
Abstract: Recent advances in data-driven approaches, such as neural operators (NOs), have shown substantial efficacy in reducing the solution time for integrated circuit (IC) thermal simulations. However, a limitation of these approaches is requiring a large amount of high-fidelity training data, such as chip parameters and temperature distributions, thereby incurring significant computational costs. To address this challenge, we propose a novel algorithm for the generation of IC thermal simulation data, named block Krylov and operator action (BlocKOA), which simultaneously accelerates the data generation process and enhances the precision of generated data. BlocKOA is specifically designed for IC applications. Initially, we use the block Krylov algorithm based on the structure of the heat equation to quickly obtain a few basic solutions. Then we combine them to get numerous temperature distributions that satisfy the physical constraints. Finally, we apply heat operators on these functions to determine the heat source distributions, efficiently generating precise data points. Theoretical analysis shows that the time complexity of BlocKOA is one order lower than the existing method. Experimental results further validate its efficiency, showing that BlocKOA achieves a 420-fold speedup in generating thermal simulation data for 5000 chips with varying physical parameters and IC structures. Even with just 4% of the generation time, data-driven approaches trained on the data generated by BlocKOA exhibits comparable performance to that using the existing method.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2510.23304</link>
<guid>https://arxiv.org/abs/2510.23304</guid>
<content:encoded><![CDATA[
arXiv:2510.23304v1 Announce Type: new 
Abstract: CNOT gates are fundamental to quantum computing, as they facilitate entanglement, a crucial resource for quantum algorithms. Certain classes of quantum circuits are constructed exclusively from CNOT gates. Given their widespread use, it is imperative to minimise the number of CNOT gates employed. This problem, known as CNOT minimisation, remains an open challenge, with its computational complexity yet to be fully characterised. In this work, we introduce a novel reinforcement learning approach to address this task. Instead of training multiple reinforcement learning agents for different circuit sizes, we use a single agent up to a fixed size $m$. Matrices of sizes different from m are preprocessed using either embedding or Gaussian striping. To assess the efficacy of our approach, we trained an agent with m = 8, and evaluated it on matrices of size n that range from 3 to 15. The results we obtained show that our method overperforms the state-of-the-art algorithm as the value of n increases.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps</title>
<link>https://arxiv.org/abs/2510.23340</link>
<guid>https://arxiv.org/abs/2510.23340</guid>
<content:encoded><![CDATA[
arXiv:2510.23340v1 Announce Type: new 
Abstract: Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach</title>
<link>https://arxiv.org/abs/2510.23384</link>
<guid>https://arxiv.org/abs/2510.23384</guid>
<content:encoded><![CDATA[
arXiv:2510.23384v1 Announce Type: new 
Abstract: Opinions are central to almost all human activities and are key influencers of our behaviors. In current times due to growth of social networking website and increase in number of e-commerce site huge amount of opinions are now available on web. Given a set of evaluative statements that contain opinions (or sentiments) about an Entity, opinion mining aims to extract attributes and components of the object that have been commented on in each statement and to determine whether the comments are positive, negative or neutral. While lot of research recently has been done in field of opinion mining and some of it dealing with ranking of entities based on review or opinion set, classifying opinions into finer granularity level and then ranking entities has never been done before. In this paper method for opinion mining from statements at a deeper level of granularity is proposed. This is done by using fuzzy logic reasoning, after which entities are ranked as per this information.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines</title>
<link>https://arxiv.org/abs/2510.23408</link>
<guid>https://arxiv.org/abs/2510.23408</guid>
<content:encoded><![CDATA[
arXiv:2510.23408v1 Announce Type: new 
Abstract: Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens</title>
<link>https://arxiv.org/abs/2510.23410</link>
<guid>https://arxiv.org/abs/2510.23410</guid>
<content:encoded><![CDATA[
arXiv:2510.23410v1 Announce Type: new 
Abstract: Auto-bidding is crucial in facilitating online advertising by automatically providing bids for advertisers. While previous work has made great efforts to model bidding environments for better ad performance, it has limitations in generalizability across environments since these models are typically tailored for specific bidding scenarios. To this end, we approach the scenario-independent principles through a unified function that estimates the achieved effect under specific bids, such as budget consumption, gross merchandise volume (GMV), page views, etc. Then, we propose a bidding foundation model Bid2X to learn this fundamental function from data in various scenarios. Our Bid2X is built over uniform series embeddings that encode heterogeneous data through tailored embedding methods. To capture complex inter-variable and dynamic temporal dependencies in bidding data, we propose two attention mechanisms separately treating embeddings of different variables and embeddings at different times as attention tokens for representation learning. On top of the learned variable and temporal representations, a variable-aware fusion module is used to perform adaptive bidding outcome prediction. To model the unique bidding data distribution, we devise a zero-inflated projection module to incorporate the estimated non-zero probability into its value prediction, which makes up a joint optimization objective containing classification and regression. The objective is proven to converge to the zero-inflated distribution. Our model has been deployed on the ad platform in Taobao, one of the world's largest e-commerce platforms. Offline evaluation on eight datasets exhibits Bid2X's superiority compared to various baselines and its generality across different scenarios. Bid2X increased GMV by 4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding foundation model in computational advertising.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Deep Q Network</title>
<link>https://arxiv.org/abs/2510.23424</link>
<guid>https://arxiv.org/abs/2510.23424</guid>
<content:encoded><![CDATA[
arXiv:2510.23424v1 Announce Type: new 
Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement learning tasks. However, their reliance on associative learning often leads to the acquisition of spurious correlations, hindering their problem-solving capabilities. In this paper, we introduce a novel approach to integrate causal principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational Causal Effect) formula for estimating causal effects. By incorporating causal reasoning during training, our proposed framework enhances the DQN's understanding of the underlying causal structure of the environment, thereby mitigating the influence of confounding factors and spurious correlations. We demonstrate that integrating DQNs with causal capabilities significantly enhances their problem-solving capabilities without compromising performance. Experimental results on standard benchmark environments showcase that our approach outperforms conventional DQNs, highlighting the effectiveness of causal reasoning in reinforcement learning. Overall, our work presents a promising avenue for advancing the capabilities of deep reinforcement learning agents through principled causal inference.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration</title>
<link>https://arxiv.org/abs/2510.23443</link>
<guid>https://arxiv.org/abs/2510.23443</guid>
<content:encoded><![CDATA[
arXiv:2510.23443v1 Announce Type: new 
Abstract: The growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases, statutes, and technical vulnerabilities. This knowledge divide hinders collaboration between legal experts and cybersecurity professionals. To address this important gap, this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyber-legal domain. We demonstrate promising initial results on multilingual tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are the odds? Risk and uncertainty about AI existential risk</title>
<link>https://arxiv.org/abs/2510.23453</link>
<guid>https://arxiv.org/abs/2510.23453</guid>
<content:encoded><![CDATA[
arXiv:2510.23453v1 Announce Type: new 
Abstract: This work is a commentary of the article \href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and Hawthorne. It is not just a commentary though, but a useful reminder of the philosophical limitations of \say{linear} models of risk. The article will focus on the model employed by the authors: first, I discuss some differences between standard Swiss Cheese models and this one. I then argue that in a situation of epistemic indifference the probability of P(D) is higher than what one might first suggest, given the structural relationships between layers. I then distinguish between risk and uncertainty, and argue that any estimation of P(D) is structurally affected by two kinds of uncertainty: option uncertainty and state-space uncertainty. Incorporating these dimensions of uncertainty into our qualitative discussion on AI existential risk can provide a better understanding of the likeliness of P(D).
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Aware Generative AI for Safe, Auditable Data Access Governance</title>
<link>https://arxiv.org/abs/2510.23474</link>
<guid>https://arxiv.org/abs/2510.23474</guid>
<content:encoded><![CDATA[
arXiv:2510.23474v1 Announce Type: new 
Abstract: Enterprises need access decisions that satisfy least privilege, comply with regulations, and remain auditable. We present a policy aware controller that uses a large language model (LLM) to interpret natural language requests against written policies and metadata, not raw data. The system, implemented with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context interpretation, user validation, data classification, business purpose test, compliance mapping, and risk synthesis) with early hard policy gates and deny by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls and a machine readable rationale. We evaluate on fourteen canonical cases across seven scenario families using a privacy preserving benchmark. Results show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny families dropping to 0, and Functional Appropriateness and Compliance Adherence at 14/14. Expert ratings of rationale quality are high, and median latency is under one minute. These findings indicate that policy constrained LLM reasoning, combined with explicit gates and audit trails, can translate human readable policies into safe, compliant, and traceable machine decisions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Collaborative Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2510.23476</link>
<guid>https://arxiv.org/abs/2510.23476</guid>
<content:encoded><![CDATA[
arXiv:2510.23476v1 Announce Type: new 
Abstract: AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy</title>
<link>https://arxiv.org/abs/2510.23487</link>
<guid>https://arxiv.org/abs/2510.23487</guid>
<content:encoded><![CDATA[
arXiv:2510.23487v1 Announce Type: new 
Abstract: This paper establishes a formal equivalence between the architectural classes of modern agentic AI systems and the abstract machines of the Chomsky hierarchy. We posit that the memory architecture of an AI agent is the definitive feature determining its computational power and that it directly maps it to a corresponding class of automaton. Specifically, we demonstrate that simple reflex agents are equivalent to Finite Automata, hierarchical task-decomposition agents are equivalent to Pushdown Automata, and agents employing readable/writable memory for reflection are equivalent to TMs. This Automata-Agent Framework provides a principled methodology for right-sizing agent architectures to optimize computational efficiency and cost. More critically, it creates a direct pathway to formal verification, enables the application of mature techniques from automata theory to guarantee agent safety and predictability. By classifying agents, we can formally delineate the boundary between verifiable systems and those whose behavior is fundamentally undecidable. We address the inherent probabilistic nature of LLM-based agents by extending the framework to probabilistic automata that allow quantitative risk analysis. The paper concludes by outlining an agenda for developing static analysis tools and grammars for agentic frameworks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier</title>
<link>https://arxiv.org/abs/2510.23506</link>
<guid>https://arxiv.org/abs/2510.23506</guid>
<content:encoded><![CDATA[
arXiv:2510.23506v1 Announce Type: new 
Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence</title>
<link>https://arxiv.org/abs/2510.23524</link>
<guid>https://arxiv.org/abs/2510.23524</guid>
<content:encoded><![CDATA[
arXiv:2510.23524v1 Announce Type: new 
Abstract: The rapid advancement of Artificial Intelligence (AI) has led to unprecedented computational demands, raising significant environmental and ethical concerns. This paper critiques the prevailing reliance on large-scale, static datasets and monolithic training paradigms, advocating for a shift toward human-inspired, sustainable AI solutions. We introduce a novel framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware optimization, and human-in-the-loop collaboration to enhance adaptability, efficiency, and accountability. By drawing parallels with biological cognition and leveraging dynamic architectures, HAI seeks to balance performance with ecological responsibility. We detail the theoretical foundations, system design, and operational principles that enable AI to learn continuously and contextually while minimizing carbon footprints and human annotation costs. Our approach addresses pressing challenges in active learning, continual adaptation, and energy-efficient model deployment, offering a pathway toward responsible, human-centered artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning</title>
<link>https://arxiv.org/abs/2510.23532</link>
<guid>https://arxiv.org/abs/2510.23532</guid>
<content:encoded><![CDATA[
arXiv:2510.23532v1 Announce Type: new 
Abstract: Designing models that can learn to reason in a systematic way is an important and long-standing challenge. In recent years, a wide range of solutions have been proposed for the specific case of systematic relational reasoning, including Neuro-Symbolic approaches, variants of the Transformer architecture, and specialised Graph Neural Networks. However, existing benchmarks for systematic relational reasoning focus on an overly simplified setting, based on the assumption that reasoning can be reduced to composing relational paths. In fact, this assumption is hard-baked into the architecture of several recent models, leading to approaches that can perform well on existing benchmarks but are difficult to generalise to other settings. To support further progress in the field of systematic relational reasoning with neural networks, we introduce NoRA, a new benchmark which adds several levels of difficulty and requires models to go beyond path-based reasoning.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence</title>
<link>https://arxiv.org/abs/2510.23538</link>
<guid>https://arxiv.org/abs/2510.23538</guid>
<content:encoded><![CDATA[
arXiv:2510.23538v1 Announce Type: new 
Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoPret: An Ontology for the Interpretation of Human Behavior</title>
<link>https://arxiv.org/abs/2510.23553</link>
<guid>https://arxiv.org/abs/2510.23553</guid>
<content:encoded><![CDATA[
arXiv:2510.23553v1 Announce Type: new 
Abstract: As human machine teaming becomes central to paradigms like Industry 5.0, a critical need arises for machines to safely and effectively interpret complex human behaviors. A research gap currently exists between techno centric robotic frameworks, which often lack nuanced models of human behavior, and descriptive behavioral ontologies, which are not designed for real time, collaborative interpretation. This paper addresses this gap by presenting OntoPret, an ontology for the interpretation of human behavior. Grounded in cognitive science and a modular engineering methodology, OntoPret provides a formal, machine processable framework for classifying behaviors, including task deviations and deceptive actions. We demonstrate its adaptability across two distinct use cases manufacturing and gameplay and establish the semantic foundations necessary for advanced reasoning about human intentions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCode: Unify Plan and Action for Universal Granularity Control</title>
<link>https://arxiv.org/abs/2510.23564</link>
<guid>https://arxiv.org/abs/2510.23564</guid>
<content:encoded><![CDATA[
arXiv:2510.23564v1 Announce Type: new 
Abstract: Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study</title>
<link>https://arxiv.org/abs/2510.23578</link>
<guid>https://arxiv.org/abs/2510.23578</guid>
<content:encoded><![CDATA[
arXiv:2510.23578v1 Announce Type: new 
Abstract: The rapid adoption of generative artificial intelligence (GenAI) technologies has led many organizations to integrate AI into their products and services, often without considering user preferences. Yet, public attitudes toward AI use, especially in impactful decision-making scenarios, are underexplored. Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488) representative of the Swiss population, we examine shifts in public attitudes toward AI before and after the launch of ChatGPT. We find that the GenAI boom is significantly associated with reduced public acceptance of AI (see Figure 1) and increased demand for human oversight in various decision-making contexts. The proportion of respondents finding AI "not acceptable at all" increased from 23% to 30%, while support for human-only decision-making rose from 18% to 26%. These shifts have amplified existing social inequalities in terms of widened educational, linguistic, and gender gaps post-boom. Our findings challenge industry assumptions about public readiness for AI deployment and highlight the critical importance of aligning technological development with evolving public preferences.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Evolve: LLM Self-Improve through Co-evolution</title>
<link>https://arxiv.org/abs/2510.23595</link>
<guid>https://arxiv.org/abs/2510.23595</guid>
<content:encoded><![CDATA[
arXiv:2510.23595v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&amp;A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alita-G: Self-Evolving Generative Agent for Agent Generation</title>
<link>https://arxiv.org/abs/2510.23601</link>
<guid>https://arxiv.org/abs/2510.23601</guid>
<content:encoded><![CDATA[
arXiv:2510.23601v1 Announce Type: new 
Abstract: Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present ALITA-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into an MCP Box. At inference time, ALITA-G performs retrieval-augmented MCP selection with the help of each tool's descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. ALITA-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI enhanced approach to the tree unimodality conjecture</title>
<link>https://arxiv.org/abs/2510.18826</link>
<guid>https://arxiv.org/abs/2510.18826</guid>
<content:encoded><![CDATA[
arXiv:2510.18826v2 Announce Type: cross 
Abstract: Given a graph $G$, its independence sequence is the integral sequence $a_1,a_2,...,a_n$, where $a_i$ is the number of independent sets of vertices of size i. In the late 80's Alavi, Erdos, Malde, Schwenk showed that this sequence need not be unimodal for general graphs, but conjectured that it is always unimodal whenever $G$ is a tree. This conjecture was then naturally generalized to claim that the independence sequence of trees should be log concave, in the sense that $a_i^2$ is always above $a_{i-1}a_{i+1}$. This conjecture stood for many years, until in 2023, Kadrawi, Levit, Yosef, and Mizrachi proved that there were exactly two trees on 26 vertices whose independence sequence was not log concave. In this paper, we use the AI architecture PatternBoost, developed by Charton, Ellenberg, Wagner, and Williamson to train a machine to find counter-examples to the log-concavity conjecture. We will discuss the successes of this approach - finding tens of thousands of new counter-examples to log-concavity with vertex set sizes varying from 27 to 101 - and some of its fascinating failures.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills</title>
<link>https://arxiv.org/abs/2510.19898</link>
<guid>https://arxiv.org/abs/2510.19898</guid>
<content:encoded><![CDATA[
arXiv:2510.19898v1 Announce Type: cross 
Abstract: High quality bugs are key to training the next generation of language model based software engineering (SWE) agents. We introduce a novel method for synthetic generation of difficult and diverse bugs. Our method instructs SWE Agents to introduce a feature into the codebase whereby they may unintentionally break tests, resulting in bugs. Prior approaches often induce an out-of-distribution effect by generating bugs intentionally (e.g. by introducing local perturbation to existing code), which does not reflect realistic development processes. We perform qualitative analysis to demonstrate that our approach for generating bugs more closely reflects the patterns found in human-authored edits. Through extensive experiments, we demonstrate that our bugs provide more efficient training data for supervised fine-tuning, outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k bugs). We train on our newly generated bugs in addition to existing bug datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over three seeds.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems</title>
<link>https://arxiv.org/abs/2510.21710</link>
<guid>https://arxiv.org/abs/2510.21710</guid>
<content:encoded><![CDATA[
arXiv:2510.21710v1 Announce Type: cross 
Abstract: Instant payment infrastructures have stringent performance requirements, processing millions of transactions daily with zero-downtime expectations. Traditional monitoring approaches fail to bridge the gap between technical infrastructure metrics and business process visibility. We introduce a novel feature engineering approach based on processing times computed between consecutive ISO 20022 message exchanges, creating a compact representation of system state. By applying anomaly detection to these features, we enable early failure detection and localization, allowing incident classification. Experimental evaluation on the TARGET Instant Payment Settlement (TIPS) system, using both real-world incidents and controlled simulations, demonstrates the approach's effectiveness in detecting diverse anomaly patterns and provides inherently interpretable explanations that enable operators to understand the business impact. By mapping features to distinct processing phases, the resulting framework differentiates between internal and external payment system issues, significantly reduces investigation time, and bridges observability gaps in distributed systems where transaction state is fragmented across multiple entities.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling</title>
<link>https://arxiv.org/abs/2510.21712</link>
<guid>https://arxiv.org/abs/2510.21712</guid>
<content:encoded><![CDATA[
arXiv:2510.21712v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal methodology for enhancing Large Language Models (LLMs) through the dynamic integration of external knowledge. To further improve RAG's flexibility, Agentic RAG introduces autonomous agents into the workflow. However, Agentic RAG faces several challenges: (1) the success of each step depends on both high-quality planning and accurate search, (2) the lack of supervision for intermediate reasoning steps, and (3) the exponentially large candidate space for planning and searching. To address these challenges, we propose DecoupleSearch, a novel framework that decouples planning and search processes using dual value models, enabling independent optimization of plan reasoning and search grounding. Our approach constructs a reasoning tree, where each node represents planning and search steps. We leverage Monte Carlo Tree Search to assess the quality of each step. During inference, Hierarchical Beam Search iteratively refines planning and search candidates with dual value models. Extensive experiments across policy models of varying parameter sizes, demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond IVR Touch-Tones: Customer Intent Routing using LLMs</title>
<link>https://arxiv.org/abs/2510.21715</link>
<guid>https://arxiv.org/abs/2510.21715</guid>
<content:encoded><![CDATA[
arXiv:2510.21715v1 Announce Type: cross 
Abstract: Widespread frustration with rigid touch-tone Interactive Voice Response (IVR) systems for customer service underscores the need for more direct and intuitive language interaction. While speech technologies are necessary, the key challenge lies in routing intents from user phrasings to IVR menu paths, a task where Large Language Models (LLMs) show strong potential. Progress, however, is limited by data scarcity, as real IVR structures and interactions are often proprietary. We present a novel LLM-based methodology to address this gap. Using three distinct models, we synthesized a realistic 23-node IVR structure, generated 920 user intents (230 base and 690 augmented), and performed the routing task. We evaluate two prompt designs: descriptive hierarchical menus and flattened path representations, across both base and augmented datasets. Results show that flattened paths consistently yield higher accuracy, reaching 89.13% on the base dataset compared to 81.30% with the descriptive format, while augmentation introduces linguistic noise that slightly reduces performance. Confusion matrix analysis further suggests that low-performing routes may reflect not only model limitations but also redundancies in menu design. Overall, our findings demonstrate proof-of-concept that LLMs can enable IVR routing through a smoother, more seamless user experience -- moving customer service one step ahead of touch-tone menus.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enhanced Operator Assistance for UNICOS Applications</title>
<link>https://arxiv.org/abs/2510.21717</link>
<guid>https://arxiv.org/abs/2510.21717</guid>
<content:encoded><![CDATA[
arXiv:2510.21717v1 Announce Type: cross 
Abstract: This project explores the development of an AI-enhanced operator assistant for UNICOS, CERN's UNified Industrial Control System. While powerful, UNICOS presents a number of challenges, including the cognitive burden of decoding widgets, manual effort required for root cause analysis, and difficulties maintainers face in tracing datapoint elements (DPEs) across a complex codebase. In situations where timely responses are critical, these challenges can increase cognitive load and slow down diagnostics. To address these issues, a multi-agent system was designed and implemented. The solution is supported by a modular architecture comprising a UNICOS-side extension written in CTRL code, a Python-based multi-agent system deployed on a virtual machine, and a vector database storing both operator documentation and widget animation code. Preliminary evaluations suggest that the system is capable of decoding widgets, performing root cause analysis by leveraging live device data and documentation, and tracing DPEs across a complex codebase. Together, these capabilities reduce the manual workload of operators and maintainers, enhance situational awareness in operations, and accelerate responses to alarms and anomalies. Beyond these immediate gains, this work highlights the potential of introducing multi-modal reasoning and retrieval augmented generation (RAG) into the domain of industrial control. Ultimately, this work represents more than a proof of concept: it provides a basis for advancing intelligent operator interfaces at CERN. By combining modular design, extensibility, and practical AI integration, this project not only alleviates current operator pain points but also points toward broader opportunities for assistive AI in accelerator operations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAMER PAT: Research as a Serious Game</title>
<link>https://arxiv.org/abs/2510.21719</link>
<guid>https://arxiv.org/abs/2510.21719</guid>
<content:encoded><![CDATA[
arXiv:2510.21719v1 Announce Type: cross 
Abstract: As generative AI increasingly outperforms students in producing academic writing, a critical question arises: how can we preserve the motivation, creativity, and intellectual growth of novice researchers in an age of automated academic achievement? This paper introduces GAMER PAT (GAme MastER, Paper Authoring Tutor), a prompt-engineered AI chatbot that reframes research paper writing as a serious game. Through role-playing mechanics, users interact with a co-author NPC and anonymous reviewer NPCs, turning feedback into "missions" and advancing through a narrative-driven writing process.
  Our study reports on 26+ gameplay chat logs, including both autoethnography and use by graduate students under supervision. Using qualitative log analysis with SCAT (Steps for Coding and Theorization), we identified an emergent four-phase scaffolding pattern: (1) question posing, (2) meta-perspective, (3) structuring, and (4) recursive reflection. These results suggest that GAMER PAT supports not only the structural development of research writing but also reflective and motivational aspects.
  We present this work as a descriptive account of concept and process, not a causal evaluation. We also include a speculative outlook envisioning how humans may continue to cultivate curiosity and agency alongside AI-driven research. This arXiv version thus provides both a descriptive report of design and usage, and a forward-looking provocation for future empirical studies.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AquaVLM: Improving Underwater Situation Awareness with Mobile Vision Language Models</title>
<link>https://arxiv.org/abs/2510.21722</link>
<guid>https://arxiv.org/abs/2510.21722</guid>
<content:encoded><![CDATA[
arXiv:2510.21722v1 Announce Type: cross 
Abstract: Underwater activities like scuba diving enable millions annually to explore marine environments for recreation and scientific research. Maintaining situational awareness and effective communication are essential for diver safety. Traditional underwater communication systems are often bulky and expensive, limiting their accessibility to divers of all levels. While recent systems leverage lightweight smartphones and support text messaging, the messages are predefined and thus restrict context-specific communication.
  In this paper, we present AquaVLM, a tap-and-send underwater communication system that automatically generates context-aware messages and transmits them using ubiquitous smartphones. Our system features a mobile vision-language model (VLM) fine-tuned on an auto-generated underwater conversation dataset and employs a hierarchical message generation pipeline. We co-design the VLM and transmission, incorporating error-resilient fine-tuning to improve the system's robustness to transmission errors. We develop a VR simulator to enable users to experience AquaVLM in a realistic underwater environment and create a fully functional prototype on the iOS platform for real-world experiments. Both subjective and objective evaluations validate the effectiveness of AquaVLM and highlight its potential for personal underwater communication as well as broader mobile VLM applications.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Dense Retriever is Secretly an Expeditious Reasoner</title>
<link>https://arxiv.org/abs/2510.21727</link>
<guid>https://arxiv.org/abs/2510.21727</guid>
<content:encoded><![CDATA[
arXiv:2510.21727v1 Announce Type: cross 
Abstract: Dense retrievers enhance retrieval by encoding queries and documents into continuous vectors, but they often struggle with reasoning-intensive queries. Although Large Language Models (LLMs) can reformulate queries to capture complex reasoning, applying them universally incurs significant computational cost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query rewriting framework. Within this framework, a Reasoner Router dynamically directs each query to either fast dense reasoning or deep LLM reasoning. The dense reasoning is achieved by the Dense Reasoner, which performs LLM-style reasoning directly in the embedding space, enabling a controllable trade-off between efficiency and accuracy. Experiments on large-scale retrieval benchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while preserving-or even improving-retrieval performance by 7%.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Bias Evolution in Fashion Recommender Systems: A System Dynamics Approach</title>
<link>https://arxiv.org/abs/2510.21728</link>
<guid>https://arxiv.org/abs/2510.21728</guid>
<content:encoded><![CDATA[
arXiv:2510.21728v1 Announce Type: cross 
Abstract: Bias in recommender systems not only distorts user experience but also perpetuates and amplifies existing societal stereotypes, particularly in sectors like fashion e-commerce. This study employs a dynamic modeling approach to scrutinize the mechanisms of bias activation and reinforcement within Fashion Recommender Systems (FRS). By leveraging system dynamics modeling and experimental simulations, we dissect the temporal evolution of bias and its multifaceted impacts on system performance. Our analysis reveals that inductive biases exert a more substantial influence on system outcomes than user biases, suggesting critical areas for intervention. We demonstrate that while current debiasing strategies, including data rebalancing and algorithmic regularization, are effective to an extent, they require further enhancement to comprehensively mitigate biases. This research underscores the necessity for advancing these strategies and extending system boundaries to incorporate broader contextual factors such as user demographics and item diversity, aiming to foster inclusivity and fairness in FRS. The findings advocate for a proactive approach in recommender system design to counteract bias propagation and ensure equitable user experiences.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora</title>
<link>https://arxiv.org/abs/2510.21729</link>
<guid>https://arxiv.org/abs/2510.21729</guid>
<content:encoded><![CDATA[
arXiv:2510.21729v1 Announce Type: cross 
Abstract: Dense embedding models have become critical for modern information retrieval, particularly in RAG pipelines, but their performance often degrades when applied to specialized corpora outside their pre-training distribution. To address thi we introduce \textbf{CustomIR}, a framework for unsupervised adaptation of pre-trained language embedding models to domain-specific corpora using synthetically generated query-document pairs. CustomIR leverages large language models (LLMs) to create diverse queries grounded in a known target corpus, paired with LLM-verified hard negatives, eliminating the need for costly human annotation. Experiments on enterprise email and messaging datasets show that CustomIR consistently improves retrieval effectiveness with small models gaining up to 2.3 points in Recall@10. This performance increase allows these small models to rival the performance of much larger alternatives, allowing for cheaper RAG deployments. These results highlight that targeted synthetic fine-tuning offers a scalable and cost-efficient strategy for increasing domain-specific performance.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data</title>
<link>https://arxiv.org/abs/2510.21735</link>
<guid>https://arxiv.org/abs/2510.21735</guid>
<content:encoded><![CDATA[
arXiv:2510.21735v1 Announce Type: cross 
Abstract: Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit distinct vehicle dynamics. EVs provide rapid acceleration, with electric motors producing peak power across a wider speed range, and achieve swift deceleration through regenerative braking. While existing microscopic models effectively capture the driving behavior of ICE vehicles, a modeling framework that accurately describes the unique car-following dynamics of EVs is lacking. Developing such a model is essential given the increasing presence of EVs in traffic, yet creating an easy-to-use and accurate analytical model remains challenging.
  To address these gaps, this study develops and validates a Phase-Aware AI (PAAI) car-following model specifically for EVs. The proposed model enhances traditional physics-based frameworks with an AI component that recognizes and adapts to different driving phases, such as rapid acceleration and regenerative braking. Using real-world trajectory data from vehicles equipped with adaptive cruise control (ACC), we conduct comprehensive simulations to validate the model's performance. The numerical results demonstrate that the PAAI model significantly improves prediction accuracy over traditional car-following models, providing an effective tool for accurately representing EV behavior in traffic simulations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn2Drive: A neural network-based framework for socially compliant automated vehicle control</title>
<link>https://arxiv.org/abs/2510.21736</link>
<guid>https://arxiv.org/abs/2510.21736</guid>
<content:encoded><![CDATA[
arXiv:2510.21736v1 Announce Type: cross 
Abstract: This study introduces a novel control framework for adaptive cruise control (ACC) in automated driving, leveraging Long Short-Term Memory (LSTM) networks and physics-informed constraints. As automated vehicles (AVs) adopt advanced features like ACC, transportation systems are becoming increasingly intelligent and efficient. However, existing AV control strategies primarily focus on optimizing the performance of individual vehicles or platoons, often neglecting their interactions with human-driven vehicles (HVs) and the broader impact on traffic flow. This oversight can exacerbate congestion and reduce overall system efficiency. To address this critical research gap, we propose a neural network-based, socially compliant AV control framework that incorporates social value orientation (SVO). This framework enables AVs to account for their influence on HVs and traffic dynamics. By leveraging AVs as mobile traffic regulators, the proposed approach promotes adaptive driving behaviors that reduce congestion, improve traffic efficiency, and lower energy consumption. Within this framework, we define utility functions for both AVs and HVs, which are optimized based on the SVO of each AV to balance its own control objectives with broader traffic flow considerations. Numerical results demonstrate the effectiveness of the proposed method in adapting to varying traffic conditions, thereby enhancing system-wide efficiency. Specifically, when the AV's control mode shifts from prioritizing energy consumption to optimizing traffic flow efficiency, vehicles in the following platoon experience at least a 58.99% increase in individual energy consumption alongside at least a 38.39% improvement in individual average speed, indicating significant enhancements in traffic dynamics.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-Generation LLM for UAV: From Natural Language to Autonomous Flight</title>
<link>https://arxiv.org/abs/2510.21739</link>
<guid>https://arxiv.org/abs/2510.21739</guid>
<content:encoded><![CDATA[
arXiv:2510.21739v1 Announce Type: cross 
Abstract: With the rapid advancement of Large Language Models (LLMs), their capabilities in various automation domains, particularly Unmanned Aerial Vehicle (UAV) operations, have garnered increasing attention. Current research remains predominantly constrained to small-scale UAV applications, with most studies focusing on isolated components such as path planning for toy drones, while lacking comprehensive investigation of medium- and long-range UAV systems in real-world operational contexts. Larger UAV platforms introduce distinct challenges, including stringent requirements for airport-based take-off and landing procedures, adherence to complex regulatory frameworks, and specialized operational capabilities with elevated mission expectations. This position paper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive demonstration and automation roadmap for integrating LLMs into multi-scale UAV operations. The NeLV system processes natural language instructions to orchestrate short-, medium-, and long-range UAV missions through five key technical components: (i) LLM-as-Parser for instruction interpretation, (ii) Route Planner for Points of Interest (POI) determination, (iii) Path Planner for waypoint generation, (iv) Control Platform for executable trajectory implementation, and (v) UAV monitoring. We demonstrate the system's feasibility through three representative use cases spanning different operational scales: multi-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the current implementation, we establish a five-level automation taxonomy that charts the evolution from current LLM-as-Parser capabilities (Level 1) to fully autonomous LLM-as-Autopilot systems (Level 5), identifying technical prerequisites and research challenges at each stage.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21740</link>
<guid>https://arxiv.org/abs/2510.21740</guid>
<content:encoded><![CDATA[
arXiv:2510.21740v1 Announce Type: cross 
Abstract: Data visualizations are vital components of many scientific articles and news stories. Current vision-language models (VLMs) still struggle on basic data visualization understanding tasks, but the causes of failure remain unclear. Are VLM failures attributable to limitations in how visual information in the data visualization is encoded, how information is transferred between the vision and language modules, or how information is processed within the language module? We developed FUGU, a suite of data visualization understanding tasks, to precisely characterize potential sources of difficulty (e.g., extracting the position of data points, distances between them, and other summary statistics). We used FUGU to investigate three widely used VLMs. To diagnose the sources of errors produced by these models, we used activation patching and linear probes to trace information flow through models across a variety of prompting strategies. We found that some models fail to generate the coordinates of individual data points correctly, and these initial errors often lead to erroneous final responses. When these models are provided with the correct coordinates, performance improves substantially. Moreover, even when the model generates an incorrect response, the correct coordinates can be successfully read out from the latent representations in the vision encoder, suggesting that the source of these errors lies in the vision-language handoff. We further found that while providing correct coordinates helps with tasks involving one or a small number of data points, it generally worsens performance for tasks that require extracting statistical relationships across many data points. Fine-tuning models on FUGU also fails to yield ceiling performance. These findings point to architectural constraints in current VLMs that might pose significant challenges for reliable data visualization understanding.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception</title>
<link>https://arxiv.org/abs/2510.21761</link>
<guid>https://arxiv.org/abs/2510.21761</guid>
<content:encoded><![CDATA[
arXiv:2510.21761v1 Announce Type: cross 
Abstract: We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot perception by providing detailed object attribute annotations within Japanese human-robot dialogue scenarios. J-ORA is designed to support three critical perception tasks, object identification, reference resolution, and next-action prediction, by leveraging a comprehensive template of attributes (e.g., category, color, shape, size, material, and spatial relations). Extensive evaluations with both proprietary and open-source Vision Language Models (VLMs) reveal that incorporating detailed object attributes substantially improves multimodal perception performance compared to without object attributes. Despite the improvement, we find that there still exists a gap between proprietary and open-source VLMs. In addition, our analysis of object affordances demonstrates varying abilities in understanding object functionality and contextual relationships across different VLMs. These findings underscore the importance of rich, context-sensitive attribute annotations in advancing robot perception in dynamic environments. See project page at https://jatuhurrra.github.io/J-ORA/.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proportion and Perspective Control for Flow-Based Image Generation</title>
<link>https://arxiv.org/abs/2510.21763</link>
<guid>https://arxiv.org/abs/2510.21763</guid>
<content:encoded><![CDATA[
arXiv:2510.21763v1 Announce Type: cross 
Abstract: While modern text-to-image diffusion models generate high-fidelity images, they offer limited control over the spatial and geometric structure of the output. To address this, we introduce and evaluate two ControlNets specialized for artistic control: (1) a proportion ControlNet that uses bounding boxes to dictate the position and scale of objects, and (2) a perspective ControlNet that employs vanishing lines to control the 3D geometry of the scene. We support the training of these modules with data pipelines that leverage vision-language models for annotation and specialized algorithms for conditioning image synthesis. Our experiments demonstrate that both modules provide effective control but exhibit limitations with complex constraints. Both models are released on HuggingFace: https://huggingface.co/obvious-research
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment</title>
<link>https://arxiv.org/abs/2510.21774</link>
<guid>https://arxiv.org/abs/2510.21774</guid>
<content:encoded><![CDATA[
arXiv:2510.21774v1 Announce Type: cross 
Abstract: We present OCR-Quality, a comprehensive human-annotated dataset designed for evaluating and developing OCR quality assessment methods. The dataset consists of 1,000 PDF pages converted to PNG images at 300 DPI, sampled from diverse real-world scenarios, including academic papers, textbooks, e-books, and multilingual documents. Each document has been processed using state-of-the-art Vision-Language Models (VLMs) and manually annotated with quality scores using a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). The dataset includes detailed source information, annotation guidelines, and representative cases across various difficulty levels. OCR-Quality addresses the critical need for reliable OCR quality assessment in real-world applications and provides a valuable benchmark for training and evaluating OCR verification systems. The dataset is publicly available at https://huggingface.co/datasets/Aslan-mingye/OCR-Quality .
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Face-MakeUpV2: Facial Consistency Learning for Controllable Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2510.21775</link>
<guid>https://arxiv.org/abs/2510.21775</guid>
<content:encoded><![CDATA[
arXiv:2510.21775v1 Announce Type: cross 
Abstract: In facial image generation, current text-to-image models often suffer from facial attribute leakage and insufficient physical consistency when responding to local semantic instructions. In this study, we propose Face-MakeUpV2, a facial image generation model that aims to maintain the consistency of face ID and physical characteristics with the reference image. First, we constructed a large-scale dataset FaceCaptionMask-1M comprising approximately one million image-text-masks pairs that provide precise spatial supervision for the local semantic instructions. Second, we employed a general text-to-image pretrained model as the backbone and introduced two complementary facial information injection channels: a 3D facial rendering channel to incorporate the physical characteristics of the image and a global facial feature channel. Third, we formulated two optimization objectives for the supervised learning of our model: semantic alignment in the model's embedding space to mitigate the attribute leakage problem and perceptual loss on facial images to preserve ID consistency. Extensive experiments demonstrated that our Face-MakeUpV2 achieves best overall performance in terms of preserving face ID and maintaining physical consistency of the reference images. These results highlight the practical potential of Face-MakeUpV2 for reliable and controllable facial editing in diverse applications.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Causes Postoperative Aspiration?</title>
<link>https://arxiv.org/abs/2510.21779</link>
<guid>https://arxiv.org/abs/2510.21779</guid>
<content:encoded><![CDATA[
arXiv:2510.21779v1 Announce Type: cross 
Abstract: Background: Aspiration, the inhalation of foreign material into the lungs, significantly impacts surgical patient morbidity and mortality. This study develops a machine learning (ML) model to predict postoperative aspiration, enabling timely preventative interventions.
  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we identified 826 surgical patients (mean age: 62, 55.7\% male) who experienced aspiration within seven days post-surgery, along with a matched non-aspiration cohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were trained using pre-surgical hospitalization data to predict postoperative aspiration. To investigate causation, we estimated Average Treatment Effects (ATE) using Augmented Inverse Probability Weighting.
  Results: Our ML model achieved an AUROC of 0.86 and 77.3\% sensitivity on a held-out test set. Maximum daily opioid dose, length of stay, and patient age emerged as the most important predictors. ATE analysis identified significant causative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/- 0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men were 1.5 times more likely to aspirate and received 27\% higher maximum daily opioid dosages compared to women.
  Conclusion: ML models can effectively predict postoperative aspiration risk, enabling targeted preventative measures. Maximum daily opioid dosage and operative site significantly influence aspiration risk. The gender disparity in both opioid administration and aspiration rates warrants further investigation. These findings have important implications for improving postoperative care protocols and aspiration prevention strategies.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast Cancer Detection</title>
<link>https://arxiv.org/abs/2510.21780</link>
<guid>https://arxiv.org/abs/2510.21780</guid>
<content:encoded><![CDATA[
arXiv:2510.21780v1 Announce Type: cross 
Abstract: In this study, we present an interpretable deep learning framework for the early detection of breast cancer using quantitative features extracted from digitized fine needle aspirate (FNA) images of breast masses. Our deep neural network, using ReLU activations, the Adam optimizer, and a binary cross-entropy loss, delivers state-of-the-art classification performance, achieving an accuracy of 0.992, precision of 1.000, recall of 0.977, and an F1 score of 0.988. These results substantially exceed the benchmarks reported in the literature. We evaluated the model under identical protocols against a suite of well-established algorithms (logistic regression, decision trees, random forests, stochastic gradient descent, K-nearest neighbors, and XGBoost) and found the deep model consistently superior on the same metrics. Recognizing that high predictive accuracy alone is insufficient for clinical adoption due to the black-box nature of deep learning models, we incorporated model-agnostic Explainable AI techniques such as SHAP and LIME to produce feature-level attributions and human-readable visualizations. These explanations quantify the contribution of each feature to individual predictions, support error analysis, and increase clinician trust, thus bridging the gap between performance and interpretability for real-world clinical use. The concave points feature of the cell nuclei is found to be the most influential feature positively impacting the classification task. This insight can be very helpful in improving the diagnosis and treatment of breast cancer by highlighting the key characteristics of breast tumor.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeSync: Accelerating Edge-Model Updates for Data Drift through Adaptive Continuous Learning</title>
<link>https://arxiv.org/abs/2510.21781</link>
<guid>https://arxiv.org/abs/2510.21781</guid>
<content:encoded><![CDATA[
arXiv:2510.21781v1 Announce Type: cross 
Abstract: Real-time video analytics systems typically deploy lightweight models on edge devices to reduce latency. However, the distribution of data features may change over time due to various factors such as changing lighting and weather conditions, leading to decreased model accuracy. Recent frameworks try to address this issue by leveraging remote servers to continuously train and adapt lightweight edge models using more complex models in the cloud. Despite these advancements, existing methods face two key challenges: first, the retraining process is compute-intensive, causing significant delays in model updates; second, the new model may not align well with the evolving data distribution of the current video stream. To address these challenges, we introduce EdgeSync, an efficient edge-model updating approach that enhances sample filtering by incorporating timeliness and inference results, thus ensuring training samples are more relevant to the current video content while reducing update delays. Additionally, EdgeSync features a dynamic training management module that optimizes the timing and sequencing of model updates to improve their timeliness. Evaluations on diverse and complex real-world datasets demonstrate that EdgeSync improves accuracy by approximately 3.4% compared to existing methods and by about 10% compared to traditional approaches.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models</title>
<link>https://arxiv.org/abs/2510.21783</link>
<guid>https://arxiv.org/abs/2510.21783</guid>
<content:encoded><![CDATA[
arXiv:2510.21783v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated powerful performance in generating high-quality images. A typical example is text-to-image generator like Stable Diffusion. However, their widespread use also poses potential privacy risks. A key concern is membership inference attacks, which attempt to determine whether a particular data sample was used in the model training process. We propose an efficient membership inference attack method against diffusion models. This method is based on the injection of slight noise and the evaluation of the aggregation degree of the noise distribution. The intuition is that the noise prediction patterns of diffusion models for training set samples and non-training set samples exhibit distinguishable differences.Specifically, we suppose that member images exhibit higher aggregation of predicted noise around a certain time step of the diffusion process. In contrast, the predicted noises of non-member images exhibit a more discrete characteristic around the certain time step. Compared with other existing methods, our proposed method requires fewer visits to the target diffusion model. We inject slight noise into the image under test and then determine its membership by analyzing the aggregation degree of the noise distribution predicted by the model. Empirical findings indicate that our method achieves superior performance across multiple datasets. At the same time, our method can also show better attack effects in ASR and AUC when facing large-scale text-to-image diffusion models, proving the scalability of our method.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction</title>
<link>https://arxiv.org/abs/2510.21786</link>
<guid>https://arxiv.org/abs/2510.21786</guid>
<content:encoded><![CDATA[
arXiv:2510.21786v1 Announce Type: cross 
Abstract: Script event induction, which aims to predict the subsequent event based on the context, is a challenging task in NLP, achieving remarkable success in practical applications. However, human events are mostly recorded and presented in the form of videos rather than scripts, yet there is a lack of related research in the realm of vision. To address this problem, we introduce AVEP (Action-centric Video Event Prediction), a task that distinguishes itself from existing video prediction tasks through its incorporation of more complex logic and richer semantic information. We present a large structured dataset, which consists of about $35K$ annotated videos and more than $178K$ video clips of event, built upon existing video event datasets to support this task. The dataset offers more fine-grained annotations, where the atomic unit is represented as a multimodal event argument node, providing better structured representations of video events. Due to the complexity of event structures, traditional visual models that take patches or frames as input are not well-suited for AVEP. We propose EventFormer, a node-graph hierarchical attention based video event prediction model, which can capture both the relationships between events and their arguments and the coreferencial relationships between arguments. We conducted experiments using several SOTA video prediction models as well as LVLMs on AVEP, demonstrating both the complexity of the task and the value of the dataset. Our approach outperforms all these video prediction models. We will release the dataset and code for replicating the experiments and annotations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making</title>
<link>https://arxiv.org/abs/2510.21788</link>
<guid>https://arxiv.org/abs/2510.21788</guid>
<content:encoded><![CDATA[
arXiv:2510.21788v1 Announce Type: cross 
Abstract: We explore the use of expert-guided bandit learning, which we refer to as online mixture-of-experts (OMoE). In this setting, given a context, a candidate committee of experts must determine how to aggregate their outputs to achieve optimal results in terms of aggregate accuracy. We propose two algorithms to address this problem. The first algorithm combines aggregate voting with UCB-driven successive elimination, efficiently pruning suboptimal exploration actions. The second algorithm employs an online weighted-majority-voting mechanism, leveraging the respective voting power of each expert proportional to their predictive power. We derive theoretical guarantees for the regret properties in the bandit setting under ideal circumstances, and empirical results are provided accordingly. As a modern study on applications, these methods are applied to the online fine-tuning of a set of expert large language models (LLMs), where after each response, the generative LLM dynamically reweighs its set of experts and/or selects the optimal committee of experts to generate the most accurate response. Our results introduce new methodologies and no-regret guarantees for combining multiple experts to improve on the performance of the an aggregate model overall.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variance-Reduction Guidance: Sampling Trajectory Optimization for Diffusion Models</title>
<link>https://arxiv.org/abs/2510.21792</link>
<guid>https://arxiv.org/abs/2510.21792</guid>
<content:encoded><![CDATA[
arXiv:2510.21792v1 Announce Type: cross 
Abstract: Diffusion models have become emerging generative models. Their sampling process involves multiple steps, and in each step the models predict the noise from a noisy sample. When the models make prediction, the output deviates from the ground truth, and we call such a deviation as \textit{prediction error}. The prediction error accumulates over the sampling process and deteriorates generation quality. This paper introduces a novel technique for statistically measuring the prediction error and proposes the Variance-Reduction Guidance (VRG) method to mitigate this error. VRG does not require model fine-tuning or modification. Given a predefined sampling trajectory, it searches for a new trajectory which has the same number of sampling steps but produces higher quality results. VRG is applicable to both conditional and unconditional generation. Experiments on various datasets and baselines demonstrate that VRG can significantly improve the generation quality of diffusion models. Source code is available at https://github.com/shifengxu/VRG.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D_3D Feature Fusion via Cross-Modal Latent Synthesis and Attention Guided Restoration for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.21793</link>
<guid>https://arxiv.org/abs/2510.21793</guid>
<content:encoded><![CDATA[
arXiv:2510.21793v1 Announce Type: cross 
Abstract: Industrial anomaly detection (IAD) increasingly benefits from integrating 2D and 3D data, but robust cross-modal fusion remains challenging. We propose a novel unsupervised framework, Multi-Modal Attention-Driven Fusion Restoration (MAFR), which synthesises a unified latent space from RGB images and point clouds using a shared fusion encoder, followed by attention-guided, modality-specific decoders. Anomalies are localised by measuring reconstruction errors between input features and their restored counterparts. Evaluations on the MVTec 3D-AD and Eyecandies benchmarks demonstrate that MAFR achieves state-of-the-art results, with a mean I-AUROC of 0.972 and 0.901, respectively. The framework also exhibits strong performance in few-shot learning settings, and ablation studies confirm the critical roles of the fusion architecture and composite loss. MAFR offers a principled approach for fusing visual and geometric information, advancing the robustness and accuracy of industrial anomaly detection. Code is available at https://github.com/adabrh/MAFR
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Inference-Time Alignment for Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21794</link>
<guid>https://arxiv.org/abs/2510.21794</guid>
<content:encoded><![CDATA[
arXiv:2510.21794v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have become essential backbones of modern multimodal intelligence, yet their outputs remain prone to hallucination-plausible text misaligned with visual inputs. Existing alignment approaches often rely on expensive fine-tuning with annotated preference data or sequence-level inference strategies that provide only coarse, delayed feedback. To overcome these limitations, we present TITA (Token-level Inference-Time Alignment), a lightweight framework that freezes the base VLM and instead trains a reward model to approximate its distribution. During inference, implicit preference signals are extracted as log-probability ratios between the reward model and the target VLM, yielding dense autoregressive feedback. This formulation can be viewed as an inference-time variant of Direct Preference Optimization (DPO), providing token-level corrective signals without retraining the backbone. Extensive evaluations on LLaVA-1.5-7B and 13B show consistent gains across 12 benchmarks, with improvements of 8.6% on MMVet and 6.7% on POPE, indicating stronger general understanding and reduced hallucinations. Additional experiments on Qwen2.5-VL-7B and DeepSeek-VL2-27.5B show comparable gains, especially in hallucination reduction and VQA accuracy, while incurring negligible inference overhead.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention</title>
<link>https://arxiv.org/abs/2510.21795</link>
<guid>https://arxiv.org/abs/2510.21795</guid>
<content:encoded><![CDATA[
arXiv:2510.21795v1 Announce Type: cross 
Abstract: The rapid advancement of time series foundation models (TSFMs) has been propelled by migrating architectures from language models. While existing TSFMs demonstrate impressive performance, their direct adoption of cross-domain architectures constrains effective capture of multiscale temporal dependencies inherent to time series data. This limitation becomes particularly pronounced during zero-shot transfer across datasets with divergent underlying patterns and sampling strategies. To address these challenges, we propose Hierarchical Interleaved Block Attention (HIBA) which employs hierarchical inter- and intra-block sparse attention to effectively capture multi-scale dependencies. Intra-block attention facilitates local information exchange, and inter-block attention operates across blocks to capture global temporal pattern interaction and dynamic evolution. Leveraging the HIBA architecture, we introduce Xihe, a scalable TSFM family spanning from an ultra-efficient 9.5M parameter configuration to high-capacity 1.5B variant. Evaluated on the comprehensive GIFT-Eval benchmark, our most compact Xihe-tiny model (9.5M) surpasses the majority of contemporary TSFMs, demonstrating remarkable parameter efficiency. More impressively, Xihe-max (1.5B) establishes new state-of-the-art zero-shot performance, surpassing previous best results by a substantial margin. This consistent performance excellence across the entire parameter spectrum provides compelling evidence for the exceptional generalization capabilities and architectural superiority of HIBA.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Guided AI Cascaded Corrector Model Significantly Extends Madden-Julian Oscillation Prediction Skill</title>
<link>https://arxiv.org/abs/2510.21796</link>
<guid>https://arxiv.org/abs/2510.21796</guid>
<content:encoded><![CDATA[
arXiv:2510.21796v1 Announce Type: cross 
Abstract: The Madden-Julian Oscillation (MJO) is an important driver of global weather and climate extremes, but its prediction in operational dynamical models remains challenging, with skillful forecasts typically limited to 3-4 weeks. Here, we introduce a novel deep learning framework, the Physics-guided Cascaded Corrector for MJO (PCC-MJO), which acts as a universal post-processor to correct MJO forecasts from dynamical models. This two-stage model first employs a physics-informed 3D U-Net to correct spatial-temporal field errors, then refines the MJO's RMM index using an LSTM optimized for forecast skill. When applied to three different operational forecasts from CMA, ECMWF and NCEP, our unified framework consistently extends the skillful forecast range (bivariate correlation > 0.5) by 2-8 days. Crucially, the model effectively mitigates the "Maritime Continent barrier", enabling more realistic eastward propagation and amplitude. Explainable AI analysis quantitatively confirms that the model's decision-making is spatially congruent with observed MJO dynamics (correlation > 0.93), demonstrating that it learns physically meaningful features rather than statistical fittings. Our work provides a promising physically consistent, computationally efficient, and highly generalizable pathway to break through longstanding barriers in subseasonal forecasting.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning</title>
<link>https://arxiv.org/abs/2510.21797</link>
<guid>https://arxiv.org/abs/2510.21797</guid>
<content:encoded><![CDATA[
arXiv:2510.21797v1 Announce Type: cross 
Abstract: Current mainstream approaches to addressing multimodal imbalance primarily focus on architectural modifications and optimization-based, often overlooking a quantitative analysis of the imbalance degree between modalities. To address this gap, our work introduces a novel method for the quantitative analysis of multi-modal imbalance, which in turn informs the design of a sample-level adaptive loss function.We begin by defining the "Modality Gap" as the difference between the Softmax scores of different modalities (e.g., audio and visual) for the ground-truth class prediction. Analysis of the Modality Gap distribution reveals that it can be effectively modeled by a bimodal Gaussian Mixture Model (GMM). These two components are found to correspond respectively to "modality-balanced" and "modality-imbalanced" data samples. Subsequently, we apply Bayes' theorem to compute the posterior probability of each sample belonging to these two distinct distributions.Informed by this quantitative analysis, we design a novel adaptive loss function with three objectives: (1) to minimize the overall Modality Gap; (2) to encourage the imbalanced sample distribution to shift towards the balanced one; and (3) to apply greater penalty weights to imbalanced samples. We employ a two-stage training strategy consisting of a warm-up phase followed by an adaptive training phase.Experimental results demonstrate that our approach achieves state-of-the-art (SOTA) performance on the public CREMA-D and AVE datasets, attaining accuracies of $80.65\%$ and $70.90\%$, respectively. This validates the effectiveness of our proposed methodology.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffGRM: Diffusion-based Generative Recommendation Model</title>
<link>https://arxiv.org/abs/2510.21805</link>
<guid>https://arxiv.org/abs/2510.21805</guid>
<content:encoded><![CDATA[
arXiv:2510.21805v1 Announce Type: cross 
Abstract: Generative recommendation (GR) is an emerging paradigm that represents each item via a tokenizer as an n-digit semantic ID (SID) and predicts the next item by autoregressively generating its SID conditioned on the user's history. However, two structural properties of SIDs make ARMs ill-suited. First, intra-item consistency: the n digits jointly specify one item, yet the left-to-right causality trains each digit only under its prefix and blocks bidirectional cross-digit evidence, collapsing supervision to a single causal path. Second, inter-digit heterogeneity: digits differ in semantic granularity and predictability, while the uniform next-token objective assigns equal weight to all digits, overtraining easy digits and undertraining hard digits. To address these two issues, we propose DiffGRM, a diffusion-based GR model that replaces the autoregressive decoder with a masked discrete diffusion model (MDM), thereby enabling bidirectional context and any-order parallel generation of SID digits for recommendation. Specifically, we tailor DiffGRM in three aspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple digits and balance per-digit information; (2) training with On-policy Coherent Noising (OCN) that prioritizes uncertain digits via coherent masking to concentrate supervision on high-value signals; and (3) inference with Confidence-guided Parallel Denoising (CPD) that fills higher-confidence digits first and generates diverse Top-K candidates. Experiments show consistent gains over strong generative and discriminative recommendation baselines on multiple datasets, improving NDCG@10 by 6.9%-15.5%. Code is available at https://github.com/liuzhao09/DiffGRM.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frame-Difference Guided Dynamic Region Perception for CLIP Adaptation in Text-Video Retrieval</title>
<link>https://arxiv.org/abs/2510.21806</link>
<guid>https://arxiv.org/abs/2510.21806</guid>
<content:encoded><![CDATA[
arXiv:2510.21806v1 Announce Type: cross 
Abstract: With the rapid growth of video data, text-video retrieval technology has become increasingly important in numerous application scenarios such as recommendation and search. Early text-video retrieval methods suffer from two critical drawbacks: first, they heavily rely on large-scale annotated video-text pairs, leading to high data acquisition costs; second, there is a significant modal gap between video and text features, which limits cross-modal alignment accuracy. With the development of vision-language model, adapting CLIP to video tasks has attracted great attention. However, existing adaptation methods generally lack enhancement for dynamic video features and fail to effectively suppress static redundant features. To address this issue, this paper proposes FDA-CLIP (Frame Difference Alpha-CLIP), which is a concise CLIP-based training framework for text-video alignment. Specifically, the method uses frame differences to generate dynamic region masks, which are input into Alpha-CLIP as an additional Alpha channel. This proactively guides the model to focus on semantically critical dynamic regions while suppressing static background redundancy. Experiments demonstrate that frame difference-guided video semantic encoding can effectively balance retrieval efficiency and accuracy.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs</title>
<link>https://arxiv.org/abs/2510.21807</link>
<guid>https://arxiv.org/abs/2510.21807</guid>
<content:encoded><![CDATA[
arXiv:2510.21807v1 Announce Type: cross 
Abstract: Recent breakthroughs in reasoning models have markedly advanced the reasoning capabilities of large language models, particularly via training on tasks with verifiable rewards. Yet, a significant gap persists in their adaptation to real world multimodal scenarios, most notably, vision language tasks, due to a heavy focus on single modal language settings. While efforts to transplant reinforcement learning techniques from NLP to VLMs have emerged, these approaches often remain confined to perception centric tasks or reduce images to textual summaries, failing to fully exploit visual context and commonsense knowledge, ultimately constraining the generalization of reasoning capabilities across diverse multimodal environments. To address this limitation, we introduce a novel fine tuning task, Masked Prediction via Context and Commonsense, which forces models to integrate visual context and commonsense reasoning by reconstructing semantically meaningful content from occluded images, thereby laying the foundation for generalized reasoning. To systematically evaluate the model performance in generalized reasoning, we developed a specialized evaluation benchmark, MPCC Eval, and employed various fine tuning strategies to guide reasoning. Among these, we introduced an innovative training method, Reinforcement Fine tuning with Prior Sampling, which not only enhances model performance but also improves its generalized reasoning capabilities in OOD and cross task scenarios.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Relation-Enhanced CLIP Adapter for Domain Adaptive Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2510.21808</link>
<guid>https://arxiv.org/abs/2510.21808</guid>
<content:encoded><![CDATA[
arXiv:2510.21808v1 Announce Type: cross 
Abstract: The high cost of data annotation has spurred research on training deep learning models in data-limited scenarios. Existing paradigms, however, fail to balance cross-domain transfer and cross-category generalization, giving rise to the demand for Domain-Adaptive Zero-Shot Learning (DAZSL). Although vision-language models (e.g., CLIP) have inherent advantages in the DAZSL field, current studies do not fully exploit their potential. Applying CLIP to DAZSL faces two core challenges: inefficient cross-category knowledge transfer due to the lack of semantic relation guidance, and degraded cross-modal alignment during target domain fine-tuning. To address these issues, we propose a Semantic Relation-Enhanced CLIP (SRE-CLIP) Adapter framework, integrating a Semantic Relation Structure Loss and a Cross-Modal Alignment Retention Strategy. As the first CLIP-based DAZSL method, SRE-CLIP achieves state-of-the-art performance on the I2AwA and I2WebV benchmarks, significantly outperforming existing approaches.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Deep Learning Framework for Enhanced Diabetic Retinopathy Detection: Integrating Traditional Features with AI-driven Insights</title>
<link>https://arxiv.org/abs/2510.21810</link>
<guid>https://arxiv.org/abs/2510.21810</guid>
<content:encoded><![CDATA[
arXiv:2510.21810v1 Announce Type: cross 
Abstract: Diabetic Retinopathy (DR), a vision-threatening complication of Dia-betes Mellitus (DM), is a major global concern, particularly in India, which has one of the highest diabetic populations. Prolonged hyperglycemia damages reti-nal microvasculature, leading to DR symptoms like microaneurysms, hemor-rhages, and fluid leakage, which, if undetected, cause irreversible vision loss. Therefore, early screening is crucial as DR is asymptomatic in its initial stages. Fundus imaging aids precise diagnosis by detecting subtle retinal lesions. This paper introduces a hybrid diagnostic framework combining traditional feature extraction and deep learning (DL) to enhance DR detection. While handcrafted features capture key clinical markers, DL automates hierarchical pattern recog-nition, improving early diagnosis. The model synergizes interpretable clinical data with learned features, surpassing standalone DL approaches that demon-strate superior classification and reduce false negatives. This multimodal AI-driven approach enables scalable, accurate DR screening, crucial for diabetes-burdened regions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Object Detection Algorithms for Surface Defect Detection</title>
<link>https://arxiv.org/abs/2510.21811</link>
<guid>https://arxiv.org/abs/2510.21811</guid>
<content:encoded><![CDATA[
arXiv:2510.21811v1 Announce Type: cross 
Abstract: This article compares the performance of six prominent object detection algorithms, YOLOv11, RetinaNet, Fast R-CNN, YOLOv8, RT-DETR, and DETR, on the NEU-DET surface defect detection dataset, comprising images representing various metal surface defects, a crucial application in industrial quality control. Each model's performance was assessed regarding detection accuracy, speed, and robustness across different defect types such as scratches, inclusions, and rolled-in scales. YOLOv11, a state-of-the-art real-time object detection algorithm, demonstrated superior performance compared to the other methods, achieving a remarkable 70% higher accuracy on average. This improvement can be attributed to YOLOv11s enhanced feature extraction capabilities and ability to process the entire image in a single forward pass, making it faster and more efficient in detecting minor surface defects. Additionally, YOLOv11's architecture optimizations, such as improved anchor box generation and deeper convolutional layers, contributed to more precise localization of defects. In conclusion, YOLOv11's outstanding performance in accuracy and speed solidifies its position as the most effective model for surface defect detection on the NEU dataset, surpassing competing algorithms by a substantial margin.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation</title>
<link>https://arxiv.org/abs/2510.21812</link>
<guid>https://arxiv.org/abs/2510.21812</guid>
<content:encoded><![CDATA[
arXiv:2510.21812v1 Announce Type: cross 
Abstract: Recommender systems have long been built upon the modeling of interactions between users and items, while recent studies have sought to broaden this paradigm by generalizing to new users and items, incorporating diverse information sources, and transferring knowledge across domains. Nevertheless, these efforts have largely focused on individual aspects, hindering their ability to tackle the complex recommendation scenarios that arise in daily consumptions across diverse domains. In this paper, we present MICRec, a unified framework that fuses inductive modeling, multimodal guidance, and cross-domain transfer to capture user contexts and latent preferences in heterogeneous and incomplete real-world data. Moving beyond the inductive backbone of INMO, our model refines expressive representations through modality-based aggregation and alleviates data sparsity by leveraging overlapping users as anchors across domains, thereby enabling robust and generalizable recommendation. Experiments show that MICRec outperforms 12 baselines, with notable gains in domains with limited training data.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image Time Series Modelling</title>
<link>https://arxiv.org/abs/2510.21813</link>
<guid>https://arxiv.org/abs/2510.21813</guid>
<content:encoded><![CDATA[
arXiv:2510.21813v1 Announce Type: cross 
Abstract: Earth Observation (EO) Foundation Modelling (FM) holds great promise for simplifying and improving the use of EO data for diverse real-world tasks. However, most existing models require additional adaptation before they can be used and are structured rigidly around particular data sources or training approaches. To address this, we take inspiration from large language models, where diverse tasks, both pre-training and downstream, are implicitly captured through next-token prediction over unified token sequences, leveraging the structure and diversity of the training data.
  We introduce SITS-DECO (Satellite Image Time Series-DECoder Only), a proof-of-concept generative model that applies this unified-sequence framing to EO data. Using a simple GPT-style decoder-only architecture, and demonstrate its ability to perform useful EO tasks (pixel-wise, multi-temporal, multi-modal crop-type classification) in a purely generative framework. Through symbolic prompting, we show that the model can perform multiple supervised and self-supervised tasks within a single unified architecture, without task- or modality-specific adaptation. Despite its simplicity and lack of spatial context, SITS-DECO outperforms much larger EO foundation models on crop-type classification (PASTIS-R) demonstrating that dense temporal sequence modelling is a critical missing ingredient in the current paradigm.
  This work exemplifies a data-centric modelling paradigm in which capability arises from the diversity and structure of the training data rather than from architectural complexity. SITS-DECO provides a lightweight, practical route to multi-modal, multi-task EO modelling, and a conceptual bridge toward future generative EO foundation models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</title>
<link>https://arxiv.org/abs/2510.21814</link>
<guid>https://arxiv.org/abs/2510.21814</guid>
<content:encoded><![CDATA[
arXiv:2510.21814v1 Announce Type: cross 
Abstract: Free-form gesture understanding is highly appealing for human-computer interaction, as it liberates users from the constraints of predefined gesture categories. However, the sole existing solution GestureGPT suffers from limited recognition accuracy and slow response times. In this paper, we propose Gestura, an end-to-end system for free-form gesture understanding. Gestura harnesses a pre-trained Large Vision-Language Model (LVLM) to align the highly dynamic and diverse patterns of free-form gestures with high-level semantic concepts. To better capture subtle hand movements across different styles, we introduce a Landmark Processing Module that compensate for LVLMs' lack of fine-grained domain knowledge by embedding anatomical hand priors. Further, a Chain-of-Thought (CoT) reasoning strategy enables step-by-step semantic inference, transforming shallow knowledge into deep semantic understanding and significantly enhancing the model's ability to interpret ambiguous or unconventional gestures. Together, these components allow Gestura to achieve robust and adaptable free-form gesture comprehension. Additionally, we have developed the first open-source dataset for free-form gesture intention reasoning and understanding with over 300,000 annotated QA pairs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDR Image Reconstruction using an Unsupervised Fusion Model</title>
<link>https://arxiv.org/abs/2510.21815</link>
<guid>https://arxiv.org/abs/2510.21815</guid>
<content:encoded><![CDATA[
arXiv:2510.21815v1 Announce Type: cross 
Abstract: High Dynamic Range (HDR) imaging aims to reproduce the wide range of brightness levels present in natural scenes, which the human visual system can perceive but conventional digital cameras often fail to capture due to their limited dynamic range. To address this limitation, we propose a deep learning-based multi-exposure fusion approach for HDR image generation. The method takes a set of differently exposed Low Dynamic Range (LDR) images, typically an underexposed and an overexposed image, and learns to fuse their complementary information using a convolutional neural network (CNN). The underexposed image preserves details in bright regions, while the overexposed image retains information in dark regions; the network effectively combines these to reconstruct a high-quality HDR output. The model is trained in an unsupervised manner, without relying on ground-truth HDR images, making it practical for real-world applications where such data is unavailable. We evaluate our results using the Multi-Exposure Fusion Structural Similarity Index Measure (MEF-SSIM) and demonstrate that our approach achieves superior visual quality compared to existing fusion methods. A customized loss function is further introduced to improve reconstruction fidelity and optimize model performance.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Biomedical Insights: Hierarchical Attention Networks for High-Dimensional Data Interpretation</title>
<link>https://arxiv.org/abs/2510.21820</link>
<guid>https://arxiv.org/abs/2510.21820</guid>
<content:encoded><![CDATA[
arXiv:2510.21820v1 Announce Type: cross 
Abstract: The proliferation of high-dimensional datasets in fields such as genomics, healthcare, and finance has created an urgent need for machine learning models that are both highly accurate and inherently interpretable. While traditional deep learning approaches deliver strong predictive performance, their lack of transparency often impedes their deployment in critical, decision-sensitive applications. In this work, we introduce the Hierarchical Attention-based Interpretable Network (HAIN), a novel architecture that unifies multi-level attention mechanisms, dimensionality reduction, and explanation-driven loss functions to deliver interpretable and robust analysis of complex biomedical data. HAIN provides feature-level interpretability via gradientweighted attention and offers global model explanations through prototype-based representations. Comprehensive evaluation on The Cancer Genome Atlas (TCGA) dataset demonstrates that HAIN achieves a classification accuracy of 94.3%, surpassing conventional post-hoc interpretability approaches such as SHAP and LIME in both transparency and explanatory power. Furthermore, HAIN effectively identifies biologically relevant cancer biomarkers, supporting its utility for clinical and research applications. By harmonizing predictive accuracy with interpretability, HAIN advances the development of transparent AI solutions for precision medicine and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt fidelity of ChatGPT4o / Dall-E3 text-to-image visualisations</title>
<link>https://arxiv.org/abs/2510.21821</link>
<guid>https://arxiv.org/abs/2510.21821</guid>
<content:encoded><![CDATA[
arXiv:2510.21821v1 Announce Type: cross 
Abstract: This study examines the prompt fidelity of ChatGPT4o / DALL-E3 text-to-image visualisations by analysing whether attributes explicitly specified in autogenously generated prompts are correctly rendered in the resulting images. Using two public-domain datasets comprising 200 visualisations of women working in the cultural and creative industries and 230 visualisations of museum curators, the study assessed accuracy across personal attributes (age, hair), appearance (attire, glasses), and paraphernalia (name tags, clipboards). While correctly rendered in most cases, DALL-E3 deviated from prompt specifications in 15.6% of all attributes (n=710). Errors were lowest for paraphernalia, moderate for personal appearance, and highest for depictions of the person themselves, particularly age. These findings demonstrate measurable prompt-to-image fidelity gaps with implications for bias detection and model evaluation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet-based GAN Fingerprint Detection using ResNet50</title>
<link>https://arxiv.org/abs/2510.21822</link>
<guid>https://arxiv.org/abs/2510.21822</guid>
<content:encoded><![CDATA[
arXiv:2510.21822v1 Announce Type: cross 
Abstract: Identifying images generated by Generative Adversarial Networks (GANs) has become a significant challenge in digital image forensics. This research presents a wavelet-based detection method that uses discrete wavelet transform (DWT) preprocessing and a ResNet50 classification layer to differentiate the StyleGAN-generated images from real ones. Haar and Daubechies wavelet filters are applied to convert the input images into multi-resolution representations, which will then be fed to a ResNet50 network for classification, capitalizing on subtle artifacts left by the generative process. Moreover, the wavelet-based models are compared to an identical ResNet50 model trained on spatial data. The Haar and Daubechies preprocessed models achieved a greater accuracy of 93.8 percent and 95.1 percent, much higher than the model developed in the spatial domain (accuracy rate of 81.5 percent). The Daubechies-based model outperforms Haar, showing that adding layers of descriptive frequency patterns can lead to even greater distinguishing power. These results indicate that the GAN-generated images have unique wavelet-domain artifacts or "fingerprints." The method proposed illustrates the effectiveness of wavelet-domain analysis to detect GAN images and emphasizes the potential of further developing the capabilities of future deepfake detection systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Deep Learning in Medical Imaging: Brain Tumor and Pneumonia Detection</title>
<link>https://arxiv.org/abs/2510.21823</link>
<guid>https://arxiv.org/abs/2510.21823</guid>
<content:encoded><![CDATA[
arXiv:2510.21823v1 Announce Type: cross 
Abstract: Deep Learning (DL) holds enormous potential for improving medical imaging diagnostics, yet the lack of interpretability in most models hampers clinical trust and adoption. This paper presents an explainable deep learning framework for detecting brain tumors in MRI scans and pneumonia in chest X-ray images using two leading Convolutional Neural Networks, ResNet50 and DenseNet121. These models were trained on publicly available Kaggle datasets comprising 7,023 brain MRI images and 5,863 chest X-ray images, achieving high classification performance. DenseNet121 consistently outperformed ResNet50 with 94.3 percent vs. 92.5 percent accuracy for brain tumors and 89.1 percent vs. 84.4 percent accuracy for pneumonia. For better explainability, Gradient-weighted Class Activation Mapping (Grad-CAM) was integrated to create heatmap visualizations superimposed on the test images, indicating the most influential image regions in the decision-making process. Interestingly, while both models produced accurate results, Grad-CAM showed that DenseNet121 consistently focused on core pathological regions, whereas ResNet50 sometimes scattered attention to peripheral or non-pathological areas. Combining deep learning and explainable AI offers a promising path toward reliable, interpretable, and clinically useful diagnostic tools.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise classification of low quality G-banded Chromosome Images by reliability metrics and data pruning classifier</title>
<link>https://arxiv.org/abs/2510.21827</link>
<guid>https://arxiv.org/abs/2510.21827</guid>
<content:encoded><![CDATA[
arXiv:2510.21827v1 Announce Type: cross 
Abstract: In the last decade, due to high resolution cameras and accurate meta-phase analyzes, the accuracy of chromosome classification has improved substantially. However, current Karyotyping systems demand large number of high quality train data to have an adequately plausible Precision per each chromosome. Such provision of high quality train data with accurate devices are not yet accomplished in some out-reached pathological laboratories. To prevent false positive detections in low-cost systems and low-quality images settings, this paper improves the classification Precision of chromosomes using proposed reliability thresholding metrics and deliberately engineered features. The proposed method has been evaluated using a variation of deep Alex-Net neural network, SVM, K Nearest-Neighbors, and their cascade pipelines to an automated filtering of semi-straight chromosome. The classification results have highly improved over 90% for the chromosomes with more common defections and translocations. Furthermore, a comparative analysis over the proposed thresholding metrics has been conducted and the best metric is bolded with its salient characteristics. The high Precision results provided for a very low-quality G-banding database verifies suitability of the proposed metrics and pruning method for Karyotyping facilities in poor countries and lowbudget pathological laboratories.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPO: Group Adaptive Policy Optimization for Real-World Code Edit</title>
<link>https://arxiv.org/abs/2510.21830</link>
<guid>https://arxiv.org/abs/2510.21830</guid>
<content:encoded><![CDATA[
arXiv:2510.21830v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods like GRPO are popular for their critic-free, normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable outliers, leading to distorted advantage computation and increased noise. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an outlier-free highest-density interval (HDI) per prompt and then uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation. This adaptive Q robustly handles skewed distributions while remaining plug-and-play and efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a large internal dataset of 51,844 real-world, history-aware code-editing tasks across 10 languages, demonstrating consistent improvements in exact match accuracy over GRPO and its variant DAPO. Code is publicly available.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal, Multitask System for Generating E Commerce Text Listings from Images</title>
<link>https://arxiv.org/abs/2510.21835</link>
<guid>https://arxiv.org/abs/2510.21835</guid>
<content:encoded><![CDATA[
arXiv:2510.21835v1 Announce Type: cross 
Abstract: Manually generating catchy descriptions and names is labor intensive and a slow process for retailers. Although generative AI provides an automation solution in form of Vision to Language Models (VLM), the current VLMs are prone to factual "hallucinations". Siloed, single task models are not only inefficient but also fail to capture interdependent relationships between features. To address these challenges, we propose an end to end, multi task system that generates factually grounded textual listings from a single image. The contributions of this study are two proposals for the model architecture. First, application of multi task learning approach for fine tuning a vision encoder where a single vision backbone is jointly trained on attribute prediction such as color, hemline and neck style and price regression. Second, introduction of a hierarchical generation process where the model's own predicted attributes are embedded in a prompt and fed to the text decoder to improve factual consistency. The experiments demonstrate the superiority of this architecture. The multi tasking approach outperforms both the independent price regression, with a 3.6% better R2 Value and attribute classification, with a 6.6% improvement F1 score. Critically, the hierarchical generation process proves highly effective, slashing the factual hallucination rate from 12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical ablation. The hierarchical approach also reduces the latency of the autoregressive text generation process by a factor of 3.5 when compared to direct vision to language model of similar size. One minor caveat is that the model does perform 3.5% worse than direct vision-to-language model on ROUGE-L score.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating ChatGPT's Performance in Classifying Pneumonia from Chest X-Ray Images</title>
<link>https://arxiv.org/abs/2510.21839</link>
<guid>https://arxiv.org/abs/2510.21839</guid>
<content:encoded><![CDATA[
arXiv:2510.21839v1 Announce Type: cross 
Abstract: In this study, we evaluate the ability of OpenAI's gpt-4o model to classify chest X-ray images as either NORMAL or PNEUMONIA in a zero-shot setting, without any prior fine-tuning. A balanced test set of 400 images (200 from each class) was used to assess performance across four distinct prompt designs, ranging from minimal instructions to detailed, reasoning-based prompts. The results indicate that concise, feature-focused prompts achieved the highest classification accuracy of 74\%, whereas reasoning-oriented prompts resulted in lower performance. These findings highlight that while ChatGPT exhibits emerging potential for medical image interpretation, its diagnostic reliability remains limited. Continued advances in visual reasoning and domain-specific adaptation are required before such models can be safely applied in clinical practice.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach</title>
<link>https://arxiv.org/abs/2510.21846</link>
<guid>https://arxiv.org/abs/2510.21846</guid>
<content:encoded><![CDATA[
arXiv:2510.21846v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) test whether a data point was part of a model's training set, posing serious privacy risks. Existing methods often depend on shadow models or heavy query access, which limits their practicality. We propose GP-MIA, an efficient and interpretable approach based on Gaussian process (GP) meta-modeling. Using post-hoc metrics such as accuracy, entropy, dataset statistics, and optional sensitivity features (e.g. gradients, NTK measures) from a single trained model, GP-MIA trains a GP classifier to distinguish members from non-members while providing calibrated uncertainty estimates. Experiments on synthetic data, real-world fraud detection data, CIFAR-10, and WikiText-2 show that GP-MIA achieves high accuracy and generalizability, offering a practical alternative to existing MIAs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TowerVision: Understanding and Improving Multilinguality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21849</link>
<guid>https://arxiv.org/abs/2510.21849</guid>
<content:encoded><![CDATA[
arXiv:2510.21849v1 Announce Type: cross 
Abstract: Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poisson Flow Consistency Training</title>
<link>https://arxiv.org/abs/2510.21857</link>
<guid>https://arxiv.org/abs/2510.21857</guid>
<content:encoded><![CDATA[
arXiv:2510.21857v1 Announce Type: cross 
Abstract: The Poisson Flow Consistency Model (PFCM) is a consistency-style model based on the robust Poisson Flow Generative Model++ (PFGM++) which has achieved success in unconditional image generation and CT image denoising. Yet the PFCM can only be trained in distillation which limits the potential of the PFCM in many data modalities. The objective of this research was to create a method to train the PFCM in isolation called Poisson Flow Consistency Training (PFCT). The perturbation kernel was leveraged to remove the pretrained PFGM++, and the sinusoidal discretization schedule and Beta noise distribution were introduced in order to facilitate adaptability and improve sample quality. The model was applied to the task of low dose computed tomography image denoising and improved the low dose image in terms of LPIPS and SSIM. It also displayed similar denoising effectiveness as models like the Consistency Model. PFCT is established as a valid method of training the PFCM from its effectiveness in denoising CT images, showing potential with competitive results to other generative models. Further study is needed in the precise optimization of PFCT and in its applicability to other generative modeling tasks. The framework of PFCT creates more flexibility for the ways in which a PFCM can be created and can be applied to the field of generative modeling.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-preserving Decision-focused Learning for Multi-energy Systems</title>
<link>https://arxiv.org/abs/2510.21858</link>
<guid>https://arxiv.org/abs/2510.21858</guid>
<content:encoded><![CDATA[
arXiv:2510.21858v1 Announce Type: cross 
Abstract: Decision-making for multi-energy system (MES) dispatch depends on accurate load forecasting. Traditionally, load forecasting and decision-making for MES are implemented separately. Forecasting models are typically trained to minimize forecasting errors, overlooking their impact on downstream decision-making. To address this, decision-focused learning (DFL) has been studied to minimize decision-making costs instead. However, practical adoption of DFL in MES faces significant challenges: the process requires sharing sensitive load data and model parameters across multiple sectors, raising serious privacy issues. To this end, we propose a privacy-preserving DFL framework tailored for MES. Our approach introduces information masking to safeguard private data while enabling recovery of decision variables and gradients required for model training. To further enhance security for DFL, we design a safety protocol combining matrix decomposition and homomorphic encryption, effectively preventing collusion and unauthorized data access. Additionally, we developed a privacy-preserving load pattern recognition algorithm, enabling the training of specialized DFL models for heterogeneous load patterns. Theoretical analysis and comprehensive case studies, including real-world MES data, demonstrate that our framework not only protects privacy but also consistently achieves lower average daily dispatch costs compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence</title>
<link>https://arxiv.org/abs/2510.21860</link>
<guid>https://arxiv.org/abs/2510.21860</guid>
<content:encoded><![CDATA[
arXiv:2510.21860v1 Announce Type: cross 
Abstract: We present Butter-Bench, a benchmark evaluating large language model (LLM) controlled robots for practical intelligence, defined as the ability to navigate the messiness of the physical world. Current state-of-the-art robotic systems use a hierarchical architecture with LLMs in charge of high-level reasoning, and a Vision Language Action (VLA) model for low-level control. Butter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs have repeatedly surpassed humans in evaluations requiring analytical intelligence, we find humans still outperform LLMs on Butter-Bench. The best LLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs struggled the most with multi-step spatial planning and social understanding. We also evaluate LLMs that are fine-tuned for embodied reasoning and conclude that this training does not improve their score on Butter-Bench.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems</title>
<link>https://arxiv.org/abs/2510.21861</link>
<guid>https://arxiv.org/abs/2510.21861</guid>
<content:encoded><![CDATA[
arXiv:2510.21861v1 Announce Type: cross 
Abstract: Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model</title>
<link>https://arxiv.org/abs/2510.21862</link>
<guid>https://arxiv.org/abs/2510.21862</guid>
<content:encoded><![CDATA[
arXiv:2510.21862v1 Announce Type: cross 
Abstract: Engineering drawings are fundamental to manufacturing communication, serving as the primary medium for conveying design intent, tolerances, and production details. However, interpreting complex multi-view drawings with dense annotations remains challenging using manual methods, generic optical character recognition (OCR) systems, or traditional deep learning approaches, due to varied layouts, orientations, and mixed symbolic-textual content. To address these challenges, this paper proposes a three-stage hybrid framework for the automated interpretation of 2D multi-view engineering drawings using modern detection and vision language models (VLMs). In the first stage, YOLOv11-det performs layout segmentation to localize key regions such as views, title blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware, fine-grained detection of annotations, including measures, GD&amp;T symbols, and surface roughness indicators. The third stage employs two Donut-based, OCR-free VLMs for semantic content parsing: the Alphabetical VLM extracts textual and categorical information from title blocks and notes, while the Numerical VLM interprets quantitative data such as measures, GD&amp;T frames, and surface roughness. Two specialized datasets were developed to ensure robustness and generalization: 1,000 drawings for layout detection and 1,406 for annotation-level training. The Alphabetical VLM achieved an overall F1 score of 0.672, while the Numerical VLM reached 0.963, demonstrating strong performance in textual and quantitative interpretation, respectively. The unified JSON output enables seamless integration with CAD and manufacturing databases, providing a scalable solution for intelligent engineering drawing analysis.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs</title>
<link>https://arxiv.org/abs/2510.21867</link>
<guid>https://arxiv.org/abs/2510.21867</guid>
<content:encoded><![CDATA[
arXiv:2510.21867v1 Announce Type: cross 
Abstract: Accurate and reliable motion forecasting is essential for the safe deployment of autonomous vehicles (AVs), particularly in rare but safety-critical scenarios known as corner cases. Existing models often underperform in these situations due to an over-representation of common scenes in training data and limited generalization capabilities. To address this limitation, we present WM-MoE, the first world model-based motion forecasting framework that unifies perception, temporal memory, and decision making to address the challenges of high-risk corner-case scenarios. The model constructs a compact scene representation that explains current observations, anticipates future dynamics, and evaluates the outcomes of potential actions. To enhance long-horizon reasoning, we leverage large language models (LLMs) and introduce a lightweight temporal tokenizer that maps agent trajectories and contextual cues into the LLM's feature space without additional training, enriching temporal context and commonsense priors. Furthermore, a mixture-of-experts (MoE) is introduced to decompose complex corner cases into subproblems and allocate capacity across scenario types, and a router assigns scenes to specialized experts that infer agent intent and perform counterfactual rollouts. In addition, we introduce nuScenes-corner, a new benchmark that comprises four real-world corner-case scenarios for rigorous evaluation. Extensive experiments on four benchmark datasets (nuScenes, NGSIM, HighD, and MoCAD) showcase that WM-MoE consistently outperforms state-of-the-art (SOTA) baselines and remains robust under corner-case and data-missing conditions, indicating the promise of world model-based architectures for robust and generalizable motion forecasting in fully AVs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer</title>
<link>https://arxiv.org/abs/2510.21872</link>
<guid>https://arxiv.org/abs/2510.21872</guid>
<content:encoded><![CDATA[
arXiv:2510.21872v1 Announce Type: cross 
Abstract: Music generation in the audio domain using artificial intelligence (AI) has witnessed steady progress in recent years. However for some instruments, particularly the guitar, controllable instrument synthesis remains limited in expressivity. We introduce GuitarFlow, a model designed specifically for electric guitar synthesis. The generative process is guided using tablatures, an ubiquitous and intuitive guitar-specific symbolic format. The tablature format easily represents guitar-specific playing techniques (e.g. bends, muted strings and legatos), which are more difficult to represent in other common music notation formats such as MIDI. Our model relies on an intermediary step of first rendering the tablature to audio using a simple sample-based virtual instrument, then performing style transfer using Flow Matching in order to transform the virtual instrument audio into more realistic sounding examples. This results in a model that is quick to train and to perform inference, requiring less than 6 hours of training data. We present the results of objective evaluation metrics, together with a listening test, in which we show significant improvement in the realism of the generated guitar audio from tablatures.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Informed Neural Network Approach for UAV Path Planning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2510.21874</link>
<guid>https://arxiv.org/abs/2510.21874</guid>
<content:encoded><![CDATA[
arXiv:2510.21874v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) operating in dynamic wind fields must generate safe and energy-efficient trajectories under physical and environmental constraints. Traditional planners, such as A* and kinodynamic RRT*, often yield suboptimal or non-smooth paths due to discretization and sampling limitations. This paper presents a physics-informed neural network (PINN) framework that embeds UAV dynamics, wind disturbances, and obstacle avoidance directly into the learning process. Without requiring supervised data, the PINN learns dynamically feasible and collision-free trajectories by minimizing physical residuals and risk-aware objectives. Comparative simulations show that the proposed method outperforms A* and Kino-RRT* in control energy, smoothness, and safety margin, while maintaining similar flight efficiency. The results highlight the potential of physics-informed learning to unify model-based and data-driven planning, providing a scalable and physically consistent framework for UAV trajectory optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Powered Urban Green Infrastructure Assessment Through Aerial Imagery of an Industrial Township</title>
<link>https://arxiv.org/abs/2510.21876</link>
<guid>https://arxiv.org/abs/2510.21876</guid>
<content:encoded><![CDATA[
arXiv:2510.21876v1 Announce Type: cross 
Abstract: Accurate assessment of urban canopy coverage is crucial for informed urban planning, effective environmental monitoring, and mitigating the impacts of climate change. Traditional practices often face limitations due to inadequate technical requirements, difficulties in scaling and data processing, and the lack of specialized expertise. This study presents an efficient approach for estimating green canopy coverage using artificial intelligence, specifically computer vision techniques, applied to aerial imageries. Our proposed methodology utilizes object-based image analysis, based on deep learning algorithms to accurately identify and segment green canopies from high-resolution drone images. This approach allows the user for detailed analysis of urban vegetation, capturing variations in canopy density and understanding spatial distribution. To overcome the computational challenges associated with processing large datasets, it was implemented over a cloud platform utilizing high-performance processors. This infrastructure efficiently manages space complexity and ensures affordable latency, enabling the rapid analysis of vast amounts of drone imageries. Our results demonstrate the effectiveness of this approach in accurately estimating canopy coverage at the city scale, providing valuable insights for urban forestry management of an industrial township. The resultant data generated by this method can be used to optimize tree plantation and assess the carbon sequestration potential of urban forests. By integrating these insights into sustainable urban planning, we can foster more resilient urban environments, contributing to a greener and healthier future.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge</title>
<link>https://arxiv.org/abs/2510.21879</link>
<guid>https://arxiv.org/abs/2510.21879</guid>
<content:encoded><![CDATA[
arXiv:2510.21879v1 Announce Type: cross 
Abstract: Recent years have witnessed an increasing interest in image-text contrastive modeling, exemplified by models such as Contrastive Language-Image Pretraining (CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational framework that converts connection weights of both vision and text encoders of CLIP into the ternary format, instead of full-precision or floating ones. TernaryCLIP incorporates quantization-aware training and distillation modules, preventing precision degradation and enabling low-cost and high-efficiency computations. Comprehensive experiments demonstrate that TernaryCLIP can achieve up to 99\% ternarized weights with 1.58-bit representation, 16.98 $\times$ compression ratio, 2.3 $\times$ inference acceleration, 16 $\times$ storage reduction, 10 $\times$ memory optimization, and 60\% sparsity while maintaining promising performance on zero-shot image classification and image-text retrieval tasks across 41 commonly used datasets. Our work highlights the feasibility of extreme quantization for large multimodal models, supporting effective and efficient deployment on resource-constrained devices. The model and code can be accessed from Hugging Face and GitHub.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Ranker: A Lightweight Ranking framework for LLM Decoding</title>
<link>https://arxiv.org/abs/2510.21883</link>
<guid>https://arxiv.org/abs/2510.21883</guid>
<content:encoded><![CDATA[
arXiv:2510.21883v1 Announce Type: cross 
Abstract: Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scaling the computation of inference time with reward models, have underscored the importance of decoding, but these methods often suffer from high computational costs and limited applicability. In this paper, we revisit LLM generation through the lens of recommender systems, conceptualizing the decoding process as analogous to the ranking stage in recommendation pipelines. From this perspective, we observe that both traditional decoding methods and reward models exhibit clear limitations such as redundancy. Motivated by this insight, we propose Language Ranker, a novel framework that introduces a lightweight module to rerank candidate responses using features extracted by the base model. Experiments across a wide range of tasks show that Language Ranker achieves performance comparable to large-scale reward models, while requiring only <0.5M additional parameters, significantly reducing the computational overhead during both training and inference stages. This highlights the efficiency and effectiveness of our method, showcasing its potential to fully unlock the capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks</title>
<link>https://arxiv.org/abs/2510.21884</link>
<guid>https://arxiv.org/abs/2510.21884</guid>
<content:encoded><![CDATA[
arXiv:2510.21884v1 Announce Type: cross 
Abstract: The growing adoption of machine learning (ML) in sensitive domains has heightened the demand for transparent and interpretable artificial intelligence. Large Language Models (LLMs) are increasingly capable of producing natural language explanations, yet it remains unclear whether these rationales faithfully capture the predictive signals that underlie decisions. This paper introduces RACE-Reasoning Alignment for Completeness of Explanations, a systematic framework to evaluate the alignment between LLM-generated explanations and interpretable feature importance scores derived from a logistic regression baseline. We analyze four widely used text classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and compare LLM rationales against top-ranked supporting and contradicting lexical features. To capture alignment at multiple levels of granularity, RACE implements token-aware, exact string, and edit-distance matching techniques. Empirical results reveal a consistent asymmetry: correct predictions exhibit higher coverage of supporting features, while incorrect predictions are associated with elevated coverage of contradicting features. Edit-distance matching further uncovers paraphrastic overlaps, boosting coverage while preserving this asymmetry. These findings demonstrate that LLM rationales combine both surface-level and flexible evidence reuse, yet can also amplify misleading cues in error cases. RACE provides new insights into the faithfulness of LLM explanations and establishes a quantitative basis for evaluating reasoning completeness in neural language models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning</title>
<link>https://arxiv.org/abs/2510.21885</link>
<guid>https://arxiv.org/abs/2510.21885</guid>
<content:encoded><![CDATA[
arXiv:2510.21885v1 Announce Type: cross 
Abstract: Large language models often lose previously aligned safety behaviors when fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior work shows that adding random safety examples can mitigate this effect, but it remains unclear which examples are most effective. We propose a behavior-aware sampling framework that selects safety examples based on two complementary factors: instruction-response behavior (e.g., refusal versus compliance) and semantic diversity across harm categories. Systematic evaluation shows that this approach substantially reduces harmful outputs while maintaining helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5% additional training data. These results highlight how targeted data selection can improve the safety and efficiency of fine-tuning at scale.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications</title>
<link>https://arxiv.org/abs/2510.21887</link>
<guid>https://arxiv.org/abs/2510.21887</guid>
<content:encoded><![CDATA[
arXiv:2510.21887v1 Announce Type: cross 
Abstract: In recent years, deep learning based generative models, particularly Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models (DMs), have been instrumental in in generating diverse, high-quality content across various domains, such as image and video synthesis. This capability has led to widespread adoption of these models and has captured strong public interest. As they continue to advance at a rapid pace, the growing volume of research, expanding application areas, and unresolved technical challenges make it increasingly difficult to stay current. To address this need, this survey introduces a comprehensive taxonomy that organizes the literature and provides a cohesive framework for understanding the development of GANs, VAEs, and DMs, including their many variants and combined approaches. We highlight key innovations that have improved the quality, diversity, and controllability of generated outputs, reflecting the expanding potential of generative artificial intelligence. In addition to summarizing technical progress, we examine rising ethical concerns, including the risks of misuse and the broader societal impact of synthetic media. Finally, we outline persistent challenges and propose future research directions, offering a structured and forward looking perspective for researchers in this fast evolving field.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Principles of Diffusion Models</title>
<link>https://arxiv.org/abs/2510.21890</link>
<guid>https://arxiv.org/abs/2510.21890</guid>
<content:encoded><![CDATA[
arXiv:2510.21890v1 Announce Type: cross 
Abstract: This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation</title>
<link>https://arxiv.org/abs/2510.21891</link>
<guid>https://arxiv.org/abs/2510.21891</guid>
<content:encoded><![CDATA[
arXiv:2510.21891v1 Announce Type: cross 
Abstract: To deploy large language models (LLMs) in high-stakes application domains that require substantively accurate responses to open-ended prompts, we need reliable, computationally inexpensive methods that assess the trustworthiness of long-form responses generated by LLMs. However, existing approaches often rely on claim-by-claim fact-checking, which is computationally expensive and brittle in long-form responses to open-ended prompts. In this work, we introduce semantic isotropy -- the degree of uniformity across normalized text embeddings on the unit sphere -- and use it to assess the trustworthiness of long-form responses generated by LLMs. To do so, we generate several long-form responses, embed them, and estimate the level of semantic isotropy of these responses as the angular dispersion of the embeddings on the unit sphere. We find that higher semantic isotropy -- that is, greater embedding dispersion -- reliably signals lower factual consistency across samples. Our approach requires no labeled data, no fine-tuning, and no hyperparameter selection, and can be used with open- or closed-weight embedding models. Across multiple domains, our method consistently outperforms existing approaches in predicting nonfactuality in long-form responses using only a handful of samples -- offering a practical, low-cost approach for integrating trust assessment into real-world LLM workflows.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Network Behaviors through Natural Language Question-Answering</title>
<link>https://arxiv.org/abs/2510.21894</link>
<guid>https://arxiv.org/abs/2510.21894</guid>
<content:encoded><![CDATA[
arXiv:2510.21894v1 Announce Type: cross 
Abstract: Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further enhance this direction, leveraging their extensive prior knowledge of network concepts and strong reasoning capabilities. However, three key challenges remain: 1) numerous router devices with lengthy configuration files challenge LLM's long-context understanding ability; 2) heterogeneity across devices and protocols impedes scalability; and 3) complex network topologies and protocols demand advanced reasoning abilities beyond the current capabilities of LLMs. To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL. Our approach introduces a tree-based configuration chunking strategy to preserve semantic coherence while enabling efficient partitioning. We then construct a unified fact graph as an intermediate representation to normalize vendor-specific configurations. Finally, we design a hybrid imperative-declarative language to reduce the reasoning burden on LLMs and enhance precision. We contribute a benchmark consisting of NL question-answer pairs paired with network configurations. Experiments demonstrate that NetMind achieves accurate and scalable network behavior understanding, outperforming existing baselines.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Literature Survey Automation with an Iterative Workflow</title>
<link>https://arxiv.org/abs/2510.21900</link>
<guid>https://arxiv.org/abs/2510.21900</guid>
<content:encoded><![CDATA[
arXiv:2510.21900v1 Announce Type: cross 
Abstract: Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often leads to noisy retrieval, fragmented structures, and context overload, ultimately limiting survey quality. Inspired by the iterative reading process of human researchers, we propose \ours, a framework based on recurrent outline generation, in which a planning agent incrementally retrieves, reads, and updates the outline to ensure both exploration and coherence. To provide faithful paper-level grounding, we design paper cards that distill each paper into its contributions, methods, and findings, and introduce a review-and-refine loop with visualization enhancement to improve textual flow and integrate multimodal elements such as figures and tables. Experiments on both established and emerging topics show that \ours\ substantially outperforms state-of-the-art baselines in content coverage, structural coherence, and citation quality, while producing more accessible and better-organized surveys. To provide a more reliable assessment of such improvements, we further introduce Survey-Arena, a pairwise benchmark that complements absolute scoring and more clearly positions machine-generated surveys relative to human-written ones. The code is available at https://github.com/HancCui/IterSurvey\_Autosurveyv2.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments</title>
<link>https://arxiv.org/abs/2510.21902</link>
<guid>https://arxiv.org/abs/2510.21902</guid>
<content:encoded><![CDATA[
arXiv:2510.21902v1 Announce Type: cross 
Abstract: Software Engineering Agents (SWE-Agents) have proven effective for traditional software engineering tasks with accessible codebases, but their performance for embodied tasks requiring well-designed information discovery remains unexplored. We present the first extended evaluation of SWE-Agents on controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks from the Minigrid environment. Our experiments compare agent performance across different information access conditions: with and without environment source code access, and with varying capabilities for interactive exploration. We quantify how different information access levels affect SWE-Agent performance for embodied tasks and analyze the relative importance of static code analysis versus dynamic exploration for task solving. This work establishes controller generation for embodied tasks as a crucial evaluation domain for SWE-Agents and provides baseline results for future research in efficient reasoning systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOM-SWE: User Mental Modeling For Software Engineering Agents</title>
<link>https://arxiv.org/abs/2510.21903</link>
<guid>https://arxiv.org/abs/2510.21903</guid>
<content:encoded><![CDATA[
arXiv:2510.21903v1 Announce Type: cross 
Abstract: Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-dependent. To bridge this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary software-engineering (SWE) agent with a lightweight theory-of-mind (ToM) partner agent dedicated to modeling the user's mental state. The ToM agent infers user goals, constraints, and preferences from instructions and interaction history, maintains a \textbf{persistent memory} of the user, and provides user-related suggestions to the SWE agent. In two software engineering benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task success rates and user satisfaction. Notably, on the stateful SWE benchmark, a newly introduced evaluation that provides agents with a user simulator along with previous interaction histories, ToM-SWE achieves a substantially higher task success rate of 59.7\% compared to 18.1\% for OpenHands, a state-of-the-art SWE agent. Furthermore, in a three-week study with professional developers using ToM-SWE in their daily work, participants found it useful 86\% of the time, underscoring the value of stateful user modeling for practical coding agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aware Cooperative Ensemble Evolutionary Optimization on Combinatorial Problems with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2510.21906</link>
<guid>https://arxiv.org/abs/2510.21906</guid>
<content:encoded><![CDATA[
arXiv:2510.21906v1 Announce Type: cross 
Abstract: Evolutionary algorithms (EAs) have proven effective in exploring the vast solution spaces typical of graph-structured combinatorial problems. However, traditional encoding schemes, such as binary or numerical representations, often fail to straightforwardly capture the intricate structural properties of networks. Through employing the image-based encoding to preserve topological context, this study utilizes multimodal large language models (MLLMs) as evolutionary operators to facilitate structure-aware optimization over graph data. To address the visual clutter inherent in large-scale network visualizations, we leverage graph sparsification techniques to simplify structures while maintaining essential structural features. To further improve robustness and mitigate bias from different sparsification views, we propose a cooperative evolutionary optimization framework that facilitates cross-domain knowledge transfer and unifies multiple sparsified variants of diverse structures. Additionally, recognizing the sensitivity of MLLMs to network layout, we introduce an ensemble strategy that aggregates outputs from various layout configurations through consensus voting. Finally, experiments on real-world networks through various tasks demonstrate that our approach improves both the quality and reliability of solutions in MLLM-driven evolutionary optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity</title>
<link>https://arxiv.org/abs/2510.21908</link>
<guid>https://arxiv.org/abs/2510.21908</guid>
<content:encoded><![CDATA[
arXiv:2510.21908v1 Announce Type: cross 
Abstract: Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case</title>
<link>https://arxiv.org/abs/2510.21933</link>
<guid>https://arxiv.org/abs/2510.21933</guid>
<content:encoded><![CDATA[
arXiv:2510.21933v1 Announce Type: cross 
Abstract: The use of Large Language Models (LLMs) to support tasks in software development has steadily increased over recent years. From assisting developers in coding activities to providing conversational agents that answer newcomers' questions. In collaboration with the Mozilla Foundation, this study evaluates the effectiveness of Retrieval-Augmented Generation (RAG) in assisting developers within the Mozilla Firefox project. We conducted an empirical analysis comparing responses from human developers, a standard GPT model, and a GPT model enhanced with RAG, using real queries from Mozilla's developer chat rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses based on helpfulness, comprehensiveness, and conciseness. The results show that RAG-assisted responses were more comprehensive than human developers (62.50% to 54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to enhance developer assistance. However, the RAG responses were not as concise and often verbose. The results show the potential to apply RAG-based tools to Open Source Software (OSS) to minimize the load to core maintainers without losing answer quality. Toning down retrieval mechanisms and making responses even shorter in the future would enhance developer assistance in massive projects like Mozilla Firefox.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing</title>
<link>https://arxiv.org/abs/2510.21935</link>
<guid>https://arxiv.org/abs/2510.21935</guid>
<content:encoded><![CDATA[
arXiv:2510.21935v1 Announce Type: cross 
Abstract: Novelty detection in large scientific datasets faces two key challenges: the noisy and high-dimensional nature of experimental data, and the necessity of making statistically robust statements about any observed outliers. While there is a wealth of literature on anomaly detection via dimensionality reduction, most methods do not produce outputs compatible with quantifiable claims of scientific discovery. In this work we directly address these challenges, presenting the first step towards a unified pipeline for novelty detection adapted for the rigorous statistical demands of science. We introduce AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive Testing), a general-purpose pipeline for detecting novelty in scientific data. AutoSciDACT begins by creating expressive low-dimensional data representations using a contrastive pre-training, leveraging the abundance of high-quality simulated data in many scientific domains alongside expertise that can guide principled data augmentation strategies. These compact embeddings then enable an extremely sensitive machine learning-based two-sample test using the New Physics Learning Machine (NPLM) framework, which identifies and statistically quantifies deviations in observed data relative to a reference distribution (null hypothesis). We perform experiments across a range of astronomical, physical, biological, image, and synthetic datasets, demonstrating strong sensitivity to small injections of anomalous data across all domains.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.21957</link>
<guid>https://arxiv.org/abs/2510.21957</guid>
<content:encoded><![CDATA[
arXiv:2510.21957v1 Announce Type: cross 
Abstract: Ransomware has become a critical threat to cybersecurity due to its rapid evolution, the necessity for early detection, and growing diversity, posing significant challenges to traditional detection methods. While AI-based approaches had been proposed by prior works to assist ransomware detection, existing methods suffer from three major limitations, ad-hoc feature dependencies, delayed response, and limited adaptability to unseen variants. In this paper, we propose a framework that integrates self-supervised contrastive learning with neural architecture search (NAS) to address these challenges. Specifically, this paper offers three important contributions. (1) We design a contrastive learning framework that incorporates hardware performance counters (HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a customized loss function that encourages early-stage detection of malicious activity, and significantly reduces the detection latency. (3) We deploy a neural architecture search (NAS) framework to automatically construct adaptive model architectures, allowing the detector to flexibly align with unseen ransomware variants. Experimental results show that our proposed method achieves significant improvements in both detection accuracy (up to 16.1%) and response time (up to 6x) compared to existing approaches while maintaining robustness under evasive attacks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities</title>
<link>https://arxiv.org/abs/2510.21966</link>
<guid>https://arxiv.org/abs/2510.21966</guid>
<content:encoded><![CDATA[
arXiv:2510.21966v1 Announce Type: cross 
Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of software development knowledge. However, locating architectural knowledge, such as architectural solutions remains challenging due to the overwhelming volume of unstructured content and fragmented discussions. Developers must manually sift through posts to find relevant architectural insights, which is time-consuming and error-prone. This study introduces ArchISMiner, a framework for mining architectural knowledge from SO. The framework comprises two complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates multiple models, including conventional ML/DL models, Pre-trained Language Models (PLMs), and Large Language Models (LLMs), and selects the best-performing model to automatically identify Architecture-Related Posts (ARPs) among programming-related discussions. ArchISPE employs an indirect supervised approach that leverages diverse features, including BERT embeddings and local TextCNN features, to extract architectural issue-solution pairs. Our evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields, achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions. A user study further validated the quality (e.g., relevance and usefulness) of the identified ARPs and the extracted issue-solution pairs. Moreover, we applied ArchISMiner to three additional forums, releasing a dataset of over 18K architectural issue-solution pairs. Overall, ArchISMiner can help architects and developers identify ARPs and extract succinct, relevant, and useful architectural knowledge from developer communities more accurately and efficiently. The replication package of this study has been provided at https://github.com/JeanMusenga/ArchISPE
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2510.21978</link>
<guid>https://arxiv.org/abs/2510.21978</guid>
<content:encoded><![CDATA[
arXiv:2510.21978v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2510.21983</link>
<guid>https://arxiv.org/abs/2510.21983</guid>
<content:encoded><![CDATA[
arXiv:2510.21983v1 Announce Type: cross 
Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak attacks that bypass alignment safeguards and elicit harmful outputs. While prior research has proposed various attack strategies differing in human readability and transferability, little attention has been paid to the linguistic and psychological mechanisms that may influence a model's susceptibility to such attacks. In this paper, we examine an interdisciplinary line of research that leverages foundational theories of persuasion from the social sciences to craft adversarial prompts capable of circumventing alignment constraints in LLMs. Drawing on well-established persuasive strategies, we hypothesize that LLMs, having been trained on large-scale human-generated text, may respond more compliantly to prompts with persuasive structures. Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive fingerprints that emerge in their jailbreak responses. Empirical evaluations across multiple aligned LLMs reveal that persuasion-aware prompts significantly bypass safeguards, demonstrating their potential to induce jailbreak behaviors. This work underscores the importance of cross-disciplinary insight in addressing the evolving challenges of LLM safety. The code and data are available.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising</title>
<link>https://arxiv.org/abs/2510.21991</link>
<guid>https://arxiv.org/abs/2510.21991</guid>
<content:encoded><![CDATA[
arXiv:2510.21991v1 Announce Type: cross 
Abstract: Diffusion models, such as diffusion policy, have achieved state-of-the-art results in robotic manipulation by imitating expert demonstrations. While diffusion models were originally developed for vision tasks like image and video generation, many of their inference strategies have been directly transferred to control domains without adaptation. In this work, we show that by tailoring the denoising process to the specific characteristics of embodied AI tasks -- particularly structured, low-dimensional nature of action distributions -- diffusion policies can operate effectively with as few as 5 neural function evaluations (NFE).
  Building on this insight, we propose a population-based sampling strategy, genetic denoising, which enhances both performance and stability by selecting denoising trajectories with low out-of-distribution risk. Our method solves challenging tasks with only 2 NFE while improving or matching performance. We evaluate our approach across 14 robotic manipulation tasks from D4RL and Robomimic, spanning multiple action horizons and inference budgets. In over 2 million evaluations, our method consistently outperforms standard diffusion-based policies, achieving up to 20\% performance gains with significantly fewer inference steps.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Temporal Difference Learning the Gold Standard for Stitching in RL?</title>
<link>https://arxiv.org/abs/2510.21995</link>
<guid>https://arxiv.org/abs/2510.21995</guid>
<content:encoded><![CDATA[
arXiv:2510.21995v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) promises to solve long-horizon tasks even when training data contains only short fragments of the behaviors. This experience stitching capability is often viewed as the purview of temporal difference (TD) methods. However, outside of small tabular settings, trajectories never intersect, calling into question this conventional wisdom. Moreover, the common belief is that Monte Carlo (MC) methods should not be able to recombine experience, yet it remains unclear whether function approximation could result in a form of implicit stitching. The goal of this paper is to empirically study whether the conventional wisdom about stitching actually holds in settings where function approximation is used. We empirically demonstrate that Monte Carlo (MC) methods can also achieve experience stitching. While TD methods do achieve slightly stronger capabilities than MC methods (in line with conventional wisdom), that gap is significantly smaller than the gap between small and large neural networks (even on quite simple tasks). We find that increasing critic capacity effectively reduces the generalization gap for both the MC and TD methods. These results suggest that the traditional TD inductive bias for stitching may be less necessary in the era of large models for RL and, in some cases, may offer diminishing returns. Additionally, our results suggest that stitching, a form of generalization unique to the RL setting, might be achieved not through specialized algorithms (temporal difference learning) but rather through the same recipe that has provided generalization in other machine learning settings (via scale). Project website: https://michalbortkiewicz.github.io/golden-standard/
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Black-box to Causal-box: Towards Building More Interpretable Models</title>
<link>https://arxiv.org/abs/2510.21998</link>
<guid>https://arxiv.org/abs/2510.21998</guid>
<content:encoded><![CDATA[
arXiv:2510.21998v1 Announce Type: cross 
Abstract: Understanding the predictions made by deep learning models remains a central challenge, especially in high-stakes applications. A promising approach is to equip models with the ability to answer counterfactual questions -- hypothetical ``what if?'' scenarios that go beyond the observed data and provide insight into a model reasoning. In this work, we introduce the notion of causal interpretability, which formalizes when counterfactual queries can be evaluated from a specific class of models and observational data. We analyze two common model classes -- blackbox and concept-based predictors -- and show that neither is causally interpretable in general. To address this gap, we develop a framework for building models that are causally interpretable by design. Specifically, we derive a complete graphical criterion that determines whether a given model architecture supports a given counterfactual query. This leads to a fundamental tradeoff between causal interpretability and predictive accuracy, which we characterize by identifying the unique maximal set of features that yields an interpretable model with maximal predictive expressiveness. Experiments corroborate the theoretical findings.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2510.22003</link>
<guid>https://arxiv.org/abs/2510.22003</guid>
<content:encoded><![CDATA[
arXiv:2510.22003v1 Announce Type: cross 
Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile software organizations, yet evidence on its effects remains scattered. We report a systematic literature review (SLR), following established SLR protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies across enterprise, solution, domain, business, and IT architect roles. GenAI most consistently supports (i) design ideation and trade-off exploration; (ii) rapid creation and refinement of artifacts (e.g., code, models, documentation); and (iii) architectural decision support and knowledge retrieval. Reported risks include opacity and bias, contextually incorrect outputs leading to rework, privacy and compliance concerns, and social loafing. We also identify emerging skills and competencies, including prompt engineering, model evaluation, and professional oversight, and organizational enablers around readiness and adaptive governance. The review contributes with (1) a mapping of GenAI use cases and risks in agile architecting, (2) implications for capability building and governance, and (3) an initial research agenda on human-AI collaboration in architecture. Overall, the findings inform responsible adoption of GenAI that accelerates digital transformation while safeguarding architectural integrity.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconnaissance Automatique des Langues des Signes : Une Approche Hybrid\'ee CNN-LSTM Bas\'ee sur Mediapipe</title>
<link>https://arxiv.org/abs/2510.22011</link>
<guid>https://arxiv.org/abs/2510.22011</guid>
<content:encoded><![CDATA[
arXiv:2510.22011v1 Announce Type: cross 
Abstract: Sign languages play a crucial role in the communication of deaf communities, but they are often marginalized, limiting access to essential services such as healthcare and education. This study proposes an automatic sign language recognition system based on a hybrid CNN-LSTM architecture, using Mediapipe for gesture keypoint extraction. Developed with Python, TensorFlow and Streamlit, the system provides real-time gesture translation. The results show an average accuracy of 92\%, with very good performance for distinct gestures such as ``Hello'' and ``Thank you''. However, some confusions remain for visually similar gestures, such as ``Call'' and ``Yes''. This work opens up interesting perspectives for applications in various fields such as healthcare, education and public services.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models</title>
<link>https://arxiv.org/abs/2510.22014</link>
<guid>https://arxiv.org/abs/2510.22014</guid>
<content:encoded><![CDATA[
arXiv:2510.22014v1 Announce Type: cross 
Abstract: Discrete optimization-based jailbreaking attacks on large language models aim to generate short, nonsensical suffixes that, when appended onto input prompts, elicit disallowed content. Notably, these suffixes are often transferable -- succeeding on prompts and models for which they were never optimized. And yet, despite the fact that transferability is surprising and empirically well-established, the field lacks a rigorous analysis of when and why transfer occurs. To fill this gap, we identify three statistical properties that strongly correlate with transfer success across numerous experimental settings: (1) how much a prompt without a suffix activates a model's internal refusal direction, (2) how strongly a suffix induces a push away from this direction, and (3) how large these shifts are in directions orthogonal to refusal. On the other hand, we find that prompt semantic similarity only weakly correlates with transfer success. These findings lead to a more fine-grained understanding of transferability, which we use in interventional experiments to showcase how our statistical analysis can translate into practical improvements in attack success.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalization in Attention Dynamics</title>
<link>https://arxiv.org/abs/2510.22026</link>
<guid>https://arxiv.org/abs/2510.22026</guid>
<content:encoded><![CDATA[
arXiv:2510.22026v1 Announce Type: cross 
Abstract: We study the effect of normalization schemes on token representations in deep transformers. Modeling their evolution as interacting particles on the sphere, we show that normalization acts as a form of speed regulation. This perspective enables a unified analysis of several schemes -- including Post-LN, Pre-LN, Mix-LN, Peri-LN, nGPT, and LN-Scaling -- revealing how they influence clustering dynamics and representation collapse. Our framework clarifies how different schemes shape token representations across layers and provides a principled basis for comparing them, identifying Peri-LN as a particularly effective choice.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Optimization for Offline Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.22027</link>
<guid>https://arxiv.org/abs/2510.22027</guid>
<content:encoded><![CDATA[
arXiv:2510.22027v1 Announce Type: cross 
Abstract: We study the problem of Offline Safe Reinforcement Learning (OSRL), where the goal is to learn a reward-maximizing policy from fixed data under a cumulative cost constraint. We propose a novel OSRL approach that frames the problem as a minimax objective and solves it by combining offline RL with online optimization algorithms. We prove the approximate optimality of this approach when integrated with an approximate offline RL oracle and no-regret online optimization. We also present a practical approximation that can be combined with any offline RL algorithm, eliminating the need for offline policy evaluation. Empirical results on the DSRL benchmark demonstrate that our method reliably enforces safety constraints under stringent cost budgets, while achieving high rewards. The code is available at https://github.com/yassineCh/O3SRL.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Constraint-Based Causal Discovery</title>
<link>https://arxiv.org/abs/2510.22031</link>
<guid>https://arxiv.org/abs/2510.22031</guid>
<content:encoded><![CDATA[
arXiv:2510.22031v1 Announce Type: cross 
Abstract: Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code and data of the proposed method are publicly available at https://github$.$com/PurdueMINDS/DAGPA.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models</title>
<link>https://arxiv.org/abs/2510.22042</link>
<guid>https://arxiv.org/abs/2510.22042</guid>
<content:encoded><![CDATA[
arXiv:2510.22042v1 Announce Type: cross 
Abstract: This work investigates how large language models (LLMs) internally represent emotion by analyzing the geometry of their hidden-state space. The paper identifies a low-dimensional emotional manifold and shows that emotional representations are directionally encoded, distributed across layers, and aligned with interpretable dimensions. These structures are stable across depth and generalize to eight real-world emotion datasets spanning five languages. Cross-domain alignment yields low error and strong linear probe performance, indicating a universal emotional subspace. Within this space, internal emotion perception can be steered while preserving semantics using a learned intervention module, with especially strong control for basic emotions across languages. These findings reveal a consistent and manipulable affective geometry in LLMs and offer insight into how they internalize and process emotion.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT</title>
<link>https://arxiv.org/abs/2510.22045</link>
<guid>https://arxiv.org/abs/2510.22045</guid>
<content:encoded><![CDATA[
arXiv:2510.22045v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) are increasingly used to evaluate multimodal content, including presentation slides, yet their slide-specific understanding remains underexplored {despite their growing role as critics in agentic, model-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework that probes VLMs along three axes: (1) element-level extraction from slide images aligned to ground truth; (2) robustness to controlled perturbations in geometry, style, and text; and (3) higher-level comprehension, such as recovering a deck's narrative order from shuffled slides. Using publicly available decks from Zenodo (https://huggingface.co/datasets/Forceless/Zenodo10K/viewer/default/pptx), we standardize ground-truth element metadata from PowerPoint XML and live renderings into a unified, verifiable schema. Empirically, VLMs underperform on pixel-accurate extraction and show non-trivial agreement, fidelity, and consistency under controlled perturbations, while performing better on single-slide content understanding; however, they do not reliably capture narrative structure across slides. These results highlight the limits of current VLMs for slide evaluation and motivate calibrated, critic-in-the-loop evaluators that drive iterative refinement and selection in agentic pipelines.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning</title>
<link>https://arxiv.org/abs/2510.22056</link>
<guid>https://arxiv.org/abs/2510.22056</guid>
<content:encoded><![CDATA[
arXiv:2510.22056v1 Announce Type: cross 
Abstract: Anomaly detection in surveillance videos remains a challenging task due to the diversity of abnormal events, class imbalance, and scene-dependent visual clutter. To address these issues, we propose a robust deep learning framework that integrates human-centric preprocessing with spatio-temporal modeling for multi-class anomaly classification. Our pipeline begins by applying YOLO-World - an open-vocabulary vision-language detector - to identify human instances in raw video clips, followed by ByteTrack for consistent identity-aware tracking. Background regions outside detected bounding boxes are suppressed via Gaussian blurring, effectively reducing scene-specific distractions and focusing the model on behaviorally relevant foreground content. The refined frames are then processed by an ImageNet-pretrained InceptionV3 network for spatial feature extraction, and temporal dynamics are captured using a bidirectional LSTM (BiLSTM) for sequence-level classification. Evaluated on a five-class subset of the UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our method achieves a mean test accuracy of 92.41% across three independent trials, with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation metrics - including confusion matrices, ROC curves, and macro/weighted averages - demonstrate strong generalization and resilience to class imbalance. The results confirm that foreground-focused preprocessing significantly enhances anomaly discrimination in real-world surveillance scenarios.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model</title>
<link>https://arxiv.org/abs/2510.22057</link>
<guid>https://arxiv.org/abs/2510.22057</guid>
<content:encoded><![CDATA[
arXiv:2510.22057v1 Announce Type: cross 
Abstract: With the rise of online and virtual learning, monitoring and enhancing student engagement have become an important aspect of effective education. Traditional methods of assessing a student's involvement might not be applicable directly to virtual environments. In this study, we focused on this problem and addressed the need to develop an automated system to detect student engagement levels during online learning. We proposed a novel training method which can discourage a model from leveraging sensitive features like gender for its predictions. The proposed method offers benefits not only in the enforcement of ethical standards, but also to enhance interpretability of the model predictions. We applied an attribute-orthogonal regularization technique to a split-model classifier, which uses multiple transfer learning strategies to achieve effective results in reducing disparity in the distribution of prediction for sensitivity groups from a Pearson correlation coefficient of 0.897 for the unmitigated model, to 0.999 for the mitigated model. The source code for this project is available on https://github.com/ashiskb/elearning-engagement-study .
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequentist Validity of Epistemic Uncertainty Estimators</title>
<link>https://arxiv.org/abs/2510.22063</link>
<guid>https://arxiv.org/abs/2510.22063</guid>
<content:encoded><![CDATA[
arXiv:2510.22063v1 Announce Type: cross 
Abstract: Decomposing prediction uncertainty into its aleatoric (irreducible) and epistemic (reducible) components is critical for the development and deployment of machine learning systems. A popular, principled measure for epistemic uncertainty is the mutual information between the response variable and model parameters. However, evaluating this measure requires access to the posterior distribution of the model parameters, which is challenging to compute. In view of this, we introduce a frequentist measure of epistemic uncertainty based on the bootstrap. Our main theoretical contribution is a novel asymptotic expansion that reveals that our proposed (frequentist) measure and the (Bayesian) mutual information are asymptotically equivalent. This provides frequentist interpretations to mutual information and new computational strategies for approximating it. Moreover, we link our proposed approach to the widely-used heuristic approach of deep ensembles, giving added perspective on their practical success.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforcement Learning for Real-World Code Repair</title>
<link>https://arxiv.org/abs/2510.22075</link>
<guid>https://arxiv.org/abs/2510.22075</guid>
<content:encoded><![CDATA[
arXiv:2510.22075v1 Announce Type: cross 
Abstract: We tackle the challenge of training reliable code-fixing agents in real repositories, where complex builds and shifting dependencies make evaluation unstable. We developed a verifiable pipeline with success defined as post-fix build validation and improved reproducibility across ~1K real issues by pinning dependencies and disabling automatic upgrades. Building on this, we introduced a scalable simplified pipeline for large-scale reinforcement learning (RL). Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and applied RL on top of the SFT model in the simplified environment. The SFT model distilled from GPT-4.1 trajectories performs on par while being 56x smaller, and RL added 7-20% absolute gains under matched train-test conditions. "Thinking mode" was on par or worse in our experiments. Both SFT and RL models failed to generalize across environments, highlighting the importance of matching train-test environments for building reliable real-world code-fixing agents.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models</title>
<link>https://arxiv.org/abs/2510.22085</link>
<guid>https://arxiv.org/abs/2510.22085</guid>
<content:encoded><![CDATA[
arXiv:2510.22085v1 Announce Type: cross 
Abstract: Large language models (LLMs) remain vulnerable to sophisticated prompt engineering attacks that exploit contextual framing to bypass safety mechanisms, posing significant risks in cybersecurity applications. We introduce Jailbreak Mimicry, a systematic methodology for training compact attacker models to automatically generate narrative-based jailbreak prompts in a one-shot manner. Our approach transforms adversarial prompt discovery from manual craftsmanship into a reproducible scientific process, enabling proactive vulnerability assessment in AI-driven security systems. Developed for the OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench, achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out test set of 200 items. Cross-model evaluation reveals significant variation in vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad applicability and model-specific defensive strengths in cybersecurity contexts. This represents a 54x improvement over direct prompting (1.5% ASR) and demonstrates systematic vulnerabilities in current safety alignment approaches. Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable, highlighting threats to AI-integrated threat detection, malware analysis, and secure systems, while physical harm categories show greater resistance (55.6% ASR). We employ automated harmfulness evaluation using Claude Sonnet 4, cross-validated with human expert assessment, ensuring reliable and scalable evaluation for cybersecurity red-teaming. Finally, we analyze failure mechanisms and discuss defensive strategies to mitigate these vulnerabilities in AI for cybersecurity.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture</title>
<link>https://arxiv.org/abs/2510.22087</link>
<guid>https://arxiv.org/abs/2510.22087</guid>
<content:encoded><![CDATA[
arXiv:2510.22087v1 Announce Type: cross 
Abstract: The field of computer architecture, which bridges high-level software abstractions and low-level hardware implementations, remains absent from current large language model (LLM) evaluations. To this end, we present QuArch (pronounced 'quark'), the first benchmark designed to facilitate the development and evaluation of LLM knowledge and reasoning capabilities specifically in computer architecture. QuArch provides a comprehensive collection of 2,671 expert-validated question-answer (QA) pairs covering various aspects of computer architecture, including processor design, memory systems, and interconnection networks. Our evaluation reveals that while frontier models possess domain-specific knowledge, they struggle with skills that require higher-order thinking in computer architecture. Frontier model accuracies vary widely (from 34% to 72%) on these advanced questions, highlighting persistent gaps in architectural reasoning across analysis, design, and implementation QAs. By holistically assessing fundamental skills, QuArch provides a foundation for building and measuring LLM capabilities that can accelerate innovation in computing systems. With over 140 contributors from 40 institutions, this benchmark represents a community effort to set the standard for architectural reasoning in LLM evaluation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Coordinate Prediction Bias from Positional Encoding Failures</title>
<link>https://arxiv.org/abs/2510.22102</link>
<guid>https://arxiv.org/abs/2510.22102</guid>
<content:encoded><![CDATA[
arXiv:2510.22102v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) excel at vision-language tasks such as VQA and document understanding, yet precise coordinate prediction remains challenging. High-resolution inputs exacerbate this difficulty by producing long token sequences that weaken positional encodings and introduce directional biases in coordinate outputs. We investigate this phenomenon by analyzing how MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed through shuffling. Our analysis reveals that such perturbations induce predictable, non-random coordinate biases rather than random errors, suggesting that models rely on internal positional priors when spatial grounding signals are degraded. Crucially, we observe similar directional error patterns in natural high-resolution datasets, indicating that positional encoding failures are a key bottleneck for accurate coordinate prediction at scale. To address this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free test-time method that leverages the directional nature of these biases for correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction while preserving coordinate format through a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable improvements, highlighting positional encoding robustness as a critical factor for spatial reasoning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation</title>
<link>https://arxiv.org/abs/2510.22107</link>
<guid>https://arxiv.org/abs/2510.22107</guid>
<content:encoded><![CDATA[
arXiv:2510.22107v1 Announce Type: cross 
Abstract: Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAR-RIS-assisted Collaborative Beamforming for Low-altitude Wireless Networks</title>
<link>https://arxiv.org/abs/2510.22108</link>
<guid>https://arxiv.org/abs/2510.22108</guid>
<content:encoded><![CDATA[
arXiv:2510.22108v1 Announce Type: cross 
Abstract: While low-altitude wireless networks (LAWNs) based on uncrewed aerial vehicles (UAVs) offer high mobility, flexibility, and coverage for urban communications, they face severe signal attenuation in dense environments due to obstructions. To address this critical issue, we consider introducing collaborative beamforming (CB) of UAVs and omnidirectional reconfigurable beamforming (ORB) of simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) to enhance the signal quality and directionality. On this basis, we formulate a joint rate and energy optimization problem (JREOP) to maximize the transmission rate of the overall system, while minimizing the energy consumption of the UAV swarm. Due to the non-convex and NP-hard nature of JREOP, we propose a heterogeneous multi-agent collaborative dynamic (HMCD) optimization framework, which has two core components. The first component is a simulated annealing (SA)-based STAR-RIS control method, which dynamically optimizes reflection and transmission coefficients to enhance signal propagation. The second component is an improved multi-agent deep reinforcement learning (MADRL) control method, which incorporates a self-attention evaluation mechanism to capture interactions between UAVs and an adaptive velocity transition mechanism to enhance training stability. Simulation results demonstrate that HMCD outperforms various baselines in terms of convergence speed, average transmission rate, and energy consumption. Further analysis reveals that the average transmission rate of the overall system scales positively with both UAV count and STAR-RIS element numbers.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows</title>
<link>https://arxiv.org/abs/2510.22109</link>
<guid>https://arxiv.org/abs/2510.22109</guid>
<content:encoded><![CDATA[
arXiv:2510.22109v1 Announce Type: cross 
Abstract: Most approaches to long-context processing increase the complexity of the transformer's internal architecture by integrating mechanisms such as recurrence or auxiliary memory modules. In this work, we introduce an alternative approach that modifies the input representation itself, rather than the transformer architecture. Inspired by cognitive models of human memory, our method applies a scale-invariant logarithmic compression to the input tokens. The resulting compressed representation is processed by a standard, unmodified transformer, preserving architectural simplicity. We evaluate this approach on the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in perplexity compared to uncompressed baselines. Moreover, performance improves consistently with longer compressed temporal contexts, showing that input-level logarithmic compression is a simple and effective way to extend a transformer's long-range memory.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation</title>
<link>https://arxiv.org/abs/2510.22115</link>
<guid>https://arxiv.org/abs/2510.22115</guid>
<content:encoded><![CDATA[
arXiv:2510.22115v1 Announce Type: cross 
Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When UAV Swarm Meets IRS: Collaborative Secure Communications in Low-altitude Wireless Networks</title>
<link>https://arxiv.org/abs/2510.22117</link>
<guid>https://arxiv.org/abs/2510.22117</guid>
<content:encoded><![CDATA[
arXiv:2510.22117v1 Announce Type: cross 
Abstract: Low-altitude wireless networks (LAWNs) represent a promising architecture that integrates unmanned aerial vehicles (UAVs) as aerial nodes to provide enhanced coverage, reliability, and throughput for diverse applications. However, these networks face significant security vulnerabilities from both known and potential unknown eavesdroppers, which may threaten data confidentiality and system integrity. To solve this critical issue, we propose a novel secure communication framework for LAWNs where the selected UAVs within a swarm function as a virtual antenna array (VAA), complemented by intelligent reflecting surface (IRS) to create a robust defense against eavesdropping attacks. Specifically, we formulate a multi-objective optimization problem that simultaneously maximizes the secrecy rate while minimizing the maximum sidelobe level and total energy consumption, requiring joint optimization of UAV excitation current weights, flight trajectories, and IRS phase shifts. This problem presents significant difficulties due to the dynamic nature of the system and heterogeneous components. Thus, we first transform the problem into a heterogeneous Markov decision process (MDP). Then, we propose a heterogeneous multi-agent control approach (HMCA) that integrates a dedicated IRS control policy with a multi-agent soft actor-critic framework for UAV control, which enables coordinated operation across heterogeneous network elements. Simulation results show that the proposed HMCA achieves superior performance compared to baseline approaches in terms of secrecy rate improvement, sidelobe suppression, and energy efficiency. Furthermore, we find that the collaborative and passive beamforming synergy between VAA and IRS creates robust security guarantees when the number of UAVs increases.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation</title>
<link>https://arxiv.org/abs/2510.22118</link>
<guid>https://arxiv.org/abs/2510.22118</guid>
<content:encoded><![CDATA[
arXiv:2510.22118v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) achieve strong performance on many vision-language tasks but often struggle with spatial reasoning\textemdash{}a prerequisite for many applications. Empirically, we find that a dataset produced by a current training data generation pipeline has a 57.6\% human validation rate. These rates stem from current limitations: single-image 3D reconstruction introduces cascading modeling errors and requires wide answer tolerances, while caption-based methods require hyper-detailed annotations and suffer from generative hallucinations. We present GRAID, built on the key insight that qualitative spatial relationships can be reliably determined from 2D geometric primitives alone. By operating exclusively on 2D bounding boxes from standard object detectors, GRAID avoids both 3D reconstruction errors and generative hallucinations, resulting in datasets that are of higher quality than existing tools that produce similar datasets as validated by human evaluations. We apply our framework to the BDD100k, NuImages, and Waymo datasets, generating over 8.5 million high-quality VQA pairs creating questions spanning spatial relations, counting, ranking, and size comparisons. We evaluate one of the datasets and find it achieves 91.16\% human-validated accuracy\textemdash{}compared to 57.6\% on a dataset generated by recent work. % or recent work Critically, we demonstrate that when trained on GRAID data, models learn spatial reasoning concepts that generalize: models fine-tuned on 6 question types improve on over 10 held-out types, with accuracy gains of 47.5\% on BDD and 37.9\% on NuImages for Llama 3.2B 11B, and when trained on all questions types, achieve improvements on several existing benchmarks such as BLINK. The GRAID framework, datasets, and additional information can be found on our \href{https://ke7.github.io/graid/}{project page}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Utility-Preserving Machine Unlearning with Implicit Gradient Surgery</title>
<link>https://arxiv.org/abs/2510.22124</link>
<guid>https://arxiv.org/abs/2510.22124</guid>
<content:encoded><![CDATA[
arXiv:2510.22124v1 Announce Type: cross 
Abstract: Machine unlearning (MU) aims to efficiently remove sensitive or harmful memory from a pre-trained model. The key challenge is to balance the potential tradeoff between unlearning efficacy and utility preservation, which involves forgetting undesirable information as defined while maintaining the model's original performance. One potential way to tackle this problem is to use multi-objective optimization to jointly optimize both the unlearning and utility preservation objectives. However, existing multi-objective methods only guarantee finding a Pareto-optimal solution without fine-grained control, which causes under-optimization of the unlearning objective. To this end, we first model MU as a constrained optimization problem, that is, optimizing the unlearning objective under the constraint of a bounded increase for utility loss. We then show that solving this optimization problem is equivalent to unilateral gradient surgery on the unlearning objective. To resolve the additional computational cost brought by gradient surgery, we propose an implicit gradient surgery method, which approximates the solution to the aforementioned constrained optimization problem via only one backpropagation, thereby achieving efficient utility-preserving MU. Theoretically, we provide a tight convergence analysis of the algorithm. Empirically, our extensive experiments show that the proposed algorithm achieves better tradeoff results than existing baselines. Codes are available at https://github.com/anseryuer/EUPMU-Efficient-Utility-Preserving-Machine-Unlearning.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Neural Combinatorial Optimization Models</title>
<link>https://arxiv.org/abs/2510.22131</link>
<guid>https://arxiv.org/abs/2510.22131</guid>
<content:encoded><![CDATA[
arXiv:2510.22131v1 Announce Type: cross 
Abstract: Neural combinatorial optimization (NCO) has achieved remarkable performance, yet its learned model representations and decision rationale remain a black box. This impedes both academic research and practical deployment, since researchers and stakeholders require deeper insights into NCO models. In this paper, we take the first critical step towards interpreting NCO models by investigating their representations through various probing tasks. Moreover, we introduce a novel probing tool named Coefficient Significance Probing (CS-Probing) to enable deeper analysis of NCO representations by examining the coefficients and statistical significance during probing. Extensive experiments and analysis reveal that NCO models encode low-level information essential for solution construction, while capturing high-level knowledge to facilitate better decisions. Using CS-Probing, we find that prevalent NCO models impose varying inductive biases on their learned representations, uncover direct evidence related to model generalization, and identify key embedding dimensions associated with specific knowledge. These insights can be potentially translated into practice, for example, with minor code modifications, we improve the generalization of the analyzed model. Our work represents a first systematic attempt to interpret black-box NCO models, showcasing probing as a promising tool for analyzing their internal mechanisms and revealing insights for the NCO community. The source code is publicly available.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power to the Clients: Federated Learning in a Dictatorship Setting</title>
<link>https://arxiv.org/abs/2510.22149</link>
<guid>https://arxiv.org/abs/2510.22149</guid>
<content:encoded><![CDATA[
arXiv:2510.22149v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics</title>
<link>https://arxiv.org/abs/2510.22158</link>
<guid>https://arxiv.org/abs/2510.22158</guid>
<content:encoded><![CDATA[
arXiv:2510.22158v1 Announce Type: cross 
Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling interactions in large-scale multi-agent systems. Despite recent advancements in reinforcement learning (RL) for MFGs, existing methods are typically limited to finite spaces or stationary models, hindering their applicability to real-world problems. This paper introduces a novel deep reinforcement learning (DRL) algorithm specifically designed for non-stationary continuous MFGs. The proposed approach builds upon a Fictitious Play (FP) methodology, leveraging DRL for best-response computation and supervised learning for average policy representation. Furthermore, it learns a representation of the time-dependent population distribution using a Conditional Normalizing Flow. To validate the effectiveness of our method, we evaluate it on three different examples of increasing complexity. By addressing critical limitations in scalability and density approximation, this work represents a significant advancement in applying DRL techniques to complex MFG problems, bringing the field closer to real-world multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Non-Parametric Sampling with Representation</title>
<link>https://arxiv.org/abs/2510.22196</link>
<guid>https://arxiv.org/abs/2510.22196</guid>
<content:encoded><![CDATA[
arXiv:2510.22196v1 Announce Type: cross 
Abstract: Scaling and architectural advances have produced strikingly photorealistic image generative models, yet their mechanisms still remain opaque. Rather than advancing scaling, our goal is to strip away complicated engineering tricks and propose a simple, non-parametric generative model. Our design is grounded in three principles of natural images-(i) spatial non-stationarity, (ii) low-level regularities, and (iii) high-level semantics-and defines each pixel's distribution from its local context window. Despite its minimal architecture and no training, the model produces high-fidelity samples on MNIST and visually compelling CIFAR-10 images. This combination of simplicity and strong empirical performance points toward a minimal theory of natural-image structure. The model's white-box nature also allows us to have a mechanistic understanding of how the model generalizes and generates diverse images. We study it by tracing each generated pixel back to its source images. These analyses reveal a simple, compositional procedure for "part-whole generalization", suggesting a hypothesis for how large neural network generative models learn to generalize.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing</title>
<link>https://arxiv.org/abs/2510.22197</link>
<guid>https://arxiv.org/abs/2510.22197</guid>
<content:encoded><![CDATA[
arXiv:2510.22197v1 Announce Type: cross 
Abstract: Task-specific pre-training is essential when task representations diverge from generic pre-training features. Existing task-general pre-training EEG models struggle with complex tasks like emotion recognition due to mismatches between task-specific features and broad pre-training approaches. This work aims to develop a task-specific multi-dataset joint pre-training framework for cross-dataset emotion recognition, tackling problems of large inter-dataset distribution shifts, inconsistent emotion category definitions, and substantial inter-subject variability. We introduce a cross-dataset covariance alignment loss to align second-order statistical properties across datasets, enabling robust generalization without the need for extensive labels or per-subject calibration. To capture the long-term dependency and complex dynamics of EEG, we propose a hybrid encoder combining a Mamba-like linear attention channel encoder and a spatiotemporal dynamics model. Our method outperforms state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for few-shot emotion recognition and 11.92% in accuracy for zero-shot generalization to a new dataset. Performance scales with the increase of datasets used in pre-training. Multi-dataset joint pre-training achieves a performance gain of 8.55% over single-dataset training. This work provides a scalable framework for task-specific pre-training and highlights its benefit in generalizable affective computing. Our code is available at https://github.com/ncclab-sustech/mdJPT_nips2025.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing for UAVs in Cluttered Environments</title>
<link>https://arxiv.org/abs/2510.22204</link>
<guid>https://arxiv.org/abs/2510.22204</guid>
<content:encoded><![CDATA[
arXiv:2510.22204v1 Announce Type: cross 
Abstract: Autonomous landing in unstructured (cluttered, uneven, and map-poor) environments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet purely vision-based or deep learning models often falter under covariate shift and provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic framework that tightly couples two complementary pipelines: (i) an offline pipeline, where Large Language Models (LLMs) and human-in-the-loop refinement synthesize Scallop code from diverse landing scenarios, distilling generalizable and verifiable symbolic knowledge; and (ii) an online pipeline, where a compact foundation-based semantic segmentation model generates probabilistic Scallop facts that are composed into semantic scene graphs for real-time deductive reasoning. This design combines the perceptual strengths of lightweight foundation models with the interpretability and verifiability of symbolic reasoning. Node attributes (e.g., flatness, area) and edge relations (adjacency, containment, proximity) are computed with geometric routines rather than learned, avoiding the data dependence and latency of train-time graph builders. The resulting Scallop program encodes landing principles (avoid water and obstacles; prefer large, flat, accessible regions) and yields calibrated safety scores with ranked Regions of Interest (ROIs) and human-readable justifications. Extensive evaluations across datasets, diverse simulation maps, and real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger robustness to covariate shift, and superior efficiency compared with state-of-the-art baselines, while advancing UAV safety and reliability in emergency response, surveillance, and delivery missions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Right Place, Right Time: Market Simulation-based RL for Execution Optimisation</title>
<link>https://arxiv.org/abs/2510.22206</link>
<guid>https://arxiv.org/abs/2510.22206</guid>
<content:encoded><![CDATA[
arXiv:2510.22206v1 Announce Type: cross 
Abstract: Execution algorithms are vital to modern trading, they enable market participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more sophisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reactive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agent's performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings highlight the potential of reinforcement learning as a powerful tool in the trader's toolkit.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation</title>
<link>https://arxiv.org/abs/2510.22210</link>
<guid>https://arxiv.org/abs/2510.22210</guid>
<content:encoded><![CDATA[
arXiv:2510.22210v1 Announce Type: cross 
Abstract: Automated unit test generation is essential for robust software development, yet existing approaches struggle to generalize across multiple programming languages and operate within real-time development. While Large Language Models (LLMs) offer a promising solution, their ability to generate high coverage test code depends on prompting a concise context of the focal method. Current solutions, such as Retrieval-Augmented Generation, either rely on imprecise similarity-based searches or demand the creation of costly, language-specific static analysis pipelines. To address this gap, we present LSPRAG, a framework for concise-context retrieval tailored for real-time, language-agnostic unit test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP) back-ends to supply LLMs with precise symbol definitions and references in real time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware context retrieval, requiring minimal per-language engineering effort. We evaluated LSPRAG on open-source projects spanning Java, Go, and Python. Compared to the best performance of baselines, LSPRAG increased line coverage by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GALA: A GlobAl-LocAl Approach for Multi-Source Active Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.22214</link>
<guid>https://arxiv.org/abs/2510.22214</guid>
<content:encoded><![CDATA[
arXiv:2510.22214v1 Announce Type: cross 
Abstract: Domain Adaptation (DA) provides an effective way to tackle target-domain tasks by leveraging knowledge learned from source domains. Recent studies have extended this paradigm to Multi-Source Domain Adaptation (MSDA), which exploits multiple source domains carrying richer and more diverse transferable information. However, a substantial performance gap still remains between adaptation-based methods and fully supervised learning. In this paper, we explore a more practical and challenging setting, named Multi-Source Active Domain Adaptation (MS-ADA), to further enhance target-domain performance by selectively acquiring annotations from the target domain. The key difficulty of MS-ADA lies in designing selection criteria that can jointly handle inter-class diversity and multi-source domain variation. To address these challenges, we propose a simple yet effective GALA strategy (GALA), which combines a global k-means clustering step for target-domain samples with a cluster-wise local selection criterion, effectively tackling the above two issues in a complementary manner. Our proposed GALA is plug-and-play and can be seamlessly integrated into existing DA frameworks without introducing any additional trainable parameters. Extensive experiments on three standard DA benchmarks demonstrate that GALA consistently outperforms prior active learning and active DA methods, achieving performance comparable to the fully-supervised upperbound while using only 1% of the target annotations.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Error of Large Language Models at Pairwise Text Comparison</title>
<link>https://arxiv.org/abs/2510.22219</link>
<guid>https://arxiv.org/abs/2510.22219</guid>
<content:encoded><![CDATA[
arXiv:2510.22219v1 Announce Type: cross 
Abstract: We measure LLMs' output error at pairwise text comparison, noting the probability of error in their preferences. Our method does not rely on the ground truth and supports two scenarios: (i) uniform error rate regardless of the order of comparison, estimated with two comparisons for each text pair with either text placed first; (ii) binary positional bias assuming distinct error rates for the two orders of comparison, estimated with repeated comparisons between the texts. The Copeland counting constructs a ranking over the compared texts from pairwise preferences; the ranking reveals the poor scalability of LLM-based pairwise comparison and helps yield the estimates for LLMs' error rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini, Grok, Qwen) with five types of text input and obtain consistent estimates of LLMs' error. In general, the measured two positional bias terms are similar, close to the uniform error. Considering both the error rates and the robustness to the variation of prompts, Claude obtained the most desirable performance in this experiment. Our model outperforms the biased Bradley-Terry model and the commutativity score in indicating LLMs' error at this task.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Silent Failures: A Framework for Verifiable AI Reliability</title>
<link>https://arxiv.org/abs/2510.22224</link>
<guid>https://arxiv.org/abs/2510.22224</guid>
<content:encoded><![CDATA[
arXiv:2510.22224v1 Announce Type: cross 
Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems introduces a new reliability paradigm: silent failures, where AI produces confident but incorrect outputs that can be dangerous. This paper introduces the Formal Assurance and Monitoring Environment (FAME), a novel framework that confronts this challenge. FAME synergizes the mathematical rigor of offline formal synthesis with the vigilance of online runtime monitoring to create a verifiable safety net around opaque AI components. We demonstrate its efficacy in an autonomous vehicle perception system, where FAME successfully detected 93.5% of critical safety violations that were otherwise silent. By contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards, we provide reliability engineers with a practical, certifiable pathway for deploying trustworthy AI. FAME represents a crucial shift from accepting probabilistic performance to enforcing provable safety in next-generation systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs</title>
<link>https://arxiv.org/abs/2510.22228</link>
<guid>https://arxiv.org/abs/2510.22228</guid>
<content:encoded><![CDATA[
arXiv:2510.22228v1 Announce Type: cross 
Abstract: Layer pruning has emerged as a widely adopted technique for improving the efficiency of large language models (LLMs). Although existing methods demonstrate strong performance retention on general knowledge tasks, their effect on long-chain reasoning, a more brittle yet crucial capability, remains largely unexplored. In this work, we study the impact of layer pruning on long-chain reasoning through the lens of test-time scaling, a key mechanism in modern LLMs that enables strong reasoning capacity by allocating more computation at inference time. With extensive experiments, we demonstrate that pruning even one or two layers can severely impair test-time scaling, with performance collapsing drastically on long reasoning benchmarks even when performance on knowledge-intensive and shallow reasoning tasks remains stable. Furthermore, we find that standard supervised fine-tuning remedies fail to recover test-time scaling once it has deteriorated. Through in-depth analyses, we identify the mechanisms underlying this fragility of test-time scaling and highlight the fundamental risks of applying layer pruning to reasoning-intensive LLMs. These findings call for a rethinking of layer pruning strategies and provide insights for developing methods that preserve the robustness of reasoning. We open-source the codebase in \href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation</title>
<link>https://arxiv.org/abs/2510.22232</link>
<guid>https://arxiv.org/abs/2510.22232</guid>
<content:encoded><![CDATA[
arXiv:2510.22232v1 Announce Type: cross 
Abstract: Cooperative systems often remain in persistently suboptimal yet stable states. This paper explains such "rational stagnation" as an equilibrium sustained by a rational adversary whose utility follows the principle of potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's Dilemma, we show that the transformation $u_{i}' = a\,u_{i} + b\,u_{j}$ and the ratio of mutual recognition $w = b/a$ generate a fragile cooperation band $[w_{\min},\,w_{\max}]$ where both (C,C) and (D,D) are equilibria. Extending to a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention costs $(C_{c},\,C_{m})$, a Bellman-style analysis yields three strategic regimes: immediate destruction, rational stagnation, and intervention abandonment. The appendix further generalizes the utility to a reference-dependent nonlinear form and proves its stability under reference shifts, ensuring robustness of the framework. Applications to social-media algorithms and political trust illustrate how adversarial rationality can deliberately preserve fragility.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading</title>
<link>https://arxiv.org/abs/2510.22242</link>
<guid>https://arxiv.org/abs/2510.22242</guid>
<content:encoded><![CDATA[
arXiv:2510.22242v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) increasingly serve as research assistants, yet their reliability in scholarly tasks remains under-evaluated. In this work, we introduce PaperAsk, a benchmark that systematically evaluates LLMs across four key research tasks: citation retrieval, content extraction, paper discovery, and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under realistic usage conditions-via web interfaces where search operations are opaque to the user. Through controlled experiments, we find consistent reliability failures: citation retrieval fails in 48-98% of multi-reference queries, section-specific content extraction fails in 72-91% of cases, and topical paper discovery yields F1 scores below 0.32, missing over 60% of relevant literature. Further human analysis attributes these failures to the uncontrolled expansion of retrieved context and the tendency of LLMs to prioritize semantically relevant text over task instructions. Across basic tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds responses rather than risk errors, whereas Gemini produces fluent but fabricated answers. To address these issues, we develop lightweight reliability classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk provides a reproducible and diagnostic framework for advancing the reliability evaluation of LLM-based scholarly assistance systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework</title>
<link>https://arxiv.org/abs/2510.22243</link>
<guid>https://arxiv.org/abs/2510.22243</guid>
<content:encoded><![CDATA[
arXiv:2510.22243v1 Announce Type: cross 
Abstract: Semantic segmentation has emerged as a fundamental problem in computer vision, gaining particular importance in real-time applications such as autonomous driving. The main challenge is achieving high accuracy while operating under computational and hardware constraints. In this research, we present an FPGA-based implementation of real-time semantic segmentation leveraging the lightweight LMIINet architecture and the Coarse-Grained Reconfigurable Array for Machine Learning (CGRA4ML) hardware framework. The model was trained using Quantization-Aware Training (QAT) with 8-bit precision on the Cityscapes dataset, reducing memory footprint by a factor of four while enabling efficient fixed-point computations. Necessary modifications were applied to adapt the model to CGRA4ML constraints, including simplifying skip connections, employing hardware-friendly operations such as depthwise-separable and 1A-1 convolutions, and redesigning parts of the Flatten Transformer. Our implementation achieves approximately 90% pixel accuracy and 45% mean Intersection-over-Union (mIoU), operating in real-time at 20 frames per second (FPS) with 50.1 ms latency on the ZCU104 FPGA board. The results demonstrate the potential of CGRA4ML, with its flexibility in mapping modern layers and off-chip memory utilization for skip connections, provides a path for implementing advanced semantic segmentation networks on FPGA for real-time applications to outperform traditional GPU solutions in terms of power efficiency while maintaining competitive accuracy. The code for this project is publicly available at https://github.com/STAmirr/ cgra4ml_semantic_segmentation
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Don't Need Prompt Engineering Anymore: The Prompting Inversion</title>
<link>https://arxiv.org/abs/2510.22251</link>
<guid>https://arxiv.org/abs/2510.22251</guid>
<content:encoded><![CDATA[
arXiv:2510.22251v1 Announce Type: cross 
Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting, significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a constrained, rule-based prompting method designed to improve upon standard CoT by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5) using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00% vs. 96.36% for CoT on full benchmark). We trace this to a "Guardrail-to-Handcuff" transition where constraints preventing common-sense errors in mid-tier models induce hyper-literalism in advanced models. Our detailed error analysis demonstrates that optimal prompting strategies must co-evolve with model capabilities, suggesting simpler prompts for more capable models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis</title>
<link>https://arxiv.org/abs/2510.22257</link>
<guid>https://arxiv.org/abs/2510.22257</guid>
<content:encoded><![CDATA[
arXiv:2510.22257v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) offers a non-invasive lens into human brain activity, but building large-scale models is hampered by topological heterogeneity: each public EEG data defines its own electrode layout, limiting generalization. We introduce LUNA (Latent Unified Network Architecture), a self-supervised foundation model that reconciles disparate electrode geometries while scaling linearly -- not quadratically -- with channel count. LUNA compresses multi-channel EEG into a fixed-size, topology-agnostic latent space via learned queries and cross-attention. Downstream transformer blocks then operate exclusively on this latent representation using patch-wise temporal self-attention, decoupling computation from electrode count. Pre-trained on TUEG and Siena (over 21,000 hours of raw EEG across diverse montages) using a masked-patch reconstruction objective, LUNA transfers effectively to four downstream tasks: abnormality detection, artifact rejection, slowing classification, and emotion recognition. It demonstrates highly competitive performance across several benchmarks, achieving state-of-the-art results on TUAR and TUSL, e.g., 0.921 AUROC on TUAR, while reducing FLOPs by 300x and trimming GPU memory use by up to 10x. Critically, these gains are consistent across all evaluated electrode configurations. Code is available at https://github.com/pulp-bio/BioFoundation
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know</title>
<link>https://arxiv.org/abs/2510.22261</link>
<guid>https://arxiv.org/abs/2510.22261</guid>
<content:encoded><![CDATA[
arXiv:2510.22261v1 Announce Type: cross 
Abstract: Machine learning has achieved remarkable successes, yet its deployment in safety-critical domains remains hindered by an inherent inability to manage uncertainty, resulting in overconfident and unreliable predictions when models encounter out-of-distribution data, adversarial perturbations, or naturally fluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling Machine Learning Models to 'Know When They Do Not Know', addresses these critical challenges by advancing the paradigm of Epistemic Artificial Intelligence, which explicitly models and quantifies epistemic uncertainty: the uncertainty arising from limited, biased, or incomplete training data, as opposed to the irreducible randomness of aleatoric uncertainty, thereby empowering models to acknowledge their limitations and refrain from overconfident decisions when uncertainty is high.
  Central to this work is the development of the Random-Set Neural Network (RS-NN), a novel methodology that leverages random set theory to predict belief functions over sets of classes, capturing the extent of epistemic uncertainty through the width of associated credal sets, applications of RS-NN, including its adaptation to Large Language Models (LLMs) and its deployment in weather classification for autonomous racing. In addition, the thesis proposes a unified evaluation framework for uncertainty-aware classifiers. Extensive experiments validate that integrating epistemic awareness into deep learning not only mitigates the risks associated with overconfident predictions but also lays the foundation for a paradigm shift in artificial intelligence, where the ability to 'know when it does not know' becomes a hallmark of robust and dependable systems. The title encapsulates the core philosophy of this work, emphasizing that true intelligence involves recognizing and managing the limits of one's own knowledge.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</title>
<link>https://arxiv.org/abs/2510.22264</link>
<guid>https://arxiv.org/abs/2510.22264</guid>
<content:encoded><![CDATA[
arXiv:2510.22264v1 Announce Type: cross 
Abstract: Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-level Analysis of Factors Associated with Student Performance: A Machine Learning Approach to the SAEB Microdata</title>
<link>https://arxiv.org/abs/2510.22266</link>
<guid>https://arxiv.org/abs/2510.22266</guid>
<content:encoded><![CDATA[
arXiv:2510.22266v1 Announce Type: cross 
Abstract: Identifying the factors that influence student performance in basic education is a central challenge for formulating effective public policies in Brazil. This study introduces a multi-level machine learning approach to classify the proficiency of 9th-grade and high school students using microdata from the System of Assessment of Basic Education (SAEB). Our model uniquely integrates four data sources: student socioeconomic characteristics, teacher professional profiles, school indicators, and director management profiles. A comparative analysis of four ensemble algorithms confirmed the superiority of a Random Forest model, which achieved 90.2% accuracy and an Area Under the Curve (AUC) of 96.7%. To move beyond prediction, we applied Explainable AI (XAI) using SHAP, which revealed that the school's average socioeconomic level is the most dominant predictor, demonstrating that systemic factors have a greater impact than individual characteristics in isolation. The primary conclusion is that academic performance is a systemic phenomenon deeply tied to the school's ecosystem. This study provides a data-driven, interpretable tool to inform policies aimed at promoting educational equity by addressing disparities between schools.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.22282</link>
<guid>https://arxiv.org/abs/2510.22282</guid>
<content:encoded><![CDATA[
arXiv:2510.22282v1 Announce Type: cross 
Abstract: Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to solve this task by treating it as a multi-modal perception and understanding problem. However, recent studies reveal that LVLMs still struggle with accurate and interpretable socio-economic predictions from visual data. To address these limitations and maximize the potential of LVLMs, we introduce \textbf{CityRiSE}, a novel framework for \textbf{R}eason\textbf{i}ng urban \textbf{S}ocio-\textbf{E}conomic status in LVLMs through pure reinforcement learning (RL). With carefully curated multi-modal data and verifiable reward design, our approach guides the LVLM to focus on semantically meaningful visual cues, enabling structured and goal-oriented reasoning for generalist socio-economic status prediction. Experiments demonstrate that CityRiSE with emergent reasoning process significantly outperforms existing baselines, improving both prediction accuracy and generalization across diverse urban contexts, particularly for prediction on unseen cities and unseen indicators. This work highlights the promise of combining RL and LVLMs for interpretable and generalist urban socio-economic sensing.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER</title>
<link>https://arxiv.org/abs/2510.22285</link>
<guid>https://arxiv.org/abs/2510.22285</guid>
<content:encoded><![CDATA[
arXiv:2510.22285v1 Announce Type: cross 
Abstract: We study clinical Named Entity Recognition (NER) on the CADEC corpus and compare three families of approaches: (i) BERT-style encoders (BERT Base, BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with supervised fine-tuning (SFT). All models are evaluated on standard NER metrics over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding). RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base, showing the limit of these family of models. Among LLM settings, simple ICL outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that the LLM achieve higher accuracy on simplified tasks, restricting classification to two labels.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Homophily Help in Robust Test-time Node Classification?</title>
<link>https://arxiv.org/abs/2510.22289</link>
<guid>https://arxiv.org/abs/2510.22289</guid>
<content:encoded><![CDATA[
arXiv:2510.22289v1 Announce Type: cross 
Abstract: Homophily, the tendency of nodes from the same class to connect, is a fundamental property of real-world graphs, underpinning structural and semantic patterns in domains such as citation networks and social networks. Existing methods exploit homophily through designing homophily-aware GNN architectures or graph structure learning strategies, yet they primarily focus on GNN learning with training graphs. However, in real-world scenarios, test graphs often suffer from data quality issues and distribution shifts, such as domain shifts across users from different regions in social networks and temporal evolution shifts in citation network graphs collected over varying time periods. These factors significantly compromise the pre-trained model's robustness, resulting in degraded test-time performance. With empirical observations and theoretical analysis, we reveal that transforming the test graph structure by increasing homophily in homophilic graphs or decreasing it in heterophilic graphs can significantly improve the robustness and performance of pre-trained GNNs on node classifications, without requiring model training or update. Motivated by these insights, a novel test-time graph structural transformation method grounded in homophily, named GrapHoST, is proposed. Specifically, a homophily predictor is developed to discriminate test edges, facilitating adaptive test-time graph structural transformation by the confidence of predicted homophily scores. Extensive experiments on nine benchmark datasets under a range of test-time data quality issues demonstrate that GrapHoST consistently achieves state-of-the-art performance, with improvements of up to 10.92%. Our code has been released at https://github.com/YanJiangJerry/GrapHoST.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</title>
<link>https://arxiv.org/abs/2510.22300</link>
<guid>https://arxiv.org/abs/2510.22300</guid>
<content:encoded><![CDATA[
arXiv:2510.22300v1 Announce Type: cross 
Abstract: Using risky text prompts, such as pornography and violent prompts, to test the safety of text-to-image (T2I) models is a critical task. However, existing risky prompt datasets are limited in three key areas: 1) limited risky categories, 2) coarse-grained annotation, and 3) low effectiveness. To address these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark designed for evaluating safety-related tasks in T2I models. Specifically, we first develop a hierarchical risk taxonomy, which consists of 6 primary categories and 14 fine-grained subcategories. Building upon this taxonomy, we construct a pipeline to collect and annotate risky prompts. Finally, we obtain 6,432 effective risky prompts, where each prompt is annotated with both hierarchical category labels and detailed risk reasons. Moreover, to facilitate the evaluation, we propose a reason-driven risky image detection method that explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt, we conduct a comprehensive evaluation of eight T2I models, nine defense methods, five safety filters, and five attack strategies, offering nine key insights into the strengths and limitations of T2I model safety. Finally, we discuss potential applications of T2I-RiskyPrompt across various research fields. The dataset and code are provided in https://github.com/datar001/T2I-RiskyPrompt.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals</title>
<link>https://arxiv.org/abs/2510.22301</link>
<guid>https://arxiv.org/abs/2510.22301</guid>
<content:encoded><![CDATA[
arXiv:2510.22301v1 Announce Type: cross 
Abstract: Timely access to laboratory values is critical for clinical decision-making, yet current approaches rely on invasive venous sampling and are intrinsically delayed. Electrocardiography (ECG), as a non-invasive and widely available signal, offers a promising modality for rapid laboratory estimation. Recent progress in deep learning has enabled the extraction of latent hematological signatures from ECGs. However, existing models are constrained by low signal-to-noise ratios, substantial inter-individual variability, limited data diversity, and suboptimal generalization, especially when adapted to low-lead wearable devices. In this work, we conduct an exploratory study leveraging transfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG foundation model, on the Multimodal Clinical Monitoring in the Emergency Department (MC-MED) dataset from Stanford. We generated a corpus of more than 20 million standardized ten-second ECG segments to enhance sensitivity to subtle biochemical correlates. On internal validation, the model demonstrated strong predictive performance (area under the curve above 0.65) for thirty-three laboratory indicators, moderate performance (between 0.55 and 0.65) for fifty-nine indicators, and limited performance (below 0.55) for sixteen indicators. This study provides an efficient artificial-intelligence driven solution and establishes the feasibility scope for real-time, non-invasive estimation of laboratory values.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery</title>
<link>https://arxiv.org/abs/2510.22312</link>
<guid>https://arxiv.org/abs/2510.22312</guid>
<content:encoded><![CDATA[
arXiv:2510.22312v1 Announce Type: cross 
Abstract: Analogical reasoning, the transfer of relational structures across contexts (e.g., planet is to sun as electron is to nucleus), is fundamental to scientific discovery. Yet human insight is often constrained by domain expertise and surface-level biases, limiting access to deeper, structure-driven analogies both within and across disciplines. Large language models (LLMs), trained on vast cross-domain data, present a promising yet underexplored tool for analogical reasoning in science. Here, we demonstrate that LLMs can generate novel battery materials by (1) retrieving cross-domain analogs and analogy-guided exemplars to steer exploration beyond conventional dopant substitutions, and (2) constructing in-domain analogical templates from few labeled examples to guide targeted exploitation. These explicit analogical reasoning strategies yield candidates outside established compositional spaces and outperform standard prompting baselines. Our findings position LLMs as interpretable, expert-like hypothesis generators that leverage analogy-driven generalization for scientific innovation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus</title>
<link>https://arxiv.org/abs/2510.22318</link>
<guid>https://arxiv.org/abs/2510.22318</guid>
<content:encoded><![CDATA[
arXiv:2510.22318v1 Announce Type: cross 
Abstract: Software testing is a critical component in the software engineering field and is important for software engineering education. Thus, it is vital for academia to continuously improve and update educational methods to reflect the current state of the field. The International Software Testing Qualifications Board (ISTQB) certification framework is globally recognized and widely adopted in industry and academia. However, ISTQB-based learning has been rarely applied with recent generative artificial intelligence advances. Despite the growing capabilities of large language models (LLMs), ISTQB-based learning and instruction with LLMs have not been thoroughly explored. This paper explores and evaluates how LLMs can complement the ISTQB framework for higher education. The findings present four key contributions: (i) the creation of a comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28 sample exams and 1,145 questions; (ii) the development of a domain-optimized prompt that enhances LLM precision and explanation quality on ISTQB tasks; (iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and (iv) actionable insights and recommendations for integrating LLMs into software testing education. These findings highlight the promise of LLMs in supporting ISTQB certification preparation and offer a foundation for their broader use in software engineering at higher education.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Target-Stance Extraction</title>
<link>https://arxiv.org/abs/2510.22334</link>
<guid>https://arxiv.org/abs/2510.22334</guid>
<content:encoded><![CDATA[
arXiv:2510.22334v1 Announce Type: cross 
Abstract: Social media enables data-driven analysis of public opinion on contested issues. Target-Stance Extraction (TSE) is the task of identifying the target discussed in a document and the document's stance towards that target. Many works classify stance towards a given target in a multilingual setting, but all prior work in TSE is English-only. This work introduces the first multilingual TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and Spanish corpora. It manages to extend the original TSE pipeline to a multilingual setting without requiring separate models for each language. Our model pipeline achieves a modest F1 score of 12.78, underscoring the increased difficulty of the multilingual task relative to English-only setups and highlighting target prediction as the primary bottleneck. We are also the first to demonstrate the sensitivity of TSE's F1 score to different target verbalizations. Together these serve as a much-needed baseline for resources, algorithms, and evaluation criteria in multilingual TSE.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction</title>
<link>https://arxiv.org/abs/2510.22335</link>
<guid>https://arxiv.org/abs/2510.22335</guid>
<content:encoded><![CDATA[
arXiv:2510.22335v1 Announce Type: cross 
Abstract: Reconstructing visual stimuli from fMRI signals is a central challenge bridging machine learning and neuroscience. Recent diffusion-based methods typically map fMRI activity to a single high-level embedding, using it as fixed guidance throughout the entire generation process. However, this fixed guidance collapses hierarchical neural information and is misaligned with the stage-dependent demands of image reconstruction. In response, we propose MindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on scale-wise autoregressive modeling. MindHier introduces three components: a Hierarchical fMRI Encoder to extract multi-level neural embeddings, a Hierarchy-to-Hierarchy Alignment scheme to enforce layer-wise correspondence with CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy to inject these embeddings into autoregression at matching scales. These designs make MindHier an efficient and cognitively-aligned alternative to diffusion-based methods by enabling a hierarchical reconstruction process that synthesizes global semantics before refining local details, akin to human visual perception. Extensive experiments on the NSD dataset show that MindHier achieves superior semantic fidelity, 4.67x faster inference, and more deterministic results than the diffusion-based baselines.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery</title>
<link>https://arxiv.org/abs/2510.22336</link>
<guid>https://arxiv.org/abs/2510.22336</guid>
<content:encoded><![CDATA[
arXiv:2510.22336v1 Announce Type: cross 
Abstract: Humanoid robots represent a central frontier in embodied intelligence, as their anthropomorphic form enables natural deployment in humans' workspace. Brain-body co-design for humanoids presents a promising approach to realizing this potential by jointly optimizing control policies and physical morphology. Within this context, fall recovery emerges as a critical capability. It not only enhances safety and resilience but also integrates naturally with locomotion systems, thereby advancing the autonomy of humanoids. In this paper, we propose RoboCraft, a scalable humanoid co-design framework for fall recovery that iteratively improves performance through the coupled updates of control policy and morphology. A shared policy pretrained across multiple designs is progressively finetuned on high-performing morphologies, enabling efficient adaptation without retraining from scratch. Concurrently, morphology search is guided by human-inspired priors and optimization algorithms, supported by a priority buffer that balances reevaluation of promising candidates with the exploration of novel designs. Experiments show that \ourmethod{} achieves an average performance gain of 44.55% on seven public humanoid robots, with morphology optimization drives at least 40% of improvements in co-designing four humanoid robots, underscoring the critical role of humanoid co-design.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.22344</link>
<guid>https://arxiv.org/abs/2510.22344</guid>
<content:encoded><![CDATA[
arXiv:2510.22344v1 Announce Type: cross 
Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models</title>
<link>https://arxiv.org/abs/2510.22366</link>
<guid>https://arxiv.org/abs/2510.22366</guid>
<content:encoded><![CDATA[
arXiv:2510.22366v1 Announce Type: cross 
Abstract: Diffusion models have advanced rapidly in recent years, producing high-fidelity images while raising concerns about intellectual property protection and the misuse of generative AI. Image watermarking for diffusion models, particularly Noise-as-Watermark (NaW) methods, encode watermark as specific standard Gaussian noise vector for image generation, embedding the infomation seamlessly while maintaining image quality. For detection, the generation process is inverted to recover the initial noise vector containing the watermark before extraction. However, existing NaW methods struggle to balance watermark robustness with generation diversity. Some methods achieve strong robustness by heavily constraining initial noise sampling, which degrades user experience, while others preserve diversity but prove too fragile for real-world deployment. To address this issue, we propose T2SMark, a two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike prior methods that simply map bits to positive or negative values, TTS enhances robustness by embedding bits exclusively in the reliable tail regions while randomly sampling the central zone to preserve the latent distribution. Our two-stage framework then ensures sampling diversity by integrating a randomly generated session key into both encryption pipelines. We evaluate T2SMark on diffusion models with both U-Net and DiT backbones. Extensive experiments show that it achieves an optimal balance between robustness and diversity. Our code is available at \href{https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2510.22370</link>
<guid>https://arxiv.org/abs/2510.22370</guid>
<content:encoded><![CDATA[
arXiv:2510.22370v1 Announce Type: cross 
Abstract: In this paper, we propose Bootstrapped Language-Image Pretraining-driven Fused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a novel multimodal reinforcement learning (RL) framework for autonomous lane-keeping (LK), in which semantic embeddings generated by a vision-language model (VLM) are directly fused with geometric states, LiDAR observations, and Proportional-Integral-Derivative-based (PID) control feedback within the agent observation space. The proposed method lets the agent learn driving rules that are aware of their surroundings and easy to understand by combining high-level scene understanding from the VLM with low-level control and spatial signals. Our architecture brings together semantic, geometric, and control-aware representations to make policy learning more robust. A hybrid reward function that includes semantic alignment, LK accuracy, obstacle avoidance, and speed regulation helps learning to be more efficient and generalizable. Our method is different from the approaches that only use semantic models to shape rewards. Instead, it directly embeds semantic features into the state representation. This cuts down on expensive runtime inference and makes sure that semantic guidance is always available. The simulation results show that the proposed model is better at LK stability and adaptability than the best vision-based and multimodal RL baselines in a wide range of difficult driving situations. We make our code publicly available.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</title>
<link>https://arxiv.org/abs/2510.22373</link>
<guid>https://arxiv.org/abs/2510.22373</guid>
<content:encoded><![CDATA[
arXiv:2510.22373v1 Announce Type: cross 
Abstract: Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
<link>https://arxiv.org/abs/2510.22379</link>
<guid>https://arxiv.org/abs/2510.22379</guid>
<content:encoded><![CDATA[
arXiv:2510.22379v1 Announce Type: cross 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Large-Deformation Medical Image Registration via Recurrent Dynamic Correlation</title>
<link>https://arxiv.org/abs/2510.22380</link>
<guid>https://arxiv.org/abs/2510.22380</guid>
<content:encoded><![CDATA[
arXiv:2510.22380v1 Announce Type: cross 
Abstract: Deformable image registration estimates voxel-wise correspondences between images through spatial transformations, and plays a key role in medical imaging. While deep learning methods have significantly reduced runtime, efficiently handling large deformations remains a challenging task. Convolutional networks aggregate local features but lack direct modeling of voxel correspondences, promoting recent works to explore explicit feature matching. Among them, voxel-to-region matching is more efficient for direct correspondence modeling by computing local correlation features whithin neighbourhoods, while region-to-region matching incurs higher redundancy due to excessive correlation pairs across large regions. However, the inherent locality of voxel-to-region matching hinders the capture of long-range correspondences required for large deformations. To address this, we propose a Recurrent Correlation-based framework that dynamically relocates the matching region toward more promising positions. At each step, local matching is performed with low cost, and the estimated offset guides the next search region, supporting efficient convergence toward large deformations. In addition, we uses a lightweight recurrent update module with memory capacity and decouples motion-related and texture features to suppress semantic redundancy. We conduct extensive experiments on brain MRI and abdominal CT datasets under two settings: with and without affine pre-registration. Results show that our method exibits a strong accuracy-computation trade-off, surpassing or matching the state-of-the-art performance. For example, it achieves comparable performance on the non-affine OASIS dataset, while using only 9.5% of the FLOPs and running 96% faster than RDP, a representative high-performing method.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization</title>
<link>https://arxiv.org/abs/2510.22383</link>
<guid>https://arxiv.org/abs/2510.22383</guid>
<content:encoded><![CDATA[
arXiv:2510.22383v1 Announce Type: cross 
Abstract: Regularization techniques play a crucial role in preventing overfitting and improving the generalization performance of neural networks. Dropout, a widely used regularization technique, randomly deactivates units during training to introduce redundancy and prevent co-adaptation among neurons. Despite its effectiveness, dropout has limitations, such as its static nature and lack of interpretability. In this paper, we propose a novel approach to regularization by substituting dropout with Conway's Game of Life (GoL), a cellular automata with simple rules that govern the evolution of a grid of cells. We introduce dynamic unit deactivation during training by representing neural network units as cells in a GoL grid and applying the game's rules to deactivate units. This approach allows for the emergence of spatial patterns that adapt to the training data, potentially enhancing the network's ability to generalize. We demonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing that dynamic unit deactivation using GoL achieves comparable performance to traditional dropout techniques while offering insights into the network's behavior through the visualization of evolving patterns. Furthermore, our discussion highlights the applicability of our proposal in deeper architectures, demonstrating how it enhances the performance of different dropout techniques.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Small and Reasoning Large Language Models Score Journal Articles for Research Quality and Do Averaging and Few-shot Help?</title>
<link>https://arxiv.org/abs/2510.22389</link>
<guid>https://arxiv.org/abs/2510.22389</guid>
<content:encoded><![CDATA[
arXiv:2510.22389v1 Announce Type: cross 
Abstract: Assessing published academic journal articles is a common task for evaluations of departments and individuals. Whilst it is sometimes supported by citation data, Large Language Models (LLMs) may give more useful indications of article quality. Evidence of this capability exists for two of the largest LLM families, ChatGPT and Gemini, and the medium sized LLM Gemma3 27b, but it is unclear whether smaller LLMs and reasoning models have similar abilities. This is important because larger models may be slow and impractical in some situations, and reasoning models may perform differently. Four relevant questions are addressed with Gemma3 variants, Llama4 Scout, Qwen3, Magistral Small and DeepSeek R1, on a dataset of 2,780 medical, health and life science papers in 6 fields, with two different gold standards, one novel. The results suggest that smaller (open weights) and reasoning LLMs have similar performance to ChatGPT 4o-mini and Gemini 2.0 Flash, but that 1b parameters may often, and 4b sometimes, be too few. Moreover, averaging scores from multiple identical queries seems to be a universally successful strategy, and few-shot prompts (four examples) tended to help but the evidence was equivocal. Reasoning models did not have a clear advantage. Overall, the results show, for the first time, that smaller LLMs >4b, including reasoning models, have a substantial capability to score journal articles for research quality, especially if score averaging is used.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-Down Semantic Refinement for Image Captioning</title>
<link>https://arxiv.org/abs/2510.22391</link>
<guid>https://arxiv.org/abs/2510.22391</guid>
<content:encoded><![CDATA[
arXiv:2510.22391v1 Announce Type: cross 
Abstract: Large Vision-Language Models (VLMs) face an inherent contradiction in image captioning: their powerful single-step generation capabilities often lead to a myopic decision-making process. This makes it difficult to maintain global narrative coherence while capturing rich details, a limitation that is particularly pronounced in tasks that require multi-step and complex scene description. To overcome this fundamental challenge, we redefine image captioning as a goal-oriented hierarchical refinement planning problem, and further propose a novel framework, named Top-Down Semantic Refinement (TDSR), which models the generation process as a Markov Decision Process (MDP). However, planning within the vast state space of a VLM presents a significant computational hurdle. Our core contribution, therefore, is the design of a highly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By incorporating a visual-guided parallel expansion and a lightweight value network, our TDSR reduces the call frequency to the expensive VLM by an order of magnitude without sacrificing planning quality. Furthermore, an adaptive early stopping mechanism dynamically matches computational overhead to the image's complexity. Extensive experiments on multiple benchmarks, including DetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a plug-and-play module, can significantly enhance the performance of existing VLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly competitive results in fine-grained description, compositional generalization, and hallucination suppression.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-guided Continual Learning for Behavioral Analytics Systems</title>
<link>https://arxiv.org/abs/2510.22405</link>
<guid>https://arxiv.org/abs/2510.22405</guid>
<content:encoded><![CDATA[
arXiv:2510.22405v1 Announce Type: cross 
Abstract: User behavior on online platforms is evolving, reflecting real-world changes in how people post, whether it's helpful messages or hate speech. Models that learn to capture this content can experience a decrease in performance over time due to data drift, which can lead to ineffective behavioral analytics systems. However, fine-tuning such a model over time with new data can be detrimental due to catastrophic forgetting. Replay-based approaches in continual learning offer a simple yet efficient method to update such models, minimizing forgetting by maintaining a buffer of important training instances from past learned tasks. However, the main limitation of this approach is the fixed size of the buffer. External knowledge bases can be utilized to overcome this limitation through data augmentation. We propose a novel augmentation-based approach to incorporate external knowledge in the replay-based continual learning framework. We evaluate several strategies with three datasets from prior studies related to deviant behavior classification to assess the integration of external knowledge in continual learning and demonstrate that augmentation helps outperform baseline replay-based approaches.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group size effects and collective misalignment in LLM multi-agent systems</title>
<link>https://arxiv.org/abs/2510.22422</link>
<guid>https://arxiv.org/abs/2510.22422</guid>
<content:encoded><![CDATA[
arXiv:2510.22422v1 Announce Type: cross 
Abstract: Multi-agent systems of large language models (LLMs) are rapidly expanding across domains, introducing dynamics not captured by single-agent evaluations. Yet, existing work has mostly contrasted the behavior of a single agent with that of a collective of fixed size, leaving open a central question: how does group size shape dynamics? Here, we move beyond this dichotomy and systematically explore outcomes across the full range of group sizes. We focus on multi-agent misalignment, building on recent evidence that interacting LLMs playing a simple coordination game can generate collective biases absent in individual models. First, we show that collective bias is a deeper phenomenon than previously assessed: interaction can amplify individual biases, introduce new ones, or override model-level preferences. Second, we demonstrate that group size affects the dynamics in a non-linear way, revealing model-dependent dynamical regimes. Finally, we develop a mean-field analytical approach and show that, above a critical population size, simulations converge to deterministic predictions that expose the basins of attraction of competing equilibria. These findings establish group size as a key driver of multi-agent dynamics and highlight the need to consider population-level effects when deploying LLM-based systems at scale.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching</title>
<link>https://arxiv.org/abs/2510.22439</link>
<guid>https://arxiv.org/abs/2510.22439</guid>
<content:encoded><![CDATA[
arXiv:2510.22439v1 Announce Type: cross 
Abstract: Room impulse response (RIR) generation remains a critical challenge for creating immersive virtual acoustic environments. Current methods suffer from two fundamental limitations: the scarcity of full-band RIR datasets and the inability of existing models to generate acoustically accurate responses from diverse input modalities. We present PromptReverb, a two-stage generative framework that addresses these challenges. Our approach combines a variational autoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and a conditional diffusion transformer model based on rectified flow matching that generates RIRs from descriptions in natural language. Empirical evaluation demonstrates that PromptReverb produces RIRs with superior perceptual quality and acoustic accuracy compared to existing methods, achieving 8.8% mean RT60 error compared to -37% for widely used baselines and yielding more realistic room-acoustic parameters. Our method enables practical applications in virtual reality, architectural acoustics, and audio production where flexible, high-quality RIR synthesis is essential.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartMixed: A Two-Phase Training Strategy for Adaptive Activation Function Learning in Neural Networks</title>
<link>https://arxiv.org/abs/2510.22450</link>
<guid>https://arxiv.org/abs/2510.22450</guid>
<content:encoded><![CDATA[
arXiv:2510.22450v1 Announce Type: cross 
Abstract: The choice of activation function plays a critical role in neural networks, yet most architectures still rely on fixed, uniform activation functions across all neurons. We introduce SmartMixed, a two-phase training strategy that allows networks to learn optimal per-neuron activation functions while preserving computational efficiency at inference. In the first phase, neurons adaptively select from a pool of candidate activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU, ELU, SELU) using a differentiable hard-mixture mechanism. In the second phase, each neuron's activation function is fixed according to the learned selection, resulting in a computationally efficient network that supports continued training with optimized vectorized operations. We evaluate SmartMixed on the MNIST dataset using feedforward neural networks of varying depths. The analysis shows that neurons in different layers exhibit distinct preferences for activation functions, providing insights into the functional diversity within neural architectures.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>