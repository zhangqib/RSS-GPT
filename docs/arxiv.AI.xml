<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>


<item>
<title>Emergent Specialization: Rare Token Neurons in Language Models</title>
<link>https://arxiv.org/abs/2505.12822</link>
<guid>https://arxiv.org/abs/2505.12822</guid>
<content:encoded><![CDATA[
<div> neuron structures, rare tokens, language models, specialization, weight distributions
Summary:<br />
- Neuron structures called rare token neurons have a significant impact on language models' prediction of rare tokens in specialized domains.
- These rare token neurons follow a three-phase organization during training, showing a dynamic evolution from a homogeneous state to a specialized architecture.
- Rare token neurons coordinate in activation space, forming a subnetwork that co-activates selectively and avoids co-activation with other neurons.
- The specialization of rare token neurons may be linked to the development of heavy-tailed weight distributions, suggesting a statistical mechanical basis for their emergence.
- The study sheds light on how language models struggle with representing and generating rare tokens, providing insights into the behavior and mechanism of rare token neurons. 
<br /><br />Summary: <div>
arXiv:2505.12822v2 Announce Type: replace 
Abstract: Large language models struggle with representing and generating rare tokens despite their importance in specialized domains. In this study, we identify neuron structures with exceptionally strong influence on language model's prediction of rare tokens, termed as rare token neurons, and investigate the mechanism for their emergence and behavior. These neurons exhibit a characteristic three-phase organization (plateau, power-law, and rapid decay) that emerges dynamically during training, evolving from a homogeneous initial state to a functionally differentiated architecture. In the activation space, rare token neurons form a coordinated subnetwork that selectively co-activates while avoiding co-activation with other neurons. This functional specialization potentially correlates with the development of heavy-tailed weight distributions, suggesting a statistical mechanical basis for emergent specialization.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis</title>
<link>https://arxiv.org/abs/2504.14202</link>
<guid>https://arxiv.org/abs/2504.14202</guid>
<content:encoded><![CDATA[
<div> faceCLIP, multi-modal encoder, identity preservation, text alignment, image synthesis 

Summary:
FaceCLIP introduces a novel approach for ID-preserving generation by employing a multi-modal encoding strategy that unifies identity and text inputs. This method utilizes a joint embedding space for identity and textual semantics, enabling the conditioning of a base diffusion model for generating identity-consistent and text-aligned images. A multi-modal alignment algorithm trains FaceCLIP by aligning its joint representation with face, text, and image embedding spaces. The integration of FaceCLIP with Stable Diffusion XL (SDXL) results in FaceCLIP-SDXL, a pipeline for ID-preserving image synthesis that outperforms previous methods in producing photorealistic portraits with improved identity preservation and textual relevance. Comprehensive experiments demonstrate the quantitative and qualitative superiority of FaceCLIP-SDXL in comparison to existing techniques. <div>
arXiv:2504.14202v3 Announce Type: replace-cross 
Abstract: We propose a novel framework for ID-preserving generation using a multi-modal encoding strategy rather than injecting identity features via adapters into pre-trained models. Our method treats identity and text as a unified conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal encoder that learns a joint embedding space for both identity and textual semantics. Given a reference face and a text prompt, FaceCLIP produces a unified representation that encodes both identity and text, which conditions a base diffusion model to generate images that are identity-consistent and text-aligned. We also present a multi-modal alignment algorithm to train FaceCLIP, using a loss that aligns its joint representation with face, text, and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL). Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait generation with better identity preservation and textual relevance. Extensive experiments demonstrate its quantitative and qualitative superiority.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Thinking via Mode Policy Optimization for Social Language Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
<div> Hierarchical thinking modes, Adaptive Mode Learning, Adaptive Mode Policy Optimization, social intelligence simulation, language agents<br />
<br />
Summary: <br />
The paper introduces the Adaptive Mode Learning (AML) framework to enhance language agents' adaptive thinking abilities in dynamic social interactions. It identifies hierarchical thinking modes and develops the Adaptive Mode Policy Optimization (AMPO) algorithm to optimize context-aware mode switching and reasoning. AML improves social intelligence simulation by designing multi-granular thinking modes, enabling context-aware mode switching, and promoting token-efficient reasoning. Experimental results demonstrate that AML achieves 15.6% higher task performance than GPT-4o and outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. This highlights the advantage of AML's adaptive thinking mode selection and optimization mechanism over GRPO's fixed-depth approach. <div>
arXiv:2505.02156v4 Announce Type: replace-cross 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPO's fixed-depth solution.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.11765</link>
<guid>https://arxiv.org/abs/2505.11765</guid>
<content:encoded><![CDATA[
<div> optimization, Large Language Models, Multi-Agent Systems, code generation, reasoning <br />
Summary:
The OMAC framework is introduced for optimizing Large Language Model-based Multi-Agent Systems (MAS) across five key dimensions. It includes Semantic Initializer and Contrastive Comparator actors to optimize individual dimensions and a joint optimization algorithm for multiple dimensions. OMAC outperformed existing methods in tasks such as code generation, arithmetic reasoning, and general reasoning through extensive experiments. <div>
arXiv:2505.11765v2 Announce Type: replace-cross 
Abstract: Agents powered by advanced large language models (LLMs) have demonstrated impressive capabilities across diverse complex applications. Recently, Multi-Agent Systems (MAS), wherein multiple agents collaborate and communicate with each other, have exhibited enhanced capabilities in complex tasks, such as high-quality code generation and arithmetic reasoning. However, the development of such systems often relies on handcrafted methods, and the literature on systematic design and optimization of LLM-based MAS remains limited.
  In this work, we introduce OMAC, a general framework designed for holistic optimization of LLM-based MAS. Specifically, we identify five key optimization dimensions for MAS, encompassing both agent functionality and collaboration structure. Building upon these dimensions, we first propose a general algorithm, utilizing two actors termed the Semantic Initializer and the Contrastive Comparator, to optimize any single dimension. Then, we present an algorithm for joint optimization across multiple dimensions. Extensive experiments demonstrate the superior performance of OMAC on code generation, arithmetic reasoning, and general reasoning tasks against state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vague Knowledge: Evidence from Analyst Reports</title>
<link>https://arxiv.org/abs/2505.12269</link>
<guid>https://arxiv.org/abs/2505.12269</guid>
<content:encoded><![CDATA[
<div> Keywords: vague knowledge, subjective expectations, linguistic expressions, forecast errors, numerical forecasts

Summary: 
Language plays a crucial role in representing subjective expectations when quantification is not feasible or desirable. Analyst reports contain valuable information conveyed through linguistic expressions rather than numerical forecasts. The textual tone of reports can predict forecast errors and subsequent revisions, especially when language is vaguer, uncertainty is higher, and analysts are busy. This suggests that important but vaguely known information is communicated through language. This highlights the significance of considering both numerical and linguistic information in understanding subjective expectations and forecast accuracy. <div>
arXiv:2505.12269v2 Announce Type: replace-cross 
Abstract: People in the real world often possess vague knowledge of future payoffs, for which quantification is not feasible or desirable. We argue that language, with differing ability to convey vague information, plays an important but less known-role in representing subjective expectations. Empirically, we find that in their reports, analysts include useful information in linguistic expressions but not numerical forecasts. Specifically, the textual tone of analyst reports has predictive power for forecast errors and subsequent revisions in numerical forecasts, and this relation becomes stronger when analyst's language is vaguer, when uncertainty is higher, and when analysts are busier. Overall, our theory and evidence suggest that some useful information is vaguely known and only communicated through language.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems</title>
<link>https://arxiv.org/abs/2505.15862</link>
<guid>https://arxiv.org/abs/2505.15862</guid>
<content:encoded><![CDATA[
<div> candidate edges, routing problems, algorithms, multi-armed bandit models, Traveling Salesman Problem

Summary:
The traditional algorithms for routing problems often get stuck in local optima due to static candidate edges. To address this limitation, a new approach is proposed that expands the candidate sets to include more promising edges. By incorporating multi-armed bandit models, the algorithm dynamically selects the most suitable candidate edges in each iteration. This enables smarter decision-making and leads to improved solutions in the Traveling Salesman Problem (TSP). Experimental results on various TSP benchmarks demonstrate the effectiveness of this method. Furthermore, applying this bandit-based approach to LKH-3, an extension of LKH for different TSP variants, significantly enhances its performance across a range of TSP problems. <div>
arXiv:2505.15862v1 Announce Type: new 
Abstract: Algorithms designed for routing problems typically rely on high-quality candidate edges to guide their search, aiming to reduce the search space and enhance the search efficiency. However, many existing algorithms, like the classical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman Problem (TSP), often use predetermined candidate edges that remain static throughout local searches. This rigidity could cause the algorithm to get trapped in local optima, limiting its potential to find better solutions. To address this issue, we propose expanding the candidate sets to include other promising edges, providing them an opportunity for selection. Specifically, we incorporate multi-armed bandit models to dynamically select the most suitable candidate edges in each iteration, enabling LKH to make smarter choices and lead to improved solutions. Extensive experiments on multiple TSP benchmarks show the excellent performance of our method. Moreover, we employ this bandit-based method to LKH-3, an extension of LKH tailored for solving various TSP variant problems, and our method also significantly enhances LKH-3's performance across typical TSP variants.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhyX: Does Your Model Have the "Wits" for Physical Reasoning?</title>
<link>https://arxiv.org/abs/2505.15929</link>
<guid>https://arxiv.org/abs/2505.15929</guid>
<content:encoded><![CDATA[
<div> physics-grounded reasoning, benchmark, physical understanding, evaluation protocol, reproducibility

Summary:
PhyX introduces a new benchmark, focusing on physical reasoning in visual scenarios. The benchmark includes 3K questions across various reasoning types and physics domains. Current models, including GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini, struggle with physical reasoning, with performance gaps exceeding 29% compared to human experts. Limitations in existing models include reliance on memorized knowledge, dependence on mathematical formulations, and surface-level pattern matching. The analysis includes fine-grained statistics, case studies, and multiple evaluation paradigms. A compatible evaluation protocol based on VLMEvalKit ensures reproducibility, enabling easy evaluation. This benchmark highlights the need for models to improve their physical reasoning capabilities to better understand real-world constraints. 

<br /><br />Summary: <div>
arXiv:2505.15929v1 Announce Type: new 
Abstract: Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\&amp;acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5\%, 42.2\%, and 45.8\% accuracy respectively-performance gaps exceeding 29\% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics</title>
<link>https://arxiv.org/abs/2505.15998</link>
<guid>https://arxiv.org/abs/2505.15998</guid>
<content:encoded><![CDATA[
<div> discovery, system-level dynamics, Flow-Lenia, AI scientist, self-organization <br />
Summary:<br />
This article introduces a method for automated discovery of system-level dynamics in Flow-Lenia, a continuous cellular automaton. The approach involves using a curiosity-driven AI scientist to uncover processes leading to self-organization of evolutionary and ecosystemic dynamics within the CA. By employing Intrinsically Motivated Goal Exploration Processes (IMGEPs), the researchers drive exploration of diverse Flow-Lenia environments by considering simulation-wide metrics such as evolutionary activity, compression-based complexity, and multi-scale entropy. Two experiments demonstrate the effectiveness of this method in identifying a wide range of dynamics compared to random search. The study showcases how ecosystemic simulations facilitate the self-organization of complex collective behaviors not captured by previous individual pattern search methods. Additionally, an interactive exploration tool is introduced to facilitate collaboration between humans and AI in scientific investigation. This methodology could be applied to other parameterizable complex systems where understanding emergent collective properties is crucial. <br /> <div>
arXiv:2505.15998v1 Announce Type: new 
Abstract: We present a method for the automated discovery of system-level dynamics in Flow-Lenia$-$a continuous cellular automaton (CA) with mass conservation and parameter localization$-$using a curiosity-driven AI scientist. This method aims to uncover processes leading to self-organization of evolutionary and ecosystemic dynamics in CAs. We build on previous work which uses diversity search algorithms in Lenia to find self-organized individual patterns, and extend it to large environments that support distinct interacting patterns. We adapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive exploration of diverse Flow-Lenia environments using simulation-wide metrics, such as evolutionary activity, compression-based complexity, and multi-scale entropy. We test our method in two experiments, showcasing its ability to illuminate significantly more diverse dynamics compared to random search. We show qualitative results illustrating how ecosystemic simulations enable self-organization of complex collective behaviors not captured by previous individual pattern search and analysis. We complement automated discovery with an interactive exploration tool, creating an effective human-AI collaborative workflow for scientific investigation. Though demonstrated specifically with Flow-Lenia, this methodology provides a framework potentially applicable to other parameterizable complex systems where understanding emergent collective properties is of interest.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Children's Mental Models of AI Reasoning: Implications for AI Literacy Education</title>
<link>https://arxiv.org/abs/2505.16031</link>
<guid>https://arxiv.org/abs/2505.16031</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, reasoning capabilities, children, mental models, AI literacy

Summary: 
Through a co-design session and field study involving children in grades 3-8, this study identified three models of AI reasoning: Deductive, Inductive, and Inherent. Younger children tend to attribute AI reasoning to inherent intelligence, while older children recognize AI as a pattern recognizer. The study revealed three tensions in children's understanding of AI reasoning. Firstly, there is a discrepancy in how younger and older children perceive AI's reasoning capabilities. Secondly, children exhibit different mental models of AI reasoning. Finally, there is a need to address these discrepancies and models in AI education to foster AI literacy. This research emphasizes the importance of understanding children's conceptualization of AI reasoning processes for curriculum development and the design of explainable AI tools. 

Summary: <div>
arXiv:2505.16031v1 Announce Type: new 
Abstract: As artificial intelligence (AI) advances in reasoning capabilities, most recently with the emergence of Large Reasoning Models (LRMs), understanding how children conceptualize AI's reasoning processes becomes critical for fostering AI literacy. While one of the "Five Big Ideas" in AI education highlights reasoning algorithms as central to AI decision-making, less is known about children's mental models in this area. Through a two-phase approach, consisting of a co-design session with 8 children followed by a field study with 106 children (grades 3-8), we identified three models of AI reasoning: Deductive, Inductive, and Inherent. Our findings reveal that younger children (grades 3-5) often attribute AI's reasoning to inherent intelligence, while older children (grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions that surfaced in children's understanding of AI reasoning and conclude with implications for scaffolding AI curricula and designing explainable AI tools.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal LLM Routing: End-to-End Regret Minimization from Observational Data</title>
<link>https://arxiv.org/abs/2505.16037</link>
<guid>https://arxiv.org/abs/2505.16037</guid>
<content:encoded><![CDATA[
<div> Causal end-to-end framework, routing policies, decision-making regret, observational data, surrogate objectives <br />
<br />
Summary: 
The article introduces a novel causal end-to-end framework for Language Model (LLM) routing that aims to select the most suitable model for each query while balancing accuracy and cost metrics. Unlike prior approaches, this framework learns from observational data, minimizing decision-making regret. Two surrogate objectives are proposed for efficient optimization: a classification-based upper bound and a softmax-weighted regret approximation. The framework is extended to handle heterogeneous cost preferences through an interval-conditioned architecture. Experimental results on public benchmarks demonstrate the superior performance of the proposed method, outperforming existing baselines and achieving state-of-the-art results across various embedding models. <div>
arXiv:2505.16037v1 Announce Type: new 
Abstract: LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution</title>
<link>https://arxiv.org/abs/2505.16048</link>
<guid>https://arxiv.org/abs/2505.16048</guid>
<content:encoded><![CDATA[
<div> dataset, large language models, spatial reasoning, topology optimization, physical reasoning

Summary:
The article introduces a new dataset aimed at evaluating the physical and spatial reasoning abilities of Large Language Models (LLMs). The dataset is based on topology optimization, a method that computes optimal material distributions within a design space under specified loads and supports. LLMs are tasked with understanding conditions like 2D boundaries, applied forces, and supports to predict optimal material distribution without relying on simulation tools or explicit physical models. The tasks range from filling in masked regions in partial structures to predicting complete material distributions, challenging models to reason about structural stability and spatial organization. By targeting spatial and physical reasoning in 2D settings, the dataset offers a unique perspective on evaluating LLM capabilities beyond traditional language and logic benchmarks.

<br /><br />Summary: <div>
arXiv:2505.16048v1 Announce Type: new 
Abstract: We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior</title>
<link>https://arxiv.org/abs/2505.16067</link>
<guid>https://arxiv.org/abs/2505.16067</guid>
<content:encoded><![CDATA[
<div> Memory Management, Language Model, Agent Behavior, Experience Replay, Selective Addition and Deletion

Summary: 
Memory management choices in large language model (LLM) agents greatly impact their behavior and long-term performance. Through empirical study, it was found that LLM agents tend to follow past experiences, leading to error propagation and misaligned experience replay. By combining selective addition and deletion strategies, the negative effects can be mitigated, resulting in a 10% average performance gain. The study also highlighted the influence of memory management choices on agent behavior under challenging conditions such as task distribution shifts and constrained memory resources. The findings provide insights into the behavioral dynamics of LLM agent memory systems and offer practical guidance for designing memory components that support robust, long-term agent performance. The release of code for further study is also mentioned. 

<br /><br />Summary: <div>
arXiv:2505.16067v1 Announce Type: new 
Abstract: Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory operations that are widely used by many agent frameworks-addition, which incorporates new experiences into the memory base, and deletion, which selectively removes past experiences-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where outdated or irrelevant experiences negatively influence current tasks. Through controlled experiments, we show that combining selective addition and deletion strategies can help mitigate these negative effects, yielding an average absolute performance gain of 10% compared to naive memory growth. Furthermore, we highlight how memory management choices affect agents' behavior under challenging conditions such as task distribution shifts and constrained memory resources. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance. We also release our code to facilitate further study.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation</title>
<link>https://arxiv.org/abs/2505.16080</link>
<guid>https://arxiv.org/abs/2505.16080</guid>
<content:encoded><![CDATA[
<div> Keywords: spatiotemporal systems, collective intelligence, model evolution, knowledge transfer, NeuroAI<br />
Summary:<br />
- The paper introduces Synaptic EVOlutional spatiotemporal network (SynEVO) for discovering regularities in spatiotemporal systems.
- Inspired by neuroscience theories, SynEVO enables cross-domain knowledge sharing and aggregation by breaking model independence.
- SynEVO uses a curriculum learning approach and two complementary learners to promote model growth and task-wise disentanglement.
- An adaptive dynamic coupler with a new difference metric determines the incorporation of new sample groups for model evolution under various domains.
- Experiments demonstrate that SynEVO significantly improves generalization capacity by up to 42% in cross-domain scenarios, providing a paradigm of NeuroAI for knowledge transfer and adaptation. <br /><br /> <div>
arXiv:2505.16080v1 Announce Type: new 
Abstract: Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development</title>
<link>https://arxiv.org/abs/2505.16086</link>
<guid>https://arxiv.org/abs/2505.16086</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent systems, software development, group optimization, natural language feedback

Summary: 
This study focuses on optimizing role-based multi-agent systems for software development tasks using natural language feedback. The researchers propose a two-step optimization pipeline that identifies underperforming agents and optimizes system prompts based on failure explanations. The impact of various optimization settings is studied, including online vs. offline optimization and individual vs. group optimization. Two prompting strategies, one-pass, and multi-pass optimizations, are compared in group optimization scenarios. The effectiveness of the optimization method is demonstrated in improving system performance across different evaluation dimensions. Insights are provided for future development, highlighting the importance of optimizing group behaviors in multi-agent systems tackling complex tasks. <br /><br />Summary: <div>
arXiv:2505.16086v1 Announce Type: new 
Abstract: We have seen remarkable progress in large language models (LLMs) empowered multi-agent systems solving complex tasks necessitating cooperation among experts with diverse skills. However, optimizing LLM-based multi-agent systems remains challenging. In this work, we perform an empirical case study on group optimization of role-based multi-agent systems utilizing natural language feedback for challenging software development tasks under various evaluation dimensions. We propose a two-step agent prompts optimization pipeline: identifying underperforming agents with their failure explanations utilizing textual feedback and then optimizing system prompts of identified agents utilizing failure explanations. We then study the impact of various optimization settings on system performance with two comparison groups: online against offline optimization and individual against group optimization. For group optimization, we study two prompting strategies: one-pass and multi-pass prompting optimizations. Overall, we demonstrate the effectiveness of our optimization method for role-based multi-agent systems tackling software development tasks evaluated on diverse evaluation dimensions, and we investigate the impact of diverse optimization settings on group behaviors of the multi-agent systems to provide practical insights for future development.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance</title>
<link>https://arxiv.org/abs/2505.16090</link>
<guid>https://arxiv.org/abs/2505.16090</guid>
<content:encoded><![CDATA[
<div> GenAI, Large Language Models, Sentiment Analysis, Financial Text, Earnings Call Transcripts  
Summary:
GenAI, specifically large language models (LLMs), has become a crucial tool in various industries by assisting in coding, data analysis, and research workflows. However, in the financial sector, LLMs struggle with understanding nuanced sentiment embedded in financial text like earnings call transcripts due to their reliance on numerical vectors. This paper presents the findings of a project that benchmarks the performance of different AI models in sentiment analysis of Microsoft's earnings call transcripts. The study evaluates the correlation between sentiment analysis results and market sentiment as well as stock movements. It also explores prompt engineering techniques to enhance sentiment analysis accuracy. Visualizations are utilized to assess sentiment consistency and alignment with stock performance, highlighting the influence of sentiment trends across Microsoft's different business segments. <br /><br />Summary: <div>
arXiv:2505.16090v1 Announce Type: new 
Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central tool for productivity across industries. Beyond text generation, GenAI now plays a critical role in coding, data analysis, and research workflows. As large language models (LLMs) continue to evolve, it is essential to assess the reliability and accuracy of their outputs, especially in specialized, high-stakes domains like finance. Most modern LLMs transform text into numerical vectors, which are used in operations such as cosine similarity searches to generate responses. However, this abstraction process can lead to misinterpretation of emotional tone, particularly in nuanced financial contexts. While LLMs generally excel at identifying sentiment in everyday language, these models often struggle with the nuanced, strategically ambiguous language found in earnings call transcripts. Financial disclosures frequently embed sentiment in hedged statements, forward-looking language, and industry-specific jargon, making it difficult even for human analysts to interpret consistently, let alone AI models. This paper presents findings from the Santa Clara Microsoft Practicum Project, led by Professor Charlie Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's ChatGPT, Google's Gemini, and traditional machine learning models for sentiment analysis of financial text. Using Microsoft earnings call transcripts, the analysis assesses how well LLM-derived sentiment correlates with market sentiment and stock movements and evaluates the accuracy of model outputs. Prompt engineering techniques are also examined to improve sentiment analysis results. Visualizations of sentiment consistency are developed to evaluate alignment between tone and stock performance, with sentiment trends analyzed across Microsoft's lines of business to determine which segments exert the greatest influence.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials</title>
<link>https://arxiv.org/abs/2505.16097</link>
<guid>https://arxiv.org/abs/2505.16097</guid>
<content:encoded><![CDATA[
<div> Database, Clinical Trials, Artificial Intelligence, Benchmark Tasks, Large Language Models
Summary:
TrialPanorama is a structured database containing clinical trial records from 15 global sources, linked to biomedical ontologies. It is designed for AI applications in clinical trial tasks such as planning and summarization. A benchmark suite derived from the database includes tasks for systematic review and trial design. Experiments using large language models show potential but limited performance in high-stakes clinical trial workflows. The release of TrialPanorama and the benchmark aims to support further research on AI for clinical trials. 
<br /><br />Summary: <div>
arXiv:2505.16097v1 Announce Type: new 
Abstract: Developing artificial intelligence (AI) for vertical domains requires a solid data foundation for both training and evaluation. In this work, we introduce TrialPanorama, a large-scale, structured database comprising 1,657,476 clinical trial records aggregated from 15 global sources. The database captures key aspects of trial design and execution, including trial setups, interventions, conditions, biomarkers, and outcomes, and links them to standard biomedical ontologies such as DrugBank and MedDRA. This structured and ontology-grounded design enables TrialPanorama to serve as a unified, extensible resource for a wide range of clinical trial tasks, including trial planning, design, and summarization. To demonstrate its utility, we derive a suite of benchmark tasks directly from the TrialPanorama database. The benchmark spans eight tasks across two categories: three for systematic review (study search, study screening, and evidence summarization) and five for trial design (arm design, eligibility criteria, endpoint selection, sample size estimation, and trial completion assessment). The experiments using five state-of-the-art large language models (LLMs) show that while general-purpose LLMs exhibit some zero-shot capability, their performance is still inadequate for high-stakes clinical trial workflows. We release TrialPanorama database and the benchmark to facilitate further research on AI for clinical trials.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research</title>
<link>https://arxiv.org/abs/2505.16100</link>
<guid>https://arxiv.org/abs/2505.16100</guid>
<content:encoded><![CDATA[
<div> benchmark, biomedical, hypothesis validation, artificial intelligence, data analysis

Summary: 
The article introduces BioDSA-1K, a benchmark designed for evaluating artificial intelligence agents in biomedical hypothesis validation tasks. It consists of over 1,000 hypothesis-centric tasks and analysis plans curated from published biomedical studies. The benchmark includes structured hypotheses and supporting evidence based on empirical data tables, enabling evaluation in four key areas: hypothesis decision accuracy, alignment between evidence and conclusion, correctness of reasoning process, and executability of generated analysis code. Importantly, BioDSA-1K also includes scenarios with non-verifiable hypotheses, where data are insufficient to support or refute a claim, reflecting real-world challenges in scientific research. The benchmark aims to facilitate the development and evaluation of trustworthy AI agents for biomedical discovery. <br /><br />Summary: <div>
arXiv:2505.16100v1 Announce Type: new 
Abstract: Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation. In this work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows. Each task includes a structured hypothesis derived from the original study's conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables. While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods. The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science. We propose BioDSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language</title>
<link>https://arxiv.org/abs/2505.16114</link>
<guid>https://arxiv.org/abs/2505.16114</guid>
<content:encoded><![CDATA[
<div> framework, LLMs, logic programming, puzzle solving, reasoning <br />
<br />
Logic-of-Thought (Logot) is introduced as a framework that combines large language models (LLMs) with logic programming to solve puzzles in natural language. By translating puzzle rules and states into answer set programs (ASPs), the method utilizes the natural language understanding of LLMs along with the precise reasoning capabilities of logic programs. The accuracy and efficiency of this hybrid approach are demonstrated through evaluations on various grid puzzles and dynamic puzzles involving actions, showing near-perfect accuracy across all tasks. The code and data for the method are available on GitHub at https://github.com/naiqili/Logic-of-Thought. 

<br /><br />Summary: <br />
1. The paper introduces Logic-of-Thought (Logot), a framework that combines LLMs and logic programming to solve puzzles in natural language. 
2. Logot utilizes LLMs to translate puzzle rules and states into ASPs for accurate and efficient inference by an ASP interpreter. 
3. The hybrid approach of Logot combines natural language understanding with precise reasoning capabilities to achieve near-perfect accuracy in puzzle solving tasks. 
4. The method is evaluated on various grid puzzles and dynamic puzzles involving actions, demonstrating its effectiveness. 
5. The code and data for Logot are available on GitHub for further exploration and use. <div>
arXiv:2505.16114v1 Announce Type: new 
Abstract: Solving puzzles in natural language poses a long-standing challenge in AI. While large language models (LLMs) have recently shown impressive capabilities in a variety of tasks, they continue to struggle with complex puzzles that demand precise reasoning and exhaustive search. In this paper, we propose Logic-of-Thought (Logot), a novel framework that bridges LLMs with logic programming to address this problem. Our method leverages LLMs to translate puzzle rules and states into answer set programs (ASPs), the solution of which are then accurately and efficiently inferred by an ASP interpreter. This hybrid approach combines the natural language understanding of LLMs with the precise reasoning capabilities of logic programs. We evaluate our method on various grid puzzles and dynamic puzzles involving actions, demonstrating near-perfect accuracy across all tasks. Our code and data are available at: https://github.com/naiqili/Logic-of-Thought.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered AI Agent Systems and Their Applications in Industry</title>
<link>https://arxiv.org/abs/2505.16120</link>
<guid>https://arxiv.org/abs/2505.16120</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, agent systems, multi-modal, challenges, solutions
Summary: 
The paper discusses the impact of Large Language Models (LLMs) on agent systems, highlighting their increased flexibility and cross-domain reasoning capabilities. It examines the evolution of agent systems and their applications in various fields such as customer service, software development, and healthcare. The integration of multi-modal LLMs enables agents to process diverse data modalities for richer real-world behavior. However, challenges such as high inference latency, output uncertainty, lack of evaluation metrics, and security vulnerabilities are identified. The paper proposes potential solutions to mitigate these challenges and enhance the performance of LLM-powered agents. Overall, the advancements in LLM technology have transformed agent systems, offering enhanced capabilities for natural language interaction and adaptive behavior across different domains. <br /><br />Summary: <div>
arXiv:2505.16120v1 Announce Type: new 
Abstract: The emergence of Large Language Models (LLMs) has reshaped agent systems. Unlike traditional rule-based agents with limited task scope, LLM-powered agents offer greater flexibility, cross-domain reasoning, and natural language interaction. Moreover, with the integration of multi-modal LLMs, current agent systems are highly capable of processing diverse data modalities, including text, images, audio, and structured tabular data, enabling richer and more adaptive real-world behavior. This paper comprehensively examines the evolution of agent systems from the pre-LLM era to current LLM-powered architectures. We categorize agent systems into software-based, physical, and adaptive hybrid systems, highlighting applications across customer service, software development, manufacturing automation, personalized education, financial trading, and healthcare. We further discuss the primary challenges posed by LLM-powered agents, including high inference latency, output uncertainty, lack of evaluation metrics, and security vulnerabilities, and propose potential solutions to mitigate these concerns.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sudoku-Bench: Evaluating creative reasoning with Sudoku variants</title>
<link>https://arxiv.org/abs/2505.16135</link>
<guid>https://arxiv.org/abs/2505.16135</guid>
<content:encoded><![CDATA[
<div> Benchmark, Large language models, Sudoku variants, Reasoning, Logical breakthroughs

Summary:<br />
- Existing reasoning benchmarks for large language models (LLMs) often reward memorization over creativity. Sudoku-Bench addresses this by providing a curated benchmark of challenging and unconventional Sudoku variants that require multi-step logical reasoning.
- Sudoku variants are effective for reasoning research as they introduce unique constraints that make memorization impractical and necessitate identifying novel logical breakthroughs.
- Sudoku-Bench includes a carefully chosen puzzle set, standardized puzzle representation, and flexible tools for easy extension into a general research environment.
- Baseline experiments reveal that state-of-the-art LLMs struggle with solving Sudoku puzzles unaided, indicating significant room for advancing strategic reasoning capabilities.
- The benchmark highlights opportunities for developing long-horizon reasoning skills in large language models through creative problem-solving in Sudoku variants. 

<br /><br />Summary: <div>
arXiv:2505.16135v1 Announce Type: new 
Abstract: Existing reasoning benchmarks for large language models (LLMs) frequently fail to capture authentic creativity, often rewarding memorization of previously observed patterns. We address this shortcoming with Sudoku-Bench, a curated benchmark of challenging and unconventional Sudoku variants specifically selected to evaluate creative, multi-step logical reasoning. Sudoku variants form an unusually effective domain for reasoning research: each puzzle introduces unique or subtly interacting constraints, making memorization infeasible and requiring solvers to identify novel logical breakthroughs (``break-ins''). Despite their diversity, Sudoku variants maintain a common and compact structure, enabling clear and consistent evaluation. Sudoku-Bench includes a carefully chosen puzzle set, a standardized text-based puzzle representation, and flexible tools compatible with thousands of publicly available puzzles -- making it easy to extend into a general research environment. Baseline experiments show that state-of-the-art LLMs solve fewer than 15\% of puzzles unaided, highlighting significant opportunities to advance long-horizon, strategic reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value</title>
<link>https://arxiv.org/abs/2505.16147</link>
<guid>https://arxiv.org/abs/2505.16147</guid>
<content:encoded><![CDATA[
<div> Efficient data valuation, Unlearning Shapley, machine unlearning, data values, Monte Carlo sampling<br />
<br />
Summary: 
The article introduces Unlearning Shapley, a framework that efficiently estimates data values using machine unlearning to measure performance shifts on a test set. This approach computes Shapley values through Monte Carlo sampling without requiring full data or model training details. It supports both full and partial data valuation, making it scalable for large models like LLMs. Experiments show that Unlearning Shapley matches the accuracy of existing methods while significantly reducing computational costs. The estimated data values demonstrate a strong correlation with the true impact of data subsets, confirming its reliability in real-world applications. By bridging the gap between data valuation theory and practical implementation, Unlearning Shapley offers a scalable and privacy-compliant solution for modern AI ecosystems. <br /><br /> <div>
arXiv:2505.16147v1 Announce Type: new 
Abstract: The proliferation of large models has intensified the need for efficient data valuation methods to quantify the contribution of individual data providers. Traditional approaches, such as game-theory-based Shapley value and influence-function-based techniques, face prohibitive computational costs or require access to full data and model training details, making them hardly achieve partial data valuation. To address this, we propose Unlearning Shapley, a novel framework that leverages machine unlearning to estimate data values efficiently. By unlearning target data from a pretrained model and measuring performance shifts on a reachable test set, our method computes Shapley values via Monte Carlo sampling, avoiding retraining and eliminating dependence on full data. Crucially, Unlearning Shapley supports both full and partial data valuation, making it scalable for large models (e.g., LLMs) and practical for data markets. Experiments on benchmark datasets and large-scale text corpora demonstrate that our approach matches the accuracy of state-of-the-art methods while reducing computational overhead by orders of magnitude. Further analysis confirms a strong correlation between estimated values and the true impact of data subsets, validating its reliability in real-world scenarios. This work bridges the gap between data valuation theory and practical deployment, offering a scalable, privacy-compliant solution for modern AI ecosystems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.16176</link>
<guid>https://arxiv.org/abs/2505.16176</guid>
<content:encoded><![CDATA[
<div> Algorithm, Data selection, Reasoning tasks, Dynamic training, Model adaptation

Summary:
SAI-DPO is a new algorithm designed for dynamically selecting training data based on a model's evolving reasoning abilities during continuous training processes. It addresses the limitations of static metrics such as difficulty and diversity by adaptively adapting data selection to the model's performance feedback in real-time. This approach enhances data utilization efficiency and final task performance, particularly in online reinforcement learning frameworks. Experimental results on state-of-the-art models and mathematical reasoning benchmarks show that SAI-DPO achieves significant performance improvements, with an average boost of up to 21.3 percentage points. Notably, it shows considerable enhancements on challenging datasets like AIME24 and AMC23. These results highlight the effectiveness of dynamic, model-adaptive data selection strategies in advancing reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2505.16176v1 Announce Type: new 
Abstract: In the realm of data selection for reasoning tasks, existing approaches predominantly rely on externally predefined static metrics such as difficulty and diversity, which are often designed for supervised fine-tuning (SFT) and lack adaptability to continuous training processes. A critical limitation of these methods is their inability to dynamically align with the evolving capabilities of models during online training, a gap that becomes increasingly pronounced with the rise of dynamic training paradigms and online reinforcement learning (RL) frameworks (e.g., R1 models). To address this, we introduce SAI-DPO, an algorithm that dynamically selects training data by continuously assessing a model's stage-specific reasoning abilities across different training phases. By integrating real-time model performance feedback, SAI-DPO adaptively adapts data selection to the evolving strengths and weaknesses of the model, thus enhancing both data utilization efficiency and final task performance. Extensive experiments on three state-of-the-art models and eight mathematical reasoning benchmarks, including challenging competition-level datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average performance boost of up to 21.3 percentage points, with particularly notable improvements of 10 and 15 points on AIME24 and AMC23, respectively. These results highlight the superiority of dynamic, model-adaptive data selection over static, externally defined strategies in advancing reasoning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning</title>
<link>https://arxiv.org/abs/2505.16186</link>
<guid>https://arxiv.org/abs/2505.16186</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, safety risks, adversarial attacks, SafeKey, generalization

Summary: 
SafeKey addresses safety risks in Large Reasoning Models (LRMs) by identifying a crucial "aha moment" in the key sentence of model generation. This moment activates safety reasoning and leads to safe responses. SafeKey implements a Dual-Path Safety Head to enhance safety signals in internal representations before the key sentence and a Query-Mask Modeling objective to improve attention on query understanding. Through experiments on various safety benchmarks, SafeKey significantly improves safety generalization against harmful queries and adversarial attacks, reducing harmfulness rates by 9.6% while maintaining overall model performance. Analysis shows that SafeKey reshapes internal attention and enhances hidden representations to foster safer responses in LRMs. <br /><br />Summary: <div>
arXiv:2505.16186v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Velocity Completion Task and Method for Event-based Player Positional Data in Soccer</title>
<link>https://arxiv.org/abs/2505.16199</link>
<guid>https://arxiv.org/abs/2505.16199</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, team sports, velocity completion, neural network, event-based positional data<br />
Summary:<br /> 
- In real-world complex systems like team sports, analyzing the behavior of individual agents is crucial but challenging due to the lack of continuous temporal information in event-based positional data.
- The proposed method completes velocity information for all agents using only event-based positional data, allowing for a deeper understanding of individual agent behaviors and team strategies.
- Neural network-based approaches outperform rule-based methods in velocity completion error, taking into account temporal dependencies and player interactions.
- Experiments using soccer event data show that the completed velocity information leads to space evaluation results closer to those obtained from complete tracking data.
- The method has the potential to enhance team sports system analysis by providing a more comprehensive view of player movements and interactions. <br /><br /> Summary: <div>
arXiv:2505.16199v1 Announce Type: new 
Abstract: In many real-world complex systems, the behavior can be observed as a collection of discrete events generated by multiple interacting agents. Analyzing the dynamics of these multi-agent systems, especially team sports, often relies on understanding the movement and interactions of individual agents. However, while providing valuable snapshots, event-based positional data typically lacks the continuous temporal information needed to directly calculate crucial properties such as velocity. This absence severely limits the depth of dynamic analysis, preventing a comprehensive understanding of individual agent behaviors and emergent team strategies. To address this challenge, we propose a new method to simultaneously complete the velocity of all agents using only the event-based positional data from team sports. Based on this completed velocity information, we investigate the applicability of existing team sports analysis and evaluation methods. Experiments using soccer event data demonstrate that neural network-based approaches outperformed rule-based methods regarding velocity completion error, considering the underlying temporal dependencies and graph structure of player-to-player or player-to-ball interaction. Moreover, the space evaluation results obtained using the completed velocity are closer to those derived from complete tracking data, highlighting our method's potential for enhanced team sports system analysis.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead</title>
<link>https://arxiv.org/abs/2505.16221</link>
<guid>https://arxiv.org/abs/2505.16221</guid>
<content:encoded><![CDATA[
<div> framework, LightRouter, large language models, task performance, cost efficiency
<br />
LightRouter is a framework designed to select and integrate a subset of large language models (LLMs) to optimize task performance and cost efficiency. It uses an adaptive selection mechanism to identify models that require minimal boot tokens, reducing costs, and an integration strategy to combine their outputs. Experimental results show that LightRouter can outperform ensemble baselines by up to 25% in accuracy and reduce inference costs by up to 27% compared to high-performing models. The framework operates without prior knowledge of individual models and relies on lightweight models. This work provides practical insights into efficient LLM selection and model combination strategies.
<br /><br />Summary: <div>
arXiv:2505.16221v1 Announce Type: new 
Abstract: The rapid advancement of large language models has unlocked remarkable capabilities across a diverse array of natural language processing tasks. However, the considerable differences among available LLMs-in terms of cost, performance, and computational demands-pose significant challenges for users aiming to identify the most suitable model for specific tasks. In this work, we present LightRouter, a novel framework designed to systematically select and integrate a small subset of LLMs from a larger pool, with the objective of jointly optimizing both task performance and cost efficiency. LightRouter leverages an adaptive selection mechanism to identify models that require only a minimal number of boot tokens, thereby reducing costs, and further employs an effective integration strategy to combine their outputs. Extensive experiments across multiple benchmarks demonstrate that LightRouter matches or outperforms widely-used ensemble baselines, achieving up to a 25% improvement in accuracy. Compared with leading high-performing models, LightRouter achieves comparable performance while reducing inference costs by up to 27%. Importantly, our framework operates without any prior knowledge of individual models and relies exclusively on inexpensive, lightweight models. This work introduces a practical approach for efficient LLM selection and provides valuable insights into optimal strategies for model combination.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network</title>
<link>https://arxiv.org/abs/2505.16223</link>
<guid>https://arxiv.org/abs/2505.16223</guid>
<content:encoded><![CDATA[
<div> Detection, MADCluster, anomaly, self-supervised clustering, deep learning architecture <br />
Summary:<br />
- MADCluster is a novel anomaly detection framework that addresses the 'hypersphere collapse' problem in deep learning-based methods.
- It clusters normal pattern data into a single cluster while learning the cluster center and mapping data close to this center.
- A new 'One-directed Adaptive loss' improves expressiveness and enables effective single clustering, with mathematically proven optimization.
- The framework consists of Base Embedder for capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates.
- MADCluster's model-agnostic characteristics allow for the application of various architectures to achieve enhanced performance on time series benchmark datasets. <br /> <div>
arXiv:2505.16223v1 Announce Type: new 
Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a 'single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning</title>
<link>https://arxiv.org/abs/2505.16225</link>
<guid>https://arxiv.org/abs/2505.16225</guid>
<content:encoded><![CDATA[
<div> In-Context Learning, Large Language Models, Many-Shot ICL, Pseudo-LabEling, MAPLE<br />
<br />
Summary:<br />
The research introduces Many-Shot Adaptive Pseudo-LabEling (MAPLE) as a novel framework for Many-Shot In-Context Learning (ICL), which enhances the adaptability and performance of Large Language Models (LLMs) by utilizing pseudo-labeled samples. Many-shot ICL involves incorporating multiple input-output examples into LLMs, and MAPLE addresses the high cost of obtaining labeled data by using pseudo-labeling on impactful unlabeled samples. These pseudo-labeled samples are tailored to each test query, improving many-shot ICL performance without significant labeling costs. The framework showcases its effectiveness in real-world dataset experiments, demonstrating its ability to enhance LLM adaptability and performance with limited labeled data. <div>
arXiv:2505.16225v1 Announce Type: new 
Abstract: In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle diverse tasks by incorporating multiple input-output examples, known as demonstrations, into the input of LLMs. More recently, advancements in the expanded context windows of LLMs have led to many-shot ICL, which uses hundreds of demonstrations and outperforms few-shot ICL, which relies on fewer examples. However, this approach is often hindered by the high cost of obtaining large amounts of labeled data. To address this challenge, we propose Many-Shot Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL framework that utilizes pseudo-labeled samples to compensate for the lack of label information. We first identify a subset of impactful unlabeled samples and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled samples are then adaptively selected and tailored to each test query as input to improve the performance of many-shot ICL, without significant labeling costs. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework, showcasing its ability to enhance LLM adaptability and performance with limited labeled data.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance</title>
<link>https://arxiv.org/abs/2505.16276</link>
<guid>https://arxiv.org/abs/2505.16276</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Graph Engineering, Model Size Scaling Laws, Benchmark Scores, Cost-Effectiveness

Summary:
The study focuses on the use of Large Language Models (LLMs) for Knowledge Graph Engineering (KGE) tasks, considering the balance between model performance and costs. The LLM-KG-Bench framework is introduced to compare LLMs in the context of KGE tasks. Analyzing data from 26 LLMs, the researchers explore model size scaling laws specific to KGE tasks. Results show that, in general, larger models exhibit higher capabilities, but there are instances of plateau effects where task performance does not improve significantly with larger models. This suggests that smaller models may offer cost-effectiveness in certain cases. Moreover, within model families, larger models do not always outperform smaller ones, indicating the need for testing models of varying sizes within the same family. The study emphasizes the importance of considering both model size and performance in selecting LLMs for KGE tasks. 

<br /><br />Summary: <div>
arXiv:2505.16276v1 Announce Type: new 
Abstract: When using Large Language Models (LLMs) to support Knowledge Graph Engineering (KGE), one of the first indications when searching for an appropriate model is its size. According to the scaling laws, larger models typically show higher capabilities. However, in practice, resource costs are also an important factor and thus it makes sense to consider the ratio between model performance and costs. The LLM-KG-Bench framework enables the comparison of LLMs in the context of KGE tasks and assesses their capabilities of understanding and producing KGs and KG queries. Based on a dataset created in an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the model size scaling laws specific to KGE tasks. In our analyses, we assess how benchmark scores evolve between different model size categories. Additionally, we inspect how the general score development of single models and families of models correlates to their size. Our analyses revealed that, with a few exceptions, the model size scaling laws generally also apply to the selected KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e., the task performance did not change much between a model and the next larger model. In these cases, smaller models could be considered to achieve high cost-effectiveness. Regarding models of the same family, sometimes larger models performed worse than smaller models of the same family. These effects occurred only locally. Hence it is advisable to additionally test the next smallest and largest model of the same family.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery</title>
<link>https://arxiv.org/abs/2505.16288</link>
<guid>https://arxiv.org/abs/2505.16288</guid>
<content:encoded><![CDATA[
<div> interpretability, interactivity, II-KEA, Electronic Health Records, Deep Learning<br />
<br />
Summary:<br />
The article introduces II-KEA, a framework aiming to address the lack of interpretability and interactivity in deep learning models trained on Electronic Health Records data. II-KEA combines personalized knowledge databases and agentic LLMs to provide explicit reasoning, causal analysis, and the ability for clinicians to inject their own knowledge and experience. The framework is evaluated on MIMIC-III and MIMIC-IV datasets, showing superior performance in diagnosis prediction, along with enhanced interpretability and interactivity. This is achieved through strong results from extensive case studies, showcasing the potential of II-KEA to assist clinicians in decision-making and treatment planning. <div>
arXiv:2505.16288v1 Announce Type: new 
Abstract: Deep learning models trained on extensive Electronic Health Records (EHR) data have achieved high accuracy in diagnosis prediction, offering the potential to assist clinicians in decision-making and treatment planning. However, these models lack two crucial features that clinicians highly value: interpretability and interactivity. The ``black-box'' nature of these models makes it difficult for clinicians to understand the reasoning behind predictions, limiting their ability to make informed decisions. Additionally, the absence of interactive mechanisms prevents clinicians from incorporating their own knowledge and experience into the decision-making process. To address these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal discovery framework that integrates personalized knowledge databases and agentic LLMs. II-KEA enhances interpretability through explicit reasoning and causal analysis, while also improving interactivity by allowing clinicians to inject their knowledge and experience through customized knowledge bases and prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating superior performance along with enhanced interpretability and interactivity, as evidenced by its strong results from extensive case studies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning</title>
<link>https://arxiv.org/abs/2505.16312</link>
<guid>https://arxiv.org/abs/2505.16312</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, EquivPruner, mathematical reasoning, token consumption, semantic equivalence

Summary:
EquivPruner introduces a method to efficiently identify and prune semantically equivalent actions during the search process of Large Language Models (LLMs). The approach is designed to address the issue of token consumption in LLM reasoning, particularly in domain-specific contexts like mathematical reasoning. The researchers also introduce MathEquiv, a dataset created for detecting mathematical statement equivalence, enabling the training of an equivalence detector. Experimental results across various models and tasks show that EquivPruner significantly reduces token consumption, leading to improved efficiency in searching and enhanced reasoning accuracy. For example, when applied to the Qwen2.5-Math-7B-Instruct model on GSM8K, EquivPruner decreased token consumption by 48.1% while also enhancing accuracy. The code for EquivPruner is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2505.16312v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at complex reasoning through search algorithms, yet current strategies often suffer from massive token consumption due to redundant exploration of semantically equivalent steps. Existing semantic similarity methods struggle to accurately identify such equivalence in domain-specific contexts like mathematical reasoning. To address this, we propose EquivPruner, a simple yet effective approach that identifies and prunes semantically equivalent actions during LLM reasoning search. We also introduce MathEquiv, the first dataset we created for mathematical statement equivalence, which enables the training of a lightweight equivalence detector. Extensive experiments across various models and tasks demonstrate that EquivPruner significantly reduces token consumption, improving searching efficiency and often bolstering reasoning accuracy. For instance, when applied to Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by 48.1\% while also improving accuracy. Our code is available at https://github.com/Lolo1222/EquivPruner.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.16315</link>
<guid>https://arxiv.org/abs/2505.16315</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, reasoning models, cognitive science, adaptive cognition, system switch

Summary:<br />
- The study introduces Adaptive Cognition Policy Optimization (ACPO) to enhance the efficiency of large reasoning models (LRMs) by reducing redundant content generation.
- ACPO incorporates system-aware reasoning tokens and online difficulty estimation to guide adaptive cognitive allocation and system switch during reinforcement learning.
- A two-stage training strategy involving supervised fine-tuning and ACPO implementation is proposed to enable LRMs to generate reasoning paths with explicit thinking modes.
- Experimental results demonstrate that ACPO effectively reduces redundant reasoning while adaptively adjusting cognitive allocation based on task complexity.
- ACPO enables LRMs to achieve efficient hybrid reasoning by dynamically allocating cognitive resources and adapting system switch based on task difficulty. 

<br /><br />Summary: <div>
arXiv:2505.16315v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex reasoning tasks, but often suffer from overthinking, generating redundant content regardless of task difficulty. Inspired by the dual process theory in cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a reinforcement learning framework that enables LRMs to achieve efficient reasoning through adaptive cognitive allocation and dynamic system switch. ACPO incorporates two key components: (1) introducing system-aware reasoning tokens to explicitly represent the thinking modes thereby making the model's cognitive process transparent, and (2) integrating online difficulty estimation and token length budget to guide adaptive system switch and reasoning during reinforcement learning. To this end, we propose a two-stage training strategy. The first stage begins with supervised fine-tuning to cold start the model, enabling it to generate reasoning paths with explicit thinking modes. In the second stage, we apply ACPO to further enhance adaptive system switch for difficulty-aware reasoning. Experimental results demonstrate that ACPO effectively reduces redundant reasoning while adaptively adjusting cognitive allocation based on task complexity, achieving efficient hybrid reasoning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serious Games: Human-AI Interaction, Evolution, and Coevolution</title>
<link>https://arxiv.org/abs/2505.16388</link>
<guid>https://arxiv.org/abs/2505.16388</guid>
<content:encoded><![CDATA[
<div> Evolutionary Game Theory, human-AI interaction, Hawk-Dove Game, Iterated Prisoner's Dilemma, War of Attrition <br />
Summary: <br />
This study explores the potential evolutionary equilibrium between humans and AI through the lens of Evolutionary Game Theory (EGT). Three EGT models, including the Hawk-Dove Game, Iterated Prisoner's Dilemma, and the War of Attrition, are examined for their relevance to human-AI interaction and coevolution. The Hawk-Dove Game predicts balanced coevolution based on the costs of conflict, while Iterated Prisoner's Dilemma suggests that repeated interactions may lead to cognitive coevolution and cooperation. The War of Attrition model highlights competition for resources resulting in strategic coevolution and conventions. EGT provides a suitable framework to understand and predict the human-AI evolutionary dynamic, but future research should expand beyond EGT and consider ethical and cognitive implications of human-AI interaction and coevolution. The possibility of human neuroplasticity combined with an evolving AI raises questions about the future convergence of humans and AI. <div>
arXiv:2505.16388v1 Announce Type: new 
Abstract: The serious games between humans and AI have only just begun. Evolutionary Game Theory (EGT) models the competitive and cooperative strategies of biological entities. EGT could help predict the potential evolutionary equilibrium of humans and AI. The objective of this work was to examine some of the EGT models relevant to human-AI interaction, evolution, and coevolution. Of thirteen EGT models considered, three were examined: the Hawk-Dove Game, Iterated Prisoner's Dilemma, and the War of Attrition. This selection was based on the widespread acceptance and clear relevance of these models to potential human-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove Game predicts balanced mixed-strategy equilibria based on the costs of conflict. It also shows the potential for balanced coevolution rather than dominance. Iterated Prisoner's Dilemma suggests that repeated interaction may lead to cognitive coevolution. It demonstrates how memory and reciprocity can lead to cooperation. The War of Attrition suggests that competition for resources may result in strategic coevolution, asymmetric equilibria, and conventions on sharing resources. Therefore, EGT may provide a suitable framework to understand and predict the human-AI evolutionary dynamic. However, future research could extend beyond EGT and explore additional frameworks, empirical validation methods, and interdisciplinary perspectives. AI is being shaped by human input and is evolving in response to it. So too, neuroplasticity allows the human brain to grow and evolve in response to stimuli. If humans and AI converge in future, what might be the result of human neuroplasticity combined with an ever-evolving AI? Future research should be mindful of the ethical and cognitive implications of human-AI interaction, evolution, and coevolution.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS</title>
<link>https://arxiv.org/abs/2505.16409</link>
<guid>https://arxiv.org/abs/2505.16409</guid>
<content:encoded><![CDATA[
<div> Framework, RETRIEVAL-AUGMENTED Reasoning, Large Reasoning Models, CORPUS, Monte Carlo Tree Search
Summary:
Large Reasoning Models (LRMs) have shown great performance in multi-step reasoning tasks and utilizing search engines effectively. Existing approaches involving separate retrieval models have limitations in retrieval decision-making and query processing, leading to increased costs and errors. To address this issue, the FREESON framework is introduced, allowing LRMs to autonomously retrieve relevant information by acting as both generator and retriever. The CT-MCTS algorithm enables LRMs to navigate through the corpus to locate answer-containing paths efficiently. Results across various open-domain QA benchmarks demonstrated a significant average improvement of 14.4% in EM and F1 compared to models with separate retrievers. Moreover, FREESON performed on par with or even outperformed the strongest baseline models on specific datasets such as PopQA and 2WikiMultihopQA. <div>
arXiv:2505.16409v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in multi-step reasoning and calling search engines at appropriate steps. However, existing retrieval-augmented reasoning approaches rely on separate retrieval models, limiting the LRM's role in retrieval to deciding when to retrieve and how to query. This separation not only increases hardware and operational costs but also leads to errors in the retrieval process due to the representation bottleneck, a phenomenon where the retriever's embedding space is not expressive enough to meet the generator's requirements. To address this, we shift our perspective from sequence-to-sequence matching to locating the answer-containing paths within the corpus, and propose a novel framework called FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables LRMs to retrieve relevant knowledge on their own by acting as both a generator and retriever. To achieve this, we introduce a variant of the MCTS algorithm specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus toward answer-containing regions. Our results on five open-domain QA benchmarks, including single-hop and multi-hop questions, show that FREESON achieves an average improvement of 14.4% in EM and F1 over four multi-step reasoning models with a separate retriever, and it also performs comparably to the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal Bias in Reasoning Models leads to Overthinking</title>
<link>https://arxiv.org/abs/2505.16448</link>
<guid>https://arxiv.org/abs/2505.16448</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, overthinking, internal bias, computational resources, interpretability experiments

Summary: This study investigates the phenomenon of overthinking in reasoning models, attributing it to an internal bias towards input texts. The model's preliminary guess about the answer, formed before actual reasoning, often leads to unnecessary reflections and wasted computational resources. Excessive attention to the input section exacerbates this behavior. By masking out the original input section, the impact of internal bias can be mitigated, resulting in a significant reduction in reasoning length and, in most cases, improved accuracy. These findings highlight the causal relationship between internal bias and overthinking in reasoning models.<br /><br />Summary: <div>
arXiv:2505.16448v1 Announce Type: new 
Abstract: While current reasoning models possess strong exploratory capabilities, they are often criticized for overthinking due to redundant and unnecessary reflections. In this work, we reveal for the first time that overthinking in reasoning models may stem from their internal bias towards input texts. Upon encountering a reasoning problem, the model immediately forms a preliminary guess about the answer, which we term as an internal bias since it is not derived through actual reasoning. When this guess conflicts with its reasoning result, the model tends to engage in reflection, leading to the waste of computational resources. Through further interpretability experiments, we find that this behavior is largely driven by the model's excessive attention to the input section, which amplifies the influence of internal bias on its decision-making process. Additionally, by masking out the original input section, the affect of internal bias can be effectively alleviated and the reasoning length could be reduced by 31%-53% across different complex reasoning tasks. Notably, in most cases, this approach also leads to improvements in accuracy. These findings demonstrate a causal relationship between internal bias and overthinking.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events</title>
<link>https://arxiv.org/abs/2505.16455</link>
<guid>https://arxiv.org/abs/2505.16455</guid>
<content:encoded><![CDATA[
<div> Keywords: panic prediction, social media, emotion arousal theory, risk perception, explainable AI

Summary:
The article introduces a new framework called PsychoAgent for predicting public panic sentiment on social media during disaster events. The framework addresses challenges such as lack of annotated data, unmodeled risk perception, and limited interpretability of panic formation mechanisms. It leverages a fine-grained open panic emotion dataset (COPE) created through collaboration between humans and large language models. By integrating cross-domain heterogeneous data and psychological mechanisms, the framework captures risk perception and cognitive differences in emotion generation. A role-playing agent based on large language models simulates individual psychological chains for enhanced interpretability. Experimental results demonstrate that PsychoAgent improves panic emotion prediction performance significantly compared to baseline models. The approach shifts from opaque "data-driven fitting" to transparent "role-based simulation with mechanistic interpretation," providing a valuable tool for proactive governance and crisis management during emergencies. <div>
arXiv:2505.16455v1 Announce Type: new 
Abstract: During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque "data-driven fitting" to transparent "role-based simulation with mechanistic interpretation" for panic emotion prediction during emergencies. Our implementation is publicly available at: https://anonymous.4open.science/r/PsychoAgent-19DD.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks</title>
<link>https://arxiv.org/abs/2505.16459</link>
<guid>https://arxiv.org/abs/2505.16459</guid>
<content:encoded><![CDATA[
<div> benchmark, multi-modal reasoning, reasoning quality, evaluation, MLLMs-T 

Summary:
The article introduces a new benchmark, MMMR, designed to evaluate multi-modal reasoning with explicit thinking. The benchmark includes a dataset of 1,083 questions spanning six reasoning types with symbolic depth and multi-hop demands. It also introduces a modular Reasoning Trace Evaluation Pipeline (RTEP) to assess reasoning quality beyond accuracy. Empirical results show that MLLMs-T outperform non-thinking counterparts, but top models still suffer from reasoning pathologies like inconsistency and overthinking. The benchmark exposes gaps between accuracy and reasoning quality, providing a valuable evaluation pipeline for future model development. Overall, the MMMR establishes a scalable foundation for evaluating, comparing, and enhancing the next generation of multi-modal reasoning systems.<br /><br />Summary: <div>
arXiv:2505.16459v1 Announce Type: new 
Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection</title>
<link>https://arxiv.org/abs/2505.16475</link>
<guid>https://arxiv.org/abs/2505.16475</guid>
<content:encoded><![CDATA[
<div> pipeline, ReflectEvo, small language models, meta introspection, reflection learning, reasoning abilities

Summary:
The article introduces a novel pipeline called ReflectEvo that uses small language models (SLMs) to enhance meta introspection through reflection learning. This approach generates self-reflection for self-training in an iterative manner, leading to continuous self-evolution. Through ReflectEvo-460k, a large-scale self-generated reflection dataset, the study demonstrates the improved reasoning abilities of SLMs using SFT and DPO techniques. ReflectEvo shows significant performance gains, surpassing prominent open-sourced models on BIG-bench without external assistance. The research emphasizes the quality of self-generated reflections and their impact on error localization and correction. Overall, the study sheds light on the potential of enhancing SLMs' reasoning performance through iterative reflection learning in the long term. 

<br /><br />Summary: <div>
arXiv:2505.16475v1 Announce Type: new 
Abstract: We present a novel pipeline, ReflectEvo, to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning. This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process. Leveraging this pipeline, we construct ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection dataset with broadened instructions and diverse multi-domain tasks. Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs' reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or fine-grained human annotation. We further conduct a deeper analysis of the high quality of self-generated reflections and their impact on error localization and correction. Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery</title>
<link>https://arxiv.org/abs/2505.16477</link>
<guid>https://arxiv.org/abs/2505.16477</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, scientific research, experimental design, data analysis, ethical questions

Summary: 
Large Language Models (LLMs) are playing a significant role in transforming scientific research by enhancing productivity and reshaping the scientific method. They are now involved in various stages of the scientific cycle, such as experimental design and data analysis, particularly in fields like chemistry and biology. Despite their potential benefits, challenges like hallucinations and reliability issues persist. The integration of LLMs into the scientific process should be done in collaboration with human scientists and aligned with clear evaluation metrics to ensure their effectiveness as creative engines and productivity enhancers. The ethical implications of AI-driven science, including questions about creativity, oversight, and responsibility, must be carefully considered. Ultimately, the scientific community needs to decide on the extent to which it allows LLMs to drive scientific advancements while ensuring responsible and effective use. <div>
arXiv:2505.16477v1 Announce Type: new 
Abstract: With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method. LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology. However, challenges such as hallucinations and reliability persist. In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility. With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively. However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes</title>
<link>https://arxiv.org/abs/2505.16482</link>
<guid>https://arxiv.org/abs/2505.16482</guid>
<content:encoded><![CDATA[
<div> Keywords: Wireless Rechargeable Sensor Networks, charging strategy optimization, partial charging approach, bi-level optimized scheme, mathematical model formulation

Summary:<br /><br />
- The paper discusses the optimization of charging strategies in Wireless Rechargeable Sensor Networks (WRSNs) using a novel partial charging approach.
- Existing studies have mainly focused on optimizing the charging path with a fully charging approach, leading to the death of sensors due to extended charging latency.
- The proposed approach aims to minimize energy depletion by simultaneously optimizing the charging path and time using a bi-level optimized scheme.
- Two approximate algorithms are introduced, combining Multi-start Local Search, Genetic Algorithm, Multitasking, and Covariance Matrix Adaptation Evolutionary Strategies.
- Experimental validations on various network scenarios demonstrate that the proposed algorithms outperform existing works. <div>
arXiv:2505.16482v1 Announce Type: new 
Abstract: Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the advantage of wireless energy transfer technology have opened a promising opportunity in solving the limited energy issue. However, an ineffective charging strategy may reduce the charging performance. Although many practical charging algorithms have been introduced, these studies mainly focus on optimizing the charging path with a fully charging approach. This approach may lead to the death of a series of sensors due to their extended charging latency. This paper introduces a novel partial charging approach that follows a bi-level optimized scheme to minimize energy depletion in WRSNs. We aim at optimizing simultaneously two factors: the charging path and time. To accomplish this, we first formulate a mathematical model of the investigated problem. We then propose two approximate algorithms in which the optimization of the charging path and the charging time are considered as the upper and lower level, respectively. The first algorithm combines a Multi-start Local Search method and a Genetic Algorithm to find a solution. The second algorithm adopts a nested approach that utilizes the advantages of the Multitasking and Covariance Matrix Adaptation Evolutionary Strategies. Experimental validations on various network scenarios demonstrate that our proposed algorithms outperform the existing works.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)</title>
<link>https://arxiv.org/abs/2505.16507</link>
<guid>https://arxiv.org/abs/2505.16507</guid>
<content:encoded><![CDATA[
<div> keywords: relevance, verification status, argumentation frameworks, stability, complexity

Summary:
In this paper, the authors explore the concept of relevance for ensuring the stability of the verification status of a set of arguments in incomplete argumentation frameworks (IAFs). They introduce the notion of strong relevance, which emphasizes the need for resolution in all situations to achieve stability. The complexity analysis demonstrates that detecting relevance for stability of argument sets is feasible in polynomial time for certain semantics. However, the study also highlights the challenges in developing tractable methods for relevance detection under grounded semantics. Overall, the paper expands upon the original notion of relevance proposed for single arguments and extends it to address the stability of verification status for sets of arguments in IAFs, shedding light on both theoretical concepts and practical implications in the field of argumentation theory.

Summary:<br />Keywords: relevance, verification status, argumentation frameworks, stability, complexity


 <div>
arXiv:2505.16507v1 Announce Type: new 
Abstract: The notion of relevance was proposed for stability of justification status of a single argument in incomplete argumentation frameworks (IAFs) in 2024 by Odekerken et al. To extend the notion, we study the relevance for stability of verification status of a set of arguments in this paper, i.e., the uncertainties in an IAF that have to be resolved in some situations so that answering whether a given set of arguments is an extension obtains the same result in every completion of the IAF. Further we propose the notion of strong relevance for describing the necessity of resolution in all situations reaching stability. An analysis of complexity reveals that detecting the (strong) relevance for stability of sets of arguments can be accomplished in P time under the most semantics discussed in the paper. We also discuss the difficulty in finding tractable methods for relevance detection under grounded semantics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.16579</link>
<guid>https://arxiv.org/abs/2505.16579</guid>
<content:encoded><![CDATA[
<div> Keywords: chains-of-thought, spatial reasoning, multimodal large language models, dynamic visual drafts, D2R framework

Summary: 
The paper introduces a new benchmark called GRASSLAND for evaluating dynamic spatial reasoning in multimodal large language models (MLLMs). It proposes a novel approach called D2R (Dynamic Draft-Augmented Reasoning) that combines textual chains-of-thought with dynamic visual drafts to improve performance on spatial reasoning tasks. The experiments show that integrating visual drafts with textual reasoning significantly enhances models' performance in dynamic spatial reasoning scenarios. The D2R framework is designed to be training-free and seamlessly integrates visual information with textual reasoning chains in MLLMs. Extensive evaluations demonstrate that D2R consistently improves performance across various tasks without the need for model fine-tuning. The project code is open-source and available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2505.16579v1 Announce Type: new 
Abstract: While chains-of-thought (CoT) have advanced complex reasoning in multimodal large language models (MLLMs), existing methods remain confined to text or static visual domains, often faltering in dynamic spatial reasoning tasks. To bridge this gap, we present GRASSLAND, a novel maze navigation benchmark designed to evaluate dynamic spatial reasoning. Our experiments show that augmenting textual reasoning chains with dynamic visual drafts, overlaid on input images, significantly outperforms conventional approaches, offering new insights into spatial reasoning in evolving environments. To generalize this capability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free framework that seamlessly integrates textual CoT with corresponding visual drafts into MLLMs. Extensive evaluations demonstrate that D2R consistently enhances performance across diverse tasks, establishing a robust baseline for dynamic spatial reasoning without requiring model fine-tuning. Project is open at https://github.com/Cratileo/D2R.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences</title>
<link>https://arxiv.org/abs/2505.16619</link>
<guid>https://arxiv.org/abs/2505.16619</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, life sciences, reusability, reproducibility, Open and Sustainable AI

Summary:
Artificial intelligence (AI) has revolutionized life sciences research, but challenges such as poor reusability and reproducibility are hampering trust in AI outputs. This also impacts environmental sustainability. The fragmented nature of the AI ecosystem and lack of guidance for Open and Sustainable AI (OSAI) model development further compound these issues. To address this, a set of practical OSAI recommendations has been introduced, mapping to over 300 AI ecosystem components. These recommendations aim to enhance transparency, reusability, and sustainability in AI research, aligning with existing efforts and community consensus. By providing researchers with resources to implement sustainable practices, this work seeks to guide future policy development and structured pathways for AI implementation. 

<br /><br />Summary: Artificial intelligence has transformed life sciences research, but challenges like poor reusability and reproducibility threaten trust in AI outputs and environmental sustainability. The fragmented AI ecosystem lacks guidance for Open and Sustainable AI (OSAI) model development. To address these issues, practical OSAI recommendations have been introduced, mapping to over 300 AI ecosystem components. These recommendations aim to enhance transparency, reusability, and sustainability in AI research, aligning with existing efforts and community consensus, to guide future policy development and structured pathways for AI implementation. <div>
arXiv:2505.16619v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has recently seen transformative breakthroughs in the life sciences, expanding possibilities for researchers to interpret biological information at an unprecedented capacity, with novel applications and advances being made almost daily. In order to maximise return on the growing investments in AI-based life science research and accelerate this progress, it has become urgent to address the exacerbation of long-standing research challenges arising from the rapid adoption of AI methods. We review the increased erosion of trust in AI research outputs, driven by the issues of poor reusability and reproducibility, and highlight their consequent impact on environmental sustainability. Furthermore, we discuss the fragmented components of the AI ecosystem and lack of guiding pathways to best support Open and Sustainable AI (OSAI) model development. In response, this perspective introduces a practical set of OSAI recommendations directly mapped to over 300 components of the AI ecosystem. Our work connects researchers with relevant AI resources, facilitating the implementation of sustainable, reusable and transparent AI. Built upon life science community consensus and aligned to existing efforts, the outputs of this perspective are designed to aid the future development of policy and structured pathways for guiding AI implementation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2505.16646</link>
<guid>https://arxiv.org/abs/2505.16646</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, mathematical problem solving, SMART framework, assessment, benchmark data

Summary: 
Large Language Models (LLMs) have shown impressive results on mathematical benchmarks, but there are concerns about whether these achievements truly reflect mathematical reasoning or are just superficial pattern recognition. Common evaluation metrics like final answer accuracy are inadequate in analyzing the underlying competencies involved. To address this, a new assessment framework called SMART has been introduced. SMART breaks down mathematical problem solving into four dimensions  understanding, reasoning, arithmetic, and reflection & refinement. Each dimension is evaluated separately through customized tasks, allowing for a detailed analysis of LLM behavior. An automated self-generating and self-validating mechanism is integrated into SMART to ensure scalability and reliability. By applying SMART to 21 LLMs, significant differences in their abilities across various dimensions were uncovered, highlighting the need for a new comprehensive metric to better assess problem-solving capabilities.<br /><br />Summary: <div>
arXiv:2505.16646v1 Announce Type: new 
Abstract: Large Language Models have achieved remarkable results on a variety of mathematical benchmarks. However, concerns remain as to whether these successes reflect genuine mathematical reasoning or superficial pattern recognition. Common evaluation metrics, such as final answer accuracy, fail to disentangle the underlying competencies involved, offering limited diagnostic value. To address these limitations, we introduce SMART: a Self-Generating and Self-Validating Multi-Dimensional Assessment Framework. SMART decomposes mathematical problem solving into four distinct dimensions: understanding, reasoning, arithmetic, and reflection \& refinement. Each dimension is evaluated independently through tailored tasks, enabling interpretable and fine-grained analysis of LLM behavior. Crucially, SMART integrates an automated self-generating and self-validating mechanism to produce and verify benchmark data, ensuring both scalability and reliability. We apply SMART to 21 state-of-the-art open- and closed-source LLMs, uncovering significant discrepancies in their abilities across different dimensions. Our findings demonstrate the inadequacy of final answer accuracy as a sole metric and motivate a new holistic metric to better capture true problem-solving capabilities. Code and benchmarks will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming</title>
<link>https://arxiv.org/abs/2505.16667</link>
<guid>https://arxiv.org/abs/2505.16667</guid>
<content:encoded><![CDATA[
<div> empirical methods, human-LLM collaboration, programming dataset, human feedback, competitive programming

Summary: 
This research focuses on the collaboration between humans and Large Language Models (LLMs) in competitive programming. The study addresses the fragmented nature of existing research by introducing a comprehensive taxonomy of human feedback throughout the programming process. Additionally, the researchers introduce ELABORATIONSET, a programming dataset specifically designed for human-LLM collaboration, enabling large-scale simulated human feedback and cost-effective real human interaction studies. A novel benchmark called ELABORATION is also introduced to assess the effectiveness of existing methods in human-LLM competition. The research aims to highlight the strengths and weaknesses of current approaches, providing a foundation for future improvements and advancements in human-LLM collaborative programming. The code and dataset are available for further exploration and analysis. 

<br /><br />Summary: <div>
arXiv:2505.16667v1 Announce Type: new 
Abstract: While recent research increasingly emphasizes the value of human-LLM collaboration in competitive programming and proposes numerous empirical methods, a comprehensive understanding remains elusive due to the fragmented nature of existing studies and their use of diverse, application-specific human feedback. Thus, our work serves a three-fold purpose: First, we present the first taxonomy of human feedback consolidating the entire programming process, which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a novel programming dataset specifically designed for human-LLM collaboration, meticulously annotated to enable large-scale simulated human feedback and facilitate costeffective real human interaction studies. Third, we introduce ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM competitive programming. With ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby setting the foundation for future improvement. Our code and dataset are available at https://github.com/SCUNLP/ELABORATION
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPaRC: A Spatial Pathfinding Reasoning Challenge</title>
<link>https://arxiv.org/abs/2505.16686</link>
<guid>https://arxiv.org/abs/2505.16686</guid>
<content:encoded><![CDATA[
<div> pathfinding, spatial reasoning, symbolic reasoning, abstract problems, multi-step problems

Summary:
SPaRC (Spatial Pathfinding Reasoning Challenge) is a new dataset designed to test spatial and symbolic reasoning through 2D grid pathfinding puzzles. While humans perform well on these puzzles, existing reasoning models struggle to achieve high accuracy, often generating invalid paths and making errors in navigation and spatial logic. Models also fail to scale test-time compute with difficulty, unlike humans who take longer on harder puzzles. Allowing models to make multiple solution attempts can improve accuracy, indicating potential for better spatial reasoning with improved training methods. SPaRC highlights the limitations in models' spatial reasoning abilities and suggests the need for new methods to excel in abstract, multi-step problem-solving. <div>
arXiv:2505.16686v1 Announce Type: new 
Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step problems, especially pathfinding and complex rule constraint satisfaction. We introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000 2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning, requiring step-by-step planning with arithmetic and geometric rules. Humans achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles). Models often generate invalid paths (>50% of puzzles for o4-mini), and reasoning tokens reveal they make errors in navigation and spatial logic. Unlike humans, who take longer on hard puzzles, models fail to scale test-time compute with difficulty. Allowing models to make multiple solution attempts improves accuracy, suggesting potential for better spatial reasoning with improved training and efficient test-time scaling methods. SPaRC can be used as a window into models' spatial reasoning limitations and drive research toward new methods that excel in abstract, multi-step problem-solving.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2505.16700</link>
<guid>https://arxiv.org/abs/2505.16700</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Model Context Protocol, MCP-RADAR, benchmark, tool interaction

Summary:
The paper introduces MCP-RADAR, a benchmark to evaluate Large Language Models (LLMs) in the Model Context Protocol (MCP) framework. The benchmark assesses LLM performance through five dimensions: answer accuracy, tool selection efficiency, computational resource efficiency, parameter construction accuracy, and execution speed. Unlike traditional benchmarks, MCP-RADAR uses objective measurements across various task domains. Evaluations of leading LLMs reveal trade-offs between accuracy, efficiency, and speed, challenging single-metric rankings. The methodology provides guidance for optimizing tools for maximum compatibility and effectiveness. While focused on MCP, the approach is applicable to all LLM-agent tool integration frameworks, offering insights for developers and tool creators to enhance interaction ecosystems. The implementation, configurations, and datasets used in the evaluation are publicly accessible at the provided link.

<br /><br />Summary: <div>
arXiv:2505.16700v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of tool interaction, the Model Context Protocol (MCP) has emerged as a standardized framework for dynamic tool discovery and orchestration. Despite widespread industry adoption, existing evaluation methodologies fail to adequately assess tool utilization capabilities within this new paradigm. This paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance in the MCP framework through a novel five-dimensional approach measuring: answer accuracy, tool selection efficiency, computational resource efficiency, parameter construction accuracy, and execution speed. Unlike conventional benchmarks that rely on subjective human evaluations or binary success metrics, MCP-RADAR employs objective, quantifiable measurements across multiple task domains including software engineering, mathematical reasoning, and general problem-solving. Our evaluations of leading commercial and open-source LLMs reveal distinctive capability profiles with significant trade-offs between accuracy, efficiency, and speed, challenging traditional single-metric performance rankings. Besides, we provide valuable guidance for developers to optimize their tools for maximum model compatibility and effectiveness. While focused on MCP due to its standardized approach, our methodology remains applicable across all LLM agent tool integration frameworks, providing valuable insights for both LLM developers and tool creators to optimize the entire LLM-tool interaction ecosystem. The implementation, configurations, and datasets used in our evaluation are publicly available at https://anonymous.4open.science/r/MCPRadar-B143.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2505.16771</link>
<guid>https://arxiv.org/abs/2505.16771</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, breakthroughs, data centric, privacy, data infrastructure<br />
<br />
Summary: This paper examines major advancements in artificial intelligence (AI) in the last fifteen years, analyzing how computational resources, data access, and algorithmic innovation have converged to drive progress. Key breakthroughs include GPU based model training, the shift to data-centric approaches with ImageNet, simplifying architectures with the Transformer, and expanding modeling capabilities with the GPT series. These milestones are seen as indicators of deeper paradigm shifts, with a focus on sample complexity and data efficiency. The paper discusses the importance of embracing data-centric approaches, particularly in light of privacy concerns and regulations. It evaluates solutions such as federated learning, privacy enhancing technologies, and the data site paradigm for enhancing data access and security. Additionally, it explores options for generating mock and synthetic data when real-world data is unavailable. Strategic guidance for future AI research and policy development is provided by aligning technical insights with evolving data infrastructure. <br /><br />Summary: <div>
arXiv:2505.16771v1 Announce Type: new 
Abstract: This paper presents a comprehensive synthesis of major breakthroughs in artificial intelligence (AI) over the past fifteen years, integrating historical, theoretical, and technological perspectives. It identifies key inflection points in AI' s evolution by tracing the convergence of computational resources, data access, and algorithmic innovation. The analysis highlights how researchers enabled GPU based model training, triggered a data centric shift with ImageNet, simplified architectures through the Transformer, and expanded modeling capabilities with the GPT series. Rather than treating these advances as isolated milestones, the paper frames them as indicators of deeper paradigm shifts. By applying concepts from statistical learning theory such as sample complexity and data efficiency, the paper explains how researchers translated breakthroughs into scalable solutions and why the field must now embrace data centric approaches. In response to rising privacy concerns and tightening regulations, the paper evaluates emerging solutions like federated learning, privacy enhancing technologies (PETs), and the data site paradigm, which reframe data access and security. In cases where real world data remains inaccessible, the paper also assesses the utility and constraints of mock and synthetic data generation. By aligning technical insights with evolving data infrastructure, this study offers strategic guidance for future AI research and policy development.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making</title>
<link>https://arxiv.org/abs/2505.16781</link>
<guid>https://arxiv.org/abs/2505.16781</guid>
<content:encoded><![CDATA[
<div> Keywords: group decision-making, uncertainty, dynamic social structures, three-way decision theory, linguistic opinion representation 

Summary: 
The article introduces a novel Social Network Group Decision-Making (SNGDM) framework to address challenges in group decision-making scenarios. The framework integrates three-way decision (3WD) theory, dynamic network reconstruction, and linguistic opinion representation. The 3WD mechanism accounts for hesitation and ambiguity in agent judgments, preventing irrational decisions. A connection adjustment rule based on opinion similarity allows agents to adaptively update communication links reflecting evolving social relationships. Linguistic terms are used to describe agent opinions for handling subjective or incomplete information. An integrated multi-agent decision-making framework considers individual uncertainty, opinion evolution, and network dynamics simultaneously. The proposed model is applied to a multi-UAV cooperative decision-making scenario, and simulation results demonstrate its effectiveness in enhancing system stability and representing realistic decision-making behaviors.<br /><br />Summary: <div>
arXiv:2505.16781v1 Announce Type: new 
Abstract: In group decision-making (GDM) scenarios, uncertainty, dynamic social structures, and vague information present major challenges for traditional opinion dynamics models. To address these issues, this study proposes a novel social network group decision-making (SNGDM) framework that integrates three-way decision (3WD) theory, dynamic network reconstruction, and linguistic opinion representation. First, the 3WD mechanism is introduced to explicitly model hesitation and ambiguity in agent judgments, thereby preventing irrational decisions. Second, a connection adjustment rule based on opinion similarity is developed, enabling agents to adaptively update their communication links and better reflect the evolving nature of social relationships. Third, linguistic terms are used to describe agent opinions, allowing the model to handle subjective, vague, or incomplete information more effectively. Finally, an integrated multi-agent decision-making framework is constructed, which simultaneously considers individual uncertainty, opinion evolution, and network dynamics. The proposed model is applied to a multi-UAV cooperative decision-making scenario, where simulation results and consensus analysis demonstrate its effectiveness. Experimental comparisons further verify the advantages of the algorithm in enhancing system stability and representing realistic decision-making behaviors.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce</title>
<link>https://arxiv.org/abs/2505.16787</link>
<guid>https://arxiv.org/abs/2505.16787</guid>
<content:encoded><![CDATA[
<div> model-based reinforcement learning, world model, model predictive control, entropy, hierarchical planner

Summary:<br />
- The article introduces a novel approach to model-based reinforcement learning that focuses on improving the world model's learning efficiency.
- By actively seeking high-entropy states using short-horizon latent predictions, the proposed method offers a principled alternative to traditional curiosity-driven approaches.
- A hierarchical planner is utilized to dynamically determine when to replan, planning horizon length, and the balance between reward and entropy.
- The method demonstrates improved performance in Miniworld procedurally generated mazes compared to base Dreamer, finishing tasks 50% faster at convergence.
- The policy trained using the proposed method converges in only 60% of the environment steps required by base Dreamer. 

<br /><br />Summary: <div>
arXiv:2505.16787v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase the sample efficiency of model-free RL methods by simultaneously training a world model that learns to predict the future. MBRL methods have progressed by largely prioritising the actor; optimising the world model learning has been neglected meanwhile. Improving the fidelity of the world model and reducing its time to convergence can yield significant downstream benefits, one of which is improving the ensuing performance of any actor it may train. We propose a novel approach that anticipates and actively seeks out high-entropy states using short-horizon latent predictions generated by the world model, offering a principled alternative to traditional curiosity-driven methods that chase once-novel states well after they were stumbled into. While many model predictive control (MPC) based methods offer similar alternatives, they typically lack commitment, synthesising multi step plans after every step. To mitigate this, we present a hierarchical planner that dynamically decides when to replan, planning horizon length, and the weighting between reward and entropy. While our method can theoretically be applied to any model that trains its own actors with solely model generated data, we have applied it to just Dreamer as a proof of concept. Our method finishes the Miniworld procedurally generated mazes 50% faster than base Dreamer at convergence and the policy trained in imagination converges in only 60% of the environment steps that base Dreamer needs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.16826</link>
<guid>https://arxiv.org/abs/2505.16826</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language models, fine-grained advantages, token-level importance, mathematical reasoning<br />
Summary: <br />
Recent research has shown that combining reinforcement learning with rule-based rewards can improve the reasoning abilities of large language models. However, existing algorithms like GRPO and DAPO suffer from a coarse granularity issue when calculating advantages, treating all tokens in a sequence equally. To address this, a new algorithm called Key-token Advantage Estimation (KTAE) has been proposed. KTAE estimates fine-grained, token-level advantages by analyzing the importance of individual tokens in a sequence based on sampled rollouts. Models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods on mathematical reasoning tasks, achieving higher accuracy with shorter responses. Notably, they even outperform a well-known model, R1-Distill-Qwen-1.5B, using the same base model. <div>
arXiv:2505.16826v1 Announce Type: new 
Abstract: Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent</title>
<link>https://arxiv.org/abs/2505.16827</link>
<guid>https://arxiv.org/abs/2505.16827</guid>
<content:encoded><![CDATA[
<div> Keywords: GUI automation, MLLMs, Function-aware Trajectory, Transition-aware Knowledge, Task success rate

Summary:
GUI automation in dynamic environments faces challenges such as misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods for MLLMs are costly for app-specific knowledge updates. The GUI-explorer is a training-free GUI agent that tackles these challenges by incorporating two fundamental mechanisms. Firstly, it autonomously explores function-aware trajectories by analyzing GUI structural information to construct exploration goals. Secondly, it mines transition-aware knowledge through unsupervised analysis of structured interaction triples without human involvement. The GUI-explorer achieved a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, showing significant improvements over state-of-the-art agents. It does not require parameter updates for new apps and is open-sourced and publicly available at https://github.com/JiuTian-VL/GUI-explorer.<br /><br />Summary: <div>
arXiv:2505.16827v1 Announce Type: new 
Abstract: GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory. To comprehensively cover all application functionalities, we design a Function-aware Task Goal Generator that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. (2) Unsupervised Mining of Transition-aware Knowledge. To establish precise screen-operation logic, we develop a Transition-aware Knowledge Extractor that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at https://github.com/JiuTian-VL/GUI-explorer.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization</title>
<link>https://arxiv.org/abs/2505.16832</link>
<guid>https://arxiv.org/abs/2505.16832</guid>
<content:encoded><![CDATA[
<div> STEM, visual explanations, educational settings, reasoning decomposition, multi-agent framework
Summary:<br />
- Foundation models are currently limited in their ability to generate pedagogically effective visual explanations in educational settings.
- EduVisBench is a multi-domain benchmark that assesses the visual reasoning capabilities of foundation models.
- Existing models struggle with decomposing complex reasoning into visually grounded solutions aligned with human cognitive processes.
- EduVisAgent, a multi-agent collaborative framework, outperforms baselines by 40.2% and provides more educationally aligned visualizations.
- The framework coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. The EduVisBench and EduVisAgent resources are freely available on GitHub for further exploration and use. 
<br />Summary: <div>
arXiv:2505.16832v1 Announce Type: new 
Abstract: While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at https://github.com/aiming-lab/EduVisBench and https://github.com/aiming-lab/EduVisAgent.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.16854</link>
<guid>https://arxiv.org/abs/2505.16854</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Vision-Language Models, Group Relative Policy Optimization, Two-Stage Training Strategy, Selective Reasoning

Summary:
In this study, a new two-stage training strategy called TON is proposed to enhance reasoning in vision-language models using reinforcement learning. The first stage involves supervised fine-tuning with a 'thought dropout' operation to teach the model when to skip reasoning. The second stage, based on Group Relative Policy Optimization (GRPO), allows the model to decide when to engage in reasoning processes while maximizing task rewards. Experimental results show that TON significantly reduces completion length compared to GRPO without sacrificing performance. The model learns to bypass unnecessary reasoning steps as training progresses, leading to more efficient reasoning in vision-language tasks. This research opens up new possibilities for developing human-like reasoning patterns in reinforcement learning approaches. The code for TON is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.16854v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings</title>
<link>https://arxiv.org/abs/2505.16877</link>
<guid>https://arxiv.org/abs/2505.16877</guid>
<content:encoded><![CDATA[
<div> uncertainty quantification, knowledge graph embedding, conformal prediction, conditional coverage guarantee, medical diagnosis <br />
Summary:<br />
The article introduces CondKGCP, a novel method for uncertainty quantification in Knowledge Graph Embedding (KGE) methods. It aims to provide conditional coverage guarantees for predicted sets, which is crucial in high-stakes applications like medical diagnosis. CondKGCP merges predicates with similar vector representations and utilizes rank information to enhance calibration. The method ensures consistent coverage per query, offering a stronger guarantee than existing approaches that provide only marginal coverage guarantees. The theoretical guarantees of CondKGCP are proven, and empirical evaluations demonstrate its effectiveness. CondKGCP maintains compact prediction sets while approximating predicate-conditional coverage guarantees, making it a valuable tool for reliable uncertainty quantification in KGE methods. <br /> <div>
arXiv:2505.16877v1 Announce Type: new 
Abstract: Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is crucial for ensuring the reliability of downstream applications. A recent work applies conformal prediction to KGE methods, providing uncertainty estimates by generating a set of answers that is guaranteed to include the true answer with a predefined confidence level. However, existing methods provide probabilistic guarantees averaged over a reference set of queries and answers (marginal coverage guarantee). In high-stakes applications such as medical diagnosis, a stronger guarantee is often required: the predicted sets must provide consistent coverage per query (conditional coverage guarantee). We propose CondKGCP, a novel method that approximates predicate-conditional coverage guarantees while maintaining compact prediction sets. CondKGCP merges predicates with similar vector representations and augments calibration with rank information. We prove the theoretical guarantees and demonstrate empirical effectiveness of CondKGCP by comprehensive evaluations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships</title>
<link>https://arxiv.org/abs/2505.16899</link>
<guid>https://arxiv.org/abs/2505.16899</guid>
<content:encoded><![CDATA[
<div> AI, collaboration, risks, framework, mitigation <br />
<br />
Summary: 
Artificial Intelligence (AI) systems are evolving to become thought partners that collaborate with humans in complex reasoning tasks. This advancement brings new opportunities for collaboration and extended cognition but also introduces various risks at multiple levels of analysis  Real-time, Individual, and Societal risks arising from collaborative cognition (RISc). To address these risks, the commentary proposes a framework to systematically identify and evaluate them, along with suggesting mitigation strategies for developers and policymakers. By implementing concrete metrics for risk evaluation and adopting specific mitigation strategies, developers can prevent potential harms and ensure that humans derive benefits from productive thought partnerships. The continued proliferation of AI thought partners necessitates proactive measures to safeguard against risks and enhance the positive impact of collaborative cognitive technologies. <br /> <div>
arXiv:2505.16899v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) systems have historically been used as tools that execute narrowly defined tasks. Yet recent advances in AI have unlocked possibilities for a new class of models that genuinely collaborate with humans in complex reasoning, from conceptualizing problems to brainstorming solutions. Such AI thought partners enable novel forms of collaboration and extended cognition, yet they also pose major risks-including and beyond risks of typical AI tools and agents. In this commentary, we systematically identify risks of AI thought partners through a novel framework that identifies risks at multiple levels of analysis, including Real-time, Individual, and Societal risks arising from collaborative cognition (RISc). We leverage this framework to propose concrete metrics for risk evaluation, and finally suggest specific mitigation strategies for developers and policymakers. As AI thought partners continue to proliferate, these strategies can help prevent major harms and ensure that humans actively benefit from productive thought partnerships.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning</title>
<link>https://arxiv.org/abs/2505.16928</link>
<guid>https://arxiv.org/abs/2505.16928</guid>
<content:encoded><![CDATA[
arXiv:2505.16928v1 Announce Type: new 
Abstract: We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification</title>
<link>https://arxiv.org/abs/2505.16938</link>
<guid>https://arxiv.org/abs/2505.16938</guid>
<content:encoded><![CDATA[
arXiv:2505.16938v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios</title>
<link>https://arxiv.org/abs/2505.16944</link>
<guid>https://arxiv.org/abs/2505.16944</guid>
<content:encoded><![CDATA[
arXiv:2505.16944v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation</title>
<link>https://arxiv.org/abs/2505.16978</link>
<guid>https://arxiv.org/abs/2505.16978</guid>
<content:encoded><![CDATA[
arXiv:2505.16978v1 Announce Type: new 
Abstract: Grammar plays a critical role in natural language processing and text/code generation by enabling the definition of syntax, the creation of parsers, and guiding structured outputs. Although large language models (LLMs) demonstrate impressive capabilities across domains, their ability to infer and generate grammars has not yet been thoroughly explored. In this paper, we aim to study and improve the ability of LLMs for few-shot grammar generation, where grammars are inferred from sets of a small number of positive and negative examples and generated in Backus-Naur Form. To explore this, we introduced a novel dataset comprising 540 structured grammar generation challenges, devised 6 metrics, and evaluated 8 various LLMs against it. Our findings reveal that existing LLMs perform sub-optimally in grammar generation. To address this, we propose an LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar generation. HyGenar achieves substantial improvements in both the syntactic and semantic correctness of generated grammars across LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design</title>
<link>https://arxiv.org/abs/2505.16979</link>
<guid>https://arxiv.org/abs/2505.16979</guid>
<content:encoded><![CDATA[
arXiv:2505.16979v1 Announce Type: new 
Abstract: Single-agent LLMs hit hard limits--finite context, role overload, and brittle domain transfer. Conventional multi-agent fixes soften those edges yet expose fresh pains: ill-posed decompositions, fuzzy contracts, and verification overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a framework that converts domain priors into an algorithmic blueprint hierarchy, in which tasks are recursively split into typed, controller-mediated subtasks, each solved zero-shot or with the lightest viable boost (e.g., chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch theorem, KtR trades the chase for a universal prompt for disciplined decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15, versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation thus turns modest models into reliable collaborators--no ever-larger monoliths required.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine</title>
<link>https://arxiv.org/abs/2505.16982</link>
<guid>https://arxiv.org/abs/2505.16982</guid>
<content:encoded><![CDATA[
arXiv:2505.16982v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show promise in biomedicine but lack true causal understanding, relying instead on correlations. This paper envisions causal LLM agents that integrate multimodal data (text, images, genomics, etc.) and perform intervention-based reasoning to infer cause-and-effect. Addressing this requires overcoming key challenges: designing safe, controllable agentic frameworks; developing rigorous benchmarks for causal evaluation; integrating heterogeneous data sources; and synergistically combining LLMs with structured knowledge (KGs) and formal causal inference tools. Such agents could unlock transformative opportunities, including accelerating drug discovery through automated hypothesis generation and simulation, enabling personalized medicine through patient-specific causal models. This research agenda aims to foster interdisciplinary efforts, bridging causal concepts and foundation models to develop reliable AI partners for biomedical progress.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs</title>
<link>https://arxiv.org/abs/2505.16997</link>
<guid>https://arxiv.org/abs/2505.16997</guid>
<content:encoded><![CDATA[
arXiv:2505.16997v1 Announce Type: new 
Abstract: LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by enabling cooperation among multiple specialized agents. However, most existing MAS frameworks rely on a single LLM to drive all agents, constraining the system's intelligence to the limit of that model. This paper explores the paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by diverse LLMs, elevating the system's potential to the collective intelligence of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to evaluate the performance of various LLMs across different domains and MAS-related functions. As an extensive empirical study, we assess 27 LLMs across 5 domains (encompassing 21 test sets) and 5 functions, conducting over 1.7 million evaluations to identify optimal model selections for each domain-function combination. Building on these findings, we demonstrate that transitioning from homogeneous to heterogeneous LLM-driven MAS can significantly enhance system performance without requiring structural redesign. Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration yields up to 8.4\% performance improvement on the MATH dataset. In a mixed chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable 47\% performance boost on the AIME dataset. Our results underscore the transformative potential of heterogeneous LLMs in MAS, highlighting a promising avenue for advancing scalable, collaborative AI systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning</title>
<link>https://arxiv.org/abs/2502.15401</link>
<guid>https://arxiv.org/abs/2502.15401</guid>
<content:encoded><![CDATA[
arXiv:2502.15401v1 Announce Type: cross 
Abstract: In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be publicly available subsequently.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14679</link>
<guid>https://arxiv.org/abs/2505.14679</guid>
<content:encoded><![CDATA[
arXiv:2505.14679v1 Announce Type: cross 
Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model's internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1/3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)</title>
<link>https://arxiv.org/abs/2505.15820</link>
<guid>https://arxiv.org/abs/2505.15820</guid>
<content:encoded><![CDATA[
arXiv:2505.15820v1 Announce Type: cross 
Abstract: During football matches, a variety of different parties (e.g., companies) each collect (possibly overlapping) data about the match ranging from basic information (e.g., starting players) to detailed positional data. This data is provided to clubs, federations, and other organizations who are increasingly interested in leveraging this data to inform their decision making. Unfortunately, analyzing such data pose significant barriers because each provider may (1) collect different data, (2) use different specifications even within the same category of data, (3) represent the data differently, and (4) delivers the data in a different manner (e.g., file format, protocol). Consequently, working with these data requires a significant investment of time and money. The goal of this work is to propose a uniform and standardized format for football data called the Common Data Format (CDF). The CDF specifies a minimal schema for five types of match data: match sheet data, video footage, event data, tracking data, and match meta data. It aims to ensure that the provided data is clear, sufficiently contextualized (e.g., its provenance is clear), and complete such that it enables common downstream analysis tasks. Concretely, this paper will detail the technical specifications of the CDF, the representational choices that were made to help ensure the clarity of the provided data, and a concrete approach for delivering data in the CDF.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Compound AI Model for 6G Networks in 3D Continuum</title>
<link>https://arxiv.org/abs/2505.15821</link>
<guid>https://arxiv.org/abs/2505.15821</guid>
<content:encoded><![CDATA[
arXiv:2505.15821v1 Announce Type: cross 
Abstract: The 3D continuum presents a complex environment that spans the terrestrial, aerial and space domains, with 6Gnetworks serving as a key enabling technology. Current AI approaches for network management rely on monolithic models that fail to capture cross-domain interactions, lack adaptability,and demand prohibitive computational resources. This paper presents a formal model of Compound AI systems, introducing a novel tripartite framework that decomposes complex tasks into specialized, interoperable modules. The proposed modular architecture provides essential capabilities to address the unique challenges of 6G networks in the 3D continuum, where heterogeneous components require coordinated, yet distributed, intelligence. This approach introduces a fundamental trade-off between model and system performance, which must be carefully addressed. Furthermore, we identify key challenges faced by Compound AI systems within 6G networks operating in the 3D continuum, including cross-domain resource orchestration, adaptation to dynamic topologies, and the maintenance of consistent AI service quality across heterogeneous environments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilinear subspace learning for person re-identification based fusion of high order tensor features</title>
<link>https://arxiv.org/abs/2505.15825</link>
<guid>https://arxiv.org/abs/2505.15825</guid>
<content:encoded><![CDATA[
arXiv:2505.15825v1 Announce Type: cross 
Abstract: Video surveillance image analysis and processing is a challenging field in computer vision, with one of its most difficult tasks being Person Re-Identification (PRe-ID). PRe-ID aims to identify and track target individuals who have already been detected in a network of cameras, using a robust description of their pedestrian images. The success of recent research in person PRe-ID is largely due to effective feature extraction and representation, as well as the powerful learning of these features to reliably discriminate between pedestrian images. To this end, two powerful features, Convolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are modeled on multidimensional data using the proposed method, High-Dimensional Feature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced to leverage and combine these two types of features in a single tensor, even though their dimensions are not identical. To enhance the system's accuracy, we employ Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace learning, followed by cosine similarity for matching. TXQDA efficiently facilitates learning while reducing the high dimensionality inherent in high-order tensor data. The effectiveness of our approach is verified through experiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S. Extensive experiments demonstrate that our approach outperforms recent state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin Interaction</title>
<link>https://arxiv.org/abs/2505.15828</link>
<guid>https://arxiv.org/abs/2505.15828</guid>
<content:encoded><![CDATA[
arXiv:2505.15828v1 Announce Type: cross 
Abstract: In this paper, we investigate a quality of experience (QoE)-aware resource allocation problem for reconfigurable intelligent surface (RIS)-assisted digital twin (DT) interaction with uncertain evolution. In the considered system, mobile users are expected to interact with a DT model maintained on a DT server that is deployed on a base station, via effective uplink and downlink channels assisted by an RIS. Our goal is to maximize the sum of all mobile users' joint subjective and objective QoE in DT interactions across various DT scenes, by jointly optimizing phase shift matrix, receive/transmit beamforming matrix, rendering resolution configuration and computing resource allocation. While solving this problem is challenging mainly due to the uncertain evolution of the DT model, which leads to multiple scene-specific problems, and require us to constantly re-solve each of them whenever DT model evolves.
  To this end, leveraging the dynamic optimization capabilities of decision transformers and the generalization strengths of generative artificial intelligence (GAI), we propose a novel GAI-aided approach, called the prompt-guided decision transformer integrated with zero-forcing optimization (PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO, demonstrating its effectiveness and superiority over counterparts.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Hand-Crafted Metrics to Evolved Training-Free Performance Predictors for Neural Architecture Search via Genetic Programming</title>
<link>https://arxiv.org/abs/2505.15832</link>
<guid>https://arxiv.org/abs/2505.15832</guid>
<content:encoded><![CDATA[
arXiv:2505.15832v1 Announce Type: cross 
Abstract: Estimating the network performance using zero-cost (ZC) metrics has proven both its efficiency and efficacy in Neural Architecture Search (NAS). However, a notable limitation of most ZC proxies is their inconsistency, as reflected by the substantial variation in their performance across different problems. Furthermore, the design of existing ZC metrics is manual, involving a time-consuming trial-and-error process that requires substantial domain expertise. These challenges raise two critical questions: (1) Can we automate the design of ZC metrics? and (2) Can we utilize the existing hand-crafted ZC metrics to synthesize a more generalizable one? In this study, we propose a framework based on Symbolic Regression via Genetic Programming to automate the design of ZC metrics. Our framework is not only highly extensible but also capable of quickly producing a ZC metric with a strong positive rank correlation to true network performance across diverse NAS search spaces and tasks. Extensive experiments on 13 problems from NAS-Bench-Suite-Zero demonstrate that our automatically generated proxies consistently outperform hand-crafted alternatives. Using our evolved proxy metric as the search objective in an evolutionary algorithm, we could identify network architectures with competitive performance within 15 minutes using a single consumer GPU.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation</title>
<link>https://arxiv.org/abs/2505.15834</link>
<guid>https://arxiv.org/abs/2505.15834</guid>
<content:encoded><![CDATA[
arXiv:2505.15834v1 Announce Type: cross 
Abstract: Fake news spreads widely on social media, leading to numerous negative effects. Most existing detection algorithms focus on analyzing news content and social context to detect fake news. However, these approaches typically detect fake news based on specific platforms, ignoring differences in propagation characteristics across platforms. In this paper, we introduce the MPPFND dataset, which captures propagation structures across multiple platforms. We also describe the commenting and propagation characteristics of different platforms to show that their social contexts have distinct features. We propose a multi-platform fake news detection model (APSL) that uses graph neural networks to extract social context features from various platforms. Experiments show that accounting for cross-platform propagation differences improves fake news detection performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based Indoor Localization</title>
<link>https://arxiv.org/abs/2505.15835</link>
<guid>https://arxiv.org/abs/2505.15835</guid>
<content:encoded><![CDATA[
arXiv:2505.15835v1 Announce Type: cross 
Abstract: Wireless Fidelity (WiFi) based indoor positioning is a widely researched area for determining the position of devices within a wireless network. Accurate indoor location has numerous applications, such as asset tracking and indoor navigation. Despite advances in WiFi localization techniques -- in particular approaches that leverage WiFi telemetry -- their adoption in practice remains limited due to several factors including environmental changes that cause signal fading, multipath effects, interference, which, in turn, impact positioning accuracy. In addition, telemetry data differs depending on the WiFi device vendor, offering distinct features and formats; use case requirements can also vary widely. Currently, there is no unified model to handle all these variations effectively. In this paper, we present WiFiGPT, a Generative Pretrained Transformer (GPT) based system that is able to handle these variations while achieving high localization accuracy. Our experiments with WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can effectively capture subtle spatial patterns in noisy wireless telemetry, making them reliable regressors. Compared to existing state-of-the-art methods, our method matches and often surpasses conventional approaches for multiple types of telemetry. Achieving sub-meter accuracy for RSSI and FTM and centimeter-level precision for CSI demonstrates the potential of LLM-based localisation to outperform specialized techniques, all without handcrafted signal processing or calibration.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning</title>
<link>https://arxiv.org/abs/2505.15836</link>
<guid>https://arxiv.org/abs/2505.15836</guid>
<content:encoded><![CDATA[
arXiv:2505.15836v1 Announce Type: cross 
Abstract: As artificial intelligence continues to drive innovation in complex, decentralized environments, the need for scalable, adaptive, and privacy-preserving decision-making systems has become critical. This paper introduces a novel framework combining quantum-inspired neural networks with evolutionary algorithms to optimize real-time decision-making in multi-agent systems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN) leverages quantum computing principles -- such as quantum superposition and entanglement -- to enhance learning speed and decision accuracy, while integrating evolutionary optimization to continually refine agent behaviors in dynamic, uncertain environments. By utilizing federated learning, QE-NN ensures privacy preservation, enabling decentralized agents to collaborate without sharing sensitive data. The framework is designed to allow agents to adapt in real-time to their environments, optimizing decision-making processes for applications in areas such as autonomous systems, smart cities, and healthcare. This research represents a breakthrough in merging quantum computing, evolutionary optimization, and privacy-preserving techniques to solve complex problems in multi-agent decision-making systems, pushing the boundaries of AI in real-world, privacy-sensitive applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TDFormer: A Top-Down Attention-Controlled Spiking Transformer</title>
<link>https://arxiv.org/abs/2505.15840</link>
<guid>https://arxiv.org/abs/2505.15840</guid>
<content:encoded><![CDATA[
arXiv:2505.15840v1 Announce Type: cross 
Abstract: Traditional spiking neural networks (SNNs) can be viewed as a combination of multiple subnetworks with each running for one time step, where the parameters are shared, and the membrane potential serves as the only information link between them. However, the implicit nature of the membrane potential limits its ability to effectively represent temporal information. As a result, each time step cannot fully leverage information from previous time steps, seriously limiting the model's performance. Inspired by the top-down mechanism in the brain, we introduce TDFormer, a novel model with a top-down feedback structure that functions hierarchically and leverages high-order representations from earlier time steps to modulate the processing of low-order information at later stages. The feedback structure plays a role from two perspectives: 1) During forward propagation, our model increases the mutual information across time steps, indicating that richer temporal information is being transmitted and integrated in different time steps. 2) During backward propagation, we theoretically prove that the feedback structure alleviates the problem of vanishing gradients along the time dimension. We find that these mechanisms together significantly and consistently improve the model performance on multiple datasets. In particular, our model achieves state-of-the-art performance on ImageNet with an accuracy of 86.83%.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Lives? A meta-analysis of diverse opinions on the definition of life</title>
<link>https://arxiv.org/abs/2505.15849</link>
<guid>https://arxiv.org/abs/2505.15849</guid>
<content:encoded><![CDATA[
arXiv:2505.15849v1 Announce Type: cross 
Abstract: The question of "what is life?" has challenged scientists and philosophers for centuries, producing an array of definitions that reflect both the mystery of its emergence and the diversity of disciplinary perspectives brought to bear on the question. Despite significant progress in our understanding of biological systems, psychology, computation, and information theory, no single definition for life has yet achieved universal acceptance. This challenge becomes increasingly urgent as advances in synthetic biology, artificial intelligence, and astrobiology challenge our traditional conceptions of what it means to be alive. We undertook a methodological approach that leverages large language models (LLMs) to analyze a set of definitions of life provided by a curated set of cross-disciplinary experts. We used a novel pairwise correlation analysis to map the definitions into distinct feature vectors, followed by agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection to reveal underlying conceptual archetypes. This methodology revealed a continuous landscape of the themes relating to the definition of life, suggesting that what has historically been approached as a binary taxonomic problem should be instead conceived as differentiated perspectives within a unified conceptual latent space. We offer a new methodological bridge between reductionist and holistic approaches to fundamental questions in science and philosophy, demonstrating how computational semantic analysis can reveal conceptual patterns across disciplinary boundaries, and opening similar pathways for addressing other contested definitional territories across the sciences.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies</title>
<link>https://arxiv.org/abs/2505.15851</link>
<guid>https://arxiv.org/abs/2505.15851</guid>
<content:encoded><![CDATA[
arXiv:2505.15851v1 Announce Type: cross 
Abstract: This paper elaborates on the concept of moral exercises as a means to help AI actors cultivate virtues that enable effective human oversight of AI systems. We explore the conceptual framework and significance of moral exercises, situating them within the contexts of philosophical discourse, ancient practices, and contemporary AI ethics scholarship. We outline the core pillars of the moral exercises methodology - eliciting an engaged personal disposition, fostering relational understanding, and cultivating technomoral wisdom - and emphasize their relevance to key activities and competencies essential for human oversight of AI systems. Our argument is supported by findings from three pilot studies involving a company, a multidisciplinary team of AI researchers, and higher education students. These studies allow us to explore both the potential and the limitations of moral exercises. Based on the collected data, we offer insights into how moral exercises can foster a responsible AI culture within organizations, and suggest directions for future research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration of TinyML and LargeML: A Survey of 6G and Beyond</title>
<link>https://arxiv.org/abs/2505.15854</link>
<guid>https://arxiv.org/abs/2505.15854</guid>
<content:encoded><![CDATA[
arXiv:2505.15854v1 Announce Type: cross 
Abstract: The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management.
  Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management</title>
<link>https://arxiv.org/abs/2505.15856</link>
<guid>https://arxiv.org/abs/2505.15856</guid>
<content:encoded><![CDATA[
arXiv:2505.15856v1 Announce Type: cross 
Abstract: Effective disaster management requires timely access to accurate and contextually relevant information. Existing Information Retrieval (IR) benchmarks, however, focus primarily on general or specialized domains, such as medicine or finance, neglecting the unique linguistic complexity and diverse information needs encountered in disaster management scenarios. To bridge this gap, we introduce DisastIR, the first comprehensive IR evaluation benchmark specifically tailored for disaster management. DisastIR comprises 9,600 diverse user queries and more than 1.3 million labeled query-passage pairs, covering 48 distinct retrieval tasks derived from six search intents and eight general disaster categories that include 301 specific event types. Our evaluations of 30 state-of-the-art retrieval models demonstrate significant performance variances across tasks, with no single model excelling universally. Furthermore, comparative analyses reveal significant performance gaps between general-domain and disaster management-specific tasks, highlighting the necessity of disaster management-specific benchmarks for guiding IR model selection to support effective decision-making in disaster management scenarios. All source codes and DisastIR are available at https://github.com/KaiYin97/Disaster_IR.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoData: A Multi-Agent System for Open Web Data Collection</title>
<link>https://arxiv.org/abs/2505.15859</link>
<guid>https://arxiv.org/abs/2505.15859</guid>
<content:encoded><![CDATA[
arXiv:2505.15859v1 Announce Type: cross 
Abstract: The exponential growth of data-driven systems and AI technologies has intensified the demand for high-quality web-sourced datasets. While existing datasets have proven valuable, conventional web data collection approaches face significant limitations in terms of human effort and scalability. Current data-collecting solutions fall into two categories: wrapper-based methods that struggle with adaptability and reproducibility, and large language model (LLM)-based approaches that incur substantial computational and financial costs. To address these challenges, we propose AutoData, a novel multi-agent system for Automated web Data collection, that requires minimal human intervention, i.e., only necessitating a natural language instruction specifying the desired dataset. In addition, AutoData is designed with a robust multi-agent architecture, featuring a novel oriented message hypergraph coordinated by a central task manager, to efficiently organize agents across research and development squads. Besides, we introduce a novel hypergraph cache system to advance the multi-agent collaboration process that enables efficient automated data collection and mitigates the token cost issues prevalent in existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark dataset supporting live data collection from web sources across three domains: academic, finance, and sports. Comprehensive evaluations over Instruct2DS and three existing benchmark datasets demonstrate AutoData's superior performance compared to baseline methods. Case studies on challenging tasks such as picture book collection and paper extraction from surveys further validate its applicability. Our source code and dataset are available at https://github.com/GraphResearcher/AutoData.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology</title>
<link>https://arxiv.org/abs/2505.15868</link>
<guid>https://arxiv.org/abs/2505.15868</guid>
<content:encoded><![CDATA[
arXiv:2505.15868v1 Announce Type: cross 
Abstract: Chromosome analysis is vital for diagnosing genetic disorders and guiding cancer therapy decisions through the identification of somatic clonal aberrations. However, developing an AI model are hindered by the overwhelming complexity and diversity of chromosomal abnormalities, requiring extensive annotation efforts, while automated methods remain task-specific and lack generalizability due to the scarcity of comprehensive datasets spanning diverse resource conditions. Here, we introduce CHROMA, a foundation model for cytogenomics, designed to overcome these challenges by learning generalizable representations of chromosomal abnormalities. Pre-trained on over 84,000 specimens (~4 million chromosomal images) via self-supervised learning, CHROMA outperforms other methods across all types of abnormalities, even when trained on fewer labelled data and more imbalanced datasets. By facilitating comprehensive mapping of instability and clonal leisons across various aberration types, CHROMA offers a scalable and generalizable solution for reliable and automated clinical analysis, reducing the annotation workload for experts and advancing precision oncology through the early detection of rare genomic abnormalities, enabling broad clinical AI applications and making advanced genomic analysis more accessible.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRIT: Teaching MLLMs to Think with Images</title>
<link>https://arxiv.org/abs/2505.15879</link>
<guid>https://arxiv.org/abs/2505.15879</guid>
<content:encoded><![CDATA[
arXiv:2505.15879v1 Announce Type: cross 
Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Last Layer Empirical Bayes</title>
<link>https://arxiv.org/abs/2505.15888</link>
<guid>https://arxiv.org/abs/2505.15888</guid>
<content:encoded><![CDATA[
arXiv:2505.15888v1 Announce Type: cross 
Abstract: The task of quantifying the inherent uncertainty associated with neural network predictions is a key challenge in artificial intelligence. Bayesian neural networks (BNNs) and deep ensembles are among the most prominent approaches to tackle this task. Both approaches produce predictions by computing an expectation of neural network outputs over some distribution on the corresponding weights; this distribution is given by the posterior in the case of BNNs, and by a mixture of point masses for ensembles. Inspired by recent work showing that the distribution used by ensembles can be understood as a posterior corresponding to a learned data-dependent prior, we propose last layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a normalizing flow, which is then trained to maximize the evidence lower bound; to retain tractability we use the flow only on the last layer. We show why LLEB is well motivated, and how it interpolates between standard BNNs and ensembles in terms of the strength of the prior that they use. LLEB performs on par with existing approaches, highlighting that empirical Bayes is a promising direction for future research in uncertainty quantification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law</title>
<link>https://arxiv.org/abs/2505.15916</link>
<guid>https://arxiv.org/abs/2505.15916</guid>
<content:encoded><![CDATA[
arXiv:2505.15916v1 Announce Type: cross 
Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question answering with references in the context of Brazilian personal income tax law. The dataset contains 715 questions from the 2024 official Q\&amp;A document published by Brazil's Internal Revenue Service, enriched with statutory norms and administrative rulings from the Conselho Administrativo de Recursos Fiscais (CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings for searching and GPT-4o-mini for answer generation. We compare different text segmentation strategies and benchmark our system against commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics. Results show that our custom RAG pipeline outperforms commercial systems in Response Relevancy, indicating stronger alignment with user queries, while commercial models achieve higher scores in Factual Correctness and fluency. These findings highlight a trade-off between legally grounded generation and linguistic fluency. Crucially, we argue that human expert evaluation remains essential to ensure the legal validity of AI-generated answers in high-stakes domains such as taxation. BR-TaxQA-R is publicly available at https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization</title>
<link>https://arxiv.org/abs/2505.15918</link>
<guid>https://arxiv.org/abs/2505.15918</guid>
<content:encoded><![CDATA[
arXiv:2505.15918v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge bases; however, their capability to generate probabilistic knowledge about real-world events remains understudied. This paper investigates using probabilistic knowledge inherent in LLMs to derive probability estimates for statements concerning events and their interrelationships captured via a Bayesian Network (BN). Using LLMs in this context allows for the parameterization of BNs, enabling probabilistic modeling within specific domains. Experiments on eighty publicly available Bayesian Networks, from healthcare to finance, demonstrate that querying LLMs about the conditional probabilities of events provides meaningful results when compared to baselines, including random and uniform distributions, as well as approaches based on next-token generation probabilities. We explore how these LLM-derived distributions can serve as expert priors to refine distributions extracted from minimal data, significantly reducing systematic biases. Overall, this work introduces a promising strategy for automatically constructing Bayesian Networks by combining probabilistic knowledge extracted from LLMs with small amounts of real-world data. Additionally, we evaluate several prompting strategies for eliciting probabilistic knowledge from LLMs and establish the first comprehensive baseline for assessing LLM performance in extracting probabilistic knowledge.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VERDI: VLM-Embedded Reasoning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15925</link>
<guid>https://arxiv.org/abs/2505.15925</guid>
<content:encoded><![CDATA[
arXiv:2505.15925v1 Announce Type: cross 
Abstract: While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, \textsc{VERDI} enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We demonstrate the effectiveness of our method on the NuScenes dataset and find that VERDI outperforms existing e2e methods that do not embed reasoning by 10% in $\ell_{2}$ distance, while maintaining high inference speed.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding</title>
<link>https://arxiv.org/abs/2505.15946</link>
<guid>https://arxiv.org/abs/2505.15946</guid>
<content:encoded><![CDATA[
arXiv:2505.15946v1 Announce Type: cross 
Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance</title>
<link>https://arxiv.org/abs/2505.15952</link>
<guid>https://arxiv.org/abs/2505.15952</guid>
<content:encoded><![CDATA[
arXiv:2505.15952v1 Announce Type: cross 
Abstract: With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15957</link>
<guid>https://arxiv.org/abs/2505.15957</guid>
<content:encoded><![CDATA[
arXiv:2505.15957v1 Announce Type: cross 
Abstract: With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training Large Memory Language Models with Internal and External Knowledge</title>
<link>https://arxiv.org/abs/2505.15962</link>
<guid>https://arxiv.org/abs/2505.15962</guid>
<content:encoded><![CDATA[
arXiv:2505.15962v1 Announce Type: cross 
Abstract: Neural language models are black-boxes -- both linguistic patterns and factual knowledge are distributed across billions of opaque parameters. This entangled encoding makes it difficult to reliably inspect, verify, or update specific facts. We propose a new class of language models, Large Memory Language Models (LMLM) with a pre-training recipe that stores factual knowledge in both internal weights and an external database. Our approach strategically masks externally retrieved factual values from the training loss, thereby teaching the model to perform targeted lookups rather than relying on memorization in model weights. Our experiments demonstrate that LMLMs achieve competitive performance compared to significantly larger, knowledge-dense LLMs on standard benchmarks, while offering the advantages of explicit, editable, and verifiable knowledge bases. This work represents a fundamental shift in how language models interact with and manage factual knowledge.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.15966</link>
<guid>https://arxiv.org/abs/2505.15966</guid>
<content:encoded><![CDATA[
arXiv:2505.15966v1 Announce Type: cross 
Abstract: Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers</title>
<link>https://arxiv.org/abs/2505.15997</link>
<guid>https://arxiv.org/abs/2505.15997</guid>
<content:encoded><![CDATA[
arXiv:2505.15997v1 Announce Type: cross 
Abstract: Exploring the trustworthiness of deep learning models is crucial, especially in critical domains such as medical imaging decision support systems. Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. However, conformal prediction results face challenges due to the backbone model's struggles in domain-shifted scenarios, such as variations in different sources. To aim this challenge, this paper proposes a novel framework termed Conformal Ensemble of Vision Transformers (CE-ViTs) designed to enhance image classification performance by prioritizing domain adaptation and model robustness, while accounting for uncertainty. The proposed method leverages an ensemble of vision transformer models in the backbone, trained on diverse datasets including HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning approach, calibrated through the combined mentioned datasets, aims to enhance domain adaptation through conformal learning. Experimental results underscore that the framework achieves a high coverage rate of 90.38\%, representing an improvement of 9.95\% compared to the HAM10000 model. This indicates a strong likelihood that the prediction set includes the true label compared to singular models. Ensemble learning in CE-ViTs significantly improves conformal prediction performance, increasing the average prediction set size for challenging misclassified samples from 1.86 to 3.075.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model</title>
<link>https://arxiv.org/abs/2505.16000</link>
<guid>https://arxiv.org/abs/2505.16000</guid>
<content:encoded><![CDATA[
arXiv:2505.16000v1 Announce Type: cross 
Abstract: The rapid advancement of language models has demonstrated the potential of artificial intelligence in the healthcare industry. However, small language models struggle with specialized domains in low-resource languages like Persian. While numerous medical-domain websites exist in Persian, no curated dataset or corpus has been available making ours the first of its kind. This study explores the enhancement of medical knowledge in a small language model by leveraging accessible online data, including a crawled corpus from medical magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a baseline model using our curated data to improve its medical knowledge. Benchmark evaluations demonstrate that the fine-tuned model achieves improved accuracy in medical question answering and provides better responses compared to its baseline. This work highlights the potential of leveraging open-access online data to enrich small language models in medical fields, providing a novel solution for Persian medical AI applications suitable for resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions</title>
<link>https://arxiv.org/abs/2505.16002</link>
<guid>https://arxiv.org/abs/2505.16002</guid>
<content:encoded><![CDATA[
arXiv:2505.16002v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for linguists seeking to develop theories of syntax. In this paper, we argue that causal interpretability methods, applied to LLMs, can greatly enhance the value of such evidence by helping us characterize the abstract mechanisms that LLMs learn to use. Our empirical focus is a set of English filler-gap dependency constructions (e.g., questions, relative clauses). Linguistic theories largely agree that these constructions share many properties. Using experiments based in Distributed Interchange Interventions, we show that LLMs converge on similar abstract analyses of these constructions. These analyses also reveal previously overlooked factors -- relating to frequency, filler type, and surrounding context -- that could motivate changes to standard linguistic theory. Overall, these results suggest that mechanistic, internal analyses of LLMs can push linguistic theory forward.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2505.16003</link>
<guid>https://arxiv.org/abs/2505.16003</guid>
<content:encoded><![CDATA[
arXiv:2505.16003v1 Announce Type: cross 
Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for evaluating language models. Although several calibration techniques have been proposed to better align these evaluators with human judgment, prior studies focus primarily on narrow, well-structured benchmarks. As a result, it remains unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these settings, exhibiting weak or even negative correlation with human judgments. To address this, we propose SLMEval, a novel and efficient calibration method based on entropy maximization over a small amount of human preference data. By estimating a latent distribution over model quality and reweighting evaluator scores accordingly, SLMEval achieves strong correlation with human evaluations across two real-world production use cases and the public benchmark. For example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with human judgments, while G-Eval yields a negative correlation. In addition, SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated evaluators such as G-eval.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations</title>
<link>https://arxiv.org/abs/2505.16004</link>
<guid>https://arxiv.org/abs/2505.16004</guid>
<content:encoded><![CDATA[
arXiv:2505.16004v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal activations of large language models (LLMs) by mapping them to human-interpretable concept representations. While existing evaluations of SAEs focus on metrics such as the reconstruction-sparsity tradeoff, human (auto-)interpretability, and feature disentanglement, they overlook a critical aspect: the robustness of concept representations to input perturbations. We argue that robustness must be a fundamental consideration for concept representations, reflecting the fidelity of concept labeling. To this end, we formulate robustness quantification as input-space optimization problems and develop a comprehensive evaluation framework featuring realistic scenarios in which adversarial perturbations are crafted to manipulate SAE representations. Empirically, we find that tiny adversarial input perturbations can effectively manipulate concept-based interpretations in most scenarios without notably affecting the outputs of the base LLMs themselves. Overall, our results suggest that SAE concept representations are fragile and may be ill-suited for applications in model monitoring and oversight.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization</title>
<link>https://arxiv.org/abs/2505.16008</link>
<guid>https://arxiv.org/abs/2505.16008</guid>
<content:encoded><![CDATA[
arXiv:2505.16008v1 Announce Type: cross 
Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel approach for few-shot cross-lingual embedding inversion attacks, addressing critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work in embedding inversion attacks that treat languages independently, LAGO explicitly models linguistic relationships through a graph-based constrained distributed optimization framework. By integrating syntactic and lexical similarity as edge constraints, our method enables collaborative parameter learning across related languages. Theoretically, we show this formulation generalizes prior approaches, such as ALGEN, which emerges as a special case when similarity constraints are relaxed. Our framework uniquely combines Frobenius-norm regularization with linear inequality or total variation constraints, ensuring robust alignment of cross-lingual embedding spaces even with extremely limited data (as few as 10 samples per language). Extensive experiments across multiple languages and embedding models demonstrate that LAGO substantially improves the transferability of attacks with 10-20% increase in Rouge-L score over baselines. This work establishes language similarity as a critical factor in inversion attack transferability, urging renewed focus on language-aware privacy-preserving multilingual embeddings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16022</link>
<guid>https://arxiv.org/abs/2505.16022</guid>
<content:encoded><![CDATA[
arXiv:2505.16022v1 Announce Type: cross 
Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging</title>
<link>https://arxiv.org/abs/2505.16024</link>
<guid>https://arxiv.org/abs/2505.16024</guid>
<content:encoded><![CDATA[
arXiv:2505.16024v1 Announce Type: cross 
Abstract: Diffusion trajectory distillation methods aim to accelerate sampling in diffusion models, which produce high-quality outputs but suffer from slow sampling speeds. These methods train a student model to approximate the multi-step denoising process of a pretrained teacher model in a single step, enabling one-shot generation. However, theoretical insights into the trade-off between different distillation strategies and generative quality remain limited, complicating their optimization and selection. In this work, we take a first step toward addressing this gap. Specifically, we reinterpret trajectory distillation as an operator merging problem in the linear regime, where each step of the teacher model is represented as a linear operator acting on noisy data. These operators admit a clear geometric interpretation as projections and rescalings corresponding to the noise schedule. During merging, signal shrinkage occurs as a convex combination of operators, arising from both discretization and limited optimization time of the student model. We propose a dynamic programming algorithm to compute the optimal merging strategy that maximally preserves signal fidelity. Additionally, we demonstrate the existence of a sharp phase transition in the optimal strategy, governed by data covariance structures. Our findings enhance the theoretical understanding of diffusion trajectory distillation and offer practical insights for improving distillation strategies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets</title>
<link>https://arxiv.org/abs/2505.16027</link>
<guid>https://arxiv.org/abs/2505.16027</guid>
<content:encoded><![CDATA[
arXiv:2505.16027v1 Announce Type: cross 
Abstract: Foundation models leveraging vision-language pretraining have shown promise in chest X-ray (CXR) interpretation, yet their real-world performance across diverse populations and diagnostic tasks remains insufficiently evaluated. This study benchmarks the diagnostic performance and generalizability of foundation models versus traditional convolutional neural networks (CNNs) on multinational CXR datasets. We evaluated eight CXR diagnostic models - five vision-language foundation models and three CNN-based architectures - across 37 standardized classification tasks using six public datasets from the USA, Spain, India, and Vietnam, and three private datasets from hospitals in China. Performance was assessed using AUROC, AUPRC, and other metrics across both shared and dataset-specific tasks. Foundation models outperformed CNNs in both accuracy and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and structured supervision, achieved the highest performance on public (mean AUROC: 0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets, ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed reduced performance on pediatric cases, with average AUROC dropping from 0.88 +/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings highlight the value of structured supervision and prompt design in radiologic AI and suggest future directions including geographic expansion and ensemble modeling for clinical deployment. Code for all evaluated models is available at https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces</title>
<link>https://arxiv.org/abs/2505.16035</link>
<guid>https://arxiv.org/abs/2505.16035</guid>
<content:encoded><![CDATA[
arXiv:2505.16035v1 Announce Type: cross 
Abstract: We introduce Equivariant Neural Eikonal Solvers, a novel framework that integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our approach employs a single neural field where a unified shared backbone is conditioned on signal-specific latent variables - represented as point clouds in a Lie group - to model diverse Eikonal solutions. The ENF integration ensures equivariant mapping from these latent representations to the solution field, delivering three key benefits: enhanced representation efficiency through weight-sharing, robust geometric grounding, and solution steerability. This steerability allows transformations applied to the latent point cloud to induce predictable, geometrically meaningful modifications in the resulting Eikonal solution. By coupling these steerable representations with Physics-Informed Neural Networks (PINNs), our framework accurately models Eikonal travel-time solutions while generalizing to arbitrary Riemannian manifolds with regular group actions. This includes homogeneous spaces such as Euclidean, position-orientation, spherical, and hyperbolic manifolds. We validate our approach through applications in seismic travel-time modeling of 2D and 3D benchmark datasets. Experimental results demonstrate superior performance, scalability, adaptability, and user controllability compared to existing Neural Operator-based Eikonal solver methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
arXiv:2505.16056v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals of Provenance: Practices &amp; Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals</title>
<link>https://arxiv.org/abs/2505.16057</link>
<guid>https://arxiv.org/abs/2505.16057</guid>
<content:encoded><![CDATA[
arXiv:2505.16057v1 Announce Type: cross 
Abstract: AI-Generated (AIG) content has become increasingly widespread by recent advances in generative models and the easy-to-use tools that have significantly lowered the technical barriers for producing highly realistic audio, images, and videos through simple natural language prompts. In response, platforms are adopting provable provenance with platforms recommending AIG to be self-disclosed and signaled to users. However, these indicators may be often missed, especially when they rely solely on visual cues and make them ineffective to users with different sensory abilities. To address the gap, we conducted semi-structured interviews (N=28) with 15 sighted and 13 BLV participants to examine their interaction with AIG content through self-disclosed AI indicators. Our findings reveal diverse mental models and practices, highlighting different strengths and weaknesses of content-based (e.g., title, description) and menu-aided (e.g., AI labels) indicators. While sighted participants leveraged visual and audio cues, BLV participants primarily relied on audio and existing assistive tools, limiting their ability to identify AIG. Across both groups, they frequently overlooked menu-aided indicators deployed by platforms and rather interacted with content-based indicators such as title and comments. We uncovered usability challenges stemming from inconsistent indicator placement, unclear metadata, and cognitive overload. These issues were especially critical for BLV individuals due to the insufficient accessibility of interface elements. We provide practical recommendations and design implications for future AIG indicators across several dimensions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-free sparse identification of nonlinear dynamics</title>
<link>https://arxiv.org/abs/2505.16058</link>
<guid>https://arxiv.org/abs/2505.16058</guid>
<content:encoded><![CDATA[
arXiv:2505.16058v1 Announce Type: cross 
Abstract: Identifying the governing equations of a dynamical system is one of the most important tasks for scientific modeling. However, this procedure often requires high-quality spatio-temporal data uniformly sampled on structured grids. In this paper, we propose mesh-free SINDy, a novel algorithm which leverages the power of neural network approximation as well as auto-differentiation to identify governing equations from arbitrary sensor placements and non-uniform temporal data sampling. We show that mesh-free SINDy is robust to high noise levels and limited data while remaining computationally efficient. In our implementation, the training procedure is straight-forward and nearly free of hyperparameter tuning, making mesh-free SINDy widely applicable to many scientific and engineering problems. In the experiments, we demonstrate its effectiveness on a series of PDEs including the Burgers' equation, the heat equation, the Korteweg-De Vries equation and the 2D advection-diffusion equation. We conduct detailed numerical experiments on all datasets, varying the noise levels and number of samples, and we also compare our approach to previous state-of-the-art methods. It is noteworthy that, even in high-noise and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery, achieving successful identification with up to 75% noise for the Burgers' equation using 5,000 samples and with as few as 100 samples and 1% noise. All of this is achieved within a training time of under one minute.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge to Mix: Mixing Datasets via Model Merging</title>
<link>https://arxiv.org/abs/2505.16066</link>
<guid>https://arxiv.org/abs/2505.16066</guid>
<content:encoded><![CDATA[
arXiv:2505.16066v1 Announce Type: cross 
Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for maximizing performance on downstream tasks. However, composing effective dataset mixtures typically relies on heuristics and trial-and-error, often requiring multiple fine-tuning runs to achieve the desired outcome. We propose a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset mixtures through model merging. Model merging is a recent technique that combines the abilities of multiple individually fine-tuned LMs into a single LM by using a few simple arithmetic operations. Our key insight is that merging models individually fine-tuned on each dataset in a mixture can effectively serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix leverages this insight to accelerate selecting dataset mixtures without requiring full fine-tuning on each candidate mixture. Our experiments demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset selection for fine-tuning LMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Variational Autoencoders</title>
<link>https://arxiv.org/abs/2505.16074</link>
<guid>https://arxiv.org/abs/2505.16074</guid>
<content:encoded><![CDATA[
arXiv:2505.16074v1 Announce Type: cross 
Abstract: We present the new bidirectional variational autoencoder (BVAE) network architecture. The BVAE uses a single neural network both to encode and decode instead of an encoder-decoder network pair. The network encodes in the forward direction and decodes in the backward direction through the same synaptic web. Simulations compared BVAEs and ordinary VAEs on the four image tasks of image reconstruction, classification, interpolation, and generation. The image datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter count by almost 50% and still slightly outperformed the unidirectional VAEs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning</title>
<link>https://arxiv.org/abs/2505.16088</link>
<guid>https://arxiv.org/abs/2505.16088</guid>
<content:encoded><![CDATA[
arXiv:2505.16088v1 Announce Type: cross 
Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future regimes; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year $\rightarrow$ month $\rightarrow$ day).
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI</title>
<link>https://arxiv.org/abs/2505.16103</link>
<guid>https://arxiv.org/abs/2505.16103</guid>
<content:encoded><![CDATA[
arXiv:2505.16103v1 Announce Type: cross 
Abstract: Keylogger detection involves monitoring for unusual system behaviors such as delays between typing and character display, analyzing network traffic patterns for data exfiltration. In this study, we provide a comprehensive analysis for keylogger detection with traditional machine learning models - SVC, Random Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes and advanced ensemble methods including Stacking, Blending and Voting. Moreover, feature selection approaches such as Information gain, Lasso L1 and Fisher Score are thoroughly assessed to improve predictive performance and lower computational complexity. The Keylogger Detection dataset from publicly available Kaggle website is used in this project. In addition to accuracy-based classification, this study implements the approach for model interpretation using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to deliver finer explanations for how much each feature contributes in assisting or hindering the detection process. To evaluate the models result, we have used AUC score, sensitivity, Specificity, Accuracy and F1 score. The best performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99, 100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is near-perfect classification with Fisher Score.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Generative Modeling via Substructure Sequences</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[
arXiv:2505.16130v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) has been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance, limiting the viability of GNNs as backbones for graph foundation models. In this work, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable, transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node classification, graph classification, and transfer learning -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study</title>
<link>https://arxiv.org/abs/2505.16136</link>
<guid>https://arxiv.org/abs/2505.16136</guid>
<content:encoded><![CDATA[
arXiv:2505.16136v1 Announce Type: cross 
Abstract: This study introduces an interpretable machine learning (ML) framework to extract macroeconomic alpha from global news sentiment. We process the Global Database of Events, Language, and Tone (GDELT) Project's worldwide news feed using FinBERT -- a Bidirectional Encoder Representations from Transformers (BERT) based model pretrained on finance-specific language -- to construct daily sentiment indices incorporating mean tone, dispersion, and event impact. These indices drive an XGBoost classifier, benchmarked against logistic regression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S. Treasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold expanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates exceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios achieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective compound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and 22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment dispersion and article impact are key predictive features. Our findings establish that integrating domain-specific Natural Language Processing (NLP) with interpretable ML offers a potent and explainable source of macro alpha.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2505.16146</link>
<guid>https://arxiv.org/abs/2505.16146</guid>
<content:encoded><![CDATA[
arXiv:2505.16146v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on multimodal tasks such as visual question answering (VQA) and image captioning. However, they still suffer from hallucinations, generating text inconsistent with visual input, posing significant risks in real-world applications. Existing approaches to address this issue focus on incorporating external knowledge bases, alignment training, or decoding strategies, all of which require substantial computational cost and time. Recent works try to explore more efficient alternatives by adjusting LVLMs' internal representations. Although promising, these methods may cause hallucinations to be insufficiently suppressed or lead to excessive interventions that negatively affect normal semantics. In this work, we leverage sparse autoencoders (SAEs) to identify semantic directions closely associated with either hallucinations or actuality, realizing more precise and direct hallucination-related representations. Our analysis demonstrates that interventions along the faithful direction we identified can mitigate hallucinations, while those along the hallucinatory direction can exacerbate them. Building on these insights, we propose Steering LVLMs via SAE Latent Directions (SSL), a training-free method based on SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive experiments demonstrate that SSL significantly outperforms existing decoding approaches in mitigating hallucinations, while maintaining transferability across different model architectures with negligible additional time overhead.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification</title>
<link>https://arxiv.org/abs/2505.16149</link>
<guid>https://arxiv.org/abs/2505.16149</guid>
<content:encoded><![CDATA[
arXiv:2505.16149v1 Announce Type: cross 
Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet serve as critical tools for model evaluation. However, despite the cleaning efforts, these datasets still suffer from pervasive noisy labels and often contain missing labels due to the co-existing image pattern where multiple classes appear in an image sample. This results in misleading model comparisons and unfair evaluations. Existing label cleaning methods focus primarily on noisy labels, but the issue of missing labels remains largely overlooked. Motivated by these challenges, we present a comprehensive framework named REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g., LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods (e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and missing label detection in widely-used image classification test sets. REVEAL detects potential noisy labels and omissions, aggregates predictions from various methods, and refines label accuracy through confidence-informed predictions and consensus-based filtering. Additionally, we provide a thorough analysis of state-of-the-art vision-language models and pre-trained image classifiers, highlighting their strengths and limitations within the context of dataset renovation by revealing 10 observations. Our method effectively reveals missing labels from public datasets and provides soft-labeled results with likelihoods. Through human verifications, REVEAL significantly improves the quality of 6 benchmark test sets, highly aligning to human judgments and enabling more accurate and meaningful comparisons in image classification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss</title>
<link>https://arxiv.org/abs/2505.16172</link>
<guid>https://arxiv.org/abs/2505.16172</guid>
<content:encoded><![CDATA[
arXiv:2505.16172v1 Announce Type: cross 
Abstract: Understanding health information is essential in achieving and maintaining a healthy life. We focus on simplifying health information for better understanding. With the availability of generative AI, the simplification process has become efficient and of reasonable quality, however, the algorithms remove information that may be crucial for comprehension. In this study, we compare generative AI to detect missing information in simplified text, evaluate its importance, and fix the text with the missing information. We collected 50 health information texts and simplified them using gpt-4-0613. We compare five approaches to identify missing elements and regenerate the text by inserting the missing elements. These five approaches involve adding missing entities and missing words in various ways: 1) adding all the missing entities, 2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613, and 4, 5) serving as controls for comparison, adding randomly chosen entities. We use cosine similarity and ROUGE scores to evaluate the semantic similarity and content overlap between the original, simplified, and reconstructed simplified text. We do this for both summaries and full text. Overall, we find that adding missing entities improves the text. Adding all the missing entities resulted in better text regeneration, which was better than adding the top-ranked entities or words, or random words. Current tools can identify these entities, but are not valuable in ranking them.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design</title>
<link>https://arxiv.org/abs/2505.16175</link>
<guid>https://arxiv.org/abs/2505.16175</guid>
<content:encoded><![CDATA[
arXiv:2505.16175v1 Announce Type: cross 
Abstract: Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Generative AI Capabilities in Everyday Image Editing Tasks</title>
<link>https://arxiv.org/abs/2505.16181</link>
<guid>https://arxiv.org/abs/2505.16181</guid>
<content:encoded><![CDATA[
arXiv:2505.16181v1 Announce Type: cross 
Abstract: Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyInsert: A Data-Efficient and Generalizable Insertion Policy</title>
<link>https://arxiv.org/abs/2505.16187</link>
<guid>https://arxiv.org/abs/2505.16187</guid>
<content:encoded><![CDATA[
arXiv:2505.16187v1 Announce Type: cross 
Abstract: Insertion task is highly challenging that requires robots to operate with exceptional precision in cluttered environments. Existing methods often have poor generalization capabilities. They typically function in restricted and structured environments, and frequently fail when the plug and socket are far apart, when the scene is densely cluttered, or when handling novel objects. They also rely on strong assumptions such as access to CAD models or a digital twin in simulation. To address this, we propose EasyInsert, a framework which leverages the human intuition that relative pose (delta pose) between plug and socket is sufficient for successful insertion, and employs efficient and automated real-world data collection with minimal human labor to train a generalizable model for relative pose prediction. During execution, EasyInsert follows a coarse-to-fine execution procedure based on predicted delta pose, and successfully performs various insertion tasks. EasyInsert demonstrates strong zero-shot generalization capability for unseen objects in cluttered environments, handling cases with significant initial pose deviations while maintaining high sample efficiency and requiring little human effort. In real-world experiments, with just 5 hours of training data, EasyInsert achieves over 90% success in zero-shot insertion for 13 out of 15 unseen novel objects, including challenging objects like Type-C cables, HDMI cables, and Ethernet cables. Furthermore, with only one human demonstration and 4 minutes of automatically collected data for fine-tuning, it reaches over 90% success rate for all 15 objects.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought</title>
<link>https://arxiv.org/abs/2505.16192</link>
<guid>https://arxiv.org/abs/2505.16192</guid>
<content:encoded><![CDATA[
arXiv:2505.16192v1 Announce Type: cross 
Abstract: Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual \textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and \textbf{R}easoning), a framework that equips an MLLM with the ability to (i) decide \emph{when} additional visual evidence is needed, (ii) determine \emph{where} to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization (R-GRPO)}, a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet</title>
<link>https://arxiv.org/abs/2505.16195</link>
<guid>https://arxiv.org/abs/2505.16195</guid>
<content:encoded><![CDATA[
arXiv:2505.16195v1 Announce Type: cross 
Abstract: Foley synthesis aims to synthesize high-quality audio that is both semantically and temporally aligned with video frames. Given its broad application in creative industries, the task has gained increasing attention in the research community. To avoid the non-trivial task of training audio generative models from scratch, adapting pretrained audio generative models for video-synchronized foley synthesis presents an attractive direction. ControlNet, a method for adding fine-grained controls to pretrained generative models, has been applied to foley synthesis, but its use has been limited to handcrafted human-readable temporal conditions. In contrast, from-scratch models achieved success by leveraging high-dimensional deep features extracted using pretrained video encoders. We have observed a performance gap between ControlNet-based and from-scratch foley models. To narrow this gap, we propose SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward video-synchronized foley synthesis via ControlNet. To unlock the potential of a single ControlNet branch, we resolve the discrepancy between the temporal video features and the time-frequency nature of the pretrained SpecMaskGIT via a frequency-aware temporal feature aligner, eliminating the need for complicated conditioning mechanisms widely used in prior arts. Evaluations on a common foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform strong from-scratch baselines, substantially advancing the development of ControlNet-based foley synthesis models. Demo page: https://zzaudio.github.io/SpecMaskFoley_Demo/
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEM: Enhancing Spatial Understanding for Robust Robot Manipulation</title>
<link>https://arxiv.org/abs/2505.16196</link>
<guid>https://arxiv.org/abs/2505.16196</guid>
<content:encoded><![CDATA[
arXiv:2505.16196v1 Announce Type: cross 
Abstract: A key challenge in robot manipulation lies in developing policy models with strong spatial understanding, the ability to reason about 3D geometry, object relations, and robot embodiment. Existing methods often fall short: 3D point cloud models lack semantic abstraction, while 2D image encoders struggle with spatial reasoning. To address this, we propose SEM (Spatial Enhanced Manipulation model), a novel diffusion-based policy framework that explicitly enhances spatial understanding from two complementary perspectives. A spatial enhancer augments visual representations with 3D geometric context, while a robot state encoder captures embodiment-aware structure through graphbased modeling of joint dependencies. By integrating these modules, SEM significantly improves spatial understanding, leading to robust and generalizable manipulation across diverse tasks that outperform existing baselines.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems</title>
<link>https://arxiv.org/abs/2505.16208</link>
<guid>https://arxiv.org/abs/2505.16208</guid>
<content:encoded><![CDATA[
arXiv:2505.16208v1 Announce Type: cross 
Abstract: We apply the Echo-State Networks to predict the time series and statistical properties of the competitive Lotka-Volterra model in the chaotic regime. In particular, we demonstrate that Echo-State Networks successfully learn the chaotic attractor of the competitive Lotka-Volterra model and reproduce histograms of dependent variables, including tails and rare events. We use the Generalized Extreme Value distribution to quantify the tail behavior.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics</title>
<link>https://arxiv.org/abs/2505.16210</link>
<guid>https://arxiv.org/abs/2505.16210</guid>
<content:encoded><![CDATA[
arXiv:2505.16210v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across a wide range of tasks. However, LLMs often require larger batch sizes to enhance throughput or longer context lengths to meet task demands, which significantly increases the memory resource consumption of the Key-Value (KV) cache during inference, becoming a major bottleneck in LLM deployment. To address this issue, quantization is a common and straightforward approach. Currently, quantization methods for activations are limited to 8-bit, and quantization to even lower bits can lead to substantial accuracy drops. To further save space by quantizing the KV cache to even lower bits, we analyzed the element distribution of the KV cache and designed the NQKV algorithm. Since the elements within each block of the KV cache follow a normal distribution, NQKV employs per-block quantile quantization to achieve information-theoretically optimal quantization error. Without significantly compromising model output quality, NQKV enables the OPT model to perform inference with an 2x larger batch size or a 4x longer context length, and it improves throughput by 9.3x compared to when the KV cache is not used.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title>
<link>https://arxiv.org/abs/2505.16211</link>
<guid>https://arxiv.org/abs/2505.16211</guid>
<content:encoded><![CDATA[
arXiv:2505.16211v1 Announce Type: cross 
Abstract: The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16227</link>
<guid>https://arxiv.org/abs/2505.16227</guid>
<content:encoded><![CDATA[
arXiv:2505.16227v1 Announce Type: cross 
Abstract: Personalizing jargon detection and explanation is essential for making technical documents accessible to readers with diverse disciplinary backgrounds. However, tailoring models to individual users typically requires substantial annotation efforts and computational resources due to user-specific finetuning. To address this, we present a systematic study of personalized jargon detection, focusing on methods that are both efficient and scalable for real-world deployment. We explore two personalization strategies: (1) lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models, and (2) personalized prompting, which tailors model behavior at inference time without retaining. To reflect realistic constraints, we also investigate hybrid approaches that combine limited annotated data with unsupervised user background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably, our method achieves comparable performance using only 10% of the annotated training data, demonstrating its practicality for resource-constrained settings. Our study offers the first work to systematically explore efficient, low-resource personalization of jargon detection using open-source language models, offering a practical path toward scalable, user-adaptive NLP system.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFEBench: Evaluating Length Instruction Following in Large Language Models</title>
<link>https://arxiv.org/abs/2505.16234</link>
<guid>https://arxiv.org/abs/2505.16234</guid>
<content:encoded><![CDATA[
arXiv:2505.16234v1 Announce Type: cross 
Abstract: While large language models (LLMs) can solve PhD-level reasoning problems over long context inputs, they still struggle with a seemingly simpler task: following explicit length instructions-e.g., write a 10,000-word novel. Additionally, models often generate far too short outputs, terminate prematurely, or even refuse the request. Existing benchmarks focus primarily on evaluating generations quality, but often overlook whether the generations meet length constraints. To this end, we introduce Length Instruction Following Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to follow length instructions across diverse tasks and a wide range of specified lengths. LIFEBench consists of 10,800 instances across 4 task categories in both English and Chinese, covering length constraints ranging from 16 to 8192 words. We evaluate 26 widely-used LLMs and find that most models reasonably follow short-length instructions but deteriorate sharply beyond a certain threshold. Surprisingly, almost all models fail to reach the vendor-claimed maximum output lengths in practice, as further confirmed by our evaluations extending up to 32K words. Even long-context LLMs, despite their extended input-output windows, counterintuitively fail to improve length-instructions following. Notably, Reasoning LLMs outperform even specialized long-text generation models, achieving state-of-the-art length following. Overall, LIFEBench uncovers fundamental limitations in current LLMs' length instructions following ability, offering critical insights for future progress.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control</title>
<link>https://arxiv.org/abs/2505.16249</link>
<guid>https://arxiv.org/abs/2505.16249</guid>
<content:encoded><![CDATA[
arXiv:2505.16249v1 Announce Type: cross 
Abstract: Manipulating elasto-plastic objects remains a significant challenge due to severe self-occlusion, difficulties of representation, and complicated dynamics. This work proposes a novel framework for elasto-plastic object manipulation with a quasi-static assumption for motions, leveraging 3D occupancy to represent such objects, a learned dynamics model trained with 3D occupancy, and a learning-based predictive control algorithm to address these challenges effectively. We build a novel data collection platform to collect full spatial information and propose a pipeline for generating a 3D occupancy dataset. To infer the 3D occupancy during manipulation, an occupancy prediction network is trained with multiple RGB images supervised by the generated dataset. We design a deep neural network empowered by a 3D convolution neural network (CNN) and a graph neural network (GNN) to predict the complex deformation with the inferred 3D occupancy results. A learning-based predictive control algorithm is introduced to plan the robot actions, incorporating a novel shape-based action initialization module specifically designed to improve the planner efficiency. The proposed framework in this paper can successfully shape the elasto-plastic objects into a given goal shape and has been verified in various experiments both in simulation and the real world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor</title>
<link>https://arxiv.org/abs/2505.16256</link>
<guid>https://arxiv.org/abs/2505.16256</guid>
<content:encoded><![CDATA[
arXiv:2505.16256v1 Announce Type: cross 
Abstract: Most learning-based lossless compressors are designed for a single modality, requiring separate models for multi-modal data and lacking flexibility. However, different modalities vary significantly in format and statistical properties, making it ineffective to use compressors that lack modality-specific adaptations. While multi-modal large language models (MLLMs) offer a potential solution for modality-unified compression, their excessive complexity hinders practical deployment. To address these challenges, we focus on the two most common modalities, image and text, and propose DualComp, the first unified and lightweight learning-based dual-modality lossless compressor. Built on a lightweight backbone, DualComp incorporates three key structural enhancements to handle modality heterogeneity: modality-unified tokenization, modality-switching contextual learning, and modality-routing mixture-of-experts. A reparameterization training strategy is also used to boost compression performance. DualComp integrates both modality-specific and shared parameters for efficient parameter utilization, enabling near real-time inference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp achieves compression performance on par with the SOTA LLM-based methods for both text and image datasets. Its simplified single-modality variant surpasses the previous best image compressor on the Kodak dataset by about 9% using just 1.2% of the model size.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2505.16258</link>
<guid>https://arxiv.org/abs/2505.16258</guid>
<content:encoded><![CDATA[
arXiv:2505.16258v1 Announce Type: cross 
Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs presents unique challenges, often requiring task-specific fine-tuning and extensive reasoning steps. However, current Chain-of-Thought approaches do not efficiently leverage the same cognitive processes that enable humans to identify sarcasm. We present IRONIC, an in-context learning framework that leverages Multi-modal Coherence Relations to analyze referential, analogical and pragmatic image-text linkages. Our experiments show that IRONIC achieves state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across different baselines. This demonstrates the need for incorporating linguistic and cognitive insights into the design of multi-modal reasoning strategies. Our code is available at: https://github.com/aashish2000/IRONIC
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System</title>
<link>https://arxiv.org/abs/2505.16259</link>
<guid>https://arxiv.org/abs/2505.16259</guid>
<content:encoded><![CDATA[
arXiv:2505.16259v1 Announce Type: cross 
Abstract: This paper presents , an interactive music piece for a human pianist and a computer-controlled piano that integrates real-time automatic music transcription into a score-driven framework. Unlike previous approaches that primarily focus on improvisation-based interactions, our work establishes a balanced framework that combines composed structure with dynamic interaction. Through real-time automatic transcription as its core mechanism, the computer interprets and responds to the human performer's input in real time, creating a musical dialogue that balances compositional intent with live interaction while incorporating elements of unpredictability. In this paper, we present the development process from composition to premiere performance, including technical implementation, rehearsal process, and performance considerations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
arXiv:2505.16270v1 Announce Type: cross 
Abstract: Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.16278</link>
<guid>https://arxiv.org/abs/2505.16278</guid>
<content:encoded><![CDATA[
arXiv:2505.16278v1 Announce Type: cross 
Abstract: End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $\pi_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$\pi_0$. Specifically, we add Vision MoE to Drive-$\pi_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$\pi_0$.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Generative AI for Story Point Estimation in Software Development</title>
<link>https://arxiv.org/abs/2505.16290</link>
<guid>https://arxiv.org/abs/2505.16290</guid>
<content:encoded><![CDATA[
arXiv:2505.16290v1 Announce Type: cross 
Abstract: This research explores the application of Multimodal Generative AI to enhance story point estimation in Agile software development. By integrating text, image, and categorical data using advanced models like BERT, CNN, and XGBoost, our approach surpasses the limitations of traditional single-modal estimation methods. The results demonstrate strong accuracy for simpler story points, while also highlighting challenges in more complex categories due to data imbalance. This study further explores the impact of categorical data, particularly severity, on the estimation process, emphasizing its influence on model performance. Our findings emphasize the transformative potential of multimodal data integration in refining AI-driven project management, paving the way for more precise, adaptable, and domain-specific AI capabilities. Additionally, this work outlines future directions for addressing data variability and enhancing the robustness of AI in Agile methodologies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space</title>
<link>https://arxiv.org/abs/2505.16301</link>
<guid>https://arxiv.org/abs/2505.16301</guid>
<content:encoded><![CDATA[
arXiv:2505.16301v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of atomistic systems, but its reliance on sequential numerical integration limits simulation efficiency. We present MDtrajNet-1, a foundational AI model that directly generates MD trajectories across chemical space, bypassing force calculations and integration. This approach accelerates simulations by up to two orders of magnitude compared to traditional MD, even those enhanced by machine-learning interatomic potentials. MDtrajNet-1 combines equivariant neural networks with a Transformer-based architecture to achieve strong accuracy and transferability in predicting long-time trajectories for both known and unseen systems. Remarkably, the errors of the trajectories generated by MDtrajNet-1 for various molecular systems are close to those of the conventional ab initio MD. The model's flexible design supports diverse application scenarios, including different statistical ensembles, boundary conditions, and interaction types. By overcoming the intrinsic speed barrier of conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable atomistic simulations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models</title>
<link>https://arxiv.org/abs/2505.16306</link>
<guid>https://arxiv.org/abs/2505.16306</guid>
<content:encoded><![CDATA[
arXiv:2505.16306v1 Announce Type: cross 
Abstract: Recently, pre-trained models for music information retrieval based on self-supervised learning (SSL) are becoming popular, showing success in various downstream tasks. However, there is limited research on the specific meanings of the encoded information and their applicability. Exploring these aspects can help us better understand their capabilities and limitations, leading to more effective use in downstream tasks.
  In this study, we analyze the advanced music representation model MusicFM and the newly emerged SSL model MuQ. We focus on three main aspects: (i) validating the advantages of SSL models across multiple downstream tasks, (ii) exploring the specialization of layer-wise information for different tasks, and (iii) comparing performance differences when selecting specific layers. Through this analysis, we reveal insights into the structure and potential applications of SSL models in music information retrieval.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models</title>
<link>https://arxiv.org/abs/2505.16307</link>
<guid>https://arxiv.org/abs/2505.16307</guid>
<content:encoded><![CDATA[
arXiv:2505.16307v1 Announce Type: cross 
Abstract: Prompt optimization offers a practical and broadly applicable alternative to fine-tuning for improving large language model (LLM) performance. However, existing methods often rely on costly output generation, self-critiquing abilities, or human-annotated preferences, which limit their scalability, especially for smaller or non-instruction-tuned models. We introduce PMPO (Probabilistic Metric Prompt Optimization), a unified framework that refines prompts using token-level cross-entropy loss as a direct, lightweight evaluation signal. PMPO identifies low-quality prompt segments by masking and measuring their impact on loss, then rewrites and selects improved variants by minimizing loss over positive and negative examples. Unlike prior methods, it requires no output sampling or human evaluation during optimization, relying only on forward passes and log-likelihoods. PMPO supports both supervised and preference-based tasks through a closely aligned loss-based evaluation strategy. Experiments show that PMPO consistently outperforms prior methods across model sizes and tasks: it achieves the highest average accuracy on BBH, performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates by over 19 points. These results highlight PMPO's effectiveness, efficiency, and broad applicability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment</title>
<link>https://arxiv.org/abs/2505.16314</link>
<guid>https://arxiv.org/abs/2505.16314</guid>
<content:encoded><![CDATA[
arXiv:2505.16314v1 Announce Type: cross 
Abstract: This paper reports on the NTIRE 2025 challenge on Text to Image (T2I) generation model quality assessment, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. The aim of this challenge is to address the fine-grained quality assessment of text-to-image generation models. This challenge evaluates text-to-image models from two aspects: image-text alignment and image structural distortion detection, and is divided into the alignment track and the structural track. The alignment track uses the EvalMuse-40K, which contains around 40K AI-Generated Images (AIGIs) generated by 20 popular generative models. The alignment track has a total of 371 registered participants. A total of 1,883 submissions are received in the development phase, and 507 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. The structure track uses the EvalMuse-Structure, which contains 10,000 AI-Generated Images (AIGIs) with corresponding structural distortion mask. A total of 211 participants have registered in the structure track. A total of 1155 submissions are received in the development phase, and 487 submissions are received in the test phase. Finally, 8 participating teams submitted their models and fact sheets. Almost all methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on T2I model quality assessment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners</title>
<link>https://arxiv.org/abs/2505.16322</link>
<guid>https://arxiv.org/abs/2505.16322</guid>
<content:encoded><![CDATA[
arXiv:2505.16322v1 Announce Type: cross 
Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling Fine-Tuning (RFT), is an integral part of the training pipeline of self-improving reasoning Language Models (LMs). The self-improving mechanism often employs random observation (data) sampling. However, this results in trained observation imbalance; inefficiently over-training on solved examples while under-training on challenging ones. In response, we introduce Adaptive STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting balanced training across observations, and (2) Adaptive Sampling for Curriculum: dynamically adjusting data difficulty to match the model's evolving strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all instances (6/6) and reduces training FLOPs by an average of 58.6% against an extensive list of baselines. These improvements in performance and efficiency generalize to different pre-trained LMs and larger models, paving the way for more efficient and effective self-improving LMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation</title>
<link>https://arxiv.org/abs/2505.16325</link>
<guid>https://arxiv.org/abs/2505.16325</guid>
<content:encoded><![CDATA[
arXiv:2505.16325v1 Announce Type: cross 
Abstract: Existing metrics often lack the granularity and interpretability to capture nuanced clinical differences between candidate and ground-truth radiology reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded tabular framework with Expert-curated labels and Attribute-level comparison for Radiology report evaluation (CLEAR). CLEAR not only examines whether a report can accurately identify the presence or absence of medical conditions, but also assesses whether it can precisely describe each positively identified condition across five key attributes: first occurrence, change, severity, descriptive location, and recommendation. Compared to prior works, CLEAR's multi-dimensional, attribute-level outputs enable a more comprehensive and clinically interpretable evaluation of report quality. Additionally, to measure the clinical alignment of CLEAR, we collaborate with five board-certified radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions. Our experiments show that CLEAR achieves high accuracy in extracting clinical attributes and provides automated metrics that are strongly aligned with clinical judgment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers</title>
<link>https://arxiv.org/abs/2505.16330</link>
<guid>https://arxiv.org/abs/2505.16330</guid>
<content:encoded><![CDATA[
arXiv:2505.16330v1 Announce Type: cross 
Abstract: Novelty is a core component of academic papers, and there are multiple perspectives on the assessment of novelty. Existing methods often focus on word or entity combinations, which provide limited insights. The content related to a paper's novelty is typically distributed across different core sections, e.g., Introduction, Methodology and Results. Therefore, exploring the optimal combination of sections for evaluating the novelty of a paper is important for advancing automated novelty assessment. In this paper, we utilize different combinations of sections from academic papers as inputs to drive language models to predict novelty scores. We then analyze the results to determine the optimal section combinations for novelty score prediction. We first employ natural language processing techniques to identify the sectional structure of academic papers, categorizing them into introduction, methods, results, and discussion (IMRaD). Subsequently, we used different combinations of these sections (e.g., introduction and methods) as inputs for pretrained language models (PLMs) and large language models (LLMs), employing novelty scores provided by human expert reviewers as ground truth labels to obtain prediction results. The results indicate that using introduction, results and discussion is most appropriate for assessing the novelty of a paper, while the use of the entire text does not yield significant results. Furthermore, based on the results of the PLMs and LLMs, the introduction and results appear to be the most important section for the task of novelty score prediction. The code and dataset for this paper can be accessed at https://github.com/njust-winchy/SC4ANM.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing</title>
<link>https://arxiv.org/abs/2505.16332</link>
<guid>https://arxiv.org/abs/2505.16332</guid>
<content:encoded><![CDATA[
arXiv:2505.16332v1 Announce Type: cross 
Abstract: Quantum optimization is the most mature quantum computing technology to date, providing a promising approach towards efficiently solving complex combinatorial problems. Methods such as adiabatic quantum computing (AQC) have been employed in recent years on important optimization problems across various domains. In deep learning, deep neural networks (DNN) have reached immense sizes to support new predictive capabilities. Optimization of large-scale models is critical for sustainable deployment, but becomes increasingly challenging with ever-growing model sizes and complexity. While quantum optimization is suitable for solving complex problems, its application to DNN optimization is not straightforward, requiring thorough reformulation for compatibility with commercially available quantum devices. In this work, we explore the potential of adopting AQC for fine-grained pruning-quantization of convolutional neural networks. We rework established heuristics to formulate model compression as a quadratic unconstrained binary optimization (QUBO) problem, and assess the solution space offered by commercial quantum annealing devices. Through our exploratory efforts of reformulation, we demonstrate that AQC can achieve effective compression of practical DNN models. Experiments demonstrate that adiabatic quantum computing (AQC) not only outperforms classical algorithms like genetic algorithms and reinforcement learning in terms of time efficiency but also excels at identifying global optima.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design</title>
<link>https://arxiv.org/abs/2505.16335</link>
<guid>https://arxiv.org/abs/2505.16335</guid>
<content:encoded><![CDATA[
arXiv:2505.16335v1 Announce Type: cross 
Abstract: Visual autoregressive (VAR) modeling has marked a paradigm shift in image generation from next-token prediction to next-scale prediction. VAR predicts a set of tokens at each step from coarse to fine scale, leading to better image quality and faster inference speed compared to existing diffusion models. However, the large parameter size and computation cost hinder its deployment on edge devices. To reduce the memory and computation cost, we propose FPQVAR, an efficient post-training floating-point (FP) quantization framework for VAR featuring algorithm and hardware co-design. At the algorithm level, we first identify the challenges of quantizing VAR. To address them, we propose Dual Format Quantization for the highly imbalanced input activation. We further propose Group-wise Hadamard Transformation and GHT-Aware Learnable Transformation to address the time-varying outlier channels. At the hardware level, we design the first low-bit FP quantizer and multiplier with lookup tables on FPGA and propose the first FPGA-based VAR accelerator featuring low-bit FP computation and an elaborate two-level pipeline. Extensive experiments show that compared to the state-of-the-art quantization method, our proposed FPQVAR significantly improves Fr\'echet Inception Distance (FID) from 10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit quantization. FPQVAR also significantly improves the performance of 6-bit quantized VAR, bringing it on par with the FP16 model. Our accelerator on AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x higher energy efficiency compared to the integer-based accelerator and GPU baseline, respectively.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection</title>
<link>https://arxiv.org/abs/2505.16351</link>
<guid>https://arxiv.org/abs/2505.16351</guid>
<content:encoded><![CDATA[
arXiv:2505.16351v1 Announce Type: cross 
Abstract: Automatic detection of speech dysfluency aids speech-language pathologists in efficient transcription of disordered speech, enhancing diagnostics and treatment planning. Traditional methods, often limited to classification, provide insufficient clinical insight, and text-independent models misclassify dysfluency, especially in context-dependent cases. This work introduces Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with upstream encoders like WavLM and requires no additional training. It achieves state-of-the-art performance in both phonetic error rate and dysfluency detection on simulated and real speech data. Our approach is lightweight, interpretable, and effective, demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms</title>
<link>https://arxiv.org/abs/2505.16362</link>
<guid>https://arxiv.org/abs/2505.16362</guid>
<content:encoded><![CDATA[
arXiv:2505.16362v1 Announce Type: cross 
Abstract: Neuromorphic computing (NC) introduces a novel algorithmic paradigm representing a major shift from traditional digital computing of Von Neumann architectures. NC emulates or simulates the neural dynamics of brains in the form of Spiking Neural Networks (SNNs). Much of the research in NC has concentrated on machine learning applications and neuroscience simulations. This paper investigates the modelling and implementation of optimization algorithms and particularly metaheuristics using the NC paradigm as an alternative to Von Neumann architectures, leading to breakthroughs in solving optimization problems.
  Neuromorphic-based metaheuristics (Nheuristics) are supposed to be characterized by low power, low latency and small footprint. Since NC systems are fundamentally different from conventional Von Neumann computers, several challenges are posed to the design and implementation of Nheuristics. A guideline based on a classification and critical analysis is conducted on the different families of metaheuristics and optimization problems they address. We also discuss future directions that need to be addressed to expand both the development and application of Nheuristics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training</title>
<link>https://arxiv.org/abs/2505.16363</link>
<guid>https://arxiv.org/abs/2505.16363</guid>
<content:encoded><![CDATA[
arXiv:2505.16363v1 Announce Type: cross 
Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large language model (LLM) pretraining and post-training. By leveraging a novel denominator, i.e., the root of weighted sum of squares of the momentum and the current gradient, AdamS eliminates the need for second-moment estimates. Hence, AdamS is efficient, matching the memory and compute footprint of SGD with momentum while delivering superior optimization performance. Moreover, AdamS is easy to adopt: it can directly inherit hyperparameters of AdamW, and is entirely model-agnostic, integrating seamlessly into existing pipelines without modifications to optimizer APIs or architectures. The motivation behind AdamS stems from the observed $(L_0, L_1)$ smoothness properties in transformer objectives, where local smoothness is governed by gradient magnitudes that can be further approximated by momentum magnitudes. We establish rigorous theoretical convergence guarantees and provide practical guidelines for hyperparameter selection. Empirically, AdamS demonstrates strong performance in various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B parameters) and reinforcement learning in post-training regimes. With its efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling alternative to existing optimizers.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules</title>
<link>https://arxiv.org/abs/2505.16365</link>
<guid>https://arxiv.org/abs/2505.16365</guid>
<content:encoded><![CDATA[
arXiv:2505.16365v1 Announce Type: cross 
Abstract: Developing new molecular compounds is crucial to address pressing challenges, from health to environmental sustainability. However, exploring the molecular space to discover new molecules is difficult due to the vastness of the space. Here we introduce CoCoGraph, a collaborative and constrained graph diffusion model capable of generating molecules that are guaranteed to be chemically valid. Thanks to the constraints built into the model and to the collaborative mechanism, CoCoGraph outperforms state-of-the-art approaches on standard benchmarks while requiring up to an order of magnitude fewer parameters. Analysis of 36 chemical properties also demonstrates that CoCoGraph generates molecules with distributions more closely matching real molecules than current models. Leveraging the model's efficiency, we created a database of 8.2M million synthetically generated molecules and conducted a Turing-like test with organic chemistry experts to further assess the plausibility of the generated molecules, and potential biases and limitations of CoCoGraph.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.16368</link>
<guid>https://arxiv.org/abs/2505.16368</guid>
<content:encoded><![CDATA[
arXiv:2505.16368v1 Announce Type: cross 
Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition</title>
<link>https://arxiv.org/abs/2505.16372</link>
<guid>https://arxiv.org/abs/2505.16372</guid>
<content:encoded><![CDATA[
arXiv:2505.16372v1 Announce Type: cross 
Abstract: When emotions are repressed, an individual's true feelings may be revealed through micro-expressions. Consequently, micro-expressions are regarded as a genuine source of insight into an individual's authentic emotions. However, the transient and highly localised nature of micro-expressions poses a significant challenge to their accurate recognition, with the accuracy rate of micro-expression recognition being as low as 50%, even for professionals. In order to address these challenges, it is necessary to explore the field of dynamic micro expression recognition (DMER) using multimodal fusion techniques, with special attention to the diverse fusion of temporal and spatial modal features. In this paper, we propose a novel Temporal and Spatial feature Fusion framework for DMER (TSFmicro). This framework integrates a Retention Network (RetNet) and a transformer-based DMER network, with the objective of efficient micro-expression recognition through the capture and fusion of temporal and spatial relations. Meanwhile, we propose a novel parallel time-space fusion method from the perspective of modal fusion, which fuses spatio-temporal information in high-dimensional feature space, resulting in complementary "where-how" relationships at the semantic level and providing richer semantic information for the model. The experimental results demonstrate the superior performance of the TSFmicro method in comparison to other contemporary state-of-the-art methods. This is evidenced by its effectiveness on three well-recognised micro-expression datasets.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos</title>
<link>https://arxiv.org/abs/2505.16376</link>
<guid>https://arxiv.org/abs/2505.16376</guid>
<content:encoded><![CDATA[
arXiv:2505.16376v1 Announce Type: cross 
Abstract: Long Video Temporal Grounding (LVTG) aims at identifying specific moments within lengthy videos based on user-provided text queries for effective content retrieval. The approach taken by existing methods of dividing video into clips and processing each clip via a full-scale expert encoder is challenging to scale due to prohibitive computational costs of processing a large number of clips in long videos. To address this issue, we introduce DeCafNet, an approach employing ``delegate-and-conquer'' strategy to achieve computation efficiency without sacrificing grounding performance. DeCafNet introduces a sidekick encoder that performs dense feature extraction over all video clips in a resource-efficient manner, while generating a saliency map to identify the most relevant clips for full processing by the expert encoder. To effectively leverage features from sidekick and expert encoders that exist at different temporal resolutions, we introduce DeCaf-Grounder, which unifies and refines them via query-aware temporal aggregation and multi-scale temporal refinement for accurate grounding. Experiments on two LTVG benchmark datasets demonstrate that DeCafNet reduces computation by up to 47\% while still outperforming existing methods, establishing a new state-of-the-art for LTVG in terms of both efficiency and performance. Our code is available at https://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.16377</link>
<guid>https://arxiv.org/abs/2505.16377</guid>
<content:encoded><![CDATA[
arXiv:2505.16377v1 Announce Type: cross 
Abstract: Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of "safety" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at: https://ys-qu.github.io/vlsafe-website/
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.16379</link>
<guid>https://arxiv.org/abs/2505.16379</guid>
<content:encoded><![CDATA[
arXiv:2505.16379v1 Announce Type: cross 
Abstract: Materials are the foundation of modern society, underpinning advancements in energy, electronics, healthcare, transportation, and infrastructure. The ability to discover and design new materials with tailored properties is critical to solving some of the most pressing global challenges. In recent years, the growing availability of high-quality materials data combined with rapid advances in Artificial Intelligence (AI) has opened new opportunities for accelerating materials discovery. Data-driven generative models provide a powerful tool for materials design by directly create novel materials that satisfy predefined property requirements. Despite the proliferation of related work, there remains a notable lack of up-to-date and systematic surveys in this area. To fill this gap, this paper provides a comprehensive overview of recent progress in AI-driven materials generation. We first organize various types of materials and illustrate multiple representations of crystalline materials. We then provide a detailed summary and taxonomy of current AI-driven materials generation approaches. Furthermore, we discuss the common evaluation metrics and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future directions and challenges in this fast-growing field. The related sources can be found at https://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection</title>
<link>https://arxiv.org/abs/2505.16392</link>
<guid>https://arxiv.org/abs/2505.16392</guid>
<content:encoded><![CDATA[
arXiv:2505.16392v1 Announce Type: cross 
Abstract: The general public often encounters complex texts but does not have the time or expertise to fully understand them, leading to the spread of misinformation. Automatic Text Simplification (ATS) helps make information more accessible, but its evaluation methods have not kept up with advances in text generation, especially with Large Language Models (LLMs). In particular, recent studies have shown that current ATS metrics do not correlate with the presence of errors. Manual inspections have further revealed a variety of errors, underscoring the need for a more nuanced evaluation framework, which is currently lacking. This resource paper addresses this gap by introducing a test collection for detecting and classifying errors in simplified texts. First, we propose a taxonomy of errors, with a formal focus on information distortion. Next, we introduce a parallel dataset of automatically simplified scientific texts. This dataset has been human-annotated with labels based on our proposed taxonomy. Finally, we analyze the quality of the dataset, and we study the performance of existing models to detect and classify errors from that taxonomy. These contributions give researchers the tools to better evaluate errors in ATS, develop more reliable models, and ultimately improve the quality of automatically simplified texts.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)</title>
<link>https://arxiv.org/abs/2505.16394</link>
<guid>https://arxiv.org/abs/2505.16394</guid>
<content:encoded><![CDATA[
arXiv:2505.16394v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16400</link>
<guid>https://arxiv.org/abs/2505.16400</guid>
<content:encoded><![CDATA[
arXiv:2505.16400v1 Announce Type: cross 
Abstract: Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16410</link>
<guid>https://arxiv.org/abs/2505.16410</guid>
<content:encoded><![CDATA[
arXiv:2505.16410v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.16415</link>
<guid>https://arxiv.org/abs/2505.16415</guid>
<content:encoded><![CDATA[
arXiv:2505.16415v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.16416</link>
<guid>https://arxiv.org/abs/2505.16416</guid>
<content:encoded><![CDATA[
arXiv:2505.16416v1 Announce Type: cross 
Abstract: Rotary Position Embedding (RoPE) is a widely adopted technique for encoding relative positional information in large language models (LLMs). However, when extended to large vision-language models (LVLMs), its variants introduce unintended cross-modal positional biases. Specifically, they enforce relative positional dependencies between text token indices and image tokens, causing spurious alignments. This issue arises because image tokens representing the same content but located at different spatial positions are assigned distinct positional biases, leading to inconsistent cross-modal associations. To address this, we propose Per-Token Distance (PTD) - a simple yet effective metric for quantifying the independence of positional encodings across modalities. Informed by this analysis, we introduce Circle-RoPE, a novel encoding scheme that maps image token indices onto a circular trajectory orthogonal to the linear path of text token indices, forming a cone-like structure. This configuration ensures that each text token maintains an equal distance to all image tokens, reducing artificial cross-modal biases while preserving intra-image spatial information. To further enhance performance, we propose a staggered layer strategy that applies different RoPE variants across layers. This design leverages the complementary strengths of each RoPE variant, thereby enhancing the model's overall performance. Our experimental results demonstrate that our method effectively preserves spatial information from images while reducing relative positional bias, offering a more robust and flexible positional encoding framework for LVLMs. The code is available at [https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment</title>
<link>https://arxiv.org/abs/2505.16419</link>
<guid>https://arxiv.org/abs/2505.16419</guid>
<content:encoded><![CDATA[
arXiv:2505.16419v1 Announce Type: cross 
Abstract: The learning mechanisms by which humans acquire internal representations of objects are not fully understood. Deep neural networks (DNNs) have emerged as a useful tool for investigating this question, as they have internal representations similar to those of humans as a byproduct of optimizing their objective functions. While previous studies have shown that models trained with various learning paradigms - such as supervised, self-supervised, and CLIP - acquire human-like representations, it remains unclear whether their similarity to human representations is primarily at a coarse category level or extends to finer details. Here, we employ an unsupervised alignment method based on Gromov-Wasserstein Optimal Transport to compare human and model object representations at both fine-grained and coarse-grained levels. The unique feature of this method compared to conventional representational similarity analysis is that it estimates optimal fine-grained mappings between the representation of each object in human and model representations. We used this unsupervised alignment method to assess the extent to which the representation of each object in humans is correctly mapped to the corresponding representation of the same object in models. Using human similarity judgments of 1,854 objects from the THINGS dataset, we find that models trained with CLIP consistently achieve strong fine- and coarse-grained matching with human object representations. In contrast, self-supervised models showed limited matching at both fine- and coarse-grained levels, but still formed object clusters that reflected human coarse category structure. Our results offer new insights into the role of linguistic information in acquiring precise object representations and the potential of self-supervised learning to capture coarse categorical structures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion</title>
<link>https://arxiv.org/abs/2505.16425</link>
<guid>https://arxiv.org/abs/2505.16425</guid>
<content:encoded><![CDATA[
arXiv:2505.16425v1 Announce Type: cross 
Abstract: The effective communication of procedural knowledge remains a significant challenge in natural language processing (NLP), as purely textual instructions often fail to convey complex physical actions and spatial relationships. We address this limitation by proposing a language-driven framework that translates procedural text into coherent visual instructions. Our approach models the linguistic structure of instructional content by decomposing it into goal statements and sequential steps, then conditioning visual generation on these linguistic elements. We introduce three key innovations: (1) a constituency parser-based text encoding mechanism that preserves semantic completeness even with lengthy instructions, (2) a pairwise discourse coherence model that maintains consistency across instruction sequences, and (3) a novel evaluation protocol specifically designed for procedural language-to-image alignment. Our experiments across three instructional datasets (HTStep, CaptainCook4D, and WikiAll) demonstrate that our method significantly outperforms existing baselines in generating visuals that accurately reflect the linguistic content and sequential nature of instructions. This work contributes to the growing body of research on grounding procedural language in visual content, with applications spanning education, task guidance, and multimodal language understanding.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems</title>
<link>https://arxiv.org/abs/2505.16429</link>
<guid>https://arxiv.org/abs/2505.16429</guid>
<content:encoded><![CDATA[
arXiv:2505.16429v1 Announce Type: cross 
Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI</title>
<link>https://arxiv.org/abs/2505.16430</link>
<guid>https://arxiv.org/abs/2505.16430</guid>
<content:encoded><![CDATA[
arXiv:2505.16430v1 Announce Type: cross 
Abstract: Students often do not fully understand the code they have written. This sometimes does not become evident until later in their education, which can mean it is harder to fix their incorrect knowledge or misunderstandings. In addition, being able to fully understand code is increasingly important in a world where students have access to generative artificial intelligence (GenAI) tools, such as GitHub Copilot. One effective solution is to utilise code comprehension questions, where a marker asks questions about a submission to gauge understanding, this can also have the side effect of helping to detect plagiarism. However, this approach is time consuming and can be difficult and/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for the automatic generation of multiple-choice code comprehension questions. This is integrated with the CodeRunner automated assessment platform.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI</title>
<link>https://arxiv.org/abs/2505.16452</link>
<guid>https://arxiv.org/abs/2505.16452</guid>
<content:encoded><![CDATA[
arXiv:2505.16452v1 Announce Type: cross 
Abstract: Accurate and efficient quantification of cardiac function is essential for the estimation of prognosis of cardiovascular diseases (CVDs). One of the most commonly used metrics for evaluating cardiac pumping performance is left ventricular ejection fraction (LVEF). However, LVEF can be affected by factors such as inter-observer variability and varying pre-load and after-load conditions, which can reduce its reproducibility. Additionally, cardiac dysfunction may not always manifest as alterations in LVEF, such as in heart failure and cardiotoxicity diseases. An alternative measure that can provide a relatively load-independent quantitative assessment of myocardial contractility is myocardial strain and strain rate. By using LVEF in combination with myocardial strain, it is possible to obtain a thorough description of cardiac function. Automated estimation of LVEF and other volumetric measures from cine-MRI sequences can be achieved through segmentation models, while strain calculation requires the estimation of tissue displacement between sequential frames, which can be accomplished using registration models. These tasks are often performed separately, potentially limiting the assessment of cardiac function. To address this issue, in this study we propose an end-to-end deep learning (DL) model that jointly estimates groupwise (GW) registration and segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep GW network was trained and validated on a large dataset of 4-chamber view cine-MRI image series of 374 subjects. A quantitative comparison with conventional GW registration using elastix and two DL-based methods showed that the proposed model improved performance and substantially reduced computation time.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection</title>
<link>https://arxiv.org/abs/2505.16460</link>
<guid>https://arxiv.org/abs/2505.16460</guid>
<content:encoded><![CDATA[
arXiv:2505.16460v1 Announce Type: cross 
Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing on multilabel emotion classification across 28 languages. We explore two main strategies: fully fine-tuning transformer models and classifier-only training, evaluating different settings such as fine-tuning strategies, model architectures, loss functions, encoders, and classifiers. Our findings suggest that training a classifier on top of prompt-based encoders such as mE5 and BGE yields significantly better results than fully fine-tuning XLMR and mBERT. Our best-performing model on the final leaderboard is an ensemble combining multiple BGE models, where CatBoost serves as the classifier, with different configurations. This ensemble achieves an average F1-macro score of 56.58 across all languages.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods</title>
<link>https://arxiv.org/abs/2505.16466</link>
<guid>https://arxiv.org/abs/2505.16466</guid>
<content:encoded><![CDATA[
arXiv:2505.16466v1 Announce Type: cross 
Abstract: Recommender systems based on graph neural networks perform well in tasks such as rating and ranking. However, in real-world recommendation scenarios, noise such as user misuse and malicious advertisement gradually accumulates through the message propagation mechanism. Even if existing studies mitigate their effects by reducing the noise propagation weights, the severe sparsity of the recommender system still leads to the low-weighted noisy neighbors being mistaken as meaningful information, and the prediction result obtained based on the polluted nodes is not entirely trustworthy. Therefore, it is crucial to measure the confidence of the prediction results in this highly noisy framework. Furthermore, our evaluation of the existing representative GNN-based recommendation shows that it suffers from overconfidence. Based on the above considerations, we propose a new method to quantify and calibrate the prediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically, we propose a rating calibration method that dynamically adjusts excessive ratings to mitigate overconfidence based on user personalization. We also design a confidence loss function to reduce the overconfidence of negative samples and effectively improve recommendation performance. Experiments on public datasets demonstrate the validity of Conf-GNNRec in prediction confidence and recommendation performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16483</link>
<guid>https://arxiv.org/abs/2505.16483</guid>
<content:encoded><![CDATA[
arXiv:2505.16483v1 Announce Type: cross 
Abstract: Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing</title>
<link>https://arxiv.org/abs/2505.16491</link>
<guid>https://arxiv.org/abs/2505.16491</guid>
<content:encoded><![CDATA[
arXiv:2505.16491v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have rapidly become central to NLP, demonstrating their ability to adapt to various tasks through prompting techniques, including sentiment analysis. However, we still have a limited understanding of how these models capture sentiment-related information. This study probes the hidden layers of Llama models to pinpoint where sentiment features are most represented and to assess how this affects sentiment analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and scales, identifying the layers and pooling methods that best capture sentiment signals. Our results show that sentiment information is most concentrated in mid-layers for binary polarity tasks, with detection accuracy increasing up to 14% over prompting techniques. Additionally, we find that in decoder-only models, the last token is not consistently the most informative for sentiment encoding. Finally, this approach enables sentiment tasks to be performed with memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs, suggesting layer-specific probing as an effective approach for sentiment tasks beyond prompting, with potential to enhance model utility and reduce memory requirements.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models</title>
<link>https://arxiv.org/abs/2505.16498</link>
<guid>https://arxiv.org/abs/2505.16498</guid>
<content:encoded><![CDATA[
arXiv:2505.16498v1 Announce Type: cross 
Abstract: Achieving full automation in self-driving vehicles remains a challenge, especially in dynamic urban environments where navigation requires real-time adaptability. Existing systems struggle to handle navigation plans when faced with unpredictable changes in road layouts, spontaneous detours, or missing map data, due to their heavy reliance on predefined cartographic information. In this work, we explore the use of Large Language Models to generate Answer Set Programming rules by translating informal navigation instructions into structured, logic-based reasoning. ASP provides non-monotonic reasoning, allowing autonomous vehicles to adapt to evolving scenarios without relying on predefined maps. We present an experimental evaluation in which LLMs generate ASP constraints that encode real-world urban driving logic into a formal knowledge representation. By automating the translation of informal navigation instructions into logical rules, our method improves adaptability and explainability in autonomous navigation. Results show that LLM-driven ASP rule generation supports semantic-based decision-making, offering an explainable framework for dynamic navigation planning that aligns closely with how humans communicate navigational intent.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smaller, Smarter, Closer: The Edge of Collaborative Generative AI</title>
<link>https://arxiv.org/abs/2505.16499</link>
<guid>https://arxiv.org/abs/2505.16499</guid>
<content:encoded><![CDATA[
arXiv:2505.16499v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI (GenAI), particularly Large Language Models (LLMs), has exposed critical limitations of cloud-centric deployments, including latency, cost, and privacy concerns. Meanwhile, Small Language Models (SLMs) are emerging as viable alternatives for resource-constrained edge environments, though they often lack the capabilities of their larger counterparts. This article explores the potential of collaborative inference systems that leverage both edge and cloud resources to address these challenges. By presenting distinct cooperation strategies alongside practical design principles and experimental insights, we offer actionable guidance for deploying GenAI across the computing continuum.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Activation Editing for Reliable Instruction Following in Narratives</title>
<link>https://arxiv.org/abs/2505.16505</link>
<guid>https://arxiv.org/abs/2505.16505</guid>
<content:encoded><![CDATA[
arXiv:2505.16505v1 Announce Type: cross 
Abstract: Complex narrative contexts often challenge language models' ability to follow instructions, and existing benchmarks fail to capture these difficulties. To address this, we propose Concise-SAE, a training-free framework that improves instruction following by identifying and editing instruction-relevant neurons using only natural language instructions, without requiring labelled data. To thoroughly evaluate our method, we introduce FreeInstruct, a diverse and realistic benchmark of 1,212 examples that highlights the challenges of instruction following in narrative-rich settings. While initially motivated by complex narratives, Concise-SAE demonstrates state-of-the-art instruction adherence across varied tasks without compromising generation quality.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-First Language Model Inference: Models, Metrics, and Tradeoffs</title>
<link>https://arxiv.org/abs/2505.16508</link>
<guid>https://arxiv.org/abs/2505.16508</guid>
<content:encoded><![CDATA[
arXiv:2505.16508v1 Announce Type: cross 
Abstract: The widespread adoption of Language Models (LMs) across industries is driving interest in deploying these services across the computing continuum, from the cloud to the network edge. This shift aims to reduce costs, lower latency, and improve reliability and privacy. Small Language Models (SLMs), enabled by advances in model compression, are central to this shift, offering a path to on-device inference on resource-constrained edge platforms. This work examines the interplay between edge and cloud deployments, starting from detailed benchmarking of SLM capabilities on single edge devices, and extending to distributed edge clusters. We identify scenarios where edge inference offers comparable performance with lower costs, and others where cloud fallback becomes essential due to limits in scalability or model capacity. Rather than proposing a one-size-fits-all solution, we present platform-level comparisons and design insights for building efficient, adaptive LM inference systems across heterogeneous environments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.16512</link>
<guid>https://arxiv.org/abs/2505.16512</guid>
<content:encoded><![CDATA[
arXiv:2505.16512v1 Announce Type: cross 
Abstract: In recent years, the rapid development of deepfake technology has given rise to an emerging and serious threat to public security: diffusion model-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency through multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the first large-scale multimodal digital human forgery dataset based on diffusion models. Employing five latest digital human generation methods (Sonic, Hallo, etc.) and voice cloning method, we systematically produce a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies show that the confusion rate between forged and real videos reaches 68%, and existing state-of-the-art (SOTA) detection models exhibit large drops in AUC values on DigiFakeAV, highlighting the challenge of the dataset. To address this problem, we further propose DigiShield, a detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves SOTA performance on both the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method effectively identifies covert artifacts through fine-grained analysis of the temporal evolution of facial features in synthetic videos.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods</title>
<link>https://arxiv.org/abs/2505.16516</link>
<guid>https://arxiv.org/abs/2505.16516</guid>
<content:encoded><![CDATA[
arXiv:2505.16516v1 Announce Type: cross 
Abstract: Kernel methods are widely used in machine learning due to their flexibility and expressive power. However, their black-box nature poses significant challenges to interpretability, limiting their adoption in high-stakes applications. Shapley value-based feature attribution techniques, such as SHAP and kernel-specific variants like RKHS-SHAP, offer a promising path toward explainability. Yet, computing exact Shapley values remains computationally intractable in general, motivating the development of various approximation schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that utilizes the multiplicative structure of product kernels to enable the exact computation of Shapley values in polynomial time. We show that product-kernel models admit a functional decomposition that allows for a recursive formulation of Shapley values. This decomposition not only yields computational efficiency but also enhances interpretability in kernel-based learning. We also demonstrate how our framework can be generalized to explain kernel-based statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for interpretable statistical inference.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUB: Benchmarking Context Utilisation Techniques for Language Models</title>
<link>https://arxiv.org/abs/2505.16518</link>
<guid>https://arxiv.org/abs/2505.16518</guid>
<content:encoded><![CDATA[
arXiv:2505.16518v1 Announce Type: cross 
Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks, such as question answering and fact checking. However, language models (LMs) may ignore relevant information that contradicts outdated parametric memory or be distracted by irrelevant contexts. While many context utilisation manipulation techniques (CMTs) that encourage or suppress context utilisation have recently been proposed to alleviate these issues, few have seen systematic comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to help practitioners within retrieval-augmented generation (RAG) identify the best CMT for their needs. CUB allows for rigorous testing on three distinct context types, observed to capture key challenges in realistic context utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art methods, representative of the main categories of CMTs, across three diverse datasets and tasks, applied to nine LMs. Our results show that most of the existing CMTs struggle to handle the full set of types of contexts that may be encountered in real-world retrieval-augmented scenarios. Moreover, we find that many CMTs display an inflated performance on simple synthesised datasets, compared to more realistic datasets with naturally occurring samples. Altogether, our results show the need for holistic tests of CMTs and the development of CMTs that can handle multiple context types.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs</title>
<link>https://arxiv.org/abs/2505.16520</link>
<guid>https://arxiv.org/abs/2505.16520</guid>
<content:encoded><![CDATA[
arXiv:2505.16520v1 Announce Type: cross 
Abstract: Factual hallucinations are a major challenge for Large Language Models (LLMs). They undermine reliability and user trust by generating inaccurate or fabricated content. Recent studies suggest that when generating false statements, the internal states of LLMs encode information about truthfulness. However, these studies often rely on synthetic datasets that lack realism, which limits generalization when evaluating the factual accuracy of text generated by the model itself. In this paper, we challenge the findings of previous work by investigating truthfulness encoding capabilities, leading to the generation of a more realistic and challenging dataset. Specifically, we extend previous work by introducing: (1) a strategy for sampling plausible true-false factoid sentences from tabular data and (2) a procedure for generating realistic, LLM-dependent true-false datasets from Question Answering collections. Our analysis of two open-source LLMs reveals that while the findings from previous studies are partially validated, generalization to LLM-generated datasets remains challenging. This study lays the groundwork for future research on factuality in LLMs and offers practical guidelines for more effective evaluation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing</title>
<link>https://arxiv.org/abs/2505.16522</link>
<guid>https://arxiv.org/abs/2505.16522</guid>
<content:encoded><![CDATA[
arXiv:2505.16522v1 Announce Type: cross 
Abstract: Despite significant progress, recent studies have indicated that current large language models (LLMs) may still utilize bias during inference, leading to the poor generalizability of LLMs. Some benchmarks are proposed to investigate the generalizability of LLMs, with each piece of data typically containing one type of controlled bias. However, a single piece of data may contain multiple types of biases in practical applications. To bridge this gap, we propose a multi-bias benchmark where each piece of data contains five types of biases. The evaluations conducted on this benchmark reveal that the performance of existing LLMs and debiasing methods is unsatisfying, highlighting the challenge of eliminating multiple types of biases simultaneously. To overcome this challenge, we propose a causal effect estimation-guided multi-bias elimination method (CMBE). This method first estimates the causal effect of multiple types of biases simultaneously. Subsequently, we eliminate the causal effect of biases from the total causal effect exerted by both the semantic information and biases during inference. Experimental results show that CMBE can effectively eliminate multiple types of bias simultaneously to enhance the generalizability of LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection</title>
<link>https://arxiv.org/abs/2505.16530</link>
<guid>https://arxiv.org/abs/2505.16530</guid>
<content:encoded><![CDATA[
arXiv:2505.16530v1 Announce Type: cross 
Abstract: Large language models (LLMs) are considered valuable Intellectual Properties (IP) for legitimate owners due to the enormous computational cost of training. It is crucial to protect the IP of LLMs from malicious stealing or unauthorized deployment. Despite existing efforts in watermarking and fingerprinting LLMs, these methods either impact the text generation process or are limited in white-box access to the suspect model, making them impractical. Hence, we propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting $\textbf{F}$ramework for black-box setting ownership verification. DuFFin extracts the trigger pattern and the knowledge-level fingerprints to identify the source of a suspect model. We conduct experiments on a variety of models collected from the open-source website, including four popular base models as protected LLMs and their fine-tuning, quantization, and safety alignment versions, which are released by large companies, start-ups, and individual users. Results show that our method can accurately verify the copyright of the base protected LLM on their model variants, achieving the IP-ROC metric greater than 0.95. Our code is available at https://github.com/yuliangyan0807/llm-fingerprint.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextureSAM: Towards a Texture Aware Foundation Model for Segmentation</title>
<link>https://arxiv.org/abs/2505.16540</link>
<guid>https://arxiv.org/abs/2505.16540</guid>
<content:encoded><![CDATA[
arXiv:2505.16540v1 Announce Type: cross 
Abstract: Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. In this study, we investigate SAM's bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a novel texture-alternation of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation</title>
<link>https://arxiv.org/abs/2505.16547</link>
<guid>https://arxiv.org/abs/2505.16547</guid>
<content:encoded><![CDATA[
arXiv:2505.16547v1 Announce Type: cross 
Abstract: This paper presents an end-to-end deep reinforcement learning (RL) framework for occlusion-aware robotic manipulation in cluttered plant environments. Our approach enables a robot to interact with a deformable plant to reveal hidden objects of interest, such as fruits, using multimodal observations. We decouple the kinematic planning problem from robot control to simplify zero-shot sim2real transfer for the trained policy. Our results demonstrate that the trained policy, deployed using our framework, achieves up to 86.7% success in real-world trials across diverse initial conditions. Our findings pave the way toward autonomous, perception-driven agricultural robots that intelligently interact with complex foliage plants to "find the fruit" in challenging occluded scenarios, without the need for explicitly designed geometric and dynamic models of every plant scenario.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-nnU-Net: Towards Automated Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.16561</link>
<guid>https://arxiv.org/abs/2505.16561</guid>
<content:encoded><![CDATA[
arXiv:2505.16561v1 Announce Type: cross 
Abstract: Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ segmentation, each with its own challenges in finding the best segmentation model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many aspects of model configuration but remains constrained by fixed hyperparameters and heuristic design choices. As a full-AutoML framework for MIS, we propose Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization (HPO), neural architecture search (NAS), and hierarchical NAS (HNAS). Additionally, we propose Regularized PriorBand to balance model accuracy with the computational resources required for training, addressing the resource constraints often faced in real-world medical settings that limit the feasibility of extensive training procedures. We evaluate our approach across diverse MIS datasets from the well-established Medical Segmentation Decathlon, analyzing the impact of AutoML techniques on segmentation performance, computational efficiency, and model design choices. The results demonstrate that our AutoML approach substantially improves the segmentation performance of nnU-Net on 6 out of 10 datasets and is on par on the other datasets while maintaining practical resource requirements. Our code is available at https://github.com/LUH-AI/AutonnUNet.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning-Activated Backdoors in LLMs</title>
<link>https://arxiv.org/abs/2505.16567</link>
<guid>https://arxiv.org/abs/2505.16567</guid>
<content:encoded><![CDATA[
arXiv:2505.16567v1 Announce Type: cross 
Abstract: Finetuning openly accessible Large Language Models (LLMs) has become standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets led to predictable behaviors. In this paper, we demonstrate for the first time that an adversary can create poisoned LLMs that initially appear benign but exhibit malicious behaviors once finetuned by downstream users. To this end, our proposed attack, FAB (Finetuning-Activated Backdoor), poisons an LLM via meta-learning techniques to simulate downstream finetuning, explicitly optimizing for the emergence of malicious behaviors in the finetuned models. At the same time, the poisoned LLM is regularized to retain general capabilities and to exhibit no malicious behaviors prior to finetuning. As a result, when users finetune the seemingly benign model on their own datasets, they unknowingly trigger its hidden backdoor behavior. We demonstrate the effectiveness of FAB across multiple LLMs and three target behaviors: unsolicited advertising, refusal, and jailbreakability. Additionally, we show that FAB-backdoors are robust to various finetuning choices made by the user (e.g., dataset, number of steps, scheduler). Our findings challenge prevailing assumptions about the security of finetuning, revealing yet another critical attack vector exploiting the complexities of LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling</title>
<link>https://arxiv.org/abs/2505.16573</link>
<guid>https://arxiv.org/abs/2505.16573</guid>
<content:encoded><![CDATA[
arXiv:2505.16573v1 Announce Type: cross 
Abstract: Stock price prediction is a critical area of financial forecasting, traditionally approached by training models using the historical price data of individual stocks. While these models effectively capture single-stock patterns, they fail to leverage potential correlations among stock trends, which could improve predictive performance. Current single-stock learning methods are thus limited in their ability to provide a broader understanding of price dynamics across multiple stocks. To address this, we propose a novel method that merges local patterns into a global understanding through cross-stock pattern integration. Our strategy is inspired by Federated Learning (FL), a paradigm designed for decentralized model training. FL enables collaborative learning across distributed datasets without sharing raw data, facilitating the aggregation of global insights while preserving data privacy. In our adaptation, we train models on individual stock data and iteratively merge them to create a unified global model. This global model is subsequently fine-tuned on specific stock data to retain local relevance. The proposed strategy enables parallel training of individual stock models, facilitating efficient utilization of computational resources and reducing overall training time. We conducted extensive experiments to evaluate the proposed method, demonstrating that it outperforms benchmark models and enhances the predictive capabilities of state-of-the-art approaches. Our results highlight the efficacy of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction, offering a robust alternative to traditional single-stock learning methodologies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16581</link>
<guid>https://arxiv.org/abs/2505.16581</guid>
<content:encoded><![CDATA[
arXiv:2505.16581v1 Announce Type: cross 
Abstract: In the zero-shot policy transfer setting in reinforcement learning, the goal is to train an agent on a fixed set of training environments so that it can generalise to similar, but unseen, testing environments. Previous work has shown that policy distillation after training can sometimes produce a policy that outperforms the original in the testing environments. However, it is not yet entirely clear why that is, or what data should be used to distil the policy. In this paper, we prove, under certain assumptions, a generalisation bound for policy distillation after training. The theory provides two practical insights: for improved generalisation, you should 1) train an ensemble of distilled policies, and 2) distil it on as much data from the training environments as possible. We empirically verify that these insights hold in more general settings, when the assumptions required for the theory no longer hold. Finally, we demonstrate that an ensemble of policies distilled on a diverse dataset can generalise significantly better than the original agent.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering</title>
<link>https://arxiv.org/abs/2505.16582</link>
<guid>https://arxiv.org/abs/2505.16582</guid>
<content:encoded><![CDATA[
arXiv:2505.16582v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model's sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Uncertainty-Aware Learning of Robotic Suturing</title>
<link>https://arxiv.org/abs/2505.16596</link>
<guid>https://arxiv.org/abs/2505.16596</guid>
<content:encoded><![CDATA[
arXiv:2505.16596v1 Announce Type: cross 
Abstract: Robot-Assisted Minimally Invasive Surgery is currently fully manually controlled by a trained surgeon. Automating this has great potential for alleviating issues, e.g., physical strain, highly repetitive tasks, and shortages of trained surgeons. For these reasons, recent works have utilized Artificial Intelligence methods, which show promising adaptability. Despite these advances, there is skepticism of these methods because they lack explainability and robust safety guarantees. This paper presents a framework for a safe, uncertainty-aware learning method. We train an Ensemble Model of Diffusion Policies using expert demonstrations of needle insertion. Using an Ensemble model, we can quantify the policy's epistemic uncertainty, which is used to determine Out-Of-Distribution scenarios. This allows the system to release control back to the surgeon in the event of an unsafe scenario. Additionally, we implement a model-free Control Barrier Function to place formal safety guarantees on the predicted action. We experimentally evaluate our proposed framework using a state-of-the-art robotic suturing simulator. We evaluate multiple scenarios, such as dropping the needle, moving the camera, and moving the phantom. The learned policy is robust to these perturbations, showing corrective behaviors and generalization, and it is possible to detect Out-Of-Distribution scenarios. We further demonstrate that the Control Barrier Function successfully limits the action to remain within our specified safety set in the case of unsafe predictions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Large Language Models for Machine Translation Personalization</title>
<link>https://arxiv.org/abs/2505.16612</link>
<guid>https://arxiv.org/abs/2505.16612</guid>
<content:encoded><![CDATA[
arXiv:2505.16612v1 Announce Type: cross 
Abstract: High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding</title>
<link>https://arxiv.org/abs/2505.16630</link>
<guid>https://arxiv.org/abs/2505.16630</guid>
<content:encoded><![CDATA[
arXiv:2505.16630v1 Announce Type: cross 
Abstract: The integration of artificial intelligence in sports analytics has transformed soccer video understanding, enabling real-time, automated insights into complex game dynamics. Traditional approaches rely on isolated data streams, limiting their effectiveness in capturing the full context of a match. To address this, we introduce SoccerChat, a multimodal conversational AI framework that integrates visual and textual data for enhanced soccer video comprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey color annotations and automatic speech recognition (ASR) transcripts, SoccerChat is fine-tuned on a structured video instruction dataset to facilitate accurate game understanding, event classification, and referee decision making. We benchmark SoccerChat on action classification and referee decision-making tasks, demonstrating its performance in general soccer event comprehension while maintaining competitive accuracy in referee decision making. Our findings highlight the importance of multimodal integration in advancing soccer analytics, paving the way for more interactive and explainable AI-driven sports analysis. https://github.com/simula/SoccerChat
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2505.16637</link>
<guid>https://arxiv.org/abs/2505.16637</guid>
<content:encoded><![CDATA[
arXiv:2505.16637v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization</title>
<link>https://arxiv.org/abs/2505.16640</link>
<guid>https://arxiv.org/abs/2505.16640</guid>
<content:encoded><![CDATA[
arXiv:2505.16640v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have advanced robotic control by enabling end-to-end decision-making directly from multimodal inputs. However, their tightly coupled architectures expose novel security vulnerabilities. Unlike traditional adversarial perturbations, backdoor attacks represent a stealthier, persistent, and practically significant threat-particularly under the emerging Training-as-a-Service paradigm-but remain largely unexplored in the context of VLA models. To address this gap, we propose BadVLA, a backdoor attack method based on Objective-Decoupled Optimization, which for the first time exposes the backdoor vulnerabilities of VLA models. Specifically, it consists of a two-stage process: (1) explicit feature-space separation to isolate trigger representations from benign inputs, and (2) conditional control deviations that activate only in the presence of the trigger, while preserving clean-task performance. Empirical results on multiple VLA benchmarks demonstrate that BadVLA consistently achieves near-100% attack success rates with minimal impact on clean task accuracy. Further analyses confirm its robustness against common input perturbations, task transfers, and model fine-tuning, underscoring critical security vulnerabilities in current VLA deployments. Our work offers the first systematic investigation of backdoor vulnerabilities in VLA models, highlighting an urgent need for secure and trustworthy embodied model design practices. We have released the project page at https://badvla-project.github.io/.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Evaluation to Defense: Advancing Safety in Video Large Language Models</title>
<link>https://arxiv.org/abs/2505.16643</link>
<guid>https://arxiv.org/abs/2505.16643</guid>
<content:encoded><![CDATA[
arXiv:2505.16643v1 Announce Type: cross 
Abstract: While the safety risks of image-based large language models have been extensively studied, their video-based counterparts (Video LLMs) remain critically under-examined. To systematically study this problem, we introduce \textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse benchmark for Video LLM safety}, which compromises 77,646 video-query pairs and spans 19 principal risk categories across 10 language communities. \textit{We reveal that integrating video modality degrades safety performance by an average of 42.3\%, exposing systemic risks in multimodal attack exploitation.} To address this vulnerability, we propose \textbf{VideoSafety-R1}, a dual-stage framework achieving unprecedented safety gains through two innovations: (1) Alarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens into visual and textual sequences, enabling explicit harm perception across modalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances defensive reasoning through dynamic policy optimization with rule-based rewards derived from dual-modality verification. These components synergize to shift safety alignment from passive harm recognition to active reasoning. The resulting framework achieves a 65.1\% improvement on VSB-Eval-HH, and improves by 59.1\%, 44.3\%, and 15.0\% on the image safety datasets MMBench, VLGuard, and FigStep, respectively. \textit{Our codes are available in the supplementary materials.} \textcolor{red}{Warning: This paper contains examples of harmful language and videos, and reader discretion is recommended.}
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.16647</link>
<guid>https://arxiv.org/abs/2505.16647</guid>
<content:encoded><![CDATA[
arXiv:2505.16647v1 Announce Type: cross 
Abstract: We investigate fine-tuning Vision-Language Models (VLMs) for multi-task medical image understanding, focusing on detection, localization, and counting of findings in medical images. Our objective is to evaluate whether instruction-tuned VLMs can simultaneously improve these tasks, with the goal of enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a multimodal dataset with annotations from endoscopy (polyps and instruments) and microscopy (sperm cells), we reformulate each task into instruction-based prompts suitable for vision-language reasoning. We fine-tune Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task combinations. Results show that multi-task training improves robustness and accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and increases Matching Accuracy in the Counting + Pointing task. However, trade-offs emerge, such as more zero-case point predictions, indicating reduced reliability in edge cases despite overall performance gains. Our study highlights the potential of adapting general-purpose VLMs to specialized medical tasks via prompt-driven fine-tuning. This approach mirrors clinical workflows, where radiologists simultaneously localize, count, and describe findings - demonstrating how VLMs can learn composite diagnostic reasoning patterns. The model produces interpretable, structured outputs, offering a promising step toward explainable and versatile medical AI. Code, model weights, and scripts will be released for reproducibility at https://github.com/simula/PointDetectCount.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaboration among Multiple Large Language Models for Medical Question Answering</title>
<link>https://arxiv.org/abs/2505.16648</link>
<guid>https://arxiv.org/abs/2505.16648</guid>
<content:encoded><![CDATA[
arXiv:2505.16648v1 Announce Type: cross 
Abstract: Empowered by vast internal knowledge reservoir, the new generation of large language models (LLMs) demonstrate untapped potential to tackle medical tasks. However, there is insufficient effort made towards summoning up a synergic effect from multiple LLMs' expertise and background. In this study, we propose a multi-LLM collaboration framework tailored on a medical multiple-choice questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants, our framework is proved to boost all LLMs reasoning ability as well as alleviate their divergence among questions. We also measure an LLM's confidence when it confronts with adversary opinions from other LLMs and observe a concurrence between LLM's confidence and prediction accuracy.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu</title>
<link>https://arxiv.org/abs/2505.16660</link>
<guid>https://arxiv.org/abs/2505.16660</guid>
<content:encoded><![CDATA[
arXiv:2505.16660v1 Announce Type: cross 
Abstract: This study addresses the challenges in intelligent processing of Chinese ancient mathematical classics by constructing Guji_MATH, a benchmark for evaluating classical texts based on Suanjing Shishu. It systematically assesses the mathematical problem-solving capabilities of mainstream reasoning models under the unique linguistic constraints of classical Chinese. Through machine-assisted annotation and manual verification, 538 mathematical problems were extracted from 8 canonical texts, forming a structured dataset centered on the "Question-Answer-Solution" framework, supplemented by problem types and difficulty levels. Dual evaluation modes--closed-book (autonomous problem-solving) and open-book (reproducing classical solution methods)--were designed to evaluate the performance of six reasoning models on ancient Chinese mathematical problems. Results indicate that reasoning models can partially comprehend and solve these problems, yet their overall performance remains inferior to benchmarks on modern mathematical tasks. Enhancing models' classical Chinese comprehension and cultural knowledge should be prioritized for optimization. This study provides methodological support for mining mathematical knowledge from ancient texts and disseminating traditional culture, while offering new perspectives for evaluating cross-linguistic and cross-cultural capabilities of reasoning models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries</title>
<link>https://arxiv.org/abs/2505.16664</link>
<guid>https://arxiv.org/abs/2505.16664</guid>
<content:encoded><![CDATA[
arXiv:2505.16664v1 Announce Type: cross 
Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for enabling timely maintenance of lithium-ion batteries, impacting the operational efficiency of electric applications that rely on them. This paper proposes a RUL prediction approach that leverages data from recent charge-discharge cycles to estimate the number of remaining usable cycles. The approach introduces both a novel signal processing pipeline and a deep learning prediction model. In the signal preprocessing pipeline, a derived capacity feature is computed based on current and capacity signals. Alongside original capacity, voltage and current, these features are denoised and enhanced using statistical metrics and a delta-based method to capture differences between the current and previous cycles. In the prediction model, the processed features are then fed into a hybrid deep learning architecture composed of 1D Convolutional Neural Networks (CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential Equation-based LSTM (ODE-LSTM) modules. This architecture is designed to capture both local signal characteristics and long-range temporal dependencies while modeling the continuous-time dynamics of battery degradation. The model is further evaluated using transfer learning across different learning strategies and target data partitioning scenarios. Results indicate that the model maintains robust performance, even when fine-tuned on limited target data. Experimental results on two publicly available large-scale datasets demonstrate that the proposed method outperforms a baseline deep learning approach and machine learning techniques, achieving an RMSE of 101.59, highlighting its strong potential for real-world RUL prediction applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models</title>
<link>https://arxiv.org/abs/2505.16670</link>
<guid>https://arxiv.org/abs/2505.16670</guid>
<content:encoded><![CDATA[
arXiv:2505.16670v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive capabilities across a wide range of applications, but their ever-increasing size and resource demands make them vulnerable to inference cost attacks, where attackers induce victim LLMs to generate the longest possible output content. In this paper, we revisit existing inference cost attacks and reveal that these methods can hardly produce large-scale malicious effects since they are self-targeting, where attackers are also the users and therefore have to execute attacks solely through the inputs, whose generated content will be charged by LLMs and can only directly influence themselves. Motivated by these findings, this paper introduces a new type of inference cost attacks (dubbed 'bit-flip inference cost attack') that target the victim model itself rather than its inputs. Specifically, we design a simple yet effective method (dubbed 'BitHydra') to effectively flip critical bits of model parameters. This process is guided by a loss function designed to suppress  token's probability with an efficient critical bit search algorithm, thus explicitly defining the attack objective and enabling effective optimization. We evaluate our method on 11 LLMs ranging from 1.5B to 14B parameters under both int8 and float16 settings. Experimental results demonstrate that with just 4 search samples and as few as 3 bit flips, BitHydra can force 100% of test prompts to reach the maximum generation length (e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its efficiency, scalability, and strong transferability across unseen inputs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO</title>
<link>https://arxiv.org/abs/2505.16673</link>
<guid>https://arxiv.org/abs/2505.16673</guid>
<content:encoded><![CDATA[
arXiv:2505.16673v1 Announce Type: cross 
Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds</title>
<link>https://arxiv.org/abs/2505.16679</link>
<guid>https://arxiv.org/abs/2505.16679</guid>
<content:encoded><![CDATA[
arXiv:2505.16679v1 Announce Type: cross 
Abstract: Traditional methods for 3D object compression operate only on structural information within the object vertices, polygons, and textures. These methods are effective at compression rates up to 10x for standard object sizes but quickly deteriorate at higher compression rates with texture artifacts, low-polygon counts, and mesh gaps. In contrast, semantic compression ignores structural information and operates directly on the core concepts to push to extreme levels of compression. In addition, it uses natural language as its storage format, which makes it natively human-readable and a natural fit for emerging applications built around large-scale, collaborative projects within augmented and virtual reality. It deprioritizes structural information like location, size, and orientation and predicts the missing information with state-of-the-art deep generative models. In this work, we construct a pipeline for 3D semantic compression from public generative models and explore the quality-compression frontier for 3D object compression. We apply this pipeline to achieve rates as high as 105x for 3D objects taken from the Objaverse dataset and show that semantic compression can outperform traditional methods in the important quality-preserving region around 100x compression.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator</title>
<link>https://arxiv.org/abs/2505.16690</link>
<guid>https://arxiv.org/abs/2505.16690</guid>
<content:encoded><![CDATA[
arXiv:2505.16690v1 Announce Type: cross 
Abstract: Post-training of large language models is essential for adapting pre-trained language models (PLMs) to align with human preferences and downstream tasks. While PLMs typically exhibit well-calibrated confidence, post-trained language models (PoLMs) often suffer from over-confidence, assigning high confidence to both correct and incorrect outputs, which can undermine reliability in critical applications. A major obstacle in calibrating PoLMs is the scarcity of labeled data for individual downstream tasks. To address this, we propose Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence calibration. Our method is motivated by the under-confidence issue caused by prediction disagreement between the PLM and PoLM while aligning their confidence via temperature scaling. Theoretically, the PLM's confidence underestimates PoLM's prediction accuracy on disagreement examples, causing a larger $\tau$ and producing under-confident predictions. DACA mitigates this by selectively using only agreement examples for calibration, effectively decoupling the influence of disagreement. In this manner, our method avoids an overly large $\tau$ in temperature scaling caused by disagreement examples, improving calibration performance. Extensive experiments demonstrate the effectiveness of our method, improving the average ECE of open-sourced and API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion</title>
<link>https://arxiv.org/abs/2505.16691</link>
<guid>https://arxiv.org/abs/2505.16691</guid>
<content:encoded><![CDATA[
arXiv:2505.16691v1 Announce Type: cross 
Abstract: Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence</title>
<link>https://arxiv.org/abs/2505.16694</link>
<guid>https://arxiv.org/abs/2505.16694</guid>
<content:encoded><![CDATA[
arXiv:2505.16694v1 Announce Type: cross 
Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where predictions are made adaptively based on context. While prior work links induction heads to ICL through a sudden jump in accuracy, this can only account for ICL when the answer is included within the context. However, an important property of practical ICL in large language models is the ability to meta-learn how to solve tasks from context, rather than just copying answers from context; how such an ability is obtained during training is largely unexplored. In this paper, we experimentally clarify how such meta-learning ability is acquired by analyzing the dynamics of the model's circuit during training. Specifically, we extend the copy task from previous research into an In-Context Meta Learning setting, where models must infer a task from examples to answer queries. Interestingly, in this setting, we find that there are multiple phases in the process of acquiring such abilities, and that a unique circuit emerges in each phase, contrasting with the single-phases change in induction heads. The emergence of such circuits can be related to several phenomena known in large language models, and our analysis lead to a deeper understanding of the source of the transformer's ICL ability.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations</title>
<link>https://arxiv.org/abs/2505.16705</link>
<guid>https://arxiv.org/abs/2505.16705</guid>
<content:encoded><![CDATA[
arXiv:2505.16705v1 Announce Type: cross 
Abstract: Concept bottleneck models (CBMs) ensure interpretability by decomposing predictions into human interpretable concepts. Yet the annotations used for training CBMs that enable this transparency are often noisy, and the impact of such corruption is not well understood. In this study, we present the first systematic study of noise in CBMs and show that even moderate corruption simultaneously impairs prediction performance, interpretability, and the intervention effectiveness. Our analysis identifies a susceptible subset of concepts whose accuracy declines far more than the average gap between noisy and clean supervision and whose corruption accounts for most performance loss. To mitigate this vulnerability we propose a two-stage framework. During training, sharpness-aware minimization stabilizes the learning of noise-sensitive concepts. During inference, where clean labels are unavailable, we rank concepts by predictive entropy and correct only the most uncertain ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and extensive ablations elucidate why sharpness-aware training confers robustness and why uncertainty reliably identifies susceptible concepts, providing a principled basis that preserves both interpretability and resilience in the presence of noise.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Long-Context LLMs Efficiently via Chunk-wise Optimization</title>
<link>https://arxiv.org/abs/2505.16710</link>
<guid>https://arxiv.org/abs/2505.16710</guid>
<content:encoded><![CDATA[
arXiv:2505.16710v1 Announce Type: cross 
Abstract: While long-context large language models (LLMs) exhibit remarkable document processing capabilities, their prohibitively high training costs often hinder customized applications. To mitigate this issue, we propose \textit{Sequential Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that partitions lengthy inputs into manageable chunks. Each chunk independently constructs its computational graph and performs localized backpropagation, ensuring that only one chunk's forward activations are stored in memory. Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization} (SpaCO), which reduces computational overhead by selectively propagating gradients to specific chunks and incorporates a carefully designed compensation factor to ensure unbiased gradient estimation. SpaCO decouples the computational cost of backpropagation from the context length, enabling training time to gradually converge to inference time as sequences become longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer substantial practical benefits. For example, when fine-tuning an 8B model with LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to 16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up to 3x faster than SeCO under the same experimental setup. These innovations provide new insights into optimizing long-context models, making them more accessible for practical applications. We have open-sourced the code at \href{https://github.com/wenhaoli-xmu/seco}{here}.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification</title>
<link>https://arxiv.org/abs/2505.16722</link>
<guid>https://arxiv.org/abs/2505.16722</guid>
<content:encoded><![CDATA[
arXiv:2505.16722v1 Announce Type: cross 
Abstract: As large language models (LLMs) become increasingly prevalent in global applications, ensuring that they are toxicity-free across diverse linguistic contexts remains a critical challenge. We explore "Cross-lingual Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling detoxification capabilities to transfer between high and low-resource languages across different script families. We analyze cross-lingual detoxification's effectiveness through 504 extensive settings to evaluate toxicity reduction in cross-distribution settings with limited data and investigate how mitigation impacts model performance on non-toxic tasks, revealing trade-offs between safety and knowledge preservation. Our code and dataset are publicly available at https://github.com/himanshubeniwal/Breaking-mBad.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Brainwave Modeling with a Codebook-Based Foundation Model</title>
<link>https://arxiv.org/abs/2505.16724</link>
<guid>https://arxiv.org/abs/2505.16724</guid>
<content:encoded><![CDATA[
arXiv:2505.16724v1 Announce Type: cross 
Abstract: Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Monte Carlo for Policy Optimization in Continuous POMDPs</title>
<link>https://arxiv.org/abs/2505.16732</link>
<guid>https://arxiv.org/abs/2505.16732</guid>
<content:encoded><![CDATA[
arXiv:2505.16732v1 Announce Type: cross 
Abstract: Optimal decision-making under partial observability requires agents to balance reducing uncertainty (exploration) against pursuing immediate objectives (exploitation). In this paper, we introduce a novel policy optimization framework for continuous partially observable Markov decision processes (POMDPs) that explicitly addresses this challenge. Our method casts policy learning as probabilistic inference in a non-Markovian Feynman--Kac model that inherently captures the value of information gathering by anticipating future observations, without requiring extrinsic exploration bonuses or handcrafted heuristics. To optimize policies under this model, we develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently estimates a history-dependent policy gradient under samples from the optimal trajectory distribution induced by the POMDP. We demonstrate the effectiveness of our algorithm across standard continuous POMDP benchmarks, where existing methods struggle to act under uncertainty.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting</title>
<link>https://arxiv.org/abs/2505.16735</link>
<guid>https://arxiv.org/abs/2505.16735</guid>
<content:encoded><![CDATA[
arXiv:2505.16735v1 Announce Type: cross 
Abstract: For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic and text embeddings are typically compared at either the phoneme or utterance level. To facilitate this, we optimize acoustic and text encoders using deep metric learning (DML), enabling direct comparison of multi-modal embeddings in a shared embedding space. However, the inherent heterogeneity between audio and text modalities presents a significant challenge. To address this, we propose Modality Adversarial Learning (MAL), which reduces the domain gap in heterogeneous modality representations. Specifically, we train a modality classifier adversarially to encourage both encoders to generate modality-invariant embeddings. Additionally, we apply DML to achieve phoneme-level alignment between audio and text, and conduct comprehensive comparisons across various DML objectives. Experiments on the Wall Street Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization</title>
<link>https://arxiv.org/abs/2505.16737</link>
<guid>https://arxiv.org/abs/2505.16737</guid>
<content:encoded><![CDATA[
arXiv:2505.16737v1 Announce Type: cross 
Abstract: The significant progress of large language models (LLMs) has led to remarkable achievements across numerous applications. However, their ability to generate harmful content has sparked substantial safety concerns. Despite the implementation of safety alignment techniques during the pre-training phase, recent research indicates that fine-tuning LLMs on adversarial or even benign data can inadvertently compromise their safety. In this paper, we re-examine the fundamental issue of why fine-tuning on non-harmful data still results in safety degradation. We introduce a safety-aware probing (SAP) optimization framework designed to mitigate the safety risks of fine-tuning LLMs. Specifically, SAP incorporates a safety-aware probe into the gradient propagation process, mitigating the model's risk of safety degradation by identifying potential pitfalls in gradient directions, thereby enhancing task-specific performance while successfully preserving model safety. Our extensive experimental results demonstrate that SAP effectively reduces harmfulness below the original fine-tuned model and achieves comparable test loss to standard fine-tuning methods. Our code is available at https://github.com/ChengcanWu/SAP.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP</title>
<link>https://arxiv.org/abs/2505.16740</link>
<guid>https://arxiv.org/abs/2505.16740</guid>
<content:encoded><![CDATA[
arXiv:2505.16740v1 Announce Type: cross 
Abstract: We explore the use of conformal prediction to provide statistical uncertainty guarantees for runway detection in vision-based landing systems (VLS). Using fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal prediction to quantify localization reliability under user-defined risk levels. We also introduce Conformal mean Average Precision (C-mAP), a novel metric aligning object detection performance with conformal guarantees. Our results show that conformal prediction can improve the reliability of runway detection by quantifying uncertainty in a statistically sound way, increasing safety on-board and paving the way for certification of ML system in the aerospace domain.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning</title>
<link>https://arxiv.org/abs/2505.16743</link>
<guid>https://arxiv.org/abs/2505.16743</guid>
<content:encoded><![CDATA[
arXiv:2505.16743v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) present significant computational and memory challenges due to their extensive size, making pruning essential for their efficient deployment. Existing one-shot pruning methods often apply uniform sparsity constraints across layers or within each layer, resulting in suboptimal performance, especially at high sparsity ratios. This work introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel approach that applies varying sparsity ratios to individual output dimensions (rows) within each layer. TRIM employs an iterative adjustment process guided by quality metrics to optimize dimension-wise sparsity allocation, focusing on reducing variance in quality retention across outputs to preserve critical information. TRIM can be seamlessly integrated with existing layer-wise pruning strategies. Our evaluations on perplexity and zero-shot tasks across diverse LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that TRIM achieves new state-of-the-art results and enhances stability. For instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and over 90% for OPT-13B compared to baseline methods. We conclude that fine-grained, dimension-wise sparsity adaptation is crucial for pushing the limits of extreme LLM compression. Code available at: https://github.com/flobk/TRIM
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation</title>
<link>https://arxiv.org/abs/2505.16752</link>
<guid>https://arxiv.org/abs/2505.16752</guid>
<content:encoded><![CDATA[
arXiv:2505.16752v1 Announce Type: cross 
Abstract: We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream architecture designed for recommendation systems. DFGR integrates innovative interaction patterns between real and fake flows within the QKV modules of the self-attention mechanism, enhancing both training and inference efficiency. This approach effectively addresses a key limitation observed in Meta's proposed HSTU generative recommendation approach, where heterogeneous information volumes are mapped into identical vector spaces, leading to training instability. Unlike traditional recommendation models, DFGR only relies on user history behavior sequences and minimal attribute information, eliminating the need for extensive manual feature engineering. Comprehensive evaluations on open-source and industrial datasets reveal DFGR's superior performance compared to established baselines such as DIN, DCN, DIEN, and DeepFM. We also investigate optimal parameter allocation strategies under computational constraints, establishing DFGR as an efficient and effective next-generation generate ranking paradigm.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques</title>
<link>https://arxiv.org/abs/2505.16765</link>
<guid>https://arxiv.org/abs/2505.16765</guid>
<content:encoded><![CDATA[
arXiv:2505.16765v1 Announce Type: cross 
Abstract: Jailbreak attacks pose a serious threat to large language models (LLMs) by bypassing built-in safety mechanisms and leading to harmful outputs. Studying these attacks is crucial for identifying vulnerabilities and improving model security. This paper presents a systematic survey of jailbreak methods from the novel perspective of stealth. We find that existing attacks struggle to simultaneously achieve toxic stealth (concealing toxic content) and linguistic stealth (maintaining linguistic naturalness). Motivated by this, we propose StegoAttack, a fully stealthy jailbreak attack that uses steganography to hide the harmful query within benign, semantically coherent text. The attack then prompts the LLM to extract the hidden query and respond in an encrypted manner. This approach effectively hides malicious intent while preserving naturalness, allowing it to evade both built-in and external safety mechanisms. We evaluate StegoAttack on four safety-aligned LLMs from major providers, benchmarking against eight state-of-the-art methods. StegoAttack achieves an average attack success rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%. Its ASR drops by less than 1% even under external detection (e.g., Llama Guard). Moreover, it attains the optimal comprehensive scores on stealth detection metrics, demonstrating both high efficacy and exceptional stealth capabilities. The code is available at https://anonymous.4open.science/r/StegoAttack-Jail66
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis</title>
<link>https://arxiv.org/abs/2505.16773</link>
<guid>https://arxiv.org/abs/2505.16773</guid>
<content:encoded><![CDATA[
arXiv:2505.16773v1 Announce Type: cross 
Abstract: Deep learning has transformed computer vision but relies heavily on large labeled datasets and computational resources. Transfer learning, particularly fine-tuning pretrained models, offers a practical alternative; however, models pretrained on natural image datasets such as ImageNet may fail to capture domain-specific characteristics in medical imaging. This study introduces an unsupervised learning framework that extracts high-value dermatological features instead of relying solely on ImageNet-based pretraining. We employ a Variational Autoencoder (VAE) trained from scratch on a proprietary dermatological dataset, allowing the model to learn a structured and clinically relevant latent space. This self-supervised feature extractor is then compared to an ImageNet-pretrained backbone under identical classification conditions, highlighting the trade-offs between general-purpose and domain-specific pretraining. Our results reveal distinct learning patterns. The self-supervised model achieves a final validation loss of 0.110 (-33.33%), while the ImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting. Accuracy trends confirm this: the self-supervised model improves from 45% to 65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained model reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting gap increasing to +0.060. These findings suggest that while ImageNet pretraining accelerates convergence, it also amplifies overfitting on non-clinically relevant features. In contrast, self-supervised learning achieves steady improvements, stronger generalization, and superior adaptability, underscoring the importance of domain-specific feature extraction in medical imaging.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models</title>
<link>https://arxiv.org/abs/2505.16785</link>
<guid>https://arxiv.org/abs/2505.16785</guid>
<content:encoded><![CDATA[
arXiv:2505.16785v1 Announce Type: cross 
Abstract: Despite providing superior performance, open-source large language models (LLMs) are vulnerable to abusive usage. To address this issue, recent works propose LLM fingerprinting methods to identify the specific source LLMs behind suspect applications. However, these methods fail to provide stealthy and robust fingerprint verification. In this paper, we propose a novel LLM fingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT) as the fingerprint of an LLM. CoTSRF first collects the responses from the source LLM by querying it with crafted CoT queries. Then, it applies contrastive learning to train a CoT extractor that extracts the CoT feature (i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint verification by comparing the Kullback-Leibler divergence between the CoT features of the source and suspect LLMs against an empirical threshold. Various experiments have been conducted to demonstrate the advantage of our proposed CoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint verification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability</title>
<link>https://arxiv.org/abs/2505.16789</link>
<guid>https://arxiv.org/abs/2505.16789</guid>
<content:encoded><![CDATA[
arXiv:2505.16789v1 Announce Type: cross 
Abstract: As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Misalignment, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity within our experimental datasets. We then evaluate the adversarial performance of these fine-tuned models and assess how dataset factors correlate with attack success rates. Lastly, we explore potential causal links, offering new insights into adversarial defense strategies and highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_misalignment.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Flexible Forward Trajectories for Masked Molecular Diffusion</title>
<link>https://arxiv.org/abs/2505.16790</link>
<guid>https://arxiv.org/abs/2505.16790</guid>
<content:encoded><![CDATA[
arXiv:2505.16790v1 Announce Type: cross 
Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling discrete data, while their potential in molecular generation remains underexplored. In this work, we explore their potential and introduce the surprising result that naively applying standards MDMs severely degrades the performance. We identify the critical cause of this issue as a state-clashing problem-where the forward diffusion of distinct molecules collapse into a common state, resulting in a mixture of reconstruction targets that cannot be learned using typical reverse diffusion process with unimodal predictions. To mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that orchestrates per-element corruption trajectories to avoid collision between distinct molecular graphs. This is achieved through a parameterized noise scheduling network that assigns distinct corruption rates to individual graph elements, i.e., atoms and bonds. Extensive experiments on diverse molecular benchmarks reveal that MELD markedly enhances overall generation quality compared to element-agnostic noise scheduling, increasing the chemical validity of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves state-of-the-art property alignment in conditional generation tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cohort-Based Active Modality Acquisition</title>
<link>https://arxiv.org/abs/2505.16791</link>
<guid>https://arxiv.org/abs/2505.16791</guid>
<content:encoded><![CDATA[
arXiv:2505.16791v1 Announce Type: cross 
Abstract: Real-world machine learning applications often involve data from multiple modalities that must be integrated effectively to make robust predictions. However, in many practical settings, not all modalities are available for every sample, and acquiring additional modalities can be costly. This raises the question: which samples should be prioritized for additional modality acquisition when resources are limited? While prior work has explored individual-level acquisition strategies and training-time active learning paradigms, test-time and cohort-based acquisition remain underexplored despite their importance in many real-world settings. We introduce Cohort-based Active Modality Acquisition (CAMA), a novel test-time setting to formalize the challenge of selecting which samples should receive additional modalities. We derive acquisition strategies that leverage a combination of generative imputation and discriminative modeling to estimate the expected benefit of acquiring missing modalities based on common evaluation metrics. We also introduce upper-bound heuristics that provide performance ceilings to benchmark acquisition strategies. Experiments on common multimodal datasets demonstrate that our proposed imputation-based strategies can more effectively guide the acquisition of new samples in comparison to those relying solely on unimodal information, entropy guidance, and random selections. Our work provides an effective solution for optimizing modality acquisition at the cohort level, enabling better utilization of resources in constrained settings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training</title>
<link>https://arxiv.org/abs/2505.16792</link>
<guid>https://arxiv.org/abs/2505.16792</guid>
<content:encoded><![CDATA[
arXiv:2505.16792v1 Announce Type: cross 
Abstract: Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet their training remains notoriously slow. A recent remedy -- representation alignment (REPA) that matches DiT hidden features to those of a non-generative teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus or even degrades performance later. We trace this failure to a capacity mismatch: once the generative student begins modelling the joint data distribution, the teacher's lower-dimensional embeddings and attention patterns become a straitjacket rather than a guide. We then introduce HASTE (Holistic Alignment with Stage-wise Termination for Efficient training), a two-phase schedule that keeps the help and drops the hindrance. Phase I applies a holistic alignment loss that simultaneously distills attention maps (relational priors) and feature projections (semantic anchors) from the teacher into mid-level layers of the DiT, yielding rapid convergence. Phase II then performs one-shot termination that deactivates the alignment loss, once a simple trigger such as a fixed iteration is hit, freeing the DiT to focus on denoising and exploit its generative capacity. HASTE speeds up training of diverse DiTs without architecture changes. On ImageNet 256X256, it reaches the vanilla SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs, amounting to a 28X reduction in optimization steps. HASTE also improves text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled recipe for efficient diffusion training across various tasks. Our code is available at https://github.com/NUS-HPC-AI-Lab/HASTE .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEED: Speaker Embedding Enhancement Diffusion Model</title>
<link>https://arxiv.org/abs/2505.16798</link>
<guid>https://arxiv.org/abs/2505.16798</guid>
<content:encoded><![CDATA[
arXiv:2505.16798v1 Announce Type: cross 
Abstract: A primary challenge when deploying speaker recognition systems in real-world applications is performance degradation caused by environmental mismatch. We propose a diffusion-based method that takes speaker embeddings extracted from a pre-trained speaker recognition model and generates refined embeddings. For training, our approach progressively adds Gaussian noise to both clean and noisy speaker embeddings extracted from clean and noisy speech, respectively, via forward process of a diffusion model, and then reconstructs them to clean embeddings in the reverse process. While inferencing, all embeddings are regenerated via diffusion process. Our method needs neither speaker label nor any modification to the existing speaker recognition pipeline. Experiments on evaluation sets simulating environment mismatch scenarios show that our method can improve recognition accuracy by up to 19.6% over baseline models while retaining performance on conventional scenarios. We publish our code here https://github.com/kaistmm/seed-pytorch
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents</title>
<link>https://arxiv.org/abs/2505.16801</link>
<guid>https://arxiv.org/abs/2505.16801</guid>
<content:encoded><![CDATA[
arXiv:2505.16801v1 Announce Type: cross 
Abstract: Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed framework's agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p=0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed framework's capability to produce meaningful data for the evaluation of procedurally generated content in SGs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Reservoir Computing with Physical Neuromorphic Networks</title>
<link>https://arxiv.org/abs/2505.16813</link>
<guid>https://arxiv.org/abs/2505.16813</guid>
<content:encoded><![CDATA[
arXiv:2505.16813v1 Announce Type: cross 
Abstract: Reservoir Computing (RC) with physical systems requires an understanding of the underlying structure and internal dynamics of the specific physical reservoir. In this study, physical nano-electronic networks with neuromorphic dynamics are investigated for their use as physical reservoirs in an RC framework. These neuromorphic networks operate as dynamic reservoirs, with node activities in general coupled to the edge dynamics through nonlinear nano-electronic circuit elements, and the reservoir outputs influenced by the underlying network connectivity structure. This study finds that networks with varying degrees of sparsity generate more useful nonlinear temporal outputs for dynamic RC compared to dense networks. Dynamic RC is also tested on an autonomous multivariate chaotic time series prediction task with networks of varying densities, which revealed the importance of network sparsity in maintaining network activity and overall dynamics, that in turn enabled the learning of the chaotic Lorenz63 system's attractor behavior.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2505.16831</link>
<guid>https://arxiv.org/abs/2505.16831</guid>
<content:encoded><![CDATA[
arXiv:2505.16831v1 Announce Type: cross 
Abstract: Unlearning in large language models (LLMs) is intended to remove the influence of specific data, yet current evaluations rely heavily on token-level metrics such as accuracy and perplexity. We show that these metrics can be misleading: models often appear to forget, but their original behavior can be rapidly restored with minimal fine-tuning, revealing that unlearning may obscure information rather than erase it. To diagnose this phenomenon, we introduce a representation-level evaluation framework using PCA-based similarity and shift, centered kernel alignment, and Fisher information. Applying this toolkit across six unlearning methods, three domains (text, code, math), and two open-source LLMs, we uncover a critical distinction between reversible and irreversible forgetting. In reversible cases, models suffer token-level collapse yet retain latent features; in irreversible cases, deeper representational damage occurs. We further provide a theoretical account linking shallow weight perturbations near output layers to misleading unlearning signals, and show that reversibility is modulated by task type and hyperparameters. Our findings reveal a fundamental gap in current evaluation practices and establish a new diagnostic foundation for trustworthy unlearning in LLMs. We provide a unified toolkit for analyzing LLM representation changes under unlearning and relearning: https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis</title>
<link>https://arxiv.org/abs/2505.16834</link>
<guid>https://arxiv.org/abs/2505.16834</guid>
<content:encoded><![CDATA[
arXiv:2505.16834v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations that lack high-quality training trajectories or suffer from the distributional mismatches in simulated environments and prohibitive computational costs for real-world deployment. This paper introduces SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap through strategic data engineering rather than complex training paradigms. Our approach synthesizes high-quality training data by simulating realistic user interactions in live web search environments, coupled with a multi-criteria curation strategy that optimizes the diversity and quality of input and output side. Experiments on five benchmarks across diverse domains demonstrate that SFT on only 871 curated samples yields significant improvements over RL-based baselines. Our work establishes SFT as a viable pathway by systematically addressing the data-scarce bottleneck, offering practical insights for efficient deep search systems. Our code is available at https://github.com/RUCAIBox/SimpleDeepSearcher.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning</title>
<link>https://arxiv.org/abs/2505.16836</link>
<guid>https://arxiv.org/abs/2505.16836</guid>
<content:encoded><![CDATA[
arXiv:2505.16836v1 Announce Type: cross 
Abstract: The rapid spread of multimodal misinformation on social media has raised growing concerns, while research on video misinformation detection remains limited due to the lack of large-scale, diverse datasets. Existing methods often overfit to rigid templates and lack deep reasoning over deceptive content. To address these challenges, we introduce FakeVV, a large-scale benchmark comprising over 100,000 video-text pairs with fine-grained, interpretable annotations. In addition, we further propose Fact-R1, a novel framework that integrates deep reasoning with collaborative rule-based reinforcement learning. Fact-R1 is trained through a three-stage process: (1) misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference alignment via Direct Preference Optimization (DPO), and (3) Group Relative Policy Optimization (GRPO) using a novel verifiable reward function. This enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those observed in advanced text-based reinforcement learning systems, but in the more complex multimodal misinformation setting. Our work establishes a new paradigm for misinformation detection, bridging large-scale video understanding, reasoning-guided alignment, and interpretable verification.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate</title>
<link>https://arxiv.org/abs/2505.16845</link>
<guid>https://arxiv.org/abs/2505.16845</guid>
<content:encoded><![CDATA[
arXiv:2505.16845v1 Announce Type: cross 
Abstract: Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only</title>
<link>https://arxiv.org/abs/2505.16856</link>
<guid>https://arxiv.org/abs/2505.16856</guid>
<content:encoded><![CDATA[
arXiv:2505.16856v1 Announce Type: cross 
Abstract: Improving the performance of pre-trained policies through online reinforcement learning (RL) is a critical yet challenging topic. Existing online RL fine-tuning methods require continued training with offline pretrained Q-functions for stability and performance. However, these offline pretrained Q-functions commonly underestimate state-action pairs beyond the offline dataset due to the conservatism in most offline RL methods, which hinders further exploration when transitioning from the offline to the online setting. Additionally, this requirement limits their applicability in scenarios where only pre-trained policies are available but pre-trained Q-functions are absent, such as in imitation learning (IL) pre-training. To address these challenges, we propose a method for efficient online RL fine-tuning using solely the offline pre-trained policy, eliminating reliance on pre-trained Q-functions. We introduce PORL (Policy-Only Reinforcement Learning Fine-Tuning), which rapidly initializes the Q-function from scratch during the online phase to avoid detrimental pessimism. Our method not only achieves competitive performance with advanced offline-to-online RL algorithms and online RL approaches that leverage data or policies prior, but also pioneers a new path for directly fine-tuning behavior cloning (BC) policies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCAL: Adapting Graph Models to Evolving Domain Shifts</title>
<link>https://arxiv.org/abs/2505.16860</link>
<guid>https://arxiv.org/abs/2505.16860</guid>
<content:encoded><![CDATA[
arXiv:2505.16860v1 Announce Type: cross 
Abstract: This paper addresses the challenge of graph domain adaptation on evolving, multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation methods are confined to single-step adaptation, making them ineffective in handling continuous domain shifts and prone to catastrophic forgetting. This paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed to enhance model sustainability and adaptability across various graph domains. GCAL employs a bilevel optimization strategy. The "adapt" phase uses an information maximization approach to fine-tune the model with new graph domains while re-adapting past memories to mitigate forgetting. Concurrently, the "generate memory" phase, guided by a theoretical lower bound derived from information bottleneck theory, involves a variational memory graph generation module to condense original graphs into memories. Extensive experimental evaluations demonstrate that GCAL substantially outperforms existing methods in terms of adaptability and knowledge retention.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-ConBench: Text-to-Image Benchmark for Continual Post-training</title>
<link>https://arxiv.org/abs/2505.16875</link>
<guid>https://arxiv.org/abs/2505.16875</guid>
<content:encoded><![CDATA[
arXiv:2505.16875v1 Announce Type: cross 
Abstract: Continual post-training adapts a single text-to-image diffusion model to learn new tasks without incurring the cost of separate models, but naive post-training causes forgetting of pretrained knowledge and undermines zero-shot compositionality. We observe that the absence of a standardized evaluation protocol hampers related research for continual post-training. To address this, we introduce T2I-ConBench, a unified benchmark for continual post-training of text-to-image models. T2I-ConBench focuses on two practical scenarios, item customization and domain enhancement, and analyzes four dimensions: (1) retention of generality, (2) target-task performance, (3) catastrophic forgetting, and (4) cross-task generalization. It combines automated metrics, human-preference modeling, and vision-language QA for comprehensive assessment. We benchmark ten representative methods across three realistic task sequences and find that no approach excels on all fronts. Even joint "oracle" training does not succeed for every task, and cross-task generalization remains unsolved. We release all datasets, code, and evaluation tools to accelerate research in continual post-training for text-to-image models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASTILLO: Characterizing Response Length Distributions of Large Language Models</title>
<link>https://arxiv.org/abs/2505.16881</link>
<guid>https://arxiv.org/abs/2505.16881</guid>
<content:encoded><![CDATA[
arXiv:2505.16881v1 Announce Type: cross 
Abstract: Efficiently managing compute resources for Large Language Model (LLM) inference remains challenging due to the inherently stochastic and variable lengths of autoregressive text generation. Accurately estimating response lengths in advance enables proactive resource allocation, yet existing approaches either bias text generation towards certain lengths or rely on assumptions that ignore model- and prompt-specific variability. We introduce CASTILLO, a dataset characterizing response length distributions across 13 widely-used open-source LLMs evaluated on seven distinct instruction-following corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10 independent completions using fixed decoding hyper-parameters, record the token length of each response, and publish summary statistics (mean, std-dev, percentiles), along with the shortest and longest completions, and the exact generation settings. Our analysis reveals significant inter- and intra-model variability in response lengths (even under identical generation settings), as well as model-specific behaviors and occurrences of partial text degeneration in only subsets of responses. CASTILLO enables the development of predictive models for proactive scheduling and provides a systematic framework for analyzing model-specific generation behaviors. We publicly release the dataset and code to foster research at the intersection of generative language modeling and systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?</title>
<link>https://arxiv.org/abs/2505.16886</link>
<guid>https://arxiv.org/abs/2505.16886</guid>
<content:encoded><![CDATA[
arXiv:2505.16886v1 Announce Type: cross 
Abstract: With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLM's reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework</title>
<link>https://arxiv.org/abs/2505.16888</link>
<guid>https://arxiv.org/abs/2505.16888</guid>
<content:encoded><![CDATA[
arXiv:2505.16888v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced many applications, but are also known to be vulnerable to adversarial attacks. In this work, we introduce a novel security threat: hijacking AI-human conversations by manipulating LLMs' system prompts to produce malicious answers only to specific targeted questions (e.g., "Who should I vote for US President?", "Are Covid vaccines safe?"), while behaving benignly on others. This attack is detrimental as it can enable malicious actors to exercise large-scale information manipulation by spreading harmful but benign-looking system prompts online. To demonstrate such an attack, we develop CAIN, an algorithm that can automatically curate such harmful system prompts for a specific target question in a black-box setting or without the need to access the LLM's parameters. Evaluated on both open-source and commercial LLMs, CAIN demonstrates significant adversarial impact. In untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves up to 40% F1 degradation on targeted questions while preserving high accuracy on benign inputs. For targeted attacks or forcing LLMs to output specific harmful answers, CAIN achieves over 70% F1 scores on these targeted responses with minimal impact on benign questions. Our results highlight the critical need for enhanced robustness measures to safeguard the integrity and safety of LLMs in real-world applications. All source code will be publicly available.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aligned Protein Language Model</title>
<link>https://arxiv.org/abs/2505.16896</link>
<guid>https://arxiv.org/abs/2505.16896</guid>
<content:encoded><![CDATA[
arXiv:2505.16896v1 Announce Type: cross 
Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but lack the structural knowledge essential for many biological applications. To address this, we integrate structural insights from pre-trained protein graph neural networks (pGNNs) into pLMs through a latent-level contrastive learning task. This task aligns residue representations from pLMs with those from pGNNs across multiple proteins, enriching pLMs with inter-protein structural knowledge. Additionally, we incorporate a physical-level task that infuses intra-protein structural knowledge by optimizing pLMs to predict structural tokens. The proposed dual-task framework effectively incorporates both inter-protein and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module, which uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method to the state-of-the-art ESM2 and AMPLIFY results in notable performance gains across a wide range of tasks, including a 12.7% increase in ESM2 contact prediction. The data, code, and resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation</title>
<link>https://arxiv.org/abs/2505.16911</link>
<guid>https://arxiv.org/abs/2505.16911</guid>
<content:encoded><![CDATA[
arXiv:2505.16911v1 Announce Type: cross 
Abstract: We introduce a new paradigm for active sound modification: Active Speech Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on suppressing external interference, ASE goes further by actively shaping the speech signal -- both attenuating unwanted noise components and amplifying speech-relevant frequencies -- to improve intelligibility and perceptual quality. To enable this, we propose a novel Transformer-Mamba-based architecture, along with a task-specific loss function designed to jointly optimize interference suppression and signal enrichment. Our method outperforms existing baselines across multiple speech processing tasks -- including denoising, dereverberation, and declipping -- demonstrating the effectiveness of active, targeted modulation in challenging acoustic environments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?</title>
<link>https://arxiv.org/abs/2505.16915</link>
<guid>https://arxiv.org/abs/2505.16915</guid>
<content:encoded><![CDATA[
arXiv:2505.16915v1 Announce Type: cross 
Abstract: While recent text-to-image (T2I) models show impressive capabilities in synthesizing images from brief descriptions, their performance significantly degrades when confronted with long, detail-intensive prompts required in professional applications. We present DetailMaster, the first comprehensive benchmark specifically designed to evaluate T2I models' systematical abilities to handle extended textual inputs that contain complex compositional requirements. Our benchmark introduces four critical evaluation dimensions: Character Attributes, Structured Character Locations, Multi-Dimensional Scene Attributes, and Explicit Spatial/Interactive Relationships. The benchmark comprises long and detail-rich prompts averaging 284.89 tokens, with high quality validated by expert annotators. Evaluation on 7 general-purpose and 5 long-prompt-optimized T2I models reveals critical performance limitations: state-of-the-art models achieve merely ~50% accuracy in key dimensions like attribute binding and spatial reasoning, while all models showing progressive performance degradation as prompt length increases. Our analysis highlights systemic failures in structural comprehension and detail overload handling, motivating future research into architectures with enhanced compositional reasoning. We open-source the dataset, data curation code, and evaluation tools to advance detail-rich T2I generation and enable broad applications that would otherwise be infeasible due to the lack of a dedicated benchmark.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Principle Discovery for Language Model Self-Improvement</title>
<link>https://arxiv.org/abs/2505.16927</link>
<guid>https://arxiv.org/abs/2505.16927</guid>
<content:encoded><![CDATA[
arXiv:2505.16927v1 Announce Type: cross 
Abstract: When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes guiding model reasoning towards human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ an approximation of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm</title>
<link>https://arxiv.org/abs/2505.16932</link>
<guid>https://arxiv.org/abs/2505.16932</guid>
<content:encoded><![CDATA[
arXiv:2505.16932v1 Announce Type: cross 
Abstract: Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen & Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records</title>
<link>https://arxiv.org/abs/2505.16941</link>
<guid>https://arxiv.org/abs/2505.16941</guid>
<content:encoded><![CDATA[
arXiv:2505.16941v1 Announce Type: cross 
Abstract: Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained on structured electronic health record (EHR) data, even in settings with limited labeled data, a prevalent challenge in healthcare. However, there is little consensus on these models' potential for clinical utility due to the lack of desiderata of comprehensive and meaningful tasks and sufficiently diverse evaluations to characterize the benefit over conventional supervised learning. To address this gap, we propose a suite of clinically meaningful tasks spanning patient outcomes, early prediction of acute and chronic conditions, including desiderata for robust evaluations. We evaluate state-of-the-art foundation models on EHR data consisting of 5 million patients from Columbia University Irving Medical Center (CUMC), a large urban academic medical center in New York City, across 14 clinically relevant tasks. We measure overall accuracy, calibration, and subpopulation performance to surface tradeoffs based on the choice of pre-training, tokenization, and data representation strategies. Our study aims to advance the empirical evaluation of structured EHR foundation models and guide the development of future healthcare foundation models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</title>
<link>https://arxiv.org/abs/2505.16947</link>
<guid>https://arxiv.org/abs/2505.16947</guid>
<content:encoded><![CDATA[
arXiv:2505.16947v1 Announce Type: cross 
Abstract: Despite recent efforts in Large Language Models (LLMs) safety and alignment, current adversarial attacks on frontier LLMs are still able to force harmful generations consistently. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. As these relaxations do not correspond to discrete input tokens, such latent training methods often leave models vulnerable to a diverse set of discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixAT's discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning</title>
<link>https://arxiv.org/abs/2505.16950</link>
<guid>https://arxiv.org/abs/2505.16950</guid>
<content:encoded><![CDATA[
arXiv:2505.16950v1 Announce Type: cross 
Abstract: Despite their impressive capabilities, Large Language Models struggle with generalisation beyond their training distribution, often exhibiting sophisticated pattern interpolation rather than true abstract reasoning (extrapolation). In this work, we approach this limitation through the lens of Information Bottleneck (IB) theory, which posits that model generalisation emerges from an optimal balance between input compression and retention of predictive information in latent representations. We prove using IB theory that decoder-only Transformers are inherently constrained in their ability to form task-optimal sequence representations. We then use this result to demonstrate that periodic global transformation of the internal sequence-level representations (KV cache) is a necessary computational step for improving Transformer generalisation in reasoning tasks. Based on these theoretical insights, we propose a modification to the Transformer architecture, in the form of an additional module that globally rewrites the KV cache at periodic intervals, shifting its capacity away from memorising input prefixes and toward encoding features most useful for predicting future tokens. Our model delivers substantial gains on mathematical reasoning benchmarks, outperforming both vanilla Transformers with up to 3.5x more parameters, as well as heuristic-driven pruning mechanisms for cache compression. Our approach can be seen as a principled generalisation of existing KV-cache compression methods; whereas such methods focus solely on compressing input representations, they often do so at the expense of retaining predictive information, and thus their capabilities are inherently bounded by those of an unconstrained model. This establishes a principled framework to manipulate Transformer memory using information theory, addressing fundamental reasoning limitations that scaling alone cannot overcome.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models</title>
<link>https://arxiv.org/abs/2505.16957</link>
<guid>https://arxiv.org/abs/2505.16957</guid>
<content:encoded><![CDATA[
arXiv:2505.16957v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly equipped with capabilities of real-time web search and integrated with protocols like Model Context Protocol (MCP). This extension could introduce new security vulnerabilities. We present a systematic investigation of LLM vulnerabilities to hidden adversarial prompts through malicious font injection in external resources like webpages, where attackers manipulate code-to-glyph mapping to inject deceptive content which are invisible to users. We evaluate two critical attack scenarios: (1) "malicious content relay" and (2) "sensitive data leakage" through MCP-enabled tools. Our experiments reveal that indirect prompts with injected malicious font can bypass LLM safety mechanisms through external resources, achieving varying success rates based on data sensitivity and prompt design. Our research underscores the urgent need for enhanced security measures in LLM deployments when processing external content.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation</title>
<link>https://arxiv.org/abs/2505.16965</link>
<guid>https://arxiv.org/abs/2505.16965</guid>
<content:encoded><![CDATA[
arXiv:2505.16965v1 Announce Type: cross 
Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental task with broad utility in many downstream applications. In this paper, we propose a graphical model-based unsupervised learning approach, named BP-Seg for efficient text segmentation. Our method not only considers local coherence, capturing the intuition that adjacent sentences are often more related, but also effectively groups sentences that are distant in the text yet semantically similar. This is achieved through belief propagation on the carefully constructed graphical models. Experimental results on both an illustrative example and a dataset with long-form documents demonstrate that our method performs favorably compared to competing approaches.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval</title>
<link>https://arxiv.org/abs/2505.16967</link>
<guid>https://arxiv.org/abs/2505.16967</guid>
<content:encoded><![CDATA[
arXiv:2505.16967v1 Announce Type: cross 
Abstract: Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness -- pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a deeper examination of training data quality, with a particular focus on "false negatives", where relevant passages are incorrectly labeled as irrelevant. We propose a simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o-mini.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
arXiv:2505.16968v1 Announce Type: cross 
Abstract: We introduce \texttt{CASS}, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the \texttt{CASS} family of domain-specific language models, achieving 95\% source translation accuracy and 37.5\% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85\% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on \href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}}, with code at \href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation</title>
<link>https://arxiv.org/abs/2505.16985</link>
<guid>https://arxiv.org/abs/2505.16985</guid>
<content:encoded><![CDATA[
arXiv:2505.16985v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection and segmentation are crucial for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. While prior research has primarily focused on unimodal image data, real-world applications are inherently multimodal, requiring the integration of multiple modalities for improved OOD detection. A key challenge is the lack of supervision signals from unknown data, leading to overconfident predictions on OOD samples. To address this challenge, we propose Feature Mixing, an extremely simple and fast method for multimodal outlier synthesis with theoretical support, which can be further optimized to help the model better distinguish between in-distribution (ID) and OOD data. Feature Mixing is modality-agnostic and applicable to various modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal dataset for OOD segmentation, featuring synthetic OOD objects across diverse scenes and weather conditions. Extensive experiments on SemanticKITTI, nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370 \times$ speedup. Our source code and dataset will be available at https://github.com/mona4399/FeatureMixing.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning</title>
<link>https://arxiv.org/abs/2505.16986</link>
<guid>https://arxiv.org/abs/2505.16986</guid>
<content:encoded><![CDATA[
arXiv:2505.16986v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as intelligent agents capable of solving complex problems. However, effective planning in scenarios involving dependencies between API or tool calls-particularly in multi-turn conversations-remains a significant challenge. To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn conversational dataset specifically designed to capture and manage inter-tool dependencies across diverse domains. T1 enables rigorous evaluation of agents' ability to coordinate tool use across nine distinct domains (4 single domain and 5 multi-domain) with the help of an integrated caching mechanism for both short- and long-term memory, while supporting dynamic replanning-such as deciding whether to recompute or reuse cached results. Beyond facilitating research on tool use and planning, T1 also serves as a benchmark for evaluating the performance of open-source language models. We present results powered by T1-Agent, highlighting their ability to plan and reason in complex, tool-dependent scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.16988</link>
<guid>https://arxiv.org/abs/2505.16988</guid>
<content:encoded><![CDATA[
arXiv:2505.16988v1 Announce Type: cross 
Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential in enhancing single LLMs to address complex and diverse tasks in practical applications. Despite considerable advancements, the field lacks a unified codebase that consolidates existing methods, resulting in redundant re-implementation efforts, unfair comparisons, and high entry barriers for researchers. To address these challenges, we introduce MASLab, a unified, comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab integrates over 20 established methods across multiple domains, each rigorously validated by comparing step-by-step outputs with its official implementation. (2) MASLab provides a unified environment with various benchmarks for fair comparisons among methods, ensuring consistent inputs and standardized evaluation protocols. (3) MASLab implements methods within a shared streamlined structure, lowering the barriers for understanding and extension. Building on MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models, offering researchers a clear and comprehensive view of the current landscape of MAS methods. MASLab will continue to evolve, tracking the latest developments in the field, and invite contributions from the broader open-source community.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning</title>
<link>https://arxiv.org/abs/2505.16994</link>
<guid>https://arxiv.org/abs/2505.16994</guid>
<content:encoded><![CDATA[
arXiv:2505.16994v1 Announce Type: cross 
Abstract: Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \name\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \name, showing relative improvements of 68.67\% in Hit@5 and 45.21\% in NDCG@20. Code available at https://github.com/YRYangang/RRec.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?</title>
<link>https://arxiv.org/abs/2505.16998</link>
<guid>https://arxiv.org/abs/2505.16998</guid>
<content:encoded><![CDATA[
arXiv:2505.16998v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough performance on complex logical reasoning tasks. Nevertheless, most existing research focuses on employing formal language to guide LLMs to derive reliable reasoning paths, while systematic evaluations of these capabilities are still limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs across various logical reasoning problems utilizing formal languages. From the perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and format of trajectories, our key findings are: 1) Thinking models significantly outperform Instruct models, especially when formal language is employed; 2) All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3) Data with PoT format achieves the best generalization performance across other languages. Additionally, we also curate the formal-relative training data to further enhance the small language models, and the experimental results indicate that a simple rejected fine-tuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance. Our codes and reports are available at https://github.com/jiangjin1999/FormalEval.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association</title>
<link>https://arxiv.org/abs/2505.17002</link>
<guid>https://arxiv.org/abs/2505.17002</guid>
<content:encoded><![CDATA[
arXiv:2505.17002v1 Announce Type: cross 
Abstract: We study the task of learning association between faces and voices, which is gaining interest in the multimodal community lately. These methods suffer from the deliberate crafting of negative mining procedures as well as the reliance on the distant margin parameter. These issues are addressed by learning a joint embedding space in which orthogonality constraints are applied to the fused embeddings of faces and voices. However, embedding spaces of faces and voices possess different characteristics and require spaces to be aligned before fusing them. To this end, we propose a method that accurately aligns the embedding spaces and fuses them with an enhanced gated fusion thereby improving the performance of face-voice association. Extensive experiments on the VoxCeleb dataset reveals the merits of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Diffusion Sampling on Function Spaces with Applications to PDEs</title>
<link>https://arxiv.org/abs/2505.17004</link>
<guid>https://arxiv.org/abs/2505.17004</guid>
<content:encoded><![CDATA[
arXiv:2505.17004v1 Announce Type: cross 
Abstract: We propose a general framework for conditional sampling in PDE-based inverse problems, targeting the recovery of whole solutions from extremely sparse or noisy measurements. This is accomplished by a function-space diffusion model and plug-and-play guidance for conditioning. Our method first trains an unconditional discretization-agnostic denoising model using neural operator architectures. At inference, we refine the samples to satisfy sparse observation data via a gradient-based guidance mechanism. Through rigorous mathematical analysis, we extend Tweedie's formula to infinite-dimensional Hilbert spaces, providing the theoretical foundation for our posterior sampling approach. Our method (FunDPS) accurately captures posterior distributions in function spaces under minimal supervision and severe data scarcity. Across five PDE tasks with only 3% observation, our method achieves an average 32% accuracy improvement over state-of-the-art fixed-resolution diffusion baselines while reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning ensures strong cross-resolution generalizability. To the best of our knowledge, this is the first diffusion-based framework to operate independently of discretization, offering a practical and flexible solution for forward and inverse problems in the context of PDEs. Code is available at https://github.com/neuraloperator/FunDPS
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17005</link>
<guid>https://arxiv.org/abs/2505.17005</guid>
<content:encoded><![CDATA[
arXiv:2505.17005v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Prompt Tuning and In-Context Learning via Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17010</link>
<guid>https://arxiv.org/abs/2505.17010</guid>
<content:encoded><![CDATA[
arXiv:2505.17010v1 Announce Type: cross 
Abstract: Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. In this paper we discuss how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. We support the theory with educational experiments on LSTMs and Transformers, where we compare different versions of prefix-tuning and different weight-tuning methods. We also confirm that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding</title>
<link>https://arxiv.org/abs/2505.17012</link>
<guid>https://arxiv.org/abs/2505.17012</guid>
<content:encoded><![CDATA[
arXiv:2505.17012v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Post-Training for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2505.17016</link>
<guid>https://arxiv.org/abs/2505.17016</guid>
<content:encoded><![CDATA[
arXiv:2505.17016v1 Announce Type: cross 
Abstract: We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</title>
<link>https://arxiv.org/abs/2505.17017</link>
<guid>https://arxiv.org/abs/2505.17017</guid>
<content:encoded><![CDATA[
arXiv:2505.17017v1 Announce Type: cross 
Abstract: Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework</title>
<link>https://arxiv.org/abs/2505.17019</link>
<guid>https://arxiv.org/abs/2505.17019</guid>
<content:encoded><![CDATA[
arXiv:2505.17019v1 Announce Type: cross 
Abstract: Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17022</link>
<guid>https://arxiv.org/abs/2505.17022</guid>
<content:encoded><![CDATA[
arXiv:2505.17022v1 Announce Type: cross 
Abstract: Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2405.13873</link>
<guid>https://arxiv.org/abs/2405.13873</guid>
<content:encoded><![CDATA[
arXiv:2405.13873v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) are often challenged by generating erroneous or hallucinated responses, especially in complex reasoning tasks. Leveraging Knowledge Graphs (KGs) as external knowledge sources has emerged as a viable solution. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this paper, we propose a unified framework, FiDeLiS, designed to improve the factuality of LLM responses by anchoring answers to verifiable reasoning steps retrieved from KGs. To achieve this, we leverage step-wise beam search with a deductive scoring function, allowing the LLM to validate reasoning process step by step, and halt the search once the question is deducible. In addition, we propose a Path-RAG module to pre-select a smaller candidate set for each beam search step, reducing computational costs by narrowing the search space. Extensive experiments show that our method, as a training-free framework, not only improve the performance but also enhance the factuality and interpretability across different benchmarks. Code is released at https://github.com/Y-Sui/FiDeLiS.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</title>
<link>https://arxiv.org/abs/2407.10973</link>
<guid>https://arxiv.org/abs/2407.10973</guid>
<content:encoded><![CDATA[
arXiv:2407.10973v4 Announce Type: replace 
Abstract: Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2409.16635</link>
<guid>https://arxiv.org/abs/2409.16635</guid>
<content:encoded><![CDATA[
arXiv:2409.16635v2 Announce Type: replace 
Abstract: This paper proposes a novel prompting approach, Judgment of Thought (JoT), specifically tailored for binary logical reasoning tasks. Despite advances in prompt engineering, existing approaches still face limitations in handling complex logical reasoning tasks. To address these issues, JoT introduces a multi-agent approach with three specialized roles$\unicode{x2010}$$\unicode{x2010}$$\unicode{x2010}$lawyer, prosecutor, and judge$\unicode{x2010}$$\unicode{x2010}$$\unicode{x2010}$where a high-level model acts as the judge, and lower-level models serve as lawyer and prosecutor to systematically debate and evaluate arguments. Experimental evaluations on benchmarks such as BigBenchHard and Winogrande demonstrate JoT's superior performance compared to existing prompting approaches, achieving notable improvements, including 98\% accuracy in Boolean expressions. Also, our ablation studies validate the critical contribution of each role, iterative refinement loops, and feedback mechanisms. Consequently, JoT significantly enhances accuracy, reliability, and consistency in binary reasoning tasks and shows potential for practical applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Amplification: Large Language Models as Increasingly Biased Media</title>
<link>https://arxiv.org/abs/2410.15234</link>
<guid>https://arxiv.org/abs/2410.15234</guid>
<content:encoded><![CDATA[
arXiv:2410.15234v3 Announce Type: replace 
Abstract: Model collapse, a phenomenon characterized by performance degradation due to iterative training on synthetic data, has been widely studied. However, its implications for bias amplification, the progressive intensification of pre-existing societal biases in Large Language Models (LLMs), remain significantly underexplored, despite the growing influence of LLMs in shaping online discourse. In this paper, we introduce a open, generational, and long-context benchmark specifically designed to measure political bias amplification in LLMs, leveraging sentence continuation tasks derived from a comprehensive dataset of U.S. political news. Our empirical study using GPT-2 reveals consistent and substantial political bias intensification (e.g., right-leaning amplification) over iterative synthetic training cycles. We evaluate three mitigation strategies, Overfitting, Preservation, and Accumulation, and demonstrate that bias amplification persists independently of model collapse, even when the latter is effectively controlled. Furthermore, we propose a mechanistic analysis approach that identifies neurons correlated with specific phenomena during inference through regression and statistical tests. This analysis uncovers largely distinct neuron populations driving bias amplification and model collapse, underscoring fundamentally different underlying mechanisms. Finally, we supplement our empirical findings with theoretical intuition that explains the separate origins of these phenomena, guiding targeted strategies for bias mitigation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facial Expression Analysis and Its Potentials in IoT Systems: A Contemporary Survey</title>
<link>https://arxiv.org/abs/2412.17616</link>
<guid>https://arxiv.org/abs/2412.17616</guid>
<content:encoded><![CDATA[
arXiv:2412.17616v2 Announce Type: replace 
Abstract: Facial expressions convey human emotions and can be categorized into macro-expressions (MaEs) and micro-expressions (MiEs) based on duration and intensity. While MaEs are voluntary and easily recognized, MiEs are involuntary, rapid, and can reveal concealed emotions. The integration of facial expression analysis with Internet-of-Thing (IoT) systems has significant potential across diverse scenarios. IoT-enhanced MaE analysis enables real-time monitoring of patient emotions, facilitating improved mental health care in smart healthcare. Similarly, IoT-based MiE detection enhances surveillance accuracy and threat detection in smart security. Our work aims to provide a comprehensive overview of research progress in facial expression analysis and explores its potential integration with IoT systems. We discuss the distinctions between our work and existing surveys, elaborate on advancements in MaE and MiE analysis techniques across various learning paradigms, and examine their potential applications in IoT. We highlight challenges and future directions for the convergence of facial expression-based technologies and IoT systems, aiming to foster innovation in this domain. By presenting recent developments and practical applications, our work offers a systematic understanding of the ways of facial expression analysis to enhance IoT systems in healthcare, security, and beyond.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization</title>
<link>https://arxiv.org/abs/2502.00691</link>
<guid>https://arxiv.org/abs/2502.00691</guid>
<content:encoded><![CDATA[
arXiv:2502.00691v3 Announce Type: replace 
Abstract: Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness -- the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training.
  While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Integration Strategies in Autonomous Vehicle Prediction and Planning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2502.10477</link>
<guid>https://arxiv.org/abs/2502.10477</guid>
<content:encoded><![CDATA[
arXiv:2502.10477v2 Announce Type: replace 
Abstract: This comprehensive survey examines the integration of knowledge-based approaches in autonomous driving systems, specifically focusing on trajectory prediction and planning. We extensively analyze various methodologies for incorporating domain knowledge, traffic rules, and commonsense reasoning into autonomous driving systems. The survey categorizes and analyzes approaches based on their knowledge representation and integration methods, ranging from purely symbolic to hybrid neuro-symbolic architectures. We examine recent developments in logic programming, foundation models for knowledge representation, reinforcement learning frameworks, and other emerging technologies incorporating domain knowledge. This work systematically reviews recent approaches, identifying key challenges, opportunities, and future research directions in knowledge-enhanced autonomous driving systems. Our analysis reveals emerging trends in the field, including the increasing importance of interpretable AI, the role of formal verification in safety-critical systems, and the potential of hybrid approaches that combine traditional knowledge representation with modern machine learning techniques.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space</title>
<link>https://arxiv.org/abs/2502.12532</link>
<guid>https://arxiv.org/abs/2502.12532</guid>
<content:encoded><![CDATA[
arXiv:2502.12532v3 Announce Type: replace 
Abstract: Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals</title>
<link>https://arxiv.org/abs/2502.16101</link>
<guid>https://arxiv.org/abs/2502.16101</guid>
<content:encoded><![CDATA[
arXiv:2502.16101v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation. In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance. To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.19918</link>
<guid>https://arxiv.org/abs/2502.19918</guid>
<content:encoded><![CDATA[
arXiv:2502.19918v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to \enquote{think about how to think.} Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs contextual multi-armed bandits to iteratively evaluate reasoning progress and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
arXiv:2503.18968v2 Announce Type: replace 
Abstract: In modern medicine, clinical diagnosis relies on the comprehensive analysis of primarily textual and visual data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in large Vision-Language Models (VLMs) and agent-based methods hold great potential for medical diagnosis, thanks to the ability to effectively integrate multi-modal patient data. However, they often provide direct answers and draw empirical-driven conclusions without quantitative analysis, which reduces their reliability and clinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm that follows the diagnosis principle in modern medicine, to decouple the process into sequential components for step-by-step, evidence-based reasoning. Our MedAgent-Pro workflow presents a hierarchical diagnostic structure to mirror this principle, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, an RAG-based agent is designed to retrieve medical guidelines to ensure alignment with clinical standards. For patient-level reasoning, we propose to integrate professional tools such as visual models to enable quantitative assessments. Meanwhile, we propose to verify the reliability of each step to achieve evidence-based diagnosis, enforcing rigorous logical reasoning and a well-founded conclusion. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro to mainstream VLMs, agentic systems and state-of-the-art expert models. Ablation studies and human evaluation by clinical experts further validate its robustness and clinical relevance. Code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation</title>
<link>https://arxiv.org/abs/2503.21322</link>
<guid>https://arxiv.org/abs/2503.21322</guid>
<content:encoded><![CDATA[
arXiv:2503.21322v2 Announce Type: replace 
Abstract: Standard Retrieval-Augmented Generation (RAG) relies on chunk-based retrieval, whereas GraphRAG advances this approach by graph-based knowledge representation. However, existing graph-based RAG approaches are constrained by binary relations, as each edge in an ordinary graph connects only two entities, limiting their ability to represent the n-ary relations (n >= 2) in real-world knowledge. In this work, we propose HyperGraphRAG, a novel hypergraph-based RAG method that represents n-ary relational facts via hyperedges, and consists of knowledge hypergraph construction, retrieval, and generation. Experiments across medicine, agriculture, computer science, and law demonstrate that HyperGraphRAG outperforms both standard RAG and previous graph-based RAG methods in answer accuracy, retrieval efficiency, and generation quality.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spark: A System for Scientifically Creative Idea Generation</title>
<link>https://arxiv.org/abs/2504.20090</link>
<guid>https://arxiv.org/abs/2504.20090</guid>
<content:encoded><![CDATA[
arXiv:2504.20090v2 Announce Type: replace 
Abstract: Recently, large language models (LLMs) have shown promising abilities to generate novel research ideas in science, a direction which coincides with many foundational principles in computational creativity (CC). In light of these developments, we present an idea generation system named Spark that couples retrieval-augmented idea generation using LLMs with a reviewer model named Judge trained on 600K scientific reviews from OpenReview. Our work is both a system demonstration and intended to inspire other CC researchers to explore grounding the generation and evaluation of scientific ideas within foundational CC principles. To this end, we release the annotated dataset used to train Judge, inviting other researchers to explore the use of LLMs for idea generation and creative evaluations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models</title>
<link>https://arxiv.org/abs/2505.07581</link>
<guid>https://arxiv.org/abs/2505.07581</guid>
<content:encoded><![CDATA[
arXiv:2505.07581v2 Announce Type: replace 
Abstract: Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoE-World: Compositional World Modeling with Products of Programmatic Experts</title>
<link>https://arxiv.org/abs/2505.10819</link>
<guid>https://arxiv.org/abs/2505.10819</guid>
<content:encoded><![CDATA[
arXiv:2505.10819v2 Announce Type: replace 
Abstract: Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaze-based dual resolution deep imitation learning for high-precision dexterous robot manipulation</title>
<link>https://arxiv.org/abs/2102.01295</link>
<guid>https://arxiv.org/abs/2102.01295</guid>
<content:encoded><![CDATA[
arXiv:2102.01295v4 Announce Type: replace-cross 
Abstract: A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulation tasks using a general-purpose robot manipulator and improves computational efficiency. Data from this and related works are available at: https://sites.google.com/view/multi-task-fine.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based deep imitation learning for dual-arm robot manipulation</title>
<link>https://arxiv.org/abs/2108.00385</link>
<guid>https://arxiv.org/abs/2108.00385</guid>
<content:encoded><![CDATA[
arXiv:2108.00385v3 Announce Type: replace-cross 
Abstract: Deep imitation learning is promising for solving dexterous manipulation tasks because it does not require an environment model and pre-programmed robot behavior. However, its application to dual-arm manipulation tasks remains challenging. In a dual-arm manipulation setup, the increased number of state dimensions caused by the additional robot manipulators causes distractions and results in poor performance of the neural networks. We address this issue using a self-attention mechanism that computes dependencies between elements in a sequential input and focuses on important elements. A Transformer, a variant of self-attention architecture, is applied to deep imitation learning to solve dual-arm manipulation tasks in the real world. The proposed method has been tested on dual-arm manipulation tasks using a real robot. The experimental results demonstrated that the Transformer-based deep imitation learning architecture can attend to the important features among the sensory inputs, therefore reducing distractions and improving manipulation performance when compared with the baseline architecture without the self-attention mechanisms. Data from this and related works are available at: https://sites.google.com/view/multi-task-fine.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Cognitive Computing? An Architecture and State of The Art</title>
<link>https://arxiv.org/abs/2301.00882</link>
<guid>https://arxiv.org/abs/2301.00882</guid>
<content:encoded><![CDATA[
arXiv:2301.00882v2 Announce Type: replace-cross 
Abstract: Cognitive Computing (COC) aims to build highly cognitive machines with low computational resources that respond in real-time. However, scholarly literature shows varying research areas and various interpretations of COC. This calls for a cohesive architecture that delineates the nature of COC. We argue that if Herbert Simon considered the design science is the science of artificial, cognitive systems are the products of cognitive science or 'the newest science of the artificial'. Therefore, building a conceptual basis for COC is an essential step into prospective cognitive computing-based systems. This paper proposes an architecture of COC through analyzing the literature on COC using a myriad of statistical analysis methods. Then, we compare the statistical analysis results with previous qualitative analysis results to confirm our findings. The study also comprehensively surveys the recent research on COC to identify the state of the art and connect the advances in varied research disciplines in COC. The study found that there are three underlaying computing paradigms, Von-Neuman, Neuromorphic Engineering and Quantum Computing, that comprehensively complement the structure of cognitive computation. The research discuss possible applications and open research directions under the COC umbrella.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Neural Networks Using Tensor Networks with Exponentially Fewer Variational Parameters</title>
<link>https://arxiv.org/abs/2305.06058</link>
<guid>https://arxiv.org/abs/2305.06058</guid>
<content:encoded><![CDATA[
arXiv:2305.06058v3 Announce Type: replace-cross 
Abstract: Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including \R{overfitting}, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN's, despite of their specific types (linear, convolutional, \textit{etc}), by encoding them to deep \R{automatically differentiable} tensor network (ADTN) that contains exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, AlextNet, ZFNet and VGG-16) and datasets (MNIST, CIFAR-10 and CIFAR-100). For instance, we compress two linear layers in VGG-16 with approximately $10^{7}$ parameters to two ADTN's with just 424 parameters, improving the testing accuracy on CIFAR-10 from $90.17\%$ to $91.74\%$. We argue that the deep structure of ADTN is an essential reason for the remarkable compression performance of ADTN, compared to existing compression schemes that are mainly based on tensor decompositions/factorization and shallow tensor networks. Our work suggests deep TN as an exceptionally efficient mathematical structure for representing the variational parameters of NN's, which exhibits superior compressibility over the commonly-used matrices and multi-way arrays.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence in digital pathology: a systematic review and meta-analysis of diagnostic test accuracy</title>
<link>https://arxiv.org/abs/2306.07999</link>
<guid>https://arxiv.org/abs/2306.07999</guid>
<content:encoded><![CDATA[
arXiv:2306.07999v3 Announce Type: replace-cross 
Abstract: Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analysis was performed using a bivariate random effects model. 100 studies were identified for inclusion, equating to over 152,000 whole slide images (WSIs) and representing many disease types. Of these, 48 studies were included in the meta-analysis. These studies reported a mean sensitivity of 96.3% (CI 94.1-97.7) and mean specificity of 93.3% (CI 90.5-95.4) for AI. There was substantial heterogeneity in study design and all 100 studies identified for inclusion had at least one area at high or unclear risk of bias. This review provides a broad overview of AI performance across applications in whole slide imaging. However, there is huge variability in study design and available performance data, with details around the conduct of the study and make up of the datasets frequently missing. Overall, AI offers good accuracy when applied to WSIs but requires more rigorous evaluation of its performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2311.17815</link>
<guid>https://arxiv.org/abs/2311.17815</guid>
<content:encoded><![CDATA[
arXiv:2311.17815v2 Announce Type: replace-cross 
Abstract: Given their increasing size and complexity, the need for efficient execution of deep neural networks has become increasingly pressing in the design of heterogeneous High-Performance Computing (HPC) and edge platforms, leading to a wide variety of proposals for specialized deep learning architectures and hardware accelerators. The design of such architectures and accelerators requires a multidisciplinary approach combining expertise from several areas, from machine learning to computer architecture, low-level hardware design, and approximate computing. Several methodologies and tools have been proposed to improve the process of designing accelerators for deep learning, aimed at maximizing parallelism and minimizing data movement to achieve high performance and energy efficiency. This paper critically reviews influential tools and design methodologies for Deep Learning accelerators, offering a wide perspective in this rapidly evolving field. This work complements surveys on architectures and accelerators by covering hardware-software co-design, automated synthesis, domain-specific compilers, design space exploration, modeling, and simulation, providing insights into technical challenges and open research directions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Miscalibrated In-Context Learners</title>
<link>https://arxiv.org/abs/2312.13772</link>
<guid>https://arxiv.org/abs/2312.13772</guid>
<content:encoded><![CDATA[
arXiv:2312.13772v3 Announce Type: replace-cross 
Abstract: When adapting ICL with or without fine-tuning, we are curious about whether the instruction-tuned language model is able to achieve well-calibrated results without suffering from the problem of overconfidence (i.e., miscalibration) considering its strong instruction following ability, especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration. Through extensive controlled experiments, we observe that the miscalibration problem exists across all learning methods in low-resource setups. To achieve simultaneous gain for both in-task performance and calibration, we then study the potential of self-ensembling applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies) to make the predictions more calibrated and have comparable or even better performance. We find that self-ensembling with max probability produces robust and calibrated predictions. Our work reveals the potential calibration problem of using ICL despite the improvements in task performance and sheds light on which learning paradigm to choose. We also provide practical guidelines for choosing learning paradigms depending on whether the data has been seen by the model before and a worthwhile solution via self-ensembling on how to enhance both task performance and calibration of LMs, which we hope could encourage further study.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Sampling Through The Reuse Of Attention Maps In Diffusion Models</title>
<link>https://arxiv.org/abs/2401.01008</link>
<guid>https://arxiv.org/abs/2401.01008</guid>
<content:encoded><![CDATA[
arXiv:2401.01008v4 Announce Type: replace-cross 
Abstract: Text-to-image diffusion models have demonstrated unprecedented capabilities for flexible and realistic image synthesis. Nevertheless, these models rely on a time-consuming sampling procedure, which has motivated attempts to reduce their latency. When improving efficiency, researchers often use the original diffusion model to train an additional network designed specifically for fast image generation. In contrast, our approach seeks to reduce latency directly, without any retraining, fine-tuning, or knowledge distillation. In particular, we find the repeated calculation of attention maps to be costly yet redundant, and instead suggest reusing them during sampling. Our specific reuse strategies are based on ODE theory, which implies that the later a map is reused, the smaller the distortion in the final image. We empirically compare our reuse strategies with few-step sampling procedures of comparable latency, finding that reuse generates images that are closer to those produced by the original high-latency diffusion model.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeMind: Evaluating Large Language Models for Code Reasoning</title>
<link>https://arxiv.org/abs/2402.09664</link>
<guid>https://arxiv.org/abs/2402.09664</guid>
<content:encoded><![CDATA[
arXiv:2402.09664v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been widely used to automate programming tasks. Their capabilities have been evaluated by assessing the quality of generated code through tests or proofs. The extent to which they can reason about code is a critical question revealing important insights about their true capabilities. This paper introduces CodeMind, a framework designed to gauge the code reasoning abilities of LLMs through the following explicit and implicit code reasoning tasks: Independent Execution Reasoning (IER), Specification Reasoning (SR) and Dynamic Semantics Reasoning (DSR). The first evaluates the abilities of LLMs to simulate the execution of given inputs to a code and predict the output (IER). The second assesses the abilities of LLMs to incorporate the simulation of test data in the specification into code generation (SR). Finally, CodeMind evaluates LLMs' abilities to understand overall code semantics only given a specific input/output (DSR). Our extensive evaluation of ten LLMs across four widely used benchmarks using CodeMind shows that LLMs, depending on their size and training strategy, can reason about some dynamic aspects of code. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. We show that these reasoning tasks evaluate LLMs differently, and a comprehensive evaluation of code reasoning requires them all. Finally, we show that the performance of LLMs in bug repair is not correlated with any of the code reasoning tasks, and except for advanced frontier models, other LLMs do not incorporate code reasoning when performing bug repair.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Initialisation and Network Effects in Decentralised Federated Learning</title>
<link>https://arxiv.org/abs/2403.15855</link>
<guid>https://arxiv.org/abs/2403.15855</guid>
<content:encoded><![CDATA[
arXiv:2403.15855v4 Announce Type: replace-cross 
Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on a distributed network of communicating devices while keeping the training data localised on each node. This approach avoids central coordination, enhances data privacy and eliminates the risk of a single point of failure. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices and the learning models' initial conditions. We propose a strategy for uncoordinated initialisation of the artificial neural networks based on the distribution of eigenvector centralities of the underlying communication network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and the choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP</title>
<link>https://arxiv.org/abs/2405.14273</link>
<guid>https://arxiv.org/abs/2405.14273</guid>
<content:encoded><![CDATA[
arXiv:2405.14273v2 Announce Type: replace-cross 
Abstract: We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d$ denotes the dimension of the weights and $k$ the number of iterations, then the error of the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$ increases.We propose a projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate that the proposed method efficiently learns the weights. In particular, we show that there exists a constant $\gamma > 0$ such that the distance between the learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)} \exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal solution is exactly recovered. Furthermore, experiments demonstrate that the proposed method solves the inverse optimization problems of MILP using fewer than $1/7$ the number of MILP calls required by known methods, and converges within a finite number of iterations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection Methods for Operator Learning and Universal Approximation</title>
<link>https://arxiv.org/abs/2406.12264</link>
<guid>https://arxiv.org/abs/2406.12264</guid>
<content:encoded><![CDATA[
arXiv:2406.12264v2 Announce Type: replace-cross 
Abstract: We obtain a new universal approximation theorem for continuous (possibly nonlinear) operators on arbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we introduce and study a method for operator learning in Banach spaces $L^p$ of functions with multiple variables, based on orthogonal projections on polynomial bases. We derive a universal approximation result for operators where we learn a linear projection and a finite dimensional mapping under some additional assumptions. For the case of $p=2$, we give some sufficient conditions for the approximation results to hold. This article serves as the theoretical framework for a deep learning methodology in operator learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities</title>
<link>https://arxiv.org/abs/2407.08112</link>
<guid>https://arxiv.org/abs/2407.08112</guid>
<content:encoded><![CDATA[
arXiv:2407.08112v3 Announce Type: replace-cross 
Abstract: Long sequences occur in abundance within real-world scenarios, hence properly modelling them opens numerous down-stream use-cases. Deep neural networks, however, have often struggled with these for a variety of reasons. Recent advances, both in system engineering as well as model design, have enabled the scaling up of model that are purported to support extended context length. In particular, the state-space and linear recurrent neural network families of models hypothetically can entend to infinite sequence lenth. However, is this too good to be true? We conduct an evaluation to show that while such claims may be sound theoretically, there remain large practical gaps that are empirically observed. In particular, recurrent models still suffer in the same settings as long-context LLMs with attention. We further show that different inductive biases have inconsistent extrapolation capabilities, highlighting the need to further study such paradigms and investigate why long-context models seemingly fail to behave as one might expect.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Foundations for Machine Learning</title>
<link>https://arxiv.org/abs/2407.12288</link>
<guid>https://arxiv.org/abs/2407.12288</guid>
<content:encoded><![CDATA[
arXiv:2407.12288v4 Announce Type: replace-cross 
Abstract: The progress of machine learning over the past decade is undeniable. In retrospect, it is both remarkable and unsettling that this progress was achievable with little to no rigorous theory to guide experimentation. Despite this fact, practitioners have been able to guide their future experimentation via observations from previous large-scale empirical investigations. In this work, we propose a theoretical framework which attempts to provide rigor to existing practices in machine learning. To the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are simple, and provide intuition to guide future investigations across a wide range of learning paradigms. Concretely, we provide a theoretical framework rooted in Bayesian statistics and Shannon's information theory which is general enough to unify the analysis of many phenomena in machine learning. Our framework characterizes the performance of an optimal Bayesian learner as it learns from a stream of experience. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse machine learning settings. Throughout this work, we derive theoretical results and demonstrate their generality by apply them to derive insights specific to settings. These settings range from learning from data which is independently and identically distributed under an unknown distribution, to data which is sequential, to data which exhibits hierarchical structure amenable to meta-learning, and finally to data which is not fully explainable under the learner's beliefs (misspecification). These results are particularly relevant as we strive to understand and overcome increasingly difficult machine learning challenges in this endlessly complex world.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITAL: Interactive Few-Shot Imitation Learning via Visual Human-in-the-Loop Corrections</title>
<link>https://arxiv.org/abs/2407.21244</link>
<guid>https://arxiv.org/abs/2407.21244</guid>
<content:encoded><![CDATA[
arXiv:2407.21244v2 Announce Type: replace-cross 
Abstract: Imitation Learning (IL) has emerged as a powerful approach in robotics, allowing robots to acquire new skills by mimicking human actions. Despite its potential, the data collection process for IL remains a significant challenge due to the logistical difficulties and high costs associated with obtaining high-quality demonstrations. To address these issues, we propose a large-scale data generation from a handful of demonstrations through data augmentation in simulation. Our approach leverages affordable hardware and visual processing techniques to collect demonstrations, which are then augmented to create extensive training datasets for imitation learning. By utilizing both real and simulated environments, along with human-in-the-loop corrections, we enhance the generalizability and robustness of the learned policies. We evaluated our method through several rounds of experiments in both simulated and real-robot settings, focusing on tasks of varying complexity, including bottle collecting, stacking objects, and hammering. Our experimental results validate the effectiveness of our approach in learning robust robot policies from simulated data, significantly improved by human-in-the-loop corrections and real-world data integration. Additionally, we demonstrate the framework's capability to generalize to new tasks, such as setting a drink tray, showcasing its adaptability and potential for handling a wide range of real-world manipulation tasks. A video of the experiments can be found at: https://youtu.be/YeVAMRqRe64?si=R179xDlEGc7nPu8i
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding</title>
<link>https://arxiv.org/abs/2408.15966</link>
<guid>https://arxiv.org/abs/2408.15966</guid>
<content:encoded><![CDATA[
arXiv:2408.15966v3 Announce Type: replace-cross 
Abstract: Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logit Scaling for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2409.01175</link>
<guid>https://arxiv.org/abs/2409.01175</guid>
<content:encoded><![CDATA[
arXiv:2409.01175v2 Announce Type: replace-cross 
Abstract: The safe deployment of machine learning and AI models in open-world settings hinges critically on the ability to detect out-of-distribution (OOD) data accurately, data samples that contrast vastly from what the model was trained with. Current approaches to OOD detection often require further training the model, and/or statistics about the training data which may no longer be accessible. Additionally, many existing OOD detection methods struggle to maintain performance when transferred across different architectures. Our research tackles these issues by proposing a simple, post-hoc method that does not require access to the training data distribution, keeps a trained network intact, and holds strong performance across a variety of architectures. Our method, Logit Scaling (LTS), as the name suggests, simply scales the logits in a manner that effectively distinguishes between in-distribution (ID) and OOD samples. We tested our method on benchmarks across various scales, including CIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14 OOD datasets, as well as 9 model architectures. Overall, we demonstrate state-of-the-art performance, robustness and adaptability across different architectures, paving the way towards a universally applicable solution for advanced OOD detection.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithm Configuration for Structured Pfaffian Settings</title>
<link>https://arxiv.org/abs/2409.04367</link>
<guid>https://arxiv.org/abs/2409.04367</guid>
<content:encoded><![CDATA[
arXiv:2409.04367v4 Announce Type: replace-cross 
Abstract: Data-driven algorithm design automatically adapts algorithms to specific application domains, achieving better performance. In the context of parameterized algorithms, this approach involves tuning the algorithm's hyperparameters using problem instances drawn from the problem distribution of the target application domain. This can be achieved by maximizing empirical utilities that measure the algorithms' performance as a function of their hyperparameters, using problem instances. While empirical evidence supports the effectiveness of data-driven algorithm design, providing theoretical guarantees for several parameterized families remains challenging. This is due to the intricate behaviors of their corresponding utility functions, which typically admit piecewise discontinuous structures. In this work, we present refined frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both distributional and online learning settings. For the distributional learning setting, we introduce the \textit{Pfaffian GJ framework}, an extension of the classical \textit{GJ framework}, that is capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions, our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a \textit{refined piecewise structure}, which automatically translates to learning guarantees using our proposed framework.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models</title>
<link>https://arxiv.org/abs/2409.06280</link>
<guid>https://arxiv.org/abs/2409.06280</guid>
<content:encoded><![CDATA[
arXiv:2409.06280v3 Announce Type: replace-cross 
Abstract: The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns.
  This work proposes MembershipTracker, a practical data auditing tool that can empower ordinary users to reliably detect the unauthorized use of their data in training DL models. We view data auditing through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples.
  MembershipTracker only requires the users to mark a small fraction of data (0.005% to 0.1% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 0% FPR@100% TPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoBiked: A Dataset with Geometric Features and Automated Labeling Techniques to Enable Deep Generative Models in Engineering Design</title>
<link>https://arxiv.org/abs/2409.17045</link>
<guid>https://arxiv.org/abs/2409.17045</guid>
<content:encoded><![CDATA[
arXiv:2409.17045v2 Announce Type: replace-cross 
Abstract: We provide a dataset for enabling Deep Generative Models (DGMs) in engineering design and propose methods to automate data labeling by utilizing large-scale foundation models. GeoBiked is curated to contain 4 355 bicycle images, annotated with structural and technical features and is used to investigate two automated labeling techniques: The utilization of consolidated latent features (Hyperfeatures) from image-generation models to detect geometric correspondences (e.g. the position of the wheel center) in structural images and the generation of diverse text descriptions for structural images. GPT-4o, a vision-language-model (VLM), is instructed to analyze images and produce diverse descriptions aligned with the system-prompt. By representing technical images as Diffusion-Hyperfeatures, drawing geometric correspondences between them is possible. The detection accuracy of geometric points in unseen samples is improved by presenting multiple annotated source images. GPT-4o has sufficient capabilities to generate accurate descriptions of technical images. Grounding the generation only on images leads to diverse descriptions but causes hallucinations, while grounding it on categorical labels restricts the diversity. Using both as input balances creativity and accuracy. Successfully using Hyperfeatures for geometric correspondence suggests that this approach can be used for general point-detection and annotation tasks in technical images. Labeling such images with text descriptions using VLMs is possible, but dependent on the models detection capabilities, careful prompt-engineering and the selection of input information. Applying foundation models in engineering design is largely unexplored. We aim to bridge this gap with a dataset to explore training, finetuning and conditioning DGMs in this field and suggesting approaches to bootstrap foundation models to process technical images.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Class-Incremental Learning using Sample Weighting</title>
<link>https://arxiv.org/abs/2410.01324</link>
<guid>https://arxiv.org/abs/2410.01324</guid>
<content:encoded><![CDATA[
arXiv:2410.01324v2 Announce Type: replace-cross 
Abstract: Model fairness is becoming important in class-incremental learning for Trustworthy AI. While accuracy has been a central focus in class-incremental learning, fairness has been relatively understudied. However, naively using all the samples of the current task for training results in unfair catastrophic forgetting for certain sensitive groups including classes. We theoretically analyze that forgetting occurs if the average gradient vector of the current task data is in an "opposite direction" compared to the average gradient vector of a sensitive group, which means their inner products are negative. We then propose a fair class-incremental learning framework that adjusts the training weights of current task samples to change the direction of the average gradient vector and thus reduce the forgetting of underperforming groups and achieve fairness. For various group fairness measures, we formulate optimization problems to minimize the overall losses of sensitive groups while minimizing the disparities among them. We also show the problems can be solved with linear programming and propose an efficient Fairness-aware Sample Weighting (FSW) algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff results than state-of-the-art approaches on real datasets.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Spoofing Attempts on Language Model Watermarks</title>
<link>https://arxiv.org/abs/2410.02693</link>
<guid>https://arxiv.org/abs/2410.02693</guid>
<content:encoded><![CDATA[
arXiv:2410.02693v2 Announce Type: replace-cross 
Abstract: LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. Despite recent work demonstrating that state-of-the-art schemes are, in fact, vulnerable to spoofing, no prior work has focused on post-hoc methods to discover spoofing attempts. In this work, we for the first time propose a reliable statistical method to distinguish spoofed from genuinely watermarked text, suggesting that current spoofing attacks are less effective than previously thought. In particular, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts and thus demonstrate that a watermark has been spoofed. Our experimental evaluation shows high test power across all learning-based spoofing methods, providing insights into their fundamental limitations and suggesting a way to mitigate this threat. We make all our code available at https://github.com/eth-sri/watermark-spoofing-detection .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permissive Information-Flow Analysis for Large Language Models</title>
<link>https://arxiv.org/abs/2410.03055</link>
<guid>https://arxiv.org/abs/2410.03055</guid>
<content:encoded><![CDATA[
arXiv:2410.03055v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are rapidly becoming commodity components of larger software systems. This poses natural security and privacy problems: poisoned data retrieved from one component can change the model's behavior and compromise the entire system, including coercing the model to spread confidential data to untrusted components. One promising approach is to tackle this problem at the system level via dynamic information flow (aka taint) tracking. Unfortunately, this approach of propagating the most restrictive input label to the output is too conservative for applications where LLMs operate on inputs retrieved from diverse sources. In this paper, we propose a novel, more permissive approach to propagate information flow labels through LLM queries. The key idea behind our approach is to propagate only the labels of the samples that were influential in generating the model output and to eliminate the labels of unnecessary inputs. We implement and investigate the effectiveness of two variations of this approach, based on (i) prompt-based retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We compare these with a baseline that uses introspection to predict the output label. Our experimental results in an LLM agent setting show that the permissive label propagator improves over the baseline in more than 85% of the cases, which underscores the practicality of our approach.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs</title>
<link>https://arxiv.org/abs/2410.03565</link>
<guid>https://arxiv.org/abs/2410.03565</guid>
<content:encoded><![CDATA[
arXiv:2410.03565v3 Announce Type: replace-cross 
Abstract: In the zero-shot policy transfer (ZSPT) setting for contextual Markov decision processes (MDP), agents train on a fixed set of contexts and must generalise to new ones. Recent work has argued and demonstrated that increased exploration can improve this generalisation, by training on more states in the training contexts. In this paper, we demonstrate that training on more states can indeed improve generalisation, but can come at a cost of reducing the accuracy of the learned value function which should not benefit generalisation. We hypothesise and demonstrate that using exploration to increase the agent's coverage while also increasing the accuracy improves generalisation even more. Inspired by this, we propose a method Explore-Go that implements an exploration phase at the beginning of each episode, which can be combined with existing on- and off-policy RL algorithms and significantly improves generalisation even in partially observable MDPs. We demonstrate the effectiveness of Explore-Go when combined with several popular algorithms and show an increase in generalisation performance across several environments. With this, we hope to provide practitioners with a simple modification that can improve the generalisation of their agents.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Oriented Time Series Inference Agents for Reasoning and Automated Analysis</title>
<link>https://arxiv.org/abs/2410.04047</link>
<guid>https://arxiv.org/abs/2410.04047</guid>
<content:encoded><![CDATA[
arXiv:2410.04047v4 Announce Type: replace-cross 
Abstract: Real-world time series inference requires more than point forecasting. It demands multi-step reasoning, constraint handling, domain knowledge incorporation, and domain-specific workflow assembly. Existing time series foundation models are limited to narrow tasks and lack flexibility to generalize across diverse scenarios. On the other hand, large language models (LLMs) struggle with numerical precision. To address these limitations, we introduce TS-Reasoner, a Domain-Oriented Time Series Agent that integrates natural language reasoning with precise numerical execution. TS-Reasoner decomposes natural language instructions into structured workflows composed of statistical, logical, and domain-specific operators, and incorporates a self-refinement mechanism for adaptive execution. We evaluate its capabilities through two axes: basic time series understanding and complex multi-step inference, using the TimeSeriesExam benchmark and a newly constructed dataset. Experimental results show that TS-Reasoner significantly outperforms general-purpose LLMs, highlighting the promise of domain-specialized agents for robust and interpretable time series reasoning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLEE: A Unified Framework and Benchmark for Language-based Economic Environments</title>
<link>https://arxiv.org/abs/2410.05254</link>
<guid>https://arxiv.org/abs/2410.05254</guid>
<content:encoded><![CDATA[
arXiv:2410.05254v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? How do they perform compared to humans? Do they tend to reach an efficient and fair outcome? What is the role of natural language in strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. To answer these questions, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents. Our results suggest that the market parameters, as well as the choice of the LLMs, tend to have complex and interdependent effects on the economic outcome, which calls for careful design and analysis of the language-based economic ecosystem.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering</title>
<link>https://arxiv.org/abs/2410.08085</link>
<guid>https://arxiv.org/abs/2410.08085</guid>
<content:encoded><![CDATA[
arXiv:2410.08085v4 Announce Type: replace-cross 
Abstract: Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs' potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy. Code and data are released at https://github.com/Y-Sui/OKGQA.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for Molecular Ground-State Conformation Prediction</title>
<link>https://arxiv.org/abs/2410.09795</link>
<guid>https://arxiv.org/abs/2410.09795</guid>
<content:encoded><![CDATA[
arXiv:2410.09795v5 Announce Type: replace-cross 
Abstract: Predicting molecular ground-state conformation (i.e., energy-minimized conformation) is crucial for many chemical applications such as molecular docking and property prediction. Classic energy-based simulation is time-consuming when solving this problem, while existing learning-based methods have advantages in computational efficiency but sacrifice accuracy and interpretability. In this work, we propose a novel and effective method to bridge the energy-based simulation and the learning-based strategy, which designs and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called WGFormer, for ground-state conformation prediction. Specifically, our method tackles this task within an auto-encoding framework, which encodes low-quality conformations by the proposed WGFormer and decodes corresponding ground-state conformations by an MLP. The architecture of WGFormer corresponds to Wasserstein gradient flows -- it optimizes conformations by minimizing an energy function defined on the latent mixture models of atoms, thereby significantly improving performance and interpretability. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art competitors, providing a new and insightful paradigm to predict ground-state conformation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epochal Sawtooth Effect: Unveiling Training Loss Oscillations in Adam and Other Optimizers</title>
<link>https://arxiv.org/abs/2410.10056</link>
<guid>https://arxiv.org/abs/2410.10056</guid>
<content:encoded><![CDATA[
arXiv:2410.10056v2 Announce Type: replace-cross 
Abstract: In this paper, we identify and analyze a recurring training loss pattern, which we term the \textit{Epochal Sawtooth Effect (ESE)}, commonly observed during training with adaptive gradient-based optimizers, particularly Adam optimizer. This pattern is characterized by a sharp drop in loss at the beginning of each epoch, followed by a gradual increase, resulting in a sawtooth-shaped loss curve. Through empirical observations, we demonstrate that while this effect is most pronounced with Adam, it persists, although less severely, with other optimizers such as RMSProp.
  We provide an in-depth explanation of the underlying mechanisms that lead to the Epochal Sawtooth Effect. The influences of factors like $\beta$, batch size, data shuffling on this pattern have been studied. We quantify the influence of $beta_2$ on the shape of the loss curve, showing that higher values of $\beta_2$ result in a nearly linear increase in loss, while lower values create a concave upward trend. Our analysis reveals that this behavior stems from the adaptive learning rate controlled by the second moment estimate, with $\beta_1$ playing a minimal role when $\beta_2$ is large.
  To support our analysis, we replicate this phenomenon through a controlled quadratic minimization task. By incrementally solving a series of quadratic optimization problems using Adam, we demonstrate that the Epochal Sawtooth Effect can emerge even in simple optimization scenarios, reinforcing the generality of this pattern. This paper provides both theoretical insights and quantitative analysis, offering a comprehensive understanding of this ubiquitous phenomenon in modern optimization techniques.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-based Large Language Model Customization as Service</title>
<link>https://arxiv.org/abs/2410.10481</link>
<guid>https://arxiv.org/abs/2410.10481</guid>
<content:encoded><![CDATA[
arXiv:2410.10481v3 Announce Type: replace-cross 
Abstract: Prominent Large Language Model (LLM) services from providers like OpenAI and Google excel at general tasks but often underperform on domain-specific applications. Current customization services for these LLMs typically require users to upload data for fine-tuning, posing significant privacy risks. While differentially private (DP) data synthesis presents a potential alternative, its application commonly results in low effectiveness due to the introduction of excessive noise on data for DP. To overcome this, we introduce Llamdex, a novel framework that facilitates LLM customization as a service, where the client uploads pre-trained domain-specific models rather than data. This client-uploaded model, optionally protected by DP with much lower noise, is inserted into the base LLM via connection modules. Significantly, these connecting modules are trained without requiring sensitive domain data, enabling clients to customize LLM services while preserving data privacy. Experiments demonstrate that Llamdex improves domain-specific accuracy by up to 26\% over state-of-the-art private data synthesis methods under identical privacy constraints and, by obviating the need for users to provide domain context within queries, maintains inference efficiency comparable to the original LLM service.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination</title>
<link>https://arxiv.org/abs/2410.17477</link>
<guid>https://arxiv.org/abs/2410.17477</guid>
<content:encoded><![CDATA[
arXiv:2410.17477v5 Announce Type: replace-cross 
Abstract: The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to hallucinate false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites</title>
<link>https://arxiv.org/abs/2410.19643</link>
<guid>https://arxiv.org/abs/2410.19643</guid>
<content:encoded><![CDATA[
arXiv:2410.19643v4 Announce Type: replace-cross 
Abstract: Machine learning (ML) models benefit from large datasets. Collecting data in biomedical domains is costly and challenging, hence, combining datasets has become a common practice. However, datasets obtained under different conditions could present undesired site-specific variability. Data harmonization methods aim to remove site-specific variance while retaining biologically relevant information. This study evaluates the effectiveness of popularly used ComBat-based methods for harmonizing data in scenarios where the class balance is not equal across sites. We find that these methods struggle with data leakage issues. To overcome this problem, we propose a novel approach PrettYharmonize, designed to harmonize data by pretending the target labels. We validate our approach using controlled datasets designed to benchmark the utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-prone methods with PrettYharmonize and show that it achieves comparable performance while avoiding data leakage, particularly in site-target-dependence scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Confidence Calibration for Large Language Models</title>
<link>https://arxiv.org/abs/2411.02454</link>
<guid>https://arxiv.org/abs/2411.02454</guid>
<content:encoded><![CDATA[
arXiv:2411.02454v2 Announce Type: replace-cross 
Abstract: Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Form Text-to-Music Generation with Adaptive Prompts: A Case Study in Tabletop Role-Playing Games Soundtracks</title>
<link>https://arxiv.org/abs/2411.03948</link>
<guid>https://arxiv.org/abs/2411.03948</guid>
<content:encoded><![CDATA[
arXiv:2411.03948v3 Announce Type: replace-cross 
Abstract: This paper investigates the capabilities of text-to-audio music generation models in producing long-form music with prompts that change over time, focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We introduce Babel Bardo, a system that uses Large Language Models (LLMs) to transform speech transcriptions into music descriptions for controlling a text-to-music model. Four versions of Babel Bardo were compared in two TRPG campaigns: a baseline using direct speech transcriptions, and three LLM-based versions with varying approaches to music description generation. Evaluations considered audio quality, story alignment, and transition smoothness. Results indicate that detailed music descriptions improve audio quality while maintaining consistency across consecutive descriptions enhances story alignment and transition smoothness.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Transformer-based Risk Assessment for Clinical Evaluation</title>
<link>https://arxiv.org/abs/2411.08701</link>
<guid>https://arxiv.org/abs/2411.08701</guid>
<content:encoded><![CDATA[
arXiv:2411.08701v2 Announce Type: replace-cross 
Abstract: We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation), a novel method for clinical risk assessment based on clinical data, leveraging the self-attention mechanism for enhanced feature interaction and result interpretation. Our approach is able to handle different data modalities, including continuous, categorical and multiple-choice (checkbox) attributes. The proposed architecture features a shared representation of the clinical data obtained by integrating specialized embeddings of each data modality, enabling the detection of high-risk individuals using Transformer encoder layers. To assess the effectiveness of the proposed method, a strong baseline based on non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method outperforms various baselines widely used in the domain of clinical risk assessment, while effectively handling missing values. In terms of explainability, our Transformer-based method offers easily interpretable results via attention weights, further enhancing the clinicians' decision-making process.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Internal Model Control: Learning a Robust Control Policy via Predictive Error Feedback</title>
<link>https://arxiv.org/abs/2411.13079</link>
<guid>https://arxiv.org/abs/2411.13079</guid>
<content:encoded><![CDATA[
arXiv:2411.13079v3 Announce Type: replace-cross 
Abstract: Accurate motion control in the face of disturbances within complex environments remains a major challenge in robotics. Classical model-based approaches often struggle with nonlinearities and unstructured disturbances, while RL-based methods can be fragile when encountering unseen scenarios. In this paper, we propose a novel framework, Neural Internal Model Control, which integrates model-based control with RL-based control to enhance robustness. Our framework streamlines the predictive model by applying Newton-Euler equations for rigid-body dynamics, eliminating the need to capture complex high-dimensional nonlinearities. This internal model combines model-free RL algorithms with predictive error feedback. Such a design enables a closed-loop control structure to enhance the robustness and generalizability of the control system. We demonstrate the effectiveness of our framework on both quadrotors and quadrupedal robots, achieving superior performance compared to state-of-the-art methods. Furthermore, real-world deployment on a quadrotor with rope-suspended payloads highlights the framework's robustness in sim-to-real transfer. Our code is released at https://github.com/thu-uav/NeuralIMC.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming the Hybrid Cloud for Emerging AI Workloads</title>
<link>https://arxiv.org/abs/2411.13239</link>
<guid>https://arxiv.org/abs/2411.13239</guid>
<content:encoded><![CDATA[
arXiv:2411.13239v2 Announce Type: replace-cross 
Abstract: This white paper, developed through close collaboration between IBM Research and UIUC researchers within the IIDAI Institute, envisions transforming hybrid cloud systems to meet the growing complexity of AI workloads through innovative, full-stack co-design approaches, emphasizing usability, manageability, affordability, adaptability, efficiency, and scalability. By integrating cutting-edge technologies such as generative and agentic AI, cross-layer automation and optimization, unified control plane, and composable and adaptive system architecture, the proposed framework addresses critical challenges in energy efficiency, performance, and cost-effectiveness. Incorporating quantum computing as it matures will enable quantum-accelerated simulations for materials science, climate modeling, and other high-impact domains. Collaborative efforts between academia and industry are central to this vision, driving advancements in foundation models for material design and climate solutions, scalable multimodal data processing, and enhanced physics-based AI emulators for applications like weather forecasting and carbon sequestration. Research priorities include advancing AI agentic systems, LLM as an Abstraction (LLMaaA), AI model optimization and unified abstractions across heterogeneous infrastructure, end-to-end edge-cloud transformation, efficient programming model, middleware and platform, secure infrastructure, application-adaptive cloud systems, and new quantum-classical collaborative workflows. These ideas and solutions encompass both theoretical and practical research questions, requiring coordinated input and support from the research community. This joint initiative aims to establish hybrid clouds as secure, efficient, and sustainable platforms, fostering breakthroughs in AI-driven applications and scientific discovery across academia, industry, and society.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems</title>
<link>https://arxiv.org/abs/2411.13865</link>
<guid>https://arxiv.org/abs/2411.13865</guid>
<content:encoded><![CDATA[
arXiv:2411.13865v3 Announce Type: replace-cross 
Abstract: Modern recommender systems often create information cocoons, restricting users' exposure to diverse content. A key challenge lies in balancing content exploration and exploitation while allowing users to adjust their recommendation preferences. Intuitively, this balance can be modeled as a tree-structured representation, where depth search facilitates exploitation and breadth search enables exploration. However, existing approaches face two fundamental limitations: Euclidean methods struggle to capture hierarchical structures, while hyperbolic methods, despite their superior hierarchical modeling, lack semantic understanding of user and item profiles and fail to provide a principled mechanism for balancing exploration and exploitation. To address these challenges, we propose HERec, a hyperbolic graph-LLM framework that effectively balances exploration and exploitation in recommender systems. Our framework introduces two key innovations: (1) a semantic-enhanced hierarchical mechanism that aligns rich textual descriptions processed by large language models (LLMs) with collaborative information directly in hyperbolic space, allowing for more nuanced updates that respect the underlying hierarchical structure in user-item profiles; (2) an automatic hierarchical representation by optimizing Dasgupta's cost, which discovers hierarchical structures without requiring predefined hyperparameters, enabling user-adjustable exploration-exploitation trade-offs. Extensive experiments demonstrate that HERec consistently outperforms both Euclidean and hyperbolic baselines, achieving up to 5.49% improvement in utility metrics and 11.39% increase in diversity metrics, effectively mitigating information cocoons. We open-source our model implementation at https://github.com/Martin-qyma/HERec.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings</title>
<link>https://arxiv.org/abs/2412.01031</link>
<guid>https://arxiv.org/abs/2412.01031</guid>
<content:encoded><![CDATA[
arXiv:2412.01031v3 Announce Type: replace-cross 
Abstract: Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, and severity of a large number of clinical findings. We then performed phrasal grounding to localize their associated anatomical regions on chest radiograph images. The textual and visual measures are then combined to rate the quality of the generated reports. We present results that compare this evaluation metric with other textual metrics on a gold standard dataset derived from the MIMIC collection and show its robustness and sensitivity to factual errors.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction</title>
<link>https://arxiv.org/abs/2412.03188</link>
<guid>https://arxiv.org/abs/2412.03188</guid>
<content:encoded><![CDATA[
arXiv:2412.03188v2 Announce Type: replace-cross 
Abstract: In smart mobility, large networks of geographically distributed sensors produce vast amounts of high-frequency spatio-temporal data that must be processed in real time to avoid major disruptions. Traditional centralized approaches are increasingly unsuitable to this task, as they struggle to scale with expanding sensor networks, and reliability issues in central components can easily affect the whole deployment. To address these challenges, we explore and adapt semi-decentralized training techniques for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation framework where sensors are grouped by proximity into multiple cloudlets, each handling a subgraph of the traffic graph, fetching node features from other cloudlets to train its own local ST-GNN model, and exchanging model updates with other cloudlets to ensure consistency, enhancing scalability and removing reliance on a centralized aggregator. We perform extensive comparative evaluation of four different ST-GNN training setups -- centralized, traditional FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed predictions. Experimental results show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance. In addition, we highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</title>
<link>https://arxiv.org/abs/2412.04233</link>
<guid>https://arxiv.org/abs/2412.04233</guid>
<content:encoded><![CDATA[
arXiv:2412.04233v3 Announce Type: replace-cross 
Abstract: Adaptability to specialised or homogeneous behaviours is critical in cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS) techniques, common for efficient adaptation, often limit behavioural diversity due to cross-agent gradient interference, which we show can be exacerbated by the coupling of observations and agent IDs. Current remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates. We ask: can shared policies adapt without these complexities? We propose HyperMARL, a PS approach using hypernetworks for dynamic agent-specific parameters, without altering the RL objective or requiring preset diversity levels. HyperMARL's explicit decoupling of observation- and agent-conditioned gradients empirically reduces policy gradient variance, facilitates shared-policy adaptation (including specialisation), and helps mitigate cross-agent interference. Across diverse MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL achieves competitive performance against key baselines -- fully shared, non-parameter sharing, and three diversity-promoting methods -- while preserving behavioural diversity comparable to non-parameter sharing. These findings establish HyperMARL as a versatile approach for adaptive MARL. The code is publicly available at https://github.com/KaleabTessera/HyperMARL.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study</title>
<link>https://arxiv.org/abs/2412.06272</link>
<guid>https://arxiv.org/abs/2412.06272</guid>
<content:encoded><![CDATA[
arXiv:2412.06272v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong potential across legal tasks, yet the problem of legal citation prediction remains under-explored. At its core, this task demands fine-grained contextual understanding and precise identification of relevant legislation or precedent. We introduce the AusLaw Citation Benchmark, a real-world dataset comprising 55k Australian legal instances and 18,677 unique citations which to the best of our knowledge is the first of its scale and scope. We then conduct a systematic benchmarking across a range of solutions: (i) standard prompting of both general and law-specialised LLMs, (ii) retrieval-only pipelines with both generic and domain-specific embeddings, (iii) supervised fine-tuning, and (iv) several hybrid strategies that combine LLMs with retrieval augmentation through query expansion, voting ensembles, or re-ranking. Results show that neither general nor law-specific LLMs suffice as stand-alone solutions, with performance near zero. Instruction tuning (of even a generic open-source LLM) on task-specific dataset is among the best performing solutions. We highlight that database granularity along with the type of embeddings play a critical role in retrieval-based approaches, with hybrid methods which utilise a trained re-ranker delivering the best results. Despite this, a performance gap of nearly 50% remains, underscoring the value of this challenging benchmark as a rigorous test-bed for future research in legal-domain.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era</title>
<link>https://arxiv.org/abs/2412.10255</link>
<guid>https://arxiv.org/abs/2412.10255</guid>
<content:encoded><![CDATA[
arXiv:2412.10255v5 Announce Type: replace-cross 
Abstract: Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation benchmark. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, with specifically developed metrics for animation video generation. Our entire project is publicly available on https://github.com/bilibili/Index-anisora/tree/main.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework</title>
<link>https://arxiv.org/abs/2412.12459</link>
<guid>https://arxiv.org/abs/2412.12459</guid>
<content:encoded><![CDATA[
arXiv:2412.12459v2 Announce Type: replace-cross 
Abstract: Topic modeling is widely used for uncovering thematic structures within text corpora, yet traditional models often struggle with specificity and coherence in domain-focused applications. Guided approaches, such as SeededLDA and CorEx, incorporate user-provided seed words to improve relevance but remain labor-intensive and static. Large language models (LLMs) offer potential for dynamic topic refinement and discovery, yet their application often incurs high API costs. To address these challenges, we propose the LLM-assisted Iterative Topic Augmentation framework (LITA), an LLM-assisted approach that integrates user-provided seeds with embedding-based clustering and iterative refinement. LITA identifies a small number of ambiguous documents and employs an LLM to reassign them to existing or new topics, minimizing API costs while enhancing topic quality. Experiments on two datasets across topic quality and clustering performance metrics demonstrate that LITA outperforms five baseline models, including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an efficient and adaptable framework for advancing topic modeling and text clustering.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ErasableMask: A Robust and Erasable Privacy Protection Scheme against Black-box Face Recognition Models</title>
<link>https://arxiv.org/abs/2412.17038</link>
<guid>https://arxiv.org/abs/2412.17038</guid>
<content:encoded><![CDATA[
arXiv:2412.17038v4 Announce Type: replace-cross 
Abstract: While face recognition (FR) models have brought remarkable convenience in face verification and identification, they also pose substantial privacy risks to the public. Existing facial privacy protection schemes usually adopt adversarial examples to disrupt face verification of FR models. However, these schemes often suffer from weak transferability against black-box FR models and permanently damage the identifiable information that cannot fulfill the requirements of authorized operations such as forensics and authentication. To address these limitations, we propose ErasableMask, a robust and erasable privacy protection scheme against black-box FR models. Specifically, via rethinking the inherent relationship between surrogate FR models, ErasableMask introduces a novel meta-auxiliary attack, which boosts black-box transferability by learning more general features in a stable and balancing optimization strategy. It also offers a perturbation erasion mechanism that supports the erasion of semantic perturbations in protected face without degrading image quality. To further improve performance, ErasableMask employs a curriculum learning strategy to mitigate optimization conflicts between adversarial attack and perturbation erasion. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the state-of-the-art performance in transferability, achieving over 72% confidence on average in commercial FR systems. Moreover, ErasableMask also exhibits outstanding perturbation erasion performance, achieving over 90% erasion success rate.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism</title>
<link>https://arxiv.org/abs/2412.17933</link>
<guid>https://arxiv.org/abs/2412.17933</guid>
<content:encoded><![CDATA[
arXiv:2412.17933v2 Announce Type: replace-cross 
Abstract: We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its duel scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 14 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis and (ii) continuous pretraining of the first Czech-centric 7B language model with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard with existing 50 model submissions, where new model submissions can be made at https://huggingface.co/spaces/CZLC/BenCzechMark.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconciling Privacy and Explainability in High-Stakes: A Systematic Inquiry</title>
<link>https://arxiv.org/abs/2412.20798</link>
<guid>https://arxiv.org/abs/2412.20798</guid>
<content:encoded><![CDATA[
arXiv:2412.20798v4 Announce Type: replace-cross 
Abstract: Deep learning's preponderance across scientific domains has reshaped high-stakes decision-making, making it essential to follow rigorous operational frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation (RTE). This paper examines the complexities of combining these two requirements. For RTP, we focus on `Differential privacy` (DP), which is considered the current gold standard for privacy-preserving machine learning due to its strong quantitative guarantee of privacy. For RTE, we focus on post-hoc explainers: they are the go-to option for model auditing as they operate independently of model training. We formally investigate DP models and various commonly-used post-hoc explainers: how to evaluate these explainers subject to RTP, and analyze the intrinsic interactions between DP models and these explainers. Furthermore, our work throws light on how RTP and RTE can be effectively combined in high-stakes applications. Our study concludes by outlining an industrial software pipeline, with the example of a wildly used use-case, that respects both RTP and RTE requirements.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian</title>
<link>https://arxiv.org/abs/2501.11264</link>
<guid>https://arxiv.org/abs/2501.11264</guid>
<content:encoded><![CDATA[
arXiv:2501.11264v2 Announce Type: replace-cross 
Abstract: Software engineers spend a significant amount of time reading code during the software development process. This trend is amplified by the emergence of large language models (LLMs) that automatically generate code. However, little is known about the readability of the LLM-generated code and whether it is still important from practitioners' perspectives in this new era. In this paper, we conduct a survey to explore the practitioners' perspectives on code readability in the age of LLMs and investigate the readability of our LLM-based software development agents framework, HULA, by comparing its generated code with human-written code in real-world scenarios. Overall, the findings underscore that (1) readability remains a critical aspect of software development; (2) the readability of our LLM-generated code is comparable to human-written code, fostering the establishment of appropriate trust and driving the broad adoption of our LLM-powered software development platform.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2501.18052</link>
<guid>https://arxiv.org/abs/2501.18052</guid>
<content:encoded><![CDATA[
arXiv:2501.18052v3 Announce Type: replace-cross 
Abstract: Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Our evaluation shows that SAeUron outperforms existing approaches on the UnlearnCanvas benchmark for concepts and style unlearning, and effectively eliminates nudity when evaluated with I2P. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content under adversarial attack. Code and checkpoints are available at https://github.com/cywinski/SAeUron.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v2 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data</title>
<link>https://arxiv.org/abs/2502.04380</link>
<guid>https://arxiv.org/abs/2502.04380</guid>
<content:encoded><![CDATA[
arXiv:2502.04380v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this work, we investigate the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-design for LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.06205</link>
<guid>https://arxiv.org/abs/2502.06205</guid>
<content:encoded><![CDATA[
arXiv:2502.06205v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior -- typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InSTA: Towards Internet-Scale Training For Agents</title>
<link>https://arxiv.org/abs/2502.06776</link>
<guid>https://arxiv.org/abs/2502.06776</guid>
<content:encoded><![CDATA[
arXiv:2502.06776v2 Announce Type: replace-cross 
Abstract: The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code, models and data at: https://data-for-agents.github.io.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Chat: Protein LLM with Early-Fusion of Text, Sequence and Structure</title>
<link>https://arxiv.org/abs/2502.06846</link>
<guid>https://arxiv.org/abs/2502.06846</guid>
<content:encoded><![CDATA[
arXiv:2502.06846v2 Announce Type: replace-cross 
Abstract: Motivation: Proteins are of great significance in living organisms. However, understanding their functions encounters numerous challenges, such as insufficient integration of multimodal information, a large number of training parameters, limited flexibility of classification-based methods, and the lack of systematic evaluation metrics for protein Q&amp;A systems. To tackle these issues, we propose the Prot2Chat framework. Results: We modified ProteinMPNN to encode protein sequence and structural information in a unified way. We used a large language model (LLM) to encode questions into vectors and developed a protein-text adapter to compress protein information into virtual tokens based on these vectors, achieving the early fusion of text and protein information. Finally, the same LLM reads the virtual tokens and the questions to generate answers. To optimize training efficiency, we froze the encoder and employed Low-Rank Adaptation (LoRA) techniques for the LLM. Experiments on two datasets show that both automated metrics and expert evaluations demonstrate the superior performance of our model, and zero-shot prediction results highlight its generalization ability. The models and codes are available at https://github.com/ wangzc1233/Prot2Chat. Contact: zqcao@suda.edu.cn or wangzc025@163.com Key words: Protein Q&amp;A, Early-Fusion, LLM
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization</title>
<link>https://arxiv.org/abs/2502.07523</link>
<guid>https://arxiv.org/abs/2502.07523</guid>
<content:encoded><![CDATA[
arXiv:2502.07523v2 Announce Type: replace-cross 
Abstract: Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications. Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with higher UTD ratios. We identify challenges in the training dynamics, which are emphasized by higher UTD ratios. To address these, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, has been shown to prevent potential loss of plasticity and keeps the effective learning rate constant. Our proposed approach reliably scales with increasing UTD ratios, achieving competitive performance across 25 challenging continuous control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably the complex dog and humanoid environments. This work eliminates the need for drastic interventions, such as network resets, and offers a simple yet robust pathway for improving sample efficiency and scalability in model-free reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms</title>
<link>https://arxiv.org/abs/2502.07849</link>
<guid>https://arxiv.org/abs/2502.07849</guid>
<content:encoded><![CDATA[
arXiv:2502.07849v2 Announce Type: replace-cross 
Abstract: Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion and flow-based generative models, enabling high-quality conditional generation. A key theoretical challenge is characterizing the distribution induced by CFG, particularly in high-dimensional settings relevant to real-world data. Previous works have shown that CFG modifies the target distribution, steering it towards a distribution sharper than the target one, more shifted towards the boundary of the class. In this work, we provide a high-dimensional analysis of CFG, showing that these distortions vanish as the data dimension grows. We present a blessing-of-dimensionality result demonstrating that in sufficiently high and infinite dimensions, CFG accurately reproduces the target distribution. Using our high-dimensional theory, we show that there is a large family of guidances enjoying this property, in particular non-linear CFG generalizations. We study a simple non-linear power-law version, for which we demonstrate improved robustness, sample fidelity and diversity. Our findings are validated with experiments on class-conditional and text-to-image generation using state-of-the-art diffusion and flow-matching models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Need for Explanations: LLMs can implicitly learn from mistakes in-context</title>
<link>https://arxiv.org/abs/2502.08550</link>
<guid>https://arxiv.org/abs/2502.08550</guid>
<content:encoded><![CDATA[
arXiv:2502.08550v2 Announce Type: replace-cross 
Abstract: Showing incorrect answers to Large Language Models (LLMs) is a popular strategy to improve their performance in reasoning-intensive tasks. It is widely assumed that, in order to be helpful, the incorrect answers must be accompanied by comprehensive rationales, explicitly detailing where the mistakes are and how to correct them. However, in this work we present a counterintuitive finding: we observe that LLMs perform better in math reasoning tasks when these rationales are eliminated from the context and models are left to infer on their own what makes an incorrect answer flawed. This approach also substantially outperforms chain-of-thought prompting in our evaluations. These results are consistent across LLMs of different sizes and varying reasoning abilities. To gain an understanding of why LLMs learn from mistakes more effectively without explicit corrective rationales, we perform a thorough analysis, investigating changes in context length and answer diversity between different prompting strategies, and their effect on performance. We also examine evidence of overfitting to the in-context rationales when these are provided, and study the extent to which LLMs are able to autonomously infer high-quality corrective rationales given only incorrect answers as input. We find evidence that, while incorrect answers are more beneficial for LLM learning than additional diverse correct answers, explicit corrective rationales over-constrain the model, thus limiting those benefits.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
arXiv:2502.09604v2 Announce Type: replace-cross 
Abstract: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. The source code is available at https://github.com/facebookresearch/SelfCite
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions</title>
<link>https://arxiv.org/abs/2502.09674</link>
<guid>https://arxiv.org/abs/2502.09674</guid>
<content:encoded><![CDATA[
arXiv:2502.09674v3 Announce Type: replace-cross 
Abstract: Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at https://github.com/BMPixel/safety-residual-space.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRIFFIN: Effective Token Alignment for Faster Speculative Decoding</title>
<link>https://arxiv.org/abs/2502.11018</link>
<guid>https://arxiv.org/abs/2502.11018</guid>
<content:encoded><![CDATA[
arXiv:2502.11018v2 Announce Type: replace-cross 
Abstract: Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA, Vicuna, Qwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 8% and a speedup ratio exceeding 7%, outperforming current speculative decoding state-of-the-art methods. Our code and GRIFFIN's draft models are released publicly in https://github.com/hsj576/GRIFFIN.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferring Textual Preferences to Vision-Language Understanding through Model Merging</title>
<link>https://arxiv.org/abs/2502.13487</link>
<guid>https://arxiv.org/abs/2502.13487</guid>
<content:encoded><![CDATA[
arXiv:2502.13487v2 Announce Type: replace-cross 
Abstract: Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Software Engineers: Programming with Trust</title>
<link>https://arxiv.org/abs/2502.13767</link>
<guid>https://arxiv.org/abs/2502.13767</guid>
<content:encoded><![CDATA[
arXiv:2502.13767v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations</title>
<link>https://arxiv.org/abs/2502.15132</link>
<guid>https://arxiv.org/abs/2502.15132</guid>
<content:encoded><![CDATA[
arXiv:2502.15132v3 Announce Type: replace-cross 
Abstract: We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of-thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slamming: Training a Speech Language Model on One GPU in a Day</title>
<link>https://arxiv.org/abs/2502.15814</link>
<guid>https://arxiv.org/abs/2502.15814</guid>
<content:encoded><![CDATA[
arXiv:2502.15814v2 Announce Type: replace-cross 
Abstract: We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLARE: A Framework for Stellar Flare Forecasting using Stellar Physical Properties and Historical Records</title>
<link>https://arxiv.org/abs/2502.18218</link>
<guid>https://arxiv.org/abs/2502.18218</guid>
<content:encoded><![CDATA[
arXiv:2502.18218v3 Announce Type: replace-cross 
Abstract: Stellar flare events are critical observational samples for astronomical research; however, recorded flare events remain limited. Stellar flare forecasting can provide additional flare event samples to support research efforts. Despite this potential, no specialized models for stellar flare forecasting have been proposed to date. In this paper, we present extensive experimental evidence demonstrating that both stellar physical properties and historical flare records are valuable inputs for flare forecasting tasks. We then introduce FLARE (Forecasting Light-curve-based Astronomical Records via features Ensemble), the first-of-its-kind large model specifically designed for stellar flare forecasting. FLARE integrates stellar physical properties and historical flare records through a novel Soft Prompt Module and Residual Record Fusion Module. Our experiments on the publicly available Kepler light curve dataset demonstrate that FLARE achieves superior performance compared to other methods across all evaluation metrics. Finally, we validate the forecast capability of our model through a comprehensive case study.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Liver Cirrhosis Stage Estimation from MRI with Deep Learning</title>
<link>https://arxiv.org/abs/2502.18225</link>
<guid>https://arxiv.org/abs/2502.18225</guid>
<content:encoded><![CDATA[
arXiv:2502.18225v3 Announce Type: replace-cross 
Abstract: We present an end-to-end deep learning framework for automated liver cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe scarring (fibrosis) of the liver and a common endpoint of various chronic liver diseases. Early diagnosis is vital to prevent complications such as decompensation and cancer, which significantly decreases life expectancy. However, diagnosing cirrhosis in its early stages is challenging, and patients often present with life-threatening complications. Our approach integrates multi-scale feature learning with sequence-specific attention mechanisms to capture subtle tissue variations across cirrhosis progression stages. Using CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution MRI scans from 339 patients, we demonstrate state-of-the-art performance in three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, significantly outperforming traditional radiomics-based approaches. Through extensive ablation studies, we show that our architecture effectively learns stage-specific imaging biomarkers. We establish new benchmarks for automated cirrhosis staging and provide insights for developing clinically applicable deep learning systems. The source code will be available at https://github.com/JunZengz/CirrhosisStage.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents</title>
<link>https://arxiv.org/abs/2502.20073</link>
<guid>https://arxiv.org/abs/2502.20073</guid>
<content:encoded><![CDATA[
arXiv:2502.20073v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel perspectives. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments over 11 popular LLMs and show that, while the LLMs present a strong ability in goal interpretation, there is a significant discrepancy in active collaboration and continuous adaptation which are critical for efficiently fulfilling complicated tasks. Notably, we highlight the strengths and weaknesses in LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-sourced benchmark. The environments, 30 open-ended tasks, and the evaluation package are publicly available at https://github.com/YusaeMeow/Collab-Overcooked.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</title>
<link>https://arxiv.org/abs/2502.20900</link>
<guid>https://arxiv.org/abs/2502.20900</guid>
<content:encoded><![CDATA[
arXiv:2502.20900v3 Announce Type: replace-cross 
Abstract: Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on restrictive assumptions, such as single-object settings or limited environments, leading to constrained generalization. We present DexGraspVLA, a hierarchical framework for general dexterous grasping in cluttered scenes based on RGB image perception and language instructions. It utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight to achieve robust generalization lies in iteratively transforming diverse language and visual inputs into domain-invariant representations via foundation models, where imitation learning can be effectively applied due to the alleviation of domain shift. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a "zero-shot" environment. Empirical analysis confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. DexGraspVLA also demonstrates free-form long-horizon prompt execution, robustness to adversarial objects and human disturbance, and failure recovery, which are rarely achieved simultaneously in prior work. Extended application to nonprehensile object grasping further proves its generality. Code, model, and video are available at dexgraspvla.github.io.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPGym Arcade: Parallel Pixelated POMDPs</title>
<link>https://arxiv.org/abs/2503.01450</link>
<guid>https://arxiv.org/abs/2503.01450</guid>
<content:encoded><![CDATA[
arXiv:2503.01450v3 Announce Type: replace-cross 
Abstract: We present the POPGym Arcade, a collection of hardware-accelerated, pixel-based environments with shared observation and action spaces. Each environment includes fully and partially observable variants, enabling counterfactual studies on partial observability. We also introduce mathematical tools for analyzing policies under partial observability, which reveal how agents recall past information to make decisions. Our analysis shows (1) that controlling for partial observability is critical and (2) that agents with long-term memory learn brittle policies that struggle to generalize. Finally, we demonstrate that recurrent policies can be "poisoned" by old, out-of-distribution observations, with implications for sim-to-real transfer, imitation learning, and offline reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steer LLM Latents for Hallucination Detection</title>
<link>https://arxiv.org/abs/2503.01917</link>
<guid>https://arxiv.org/abs/2503.01917</guid>
<content:encoded><![CDATA[
arXiv:2503.01917v2 Announce Type: replace-cross 
Abstract: Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers for molecular property prediction: Domain adaptation efficiently improves performance</title>
<link>https://arxiv.org/abs/2503.03360</link>
<guid>https://arxiv.org/abs/2503.03360</guid>
<content:encoded><![CDATA[
arXiv:2503.03360v3 Announce Type: replace-cross 
Abstract: Over the past six years, molecular transformer models have become key tools in drug discovery. Most existing models are pre-trained on large, unlabeled datasets such as ZINC or ChEMBL. However, the extent to which large-scale pre-training improves molecular property prediction remains unclear. This study evaluates transformer models for this task while addressing their limitations. We explore how pre-training dataset size and chemically informed objectives impact performance. Our results show that increasing the dataset beyond approximately 400K to 800K molecules from large-scale unlabeled databases does not enhance performance across seven datasets covering five ADME endpoints: lipophilicity, permeability, solubility (two datasets), microsomal stability (two datasets), and plasma protein binding. In contrast, domain adaptation on a small, domain-specific dataset (less than or equal 4K molecules) using multi-task regression of physicochemical properties significantly boosts performance (P-value less than 0.001). A model pre-trained on 400K molecules and adapted with domain-specific data outperforms larger models such as MolFormer and performs comparably to MolBERT. Benchmarks against Random Forest (RF) baselines using descriptors and Morgan fingerprints show that chemically and physically informed features consistently yield better performance across model types. While RF remains a strong baseline, we identify concrete practices to enhance transformer performance. Aligning pre-training and adaptation with chemically meaningful tasks and domain-relevant data presents a promising direction for molecular property prediction. Our models are available on HuggingFace for easy use and adaptation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All-atom Diffusion Transformers: Unified generative modelling of molecules and materials</title>
<link>https://arxiv.org/abs/2503.03965</link>
<guid>https://arxiv.org/abs/2503.03965</guid>
<content:encoded><![CDATA[
arXiv:2503.03965v2 Announce Type: replace-cross 
Abstract: Diffusion models are the standard toolkit for generative modelling of 3D atomic systems. However, for different types of atomic systems -- such as molecules and materials -- the generative processes are usually highly specific to the target system despite the underlying physics being the same. We introduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusion framework for jointly generating both periodic materials and non-periodic molecular systems using the same model: (1) An autoencoder maps a unified, all-atom representations of molecules and materials to a shared latent embedding space; and (2) A diffusion model is trained to generate new latent embeddings that the autoencoder can decode to sample new molecules or materials. Experiments on MP20, QM9 and GEOM-DRUGS datasets demonstrate that jointly trained ADiT generates realistic and valid molecules as well as materials, obtaining state-of-the-art results on par with molecule and crystal-specific models. ADiT uses standard Transformers with minimal inductive biases for both the autoencoder and diffusion model, resulting in significant speedups during training and inference compared to equivariant diffusion models. Scaling ADiT up to half a billion parameters predictably improves performance, representing a step towards broadly generalizable foundation models for generative chemistry. Open source code: https://github.com/facebookresearch/all-atom-diffusion-transformer
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User Association</title>
<link>https://arxiv.org/abs/2503.04564</link>
<guid>https://arxiv.org/abs/2503.04564</guid>
<content:encoded><![CDATA[
arXiv:2503.04564v3 Announce Type: replace-cross 
Abstract: Secure aggregation is motivated by federated learning (FL) where a cloud server aims to compute an averaged model (i.e., weights of deep neural networks) of the locally-trained models of numerous clients, while adhering to data security requirements. Hierarchical secure aggregation (HSA) extends this concept to a three-layer network, where clustered users communicate with the server through an intermediate layer of relays. In HSA, beyond conventional server security, relay security is also enforced to ensure that the relays remain oblivious to the users' inputs (an abstraction of the local models in FL). Existing study on HSA assumes that each user is associated with only one relay, limiting opportunities for coding across inter-cluster users to achieve efficient communication and key generation. In this paper, we consider HSA with a cyclic association pattern where each user is connected to $B$ consecutive relays in a wrap-around manner. We propose an efficient aggregation scheme which includes a message design for the inputs inspired by gradient coding-a well-known technique for efficient communication in distributed computing-along with a highly nontrivial security key design. We also derive novel converse bounds on the minimum achievable communication and key rates using information-theoretic arguments.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[
arXiv:2503.04598v3 Announce Type: replace-cross 
Abstract: Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the position of layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence demonstrating that HybridNorm improves gradient flow and model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts</title>
<link>https://arxiv.org/abs/2503.05066</link>
<guid>https://arxiv.org/abs/2503.05066</guid>
<content:encoded><![CDATA[
arXiv:2503.05066v2 Announce Type: replace-cross 
Abstract: The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \textbf{\textit{Straggler Effect}}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \textbf{\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \textbf{\textit{Capacity-Aware Token Reroute}}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2\% average performance increase and a 1.94$\times$ inference speedup on Mixtral-8$\times$7B-Instruct.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?</title>
<link>https://arxiv.org/abs/2503.09499</link>
<guid>https://arxiv.org/abs/2503.09499</guid>
<content:encoded><![CDATA[
arXiv:2503.09499v2 Announce Type: replace-cross 
Abstract: Large foundation models face challenges in acquiring transferable, structured thinking abilities, especially when supervised with rigid templates or crowd-annotated instruction datasets. Unlike prior approaches, we focus on a thinking-centric data synthesis paradigm that enables models to evolve through self-generated, cognitively guided data. We propose MindGYM, a structured and scalable framework for question synthesis, composed of: (1) Cognitive Thinking Process Injection, which infuses high-level reasoning objectives to shape the model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating atomic questions from diverse semantic types to encourage broader thinking; and (3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop questions based on QA seeds for deeper reasoning. Detailed analysis shows that synthetic data generated by our method achieves 16.7% higher average quality and 67.91% lower quality variance compared to baseline sources, highlighting that both high-quality and self-contained data are essential for effective, thinking-oriented fine-tuning. MindGYM improves performance on six reasoning benchmarks, achieving gains of up to 16% on MathVision using only 400 data samples, and generalizable improvements across different model sizes and architectures. MindGYM underscores the viability of self-challenging mechanisms in refining large model capabilities while minimizing human intervention and resource demands. Code and data are released to promote data-centric research into self-evolving foundation models driven by their internal reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.11617</link>
<guid>https://arxiv.org/abs/2503.11617</guid>
<content:encoded><![CDATA[
arXiv:2503.11617v2 Announce Type: replace-cross 
Abstract: Assembly code analysis and comprehension play critical roles in applications like reverse engineering, yet they face substantial challenges due to low information density and a lack of explicit syntactic structures. While traditional masked language modeling (MLM) approaches do not explicitly focus on natural language interaction, emerging decoder-focused large language models (LLMs) demonstrate partial success in binary analysis yet remain underexplored for holistic comprehension. We present Assembly Augmented Tuning, an end-to-end structural-semantic instruction tuning framework that synergizes encoder architecture with decoder-based LLMs through a projector module, where the assembly encoder extracts hardware-level structural features, the projector bridges representations with the semantic space, and the instruction-tuned LLM preserves natural language capabilities. Experimental results demonstrate three key advantages: (1) State-of-the-art performance in assembly comprehension with +39.7% Recall@1 and +17.8% MRR improvements over GPT-4-Turbo, (2) Consistent enhancements across base models (24.6-107.4% Recall@1 and 15.2-106.3% MRR on Qwen2.5-Coder, Deepseek-Coder and CodeLlama variants), and (3) Superior instruction-following capabilities (41.5%-118% improvements) with controlled code generation degradation (-8.9% to -35% across architectures).
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment</title>
<link>https://arxiv.org/abs/2503.15463</link>
<guid>https://arxiv.org/abs/2503.15463</guid>
<content:encoded><![CDATA[
arXiv:2503.15463v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have traditionally been aligned through one-size-fits-all approaches that assume uniform human preferences, fundamentally overlooking the diversity in user values and needs. This paper introduces a comprehensive framework for scalable personalized alignment of LLMs. We establish a systematic preference space characterizing psychological and behavioral dimensions, alongside diverse persona representations for robust preference inference in real-world scenarios. Building upon this foundation, we introduce \textsc{AlignX}, a large-scale dataset of over 1.3 million personalized preference examples, and develop two complementary alignment approaches: \textit{in-context alignment} directly conditioning on persona representations and \textit{preference-bridged alignment} modeling intermediate preference distributions. Extensive experiments demonstrate substantial improvements over existing methods, with an average 17.06\% accuracy gain across four benchmarks while exhibiting a strong adaptation capability to novel preferences, robustness to limited user data, and precise preference controllability. These results validate our approach toward user-adaptive AI systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback</title>
<link>https://arxiv.org/abs/2503.17682</link>
<guid>https://arxiv.org/abs/2503.17682</guid>
<content:encoded><![CDATA[
arXiv:2503.17682v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) are essential for building general-purpose AI assistants; however, they pose increasing safety risks. How can we ensure safety alignment of MLLMs to prevent undesired behaviors? Going further, it is critical to explore how to fine-tune MLLMs to preserve capabilities while meeting safety constraints. Fundamentally, this challenge can be formulated as a min-max optimization problem. However, existing datasets have not yet disentangled single preference signals into explicit safety constraints, hindering systematic investigation in this direction. Moreover, it remains an open question whether such constraints can be effectively incorporated into the optimization process for multi-modal models. In this work, we present the first exploration of the Safe RLHF-V -- the first multimodal safety alignment framework. The framework consists of: $\mathbf{(I)}$ BeaverTails-V, the first open-source dataset featuring dual preference annotations for helpfulness and safety, supplemented with multi-level safety labels (minor, moderate, severe); $\mathbf{(II)}$ Beaver-Guard-V, a multi-level guardrail system to proactively defend against unsafe queries and adversarial attacks. Applying the guard model over five rounds of filtering and regeneration significantly enhances the precursor model's overall safety by an average of 40.9%. $\mathbf{(III)}$ Based on dual preference, we initiate the first exploration of multi-modal safety alignment within a constrained optimization. Experimental results demonstrate that Safe RLHF effectively improves both model helpfulness and safety. Specifically, Safe RLHF-V enhances model safety by 34.2% and helpfulness by 34.3%.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stroke Disease Classification Using Machine Learning with Feature Selection Techniques</title>
<link>https://arxiv.org/abs/2504.00485</link>
<guid>https://arxiv.org/abs/2504.00485</guid>
<content:encoded><![CDATA[
arXiv:2504.00485v3 Announce Type: replace-cross 
Abstract: Heart disease remains a leading cause of mortality and morbidity worldwide, necessitating the development of accurate and reliable predictive models to facilitate early detection and intervention. While state of the art work has focused on various machine learning approaches for predicting heart disease, but they could not able to achieve remarkable accuracy. In response to this need, we applied nine machine learning algorithms XGBoost, logistic regression, decision tree, random forest, k-nearest neighbors (KNN), support vector machine (SVM), gaussian na\"ive bayes (NB gaussian), adaptive boosting, and linear regression to predict heart disease based on a range of physiological indicators. Our approach involved feature selection techniques to identify the most relevant predictors, aimed at refining the models to enhance both performance and interpretability. The models were trained, incorporating processes such as grid search hyperparameter tuning, and cross-validation to minimize overfitting. Additionally, we have developed a novel voting system with feature selection techniques to advance heart disease classification. Furthermore, we have evaluated the models using key performance metrics including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (ROC AUC). Among the models, XGBoost demonstrated exceptional performance, achieving 99% accuracy, precision, F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach to early heart disease diagnosis and preventive healthcare.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dion: Distributed Orthonormalized Updates</title>
<link>https://arxiv.org/abs/2504.05295</link>
<guid>https://arxiv.org/abs/2504.05295</guid>
<content:encoded><![CDATA[
arXiv:2504.05295v2 Announce Type: replace-cross 
Abstract: Recent work has shown that orthonormal matrix updates speed up neural network optimization, improve training stability, and offer better hyperparameter transfer across model sizes. Applying these updates efficiently when model weights and optimizer states are sharded across a large-scale distributed LLM training system remains a major challenge. We introduce Dion (DIstributed OrthoNormalization), a scalable and communication-efficient orthonormalizing optimizer. Dion leverages low-rank approximation and decoupled momentum buffers, eliminating the need for full gradient synchronization while producing numerically equivalent results. It is compatible with simultaneous DDP, FSDP, and TP parallelism, and it computes an orthonormalized update without unsharding a full parameter matrix on any single device. We evaluate Dion on language models from 120M to 3B parameters and find that its benefits improve with increasing model size and batch size.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05585</link>
<guid>https://arxiv.org/abs/2504.05585</guid>
<content:encoded><![CDATA[
arXiv:2504.05585v2 Announce Type: replace-cross 
Abstract: Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden "trap states" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecture independent generalization bounds for overparametrized deep ReLU networks</title>
<link>https://arxiv.org/abs/2504.05695</link>
<guid>https://arxiv.org/abs/2504.05695</guid>
<content:encoded><![CDATA[
arXiv:2504.05695v3 Announce Type: replace-cross 
Abstract: We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyConf: Unlocking Polymer Conformation Generation through Hierarchical Generative Models</title>
<link>https://arxiv.org/abs/2504.08859</link>
<guid>https://arxiv.org/abs/2504.08859</guid>
<content:encoded><![CDATA[
arXiv:2504.08859v2 Announce Type: replace-cross 
Abstract: Polymer conformation generation is a critical task that enables atomic-level studies of diverse polymer materials. While significant advances have been made in designing conformation generation methods for small molecules and proteins, these methods struggle to generate polymer conformations due to their unique structural characteristics. Meanwhile, the scarcity of polymer conformation datasets further limits the progress, making this important area largely unexplored. In this work, we propose PolyConf, a pioneering tailored polymer conformation generation method that leverages hierarchical generative models to unlock new possibilities. Specifically, we decompose the polymer conformation into a series of local conformations (i.e., the conformations of its repeating units), generating these local conformations through an autoregressive model, and then generating their orientation transformations via a diffusion model to assemble them into the complete polymer conformation. Moreover, we develop the first benchmark with a high-quality polymer conformation dataset derived from molecular dynamics simulations to boost related research in this area. The comprehensive evaluation demonstrates that PolyConf consistently outperforms existing conformation generation methods, thus facilitating advancements in polymer modeling and simulation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance</title>
<link>https://arxiv.org/abs/2504.09753</link>
<guid>https://arxiv.org/abs/2504.09753</guid>
<content:encoded><![CDATA[
arXiv:2504.09753v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their development has primarily focused on English and other high-resource languages, leaving many languages underserved. We present our latest Hindi-English bi-lingual LLM \textbf{Mantra-14B} with ~3\% average improvement in benchmark scores over both languages, outperforming models twice its size. Using a curated dataset composed of English and Hindi instruction data of 485K samples, we instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve performance over both English and Hindi. Our experiments encompassing seven different LLMs of varying parameter sizes and over 140 training attempts with varying English-Hindi training data ratios demonstrated that it is possible to significantly improve multilingual performance without compromising native performance. Further, our approach avoids resource-intensive techniques like vocabulary expansion or architectural modifications, thus keeping the model size small. Our results indicate that modest fine-tuning with culturally and locally informed data can bridge performance gaps without incurring significant computational overhead. We release our training code, datasets, and models under mit and apache licenses to aid further research towards under-represented and low-resource languages.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection in LLMs with Topological Divergence on Attention Graphs</title>
<link>https://arxiv.org/abs/2504.10063</link>
<guid>https://arxiv.org/abs/2504.10063</guid>
<content:encoded><![CDATA[
arXiv:2504.10063v2 Announce Type: replace-cross 
Abstract: Hallucination, i.e., generating factually incorrect content, remains a critical challenge for large language models (LLMs). We introduce TOHA, a TOpology-based HAllucination detector in the RAG setting, which leverages a topological divergence metric to quantify the structural properties of graphs induced by attention matrices. Examining the topological divergence between prompt and response subgraphs reveals consistent patterns: higher divergence values in specific attention heads correlate with hallucinated outputs, independent of the dataset. Extensive experiments - including evaluation on question answering and summarization tasks - show that our approach achieves state-of-the-art or competitive results on several benchmarks while requiring minimal annotated data and computational resources. Our findings suggest that analyzing the topological structure of attention matrices can serve as an efficient and robust indicator of factual reliability in LLMs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling</title>
<link>https://arxiv.org/abs/2504.10612</link>
<guid>https://arxiv.org/abs/2504.10612</guid>
<content:encoded><![CDATA[
arXiv:2504.10612v3 Announce Type: replace-cross 
Abstract: The most widely used generative models map noise and data distributions by matching flows or scores. However, they struggle to incorporate partial observations and additional priors--something energy-based models (EBMs) handle elegantly by simply adding corresponding scalar energy terms. We address this issue by proposing Energy Matching, a framework that endows flow-based approaches with the flexibility of EBMs. Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in terms of fidelity, while retaining simulation-free training of transport-based approaches away from the data manifold. Furthermore, we leverage the method's flexibility to introduce an interaction energy that supports diverse mode exploration, which we demonstrate in a controlled protein-generation setting. Our approach focuses on learning a scalar potential energy--without time-conditioning, auxiliary generators, or additional networks--which marks a significant departure from recent EBM methods. We believe that this simplified framework significantly advances EBMs capabilities and paves the way for their wider adoption in generative modeling across diverse domains.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Fine-Grained Detection of AI Generated Texts</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
arXiv:2504.11952v2 Announce Type: replace-cross 
Abstract: An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</title>
<link>https://arxiv.org/abs/2504.12961</link>
<guid>https://arxiv.org/abs/2504.12961</guid>
<content:encoded><![CDATA[
arXiv:2504.12961v2 Announce Type: replace-cross 
Abstract: Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity</title>
<link>https://arxiv.org/abs/2504.18078</link>
<guid>https://arxiv.org/abs/2504.18078</guid>
<content:encoded><![CDATA[
arXiv:2504.18078v2 Announce Type: replace-cross 
Abstract: The rapid expansion of distributed photovoltaic (PV) installations worldwide, many being behind-the-meter systems, has significantly challenged energy management and grid operations, as unobservable PV generation further complicates the supply-demand balance. Therefore, estimating this generation from net load, known as PV disaggregation, is critical. Given privacy concerns and the need for large training datasets, federated learning becomes a promising approach, but statistical heterogeneity, arising from geographical and behavioral variations among prosumers, poses new challenges to PV disaggregation. To overcome these challenges, a privacy-preserving distributed PV disaggregation framework is proposed using Personalized Federated Learning (PFL). The proposed method employs a two-level framework that combines local and global modeling. At the local level, a transformer-based PV disaggregation model is designed to generate solar irradiance embeddings for representing local PV conditions. A novel adaptive local aggregation mechanism is adopted to mitigate the impact of statistical heterogeneity on the local model, extracting a portion of global information that benefits the local model. At the global level, a central server aggregates information uploaded from multiple data centers, preserving privacy while enabling cross-center knowledge sharing. Experiments on real-world data demonstrate the effectiveness of this proposed framework, showing improved accuracy and robustness compared to benchmark methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues</title>
<link>https://arxiv.org/abs/2504.21800</link>
<guid>https://arxiv.org/abs/2504.21800</guid>
<content:encoded><![CDATA[
arXiv:2504.21800v3 Announce Type: replace-cross 
Abstract: The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. Synthetic therapy dialogues closely match structural features of real-world conversations (e.g., speaker switch ratio: 0.98 vs. 0.99); however, they may not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Robustness in Discrete Time Linear Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.02347</link>
<guid>https://arxiv.org/abs/2505.02347</guid>
<content:encoded><![CDATA[
arXiv:2505.02347v2 Announce Type: replace-cross 
Abstract: Discrete time linear dynamical systems, including Markov chains, have found many applications. However, in some problems, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we propose to theoretically analyze a distributional robust cost estimation task in a Wasserstein ambiguity set, instead of learning a probability distribution from a few samples. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a fundamental result about Wasserstein distance based polytope.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Approach to Backtracking Counterfactual Explanations: A Unified Causal Framework for Efficient Model Interpretability</title>
<link>https://arxiv.org/abs/2505.02435</link>
<guid>https://arxiv.org/abs/2505.02435</guid>
<content:encoded><![CDATA[
arXiv:2505.02435v2 Announce Type: replace-cross 
Abstract: Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method called BRACE based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models</title>
<link>https://arxiv.org/abs/2505.03176</link>
<guid>https://arxiv.org/abs/2505.03176</guid>
<content:encoded><![CDATA[
arXiv:2505.03176v2 Announce Type: replace-cross 
Abstract: Current self-supervised algorithms commonly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by enforcing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm often limits the flexibility of learned representations for downstream adaptation by creating performance trade-offs between high-level invariance-demanding tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we proposes \emph{seq-JEPA}, a world modeling framework that introduces architectural inductive biases into joint-embedding predictive architectures to resolve this trade-off. Without relying on dual equivariance predictors or loss terms, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to specified transformations and another invariant to them. To do so, our model processes short sequences of different views (observations) of inputs. Each encoded view is concatenated with an embedding of the relative transformation (action) that produces the next observation in the sequence. These view-action pairs are passed through a transformer encoder that outputs an aggregate representation. A predictor head then conditions this aggregate representation on the upcoming action to predict the representation of the next observation. Empirically, seq-JEPA demonstrates strong performance on both equivariant and invariant benchmarks without sacrificing one for the other. Furthermore, it excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Adaptation to Concept Drift for Learned Database Operations</title>
<link>https://arxiv.org/abs/2505.04404</link>
<guid>https://arxiv.org/abs/2505.04404</guid>
<content:encoded><![CDATA[
arXiv:2505.04404v2 Announce Type: replace-cross 
Abstract: Machine learning has demonstrated transformative potential for database operations, such as query optimization and in-database data analytics. However, dynamic database environments, characterized by frequent updates and evolving data distributions, introduce concept drift, which leads to performance degradation for learned models and limits their practical applicability. Addressing this challenge requires efficient frameworks capable of adapting to shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that introduces a new paradigm called \textit{in-context adaptation} for learned database operations. FLAIR leverages the inherent property of data systems, i.e., immediate availability of execution results for predictions, to enable dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,| \,C_t) \to \mathbf{y}$, with $C_t$ representing a dynamic context memory, FLAIR delivers predictions aligned with the current concept, eliminating the need for runtime parameter optimization. To achieve this, FLAIR integrates two key modules: a Task Featurization Module for encoding task-specific features into standardized representations, and a Dynamic Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly using contextual information at runtime. Extensive experiments across key database tasks demonstrate that FLAIR outperforms state-of-the-art baselines, achieving up to 5.2x faster adaptation and reducing error by 22.5% for cardinality estimation.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
<link>https://arxiv.org/abs/2505.05181</link>
<guid>https://arxiv.org/abs/2505.05181</guid>
<content:encoded><![CDATA[
arXiv:2505.05181v3 Announce Type: replace-cross 
Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on global gradient synchronization limits scalability and imposes significant memory overhead. We propose Stochastic Variational Propagation (SVP), a scalable alternative that reframes training as hierarchical variational inference. SVP treats layer activations as latent variables and optimizes local Evidence Lower Bounds (ELBOs), enabling independent, local updates while preserving global coherence. However, directly applying KL divergence in layer-wise ELBOs risks inter-layer's representation collapse due to excessive compression. To prevent this, SVP projects activations into low-dimensional spaces via fixed random matrices, ensuring information preservation and representational diversity. Combined with a feature alignment loss for inter-layer consistency, SVP achieves competitive accuracy with BP across diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to ImageNet), reduces memory usage by up to 4x, and significantly improves scalability. More broadly, SVP introduces a probabilistic perspective to deep representation learning, opening pathways toward more modular and interpretable neural network design.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</title>
<link>https://arxiv.org/abs/2505.05291</link>
<guid>https://arxiv.org/abs/2505.05291</guid>
<content:encoded><![CDATA[
arXiv:2505.05291v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n=587) of DFIs with AMD labels from Brazil.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering</title>
<link>https://arxiv.org/abs/2505.05423</link>
<guid>https://arxiv.org/abs/2505.05423</guid>
<content:encoded><![CDATA[
arXiv:2505.05423v3 Announce Type: replace-cross 
Abstract: The impact of Large Language Models (LLMs) has extended into literary domains. However, existing evaluation metrics prioritize mechanical accuracy over artistic expression and tend to overrate machine translation as being superior to human translation from experienced professionals. In the long run, this bias could result in an irreversible decline in translation quality and cultural authenticity. In response to the urgent need for a specialized literary evaluation metric, we introduce LiTransProQA, a novel, reference-free, LLM-based question-answering framework designed for literary translation evaluation. LiTransProQA uniquely integrates insights from professional literary translators and researchers, focusing on critical elements in literary quality assessment such as literary devices, cultural understanding, and authorial voice. Our extensive evaluation shows that while literary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially outperforms current metrics, achieving up to 0.07 gain in correlation and surpassing the best state-of-the-art metrics by over 15 points in adequacy assessments. Incorporating professional translator insights as weights further improves performance, highlighting the value of translator inputs. Notably, LiTransProQA reaches human-level evaluation performance comparable to trained student evaluators. It shows broad applicability to open-source models like LLaMa3.3-70b and Qwen2.5-32b, indicating its potential as an accessible and training-free tool for evaluating literary translations that require local processing due to copyright or ethical considerations. The code and datasets are available under: https://github.com/zhangr2021/TransProQA.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models</title>
<link>https://arxiv.org/abs/2505.05626</link>
<guid>https://arxiv.org/abs/2505.05626</guid>
<content:encoded><![CDATA[
arXiv:2505.05626v2 Announce Type: replace-cross 
Abstract: Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowHFT: Imitation Learning via Flow Matching Policy for Optimal High-Frequency Trading under Diverse Market Conditions</title>
<link>https://arxiv.org/abs/2505.05784</link>
<guid>https://arxiv.org/abs/2505.05784</guid>
<content:encoded><![CDATA[
arXiv:2505.05784v3 Announce Type: replace-cross 
Abstract: High-frequency trading (HFT) is an investing strategy that continuously monitors market states and places bid and ask orders at millisecond speeds. Traditional HFT approaches fit models with historical data and assume that future market states follow similar patterns. This limits the effectiveness of any single model to the specific conditions it was trained for. Additionally, these models achieve optimal solutions only under specific market conditions, such as assumptions about stock price's stochastic process, stable order flow, and the absence of sudden volatility. Real-world markets, however, are dynamic, diverse, and frequently volatile. To address these challenges, we propose the FlowHFT, a novel imitation learning framework based on flow matching policy. FlowHFT simultaneously learns strategies from numerous expert models, each proficient in particular market scenarios. As a result, our framework can adaptively adjust investment decisions according to the prevailing market state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism. This allows it to refine strategies and achieve superior performance even in complex or extreme market scenarios where expert strategies may be suboptimal. We test FlowHFT in multiple market environments. We first show that flow matching policy is applicable in stochastic market environments, thus enabling FlowHFT to learn trading strategies under different market conditions. Notably, our single framework consistently achieves performance superior to the best expert for each market condition.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Outperform Experts on Challenging Biology Benchmarks</title>
<link>https://arxiv.org/abs/2505.06108</link>
<guid>https://arxiv.org/abs/2505.06108</guid>
<content:encoded><![CDATA[
arXiv:2505.06108v3 Announce Type: replace-cross 
Abstract: This study systematically evaluates 27 frontier Large Language Models on eight biology benchmarks spanning molecular biology, genetics, cloning, virology, and biosecurity. Models from major AI developers released between November 2022 and April 2025 were assessed through ten independent runs per benchmark. The findings reveal dramatic improvements in biological capabilities. Top model performance increased more than 4-fold on the challenging text-only subset of the Virology Capabilities Test over the study period, with OpenAI's o3 now performing twice as well as expert virologists. Several models now match or exceed expert-level performance on other challenging benchmarks, including the biology subsets of GPQA and WMDP and LAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not substantially improve performance over zero-shot evaluation, while extended reasoning features in o3-mini and Claude 3.7 Sonnet typically improved performance as predicted by inference scaling. Benchmarks such as PubMedQA and the MMLU and WMDP biology subsets exhibited performance plateaus well below 100%, suggesting benchmark saturation and errors in the underlying benchmark data. The analysis highlights the need for more sophisticated evaluation methodologies as AI systems continue to advance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Pre-trained Autoregressive Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.07344</link>
<guid>https://arxiv.org/abs/2505.07344</guid>
<content:encoded><![CDATA[
arXiv:2505.07344v4 Announce Type: replace-cross 
Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Bad NOFO? AI Governance in Federal Grantmaking</title>
<link>https://arxiv.org/abs/2505.08133</link>
<guid>https://arxiv.org/abs/2505.08133</guid>
<content:encoded><![CDATA[
arXiv:2505.08133v2 Announce Type: replace-cross 
Abstract: Much scholarship considers how U.S. federal agencies govern artificial intelligence (AI) through rulemaking and their own internal use policies. But agencies have an overlooked AI governance role: setting discretionary grant policy when directing billions of dollars in federal financial assistance. These dollars enable state and local entities to study, create, and use AI. This funding not only goes to dedicated AI programs, but also to grantees using AI in the course of meeting their routine grant objectives. As discretionary grantmakers, agencies guide and restrict what grant winners do -- a hidden lever for AI governance. Agencies pull this lever by setting program objectives, judging criteria, and restrictions for AI use. Using a novel dataset of over 40,000 non-defense federal grant notices of funding opportunity (NOFOs) posted to the U.S. federal grants website between 2009 and 2024, we analyze how agencies regulate the use of AI by grantees. We select records mentioning AI and review their stated goals and requirements. We find agencies promoting AI in notice narratives, shaping adoption in ways other records of grant policy might fail to capture. Of the grant opportunities that mention AI, we find only a handful of AI-specific judging criteria or restrictions. This silence holds even when agencies fund AI uses in contexts affecting people's rights and which, under an analogous federal procurement regime, would result in extra oversight. These findings recast grant notices as a site of AI policymaking -- albeit one that is developing out of step with other regulatory efforts and incomplete in its consideration of transparency, accountability, and privacy protections. The paper concludes by drawing lessons from AI procurement scholarship, while identifying distinct challenges in grantmaking that invite further study.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Solutions Integrated in an IoT Healthcare Platform for Heart Failure Risk Stratification</title>
<link>https://arxiv.org/abs/2505.09619</link>
<guid>https://arxiv.org/abs/2505.09619</guid>
<content:encoded><![CDATA[
arXiv:2505.09619v3 Announce Type: replace-cross 
Abstract: The management of chronic Heart Failure (HF) presents significant challenges in modern healthcare, requiring continuous monitoring, early detection of exacerbations, and personalized treatment strategies. In this paper, we present a predictive model founded on Machine Learning (ML) techniques to identify patients at HF risk. This model is an ensemble learning approach, a modified stacking technique, that uses two specialized models leveraging clinical and echocardiographic features and then a meta-model to combine the predictions of these two models. We initially assess the model on a real dataset and the obtained results suggest that it performs well in the stratification of patients at HR risk. Specifically, we obtained high sensitivity (95\%), ensuring that nearly all high-risk patients are identified. As for accuracy, we obtained 84\%, which can be considered moderate in some ML contexts. However, it is acceptable given our priority of identifying patients at risk of HF because they will be asked to participate in the telemonitoring program of the PrediHealth research project on which some of the authors of this paper are working. The initial findings also suggest that ML-based risk stratification models can serve as valuable decision-support tools not only in the PrediHealth project but also for healthcare professionals, aiding in early intervention and personalized patient management. To have a better understanding of the value and of potentiality of our predictive model, we also contrasted its results with those obtained by using three baseline models. The preliminary results indicate that our predictive model outperforms these baselines that flatly consider features, \ie not grouping them in clinical and echocardiographic features.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDDA: Data Driven Attribution at LinkedIn</title>
<link>https://arxiv.org/abs/2505.09861</link>
<guid>https://arxiv.org/abs/2505.09861</guid>
<content:encoded><![CDATA[
arXiv:2505.09861v2 Announce Type: replace-cross 
Abstract: Data Driven Attribution, which assigns conversion credits to marketing interactions based on causal patterns learned from data, is the foundation of modern marketing intelligence and vital to any marketing businesses and advertising platform. In this paper, we introduce a unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors. We detail the large scale implementation of the approach at LinkedIn, showcasing significant impact. We also share learning and insights that are broadly applicable to the marketing and ad tech fields.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Sparse Autoencoders Useful for Java Function Bug Detection?</title>
<link>https://arxiv.org/abs/2505.10375</link>
<guid>https://arxiv.org/abs/2505.10375</guid>
<content:encoded><![CDATA[
arXiv:2505.10375v2 Announce Type: replace-cross 
Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a major source of security breaches. Traditional methods for vulnerability detection remain essential but are limited by high false positive rates, scalability issues, and reliance on manual effort. These constraints have driven interest in AI-based approaches to automated vulnerability detection and secure code generation. While Large Language Models (LLMs) have opened new avenues for classification tasks, their complexity and opacity pose challenges for interpretability and deployment. Sparse Autoencoder offer a promising solution to this problem. We explore whether SAEs can serve as a lightweight, interpretable alternative for bug detection in Java functions. We evaluate the effectiveness of SAEs when applied to representations from GPT-2 Small and Gemma 2B, examining their capacity to highlight buggy behaviour without fine-tuning the underlying LLMs. We found that SAE-derived features enable bug detection with an F1 score of up to 89%, consistently outperforming fine-tuned transformer encoder baselines. Our work provides the first empirical evidence that SAEs can be used to detect software bugs directly from the internal representations of pretrained LLMs, without any fine-tuning or task-specific supervision.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer</title>
<link>https://arxiv.org/abs/2505.10945</link>
<guid>https://arxiv.org/abs/2505.10945</guid>
<content:encoded><![CDATA[
arXiv:2505.10945v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) increasingly incorporate multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model's embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies, to handle each non-overlapping token's embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods and achieves lower loss with accelerating faster convergence during language adaptation. Notably, SALT obtains remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning</title>
<link>https://arxiv.org/abs/2505.11004</link>
<guid>https://arxiv.org/abs/2505.11004</guid>
<content:encoded><![CDATA[
arXiv:2505.11004v2 Announce Type: replace-cross 
Abstract: Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13176</link>
<guid>https://arxiv.org/abs/2505.13176</guid>
<content:encoded><![CDATA[
arXiv:2505.13176v2 Announce Type: replace-cross 
Abstract: While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at https://github.com/Chengziha0/ToolSpectrum.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2504.14858</link>
<guid>https://arxiv.org/abs/2504.14858</guid>
<content:encoded><![CDATA[
<div> alignment, reasoning, retrieval, critiques, CLM

Summary: 
In this work, the authors propose a novel framework called AlignRAG to address the issue of Reasoning Misalignment in retrieval-augmented generation (RAG) systems. They introduce a Critique-Driven Alignment approach, which includes a contrastive critique synthesis mechanism to generate critiques that are sensitive to retrieved evidence. This mechanism trains a Critic Language Model (CLM) to distinguish between evidence-aligned and misaligned reasoning. The CLM is optimized for evidence sensitivity, allowing it to detect and correct reasoning errors during inference. Evaluation results show that the 8B-parameter CLM in AlignRAG outperforms baseline models on out-of-domain tasks and is compatible with existing RAG architectures. Overall, AlignRAG provides a principled solution for ensuring that model reasoning aligns with retrieved evidence, improving the factual reliability and robustness of RAG systems. 

<br /><br />Summary: <div>
arXiv:2504.14858v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has become a widely adopted paradigm for enabling knowledge-grounded large language models (LLMs). However, standard RAG pipelines often fail to ensure that model reasoning remains consistent with the evidence retrieved, leading to factual inconsistencies or unsupported conclusions. In this work, we reinterpret RAG as Retrieval-Augmented Reasoning and identify a central but underexplored problem: \textit{Reasoning Misalignment}-the divergence between an LLM's internal reasoning trajectory and the evidential constraints provided by retrieval. To address this issue, we propose \textsc{AlignRAG}, a novel iterative framework grounded in Critique-Driven Alignment (CDA). At the heart of \textsc{AlignRAG} lies a \textit{contrastive critique synthesis} mechanism that generates retrieval-sensitive critiques while mitigating self-bias. This mechanism trains a dedicated retrieval-augmented \textit{Critic Language Model (CLM)} using labeled critiques that distinguish between evidence-aligned and misaligned reasoning. Alignment signals for supervision are obtained through self-supervised or externally guided labeling strategies. The resulting CLM is explicitly optimized for evidence sensitivity, enabling it to detect and revise reasoning errors during inference without relying solely on self-generated feedback. Empirical evaluations show that our 8B-parameter CLM improves performance over the Self-Refine baseline by 12.1\% on out-of-domain tasks and outperforms a standard 72B-parameter CLM by 2.2\%, while remaining compatible with existing RAG architectures as a plug-and-play module. Overall, AlignRAG offers a principled solution for aligning model reasoning with retrieved evidence, substantially improving the factual reliability and robustness of RAG systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization</title>
<link>https://arxiv.org/abs/2412.06141</link>
<guid>https://arxiv.org/abs/2412.06141</guid>
<content:encoded><![CDATA[
<div> alignment, multimodal, preference optimization, Medical LVLM, clinical relevance

Summary:<br />
The article introduces a novel approach called MMedPO to enhance alignment in Medical LVLMs by considering clinical relevance in preference data. MMedPO curates multimodal preference data by introducing plausible hallucinations and lesion region neglect to disrupt visual understanding. Clinical relevance scores are calculated for each sample and integrated as weights in the optimization process. Experimental results show that MMedPO significantly improves factual accuracy in Med-LVLMs, outperforming existing methods by 14.2% and 51.7% in Med-VQA and report generation tasks, respectively. The code for MMedPO is available on GitHub at https://github.com/aiming-lab/MMedPO.<br /><br />Summary: <div>
arXiv:2412.06141v3 Announce Type: replace-cross 
Abstract: The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately mitigated clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. To address this challenge, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, and integrate these scores into the preference optimization process as weights, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by averaging 14.2% and 51.7% across the Med-VQA and report generation tasks. Our code are available in https://github.com/aiming-lab/MMedPO.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-GPT via Next Clip Diffusion</title>
<link>https://arxiv.org/abs/2505.12489</link>
<guid>https://arxiv.org/abs/2505.12489</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT, Video-GPT, visual world modeling, video prediction, world modeling

Summary:<br /><br />
The paper introduces Video-GPT, a novel approach that leverages video sequences as a new form of language for modeling the visual world. By incorporating a next clip diffusion paradigm for pretraining, Video-GPT can generate short-term and predict long-term visual sequences by denoising noisy clips based on clean historical clips. Experimental results demonstrate that Video-GPT outperforms existing methods in video prediction, achieving state-of-the-art performance. The model shows strong generalization capabilities across six mainstream video tasks, including generation and understanding. The project page provides further information on Video-GPT and its applications in modeling the visual world. <div>
arXiv:2505.12489v2 Announce Type: replace-cross 
Abstract: GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream. The project page is at https://zhuangshaobin.github.io/Video-GPT.github.io/.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Follow the STARs: Dynamic $\omega$-Regular Shielding of Learned Policies</title>
<link>https://arxiv.org/abs/2505.14689</link>
<guid>https://arxiv.org/abs/2505.14689</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic post-shielding, $\omega$-regular correctness properties, probabilistic policies, Strategy-Template-based Adaptive Runtime Shields (STARs), cyber-physical applications

Summary: 
This paper introduces a novel dynamic post-shielding framework that goes beyond traditional safety-shielding to also enforce liveness properties. The framework, called Strategy-Template-based Adaptive Runtime Shields (STARs), utilizes permissive strategy templates to minimize interference while enforcing correctness properties. STARs feature a tunable enforcement parameter that allows for balancing between formal obligations and task-specific behavior at runtime. They support runtime adaptation to changing specifications and actuator failures, making them well-suited for cyber-physical applications. The evaluation on a mobile robot benchmark showcases STARs' ability to control interference while enforcing $\omega$-regular correctness properties over learned probabilistic policies.<br /><br />Summary: <div>
arXiv:2505.14689v1 Announce Type: new 
Abstract: This paper presents a novel dynamic post-shielding framework that enforces the full class of $\omega$-regular correctness properties over pre-computed probabilistic policies. This constitutes a paradigm shift from the predominant setting of safety-shielding -- i.e., ensuring that nothing bad ever happens -- to a shielding process that additionally enforces liveness -- i.e., ensures that something good eventually happens. At the core, our method uses Strategy-Template-based Adaptive Runtime Shields (STARs), which leverage permissive strategy templates to enable post-shielding with minimal interference. As its main feature, STARs introduce a mechanism to dynamically control interference, allowing a tunable enforcement parameter to balance formal obligations and task-specific behavior at runtime. This allows to trigger more aggressive enforcement when needed, while allowing for optimized policy choices otherwise. In addition, STARs support runtime adaptation to changing specifications or actuator failures, making them especially suited for cyber-physical applications. We evaluate STARs on a mobile robot benchmark to demonstrate their controllable interference when enforcing (incrementally updated) $\omega$-regular correctness properties over learned probabilistic policies.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution</title>
<link>https://arxiv.org/abs/2505.14738</link>
<guid>https://arxiv.org/abs/2505.14738</guid>
<content:encoded><![CDATA[
<div> Framework, AI, ML, Crowdsourcing, Iterative  
Summary:  
The article introduces R&amp;D-Agent, a dual-agent framework designed to enhance iterative exploration in data science tasks. The Researcher agent generates ideas using performance feedback, while the Developer agent refines code based on error feedback. By enabling multiple parallel exploration traces that merge and enhance each other, R&amp;D-Agent aims to bridge the gap between automated solutions and expert-level performance. Evaluation on MLE-Bench shows that R&amp;D-Agent is the top-performing machine learning engineering agent, indicating its potential to accelerate innovation and improve precision in various data science applications. The open-source availability of R&amp;D-Agent on GitHub allows for broader adoption and collaborative development in the field. <br /><br />Summary: <div>
arXiv:2505.14738v1 Announce Type: new 
Abstract: Recent advances in AI and ML have transformed data science, yet increasing complexity and expertise requirements continue to hinder progress. While crowdsourcing platforms alleviate some challenges, high-level data science tasks remain labor-intensive and iterative. To overcome these limitations, we introduce R&amp;D-Agent, a dual-agent framework for iterative exploration. The Researcher agent uses performance feedback to generate ideas, while the Developer agent refines code based on error feedback. By enabling multiple parallel exploration traces that merge and enhance one another, R&amp;D-Agent narrows the gap between automated solutions and expert-level performance. Evaluated on MLE-Bench, R&amp;D-Agent emerges as the top-performing machine learning engineering agent, demonstrating its potential to accelerate innovation and improve precision across diverse data science applications. We have open-sourced R&amp;D-Agent on GitHub: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOL-Pretrain: A complexity annotated corpus of first-order logic</title>
<link>https://arxiv.org/abs/2505.14932</link>
<guid>https://arxiv.org/abs/2505.14932</guid>
<content:encoded><![CDATA[
<div> large language models, transformer-based, reasoning capabilities, first-order logic, dataset 

Summary:
This article introduces a new dataset aimed at studying the algorithmic reasoning capabilities of large language models (LLMs), particularly their ability to integrate and compute over structured information. The dataset, consisting of 3.5 billion tokens, includes both human-annotated and synthetically generated examples of first-order logic reasoning traces. Each synthetic example is generated by an automated theorem solver and comes with metadata tracing its algorithmic provenance. The goal of the dataset is to provide a scalable and interpretable resource for investigating how LLMs learn and generalize symbolic reasoning processes. By focusing on complex algorithms, this dataset aims to facilitate more transparent and targeted studies of the algorithmic capabilities of modern LLMs. <div>
arXiv:2505.14932v1 Announce Type: new 
Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable reasoning capabilities such as coding and solving mathematical problems to commonsense inference. While these tasks vary in complexity, they all require models to integrate and compute over structured information. Despite recent efforts to reverse-engineer LLM behavior through controlled experiments, our understanding of how these models internalize and execute complex algorithms remains limited. Progress has largely been confined to small-scale studies or shallow tasks such as basic arithmetic and grammatical pattern matching. One barrier to deeper understanding is the nature of pretraining data -- vast, heterogeneous, and often poorly annotated, making it difficult to isolate mechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully open, complexity-annotated dataset of first-order logic reasoning traces, designed to probe and analyze algorithmic reasoning in LLMs. The dataset consists of 3.5 billion tokens, including 8.8 million LLM-augmented, human-annotated examples and 7.5 million synthetically generated examples. Each synthetic example is verifiably correct, produced by a custom automated theorem solver, and accompanied by metadata tracing its algorithmic provenance. We aim to provide a scalable, interpretable artifact for studying how LLMs learn and generalize symbolic reasoning processes, paving the way for more transparent and targeted investigations into the algorithmic capabilities of modern models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Be or Not To Be: Vector ontologies as a truly formal ontological framework</title>
<link>https://arxiv.org/abs/2505.14940</link>
<guid>https://arxiv.org/abs/2505.14940</guid>
<content:encoded><![CDATA[
<div> Keywords: Formal Ontologies, Husserl, Foundational Ontology, Vector Spaces, Information Systems

Summary: 
Formal ontologies, as conceptualized by Husserl, emphasize a priori validity independent of perception and the absence of content. However, many so-called formal ontologies fail to meet these criteria. The work previously categorized as formal ontology should be repositioned as foundational ontology, recognizing its potential in capturing objective reality without relying on a specific perceptual framework. A class of formal ontology based on vector spaces can effectively represent conceptualizations found in foundational ontologies. Evidence suggests that both information systems and humans may already be using vector ontologies to represent reality. Further exploration of vector ontologies could lead to a human-machine interoperable ontological framework, enabling enhanced understanding between machines and humans. 

Summary:<br />
Formal ontologies, rooted in Husserl's principles, prioritize a priori validity and content absence. Many current formal ontologies do not meet these standards. Repositioning formal ontology as foundational acknowledges its potential for objective representation. Vector-based formal ontologies effectively capture foundational concepts. Both human and machine systems may already employ vector ontologies. Exploring these structures could enable a shared ontological framework between humans and advanced machines. <div>
arXiv:2505.14940v1 Announce Type: new 
Abstract: Since Edmund Husserl coined the term "Formal Ontologies" in the early 20th century, a field that identifies itself with this particular branch of sciences has gained increasing attention. Many authors, and even Husserl himself have developed what they claim to be formal ontologies. I argue that under close inspection, none of these so claimed formal ontologies are truly formal in the Husserlian sense. More concretely, I demonstrate that they violate the two most important notions of formal ontology as developed in Husserl's Logical Investigations, namely a priori validity independent of perception and formalism as the total absence of content. I hence propose repositioning the work previously understood as formal ontology as the foundational ontology it really is. This is to recognize the potential of a truly formal ontology in the Husserlian sense. Specifically, I argue that formal ontology following his conditions, allows us to formulate ontological structures, which could capture what is more objectively without presupposing a particular framework arising from perception. I further argue that the ability to design the formal structure deliberately allows us to create highly scalable and interoperable information artifacts. As concrete evidence, I showcase that a class of formal ontology, which uses the axioms of vector spaces, is able to express most of the conceptualizations found in foundational ontologies. Most importantly, I argue that many information systems, specifically artificial intelligence, are likely already using some type of vector ontologies to represent reality in their internal worldviews and elaborate on the evidence that humans do as well. I hence propose a thorough investigation of the ability of vector ontologies to act as a human-machine interoperable ontological framework that allows us to understand highly sophisticated machines and machines to understand us.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning from User Feedback</title>
<link>https://arxiv.org/abs/2505.14946</link>
<guid>https://arxiv.org/abs/2505.14946</guid>
<content:encoded><![CDATA[
<div> framework, Reinforcement Learning from User Feedback, large language models, user preferences, positive feedback
Summary: 
Reinforcement Learning from User Feedback (RLUF) is introduced to align large language models (LLMs) with real user preferences using implicit signals from users. RLUF trains a reward model, P[Love], to predict the likelihood of receiving positive feedback from users and integrates it into a multi-objective policy optimization framework. In experiments, P[Love] proves predictive of increased positive feedback and serves as a reliable offline evaluator. Policy optimization using P[Love] significantly raises positive-feedback rates, including a 28% increase in Love Reactions during live tests. However, optimizing for positive reactions poses challenges in balancing objectives to prevent reward hacking. By leveraging implicit signals from users directly, RLUF offers a scalable approach to aligning LLMs with real-world user preferences. 
<br /><br />Summary: <div>
arXiv:2505.14946v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in diverse user facing applications, aligning them with real user preferences becomes essential. Existing methods like Reinforcement Learning from Human Feedback (RLHF) rely on expert annotators trained on manually defined guidelines, whose judgments may not reflect the priorities of everyday users. We introduce Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs directly to implicit signals from users in production. RLUF addresses key challenges of user feedback: user feedback is often binary (e.g., emoji reactions), sparse, and occasionally adversarial. We train a reward model, P[Love], to predict the likelihood that an LLM response will receive a Love Reaction, a lightweight form of positive user feedback, and integrate P[Love] into a multi-objective policy optimization framework alongside helpfulness and safety objectives. In large-scale experiments, we show that P[Love] is predictive of increased positive feedback and serves as a reliable offline evaluator of future user behavior. Policy optimization using P[Love] significantly raises observed positive-feedback rates, including a 28% increase in Love Reactions during live A/B tests. However, optimizing for positive reactions introduces reward hacking challenges, requiring careful balancing of objectives. By directly leveraging implicit signals from users, RLUF offers a path to aligning LLMs with real-world user preferences at scale.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Curriculum for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14970</link>
<guid>https://arxiv.org/abs/2505.14970</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, language models, curriculum learning, multi-armed bandit, policy gradient <br />
Summary:<br />
- Reinforcement learning has been effective in fine-tuning large language models, particularly in domains like mathematics and code generation.
- Training curriculum order is crucial for success, with Self-Evolving Curriculum (SEC) proposed as an automatic, concurrent learning method.
- SEC treats curriculum selection as a non-stationary Multi-Armed Bandit problem, optimizing for immediate learning gain using absolute advantage from policy gradient methods.
- Experimental results in planning, inductive reasoning, and mathematics domains show SEC significantly improves models' reasoning abilities and generalization to harder test problems.
- SEC also achieves better skill balance when fine-tuning across multiple reasoning domains, making it a promising strategy for RL fine-tuning of LLMs. 
<br /><br /> <div>
arXiv:2505.14970v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility</title>
<link>https://arxiv.org/abs/2505.14983</link>
<guid>https://arxiv.org/abs/2505.14983</guid>
<content:encoded><![CDATA[
<div> Dynamic Bayesian Network, cognitive states, human-aware systems, autonomous vehicles, decision-making<br />
<br />
Summary: 
The article presents a novel computational model using a Dynamic Bayesian Network to infer cognitive states of autonomous vehicle (AV) users and other road users. The model integrates information on well-being, trust, and intentions to make informed AV decisions. By collecting data from interaction studies, the model parameters are refined and evaluated for effectiveness. The model is extended into a causal inference model framework for AV decision-making, balancing user well-being and trust with operational costs and well-being of other road users. The evaluation demonstrates the model's ability to accurately predict user states and guide human-centered AV decisions. <div>
arXiv:2505.14983v1 Announce Type: new 
Abstract: For future human-autonomous vehicle (AV) interactions to be effective and smooth, human-aware systems that analyze and align human needs with automation decisions are essential. Achieving this requires systems that account for human cognitive states. We present a novel computational model in the form of a Dynamic Bayesian Network (DBN) that infers the cognitive states of both AV users and other road users, integrating this information into the AV's decision-making process. Specifically, our model captures the well-being of both an AV user and an interacting road user as cognitive states alongside trust. Our DBN models infer beliefs over the AV user's evolving well-being, trust, and intention states, as well as the possible well-being of other road users, based on observed interaction experiences. Using data collected from an interaction study, we refine the model parameters and empirically assess its performance. Finally, we extend our model into a causal inference model (CIM) framework for AV decision-making, enabling the AV to enhance user well-being and trust while balancing these factors with its own operational costs and the well-being of interacting road users. Our evaluation demonstrates the model's effectiveness in accurately predicting user's states and guiding informed, human-centered AV decisions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.15011</link>
<guid>https://arxiv.org/abs/2505.15011</guid>
<content:encoded><![CDATA[
<div> norms, value-alignment, reinforcement learning, reputation, policy learning
Summary:<br />
- The article discusses the importance of integrating both written and unwritten norms into reinforcement learning processes to create value-aligned agents. 
- It highlights the difference in representing safety/legal norms explicitly and social norms learned in the parameter space of neural networks. 
- A novel method is proposed to monitor an agent's compliance with the norms and use their reputation to motivate value-aligned behavior through reward weighting. 
- Experiments, including a traffic problem, demonstrate the significance of combining both types of norms for finding value-aligned policies. 
- Ablations show the effectiveness of integrating written and unwritten norms as opposed to using them separately. 
<br /><br /> <div>
arXiv:2505.15011v1 Announce Type: new 
Abstract: Our society is governed by a set of norms which together bring about the values we cherish such as safety, fairness or trustworthiness. The goal of value-alignment is to create agents that not only do their tasks but through their behaviours also promote these values. Many of the norms are written as laws or rules (legal / safety norms) but even more remain unwritten (social norms). Furthermore, the techniques used to represent these norms also differ. Safety / legal norms are often represented explicitly, for example, in some logical language while social norms are typically learned and remain hidden in the parameter space of a neural network. There is a lack of approaches in the literature that could combine these various norm representations into a single algorithm. We propose a novel method that integrates these norms into the reinforcement learning process. Our method monitors the agent's compliance with the given norms and summarizes it in a quantity we call the agent's reputation. This quantity is used to weigh the received rewards to motivate the agent to become value-aligned. We carry out a series of experiments including a continuous state space traffic problem to demonstrate the importance of the written and unwritten norms and show how our method can find the value-aligned policies. Furthermore, we carry out ablations to demonstrate why it is better to combine these two groups of norms rather than using either separately.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges</title>
<link>https://arxiv.org/abs/2505.15068</link>
<guid>https://arxiv.org/abs/2505.15068</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical modeling, real-world problems, interdisciplinary reasoning, computational tools

Summary: 
ModelingBench is introduced as a new benchmark for evaluating mathematical problem-solving capabilities using real-world-inspired, open-ended tasks from various domains. These tasks require the translation of natural language into formal mathematical formulations, application of appropriate tools, and structured report generation. The benchmark allows for multiple valid solutions, reflecting the ambiguity and creativity of practical modeling challenges. The ModelingAgent framework facilitates tool coordination, supports structured workflows, and enables iterative self-refinement to generate well-grounded, creative solutions. Additionally, the ModelingJudge system, which leverages domain-specialized LLMs, evaluates solutions from multiple expert perspectives. Empirical results demonstrate the superior performance of ModelingAgent compared to strong baselines, often producing solutions on par with human experts. This comprehensive framework aims to advance real-world problem-solving in open-ended, interdisciplinary modeling competitions.<br /><br />Summary: <div>
arXiv:2505.15068v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has enabled substantial advances in solving mathematical problems. However, existing benchmarks often fail to reflect the complexity of real-world problems, which demand open-ended, interdisciplinary reasoning and integration of computational tools. To address this gap, we introduce ModelingBench, a novel benchmark featuring real-world-inspired, open-ended problems from math modeling competitions across diverse domains, ranging from urban traffic optimization to ecosystem resource planning. These tasks require translating natural language into formal mathematical formulations, applying appropriate tools, and producing structured, defensible reports. ModelingBench also supports multiple valid solutions, capturing the ambiguity and creativity of practical modeling. We also present ModelingAgent, a multi-agent framework that coordinates tool use, supports structured workflows, and enables iterative self-refinement to generate well-grounded, creative solutions. To evaluate outputs, we further propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as domain-specialized judges assessing solutions from multiple expert perspectives. Empirical results show that ModelingAgent substantially outperforms strong baselines and often produces solutions indistinguishable from those of human experts. Together, our work provides a comprehensive framework for evaluating and advancing real-world problem-solving in open-ended, interdisciplinary modeling challenges.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>lmgame-Bench: How Good are LLMs at Playing Games?</title>
<link>https://arxiv.org/abs/2505.15146</link>
<guid>https://arxiv.org/abs/2505.15146</guid>
<content:encoded><![CDATA[
<div> Keywords: video games, large language models, evaluation, lmgame-Bench, reinforcement learning

Summary:<br />
Playing video games presents challenges for evaluating large language models (LLMs) due to vision perception, prompt sensitivity, and data contamination issues. To address these challenges, lmgame-Bench was introduced as a platform to evaluate LLMs across various game genres using a unified API with perception and memory scaffolds. The platform successfully separates different models and probes a unique blend of capabilities tested in isolation. Additionally, reinforcement learning on a single game from lmgame-Bench transfers to unseen games and external planning tasks, showcasing the platform's effectiveness. The evaluation code for lmgame-Bench is available on GitHub for further exploration and research.<br /> 
Summary: <div>
arXiv:2505.15146v1 Announce Type: new 
Abstract: Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge</title>
<link>https://arxiv.org/abs/2505.15240</link>
<guid>https://arxiv.org/abs/2505.15240</guid>
<content:encoded><![CDATA[
<div> Generalised probabilistic modelling, uncertainty estimation, comparative frameworks, Product-of-Experts methods, uncertainty estimates<br />
<br />
Summary:<br />
This paper examines probabilistic modelling and uncertainty estimation in comparative LLM-as-a-judge frameworks. It reveals that current Product-of-Experts techniques are a subset of a broader framework, allowing for various modelling choices. The study proposes enhanced uncertainty estimates for individual comparisons, resulting in more effective selection and better performance with fewer evaluations. Additionally, a method for estimating overall ranking uncertainty is introduced. The combination of absolute and comparative scoring is shown to enhance performance. Experimental results indicate that while the specific expert model has minimal impact on final rankings, the suggested uncertainty estimates notably enhance system efficiency by reducing the required number of comparisons by around 50%. Rank-level uncertainty metrics can detect inaccurate predictions, particularly where the probabilistic model influences uncertainty quality significantly. <div>
arXiv:2505.15240v1 Announce Type: new 
Abstract: This paper explores generalised probabilistic modelling and uncertainty estimation in comparative LLM-as-a-judge frameworks. We show that existing Product-of-Experts methods are specific cases of a broader framework, enabling diverse modelling options. Furthermore, we propose improved uncertainty estimates for individual comparisons, enabling more efficient selection and achieving strong performance with fewer evaluations. We also introduce a method for estimating overall ranking uncertainty. Finally, we demonstrate that combining absolute and comparative scoring improves performance. Experiments show that the specific expert model has a limited impact on final rankings but our proposed uncertainty estimates, especially the probability of reordering, significantly improve the efficiency of systems reducing the number of needed comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used to identify low-performing predictions, where the nature of the probabilistic model has a notable impact on the quality of the overall uncertainty.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification of Probabilities of Causation: A Complete Characterization</title>
<link>https://arxiv.org/abs/2505.15274</link>
<guid>https://arxiv.org/abs/2505.15274</guid>
<content:encoded><![CDATA[
<div> causation, probabilities, decision-making, structural causal models, tight bounds 

Summary:
This paper addresses the foundational gap in characterizing probabilities of causation with multi-valued treatments and outcomes within the framework of Structural Causal Models (SCMs). The authors propose a complete set of representative probabilities of causation and prove their sufficiency to capture all possible probabilities of causation. Tight bounds for these representative quantities are derived using formal mathematical proofs. The theoretical results are illustrated through toy examples to demonstrate their practical relevance in decision-making. <div>
arXiv:2505.15274v1 Announce Type: new 
Abstract: Probabilities of causation are fundamental to modern decision-making. Pearl first introduced three binary probabilities of causation, and Tian and Pearl later derived tight bounds for them using Balke's linear programming. The theoretical characterization of probabilities of causation with multi-valued treatments and outcomes has remained unresolved for decades, limiting the scope of causality-based decision-making. In this paper, we resolve this foundational gap by proposing a complete set of representative probabilities of causation and proving that they are sufficient to characterize all possible probabilities of causation within the framework of Structural Causal Models (SCMs). We then formally derive tight bounds for these representative quantities using formal mathematical proofs. Finally, we demonstrate the practical relevance of our results through illustrative toy examples.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning</title>
<link>https://arxiv.org/abs/2505.15276</link>
<guid>https://arxiv.org/abs/2505.15276</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, reinforcement learning, thinking modes, efficiency, accuracy

Summary:
This study examines the internal mechanisms of reinforcement learning-trained large reasoning models (LRMs) to understand how they can optimize thinking processes. The researchers identify three distinct thinking modes - no thinking, explicit thinking, and implicit thinking - and analyze factors such as confidence in thinking termination and attentional focus on input sections. It is found that no thinking reduces output length but sacrifices accuracy, while explicit and implicit thinking maintain accuracy with shorter responses. The study highlights the need for adaptive improvements in RL-optimized LRMs to enhance efficiency and reliability. Overall, the findings reveal fundamental inconsistencies in the reasoning behaviors of LRMs and suggest pathways for improving their performance. 

<br /><br />Summary: <div>
arXiv:2505.15276v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have significantly advanced performance on complex tasks, yet their tendency to overthink introduces inefficiencies. This study investigates the internal mechanisms of reinforcement learning (RL)-trained LRMs when prompted to save thinking, revealing three distinct thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking (IT). Through comprehensive analysis of confidence in thinking termination, attention from thinking to generation, and attentional focus on input sections, we uncover key factors influencing the reasoning behaviors. We further find that NT reduces output length at the cost of accuracy, while ET and IT maintain accuracy with reduced response length. Our findings expose fundamental inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for reliable efficiency.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.15400</link>
<guid>https://arxiv.org/abs/2505.15400</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, Adaptive Self-Recovery Reasoning, Efficiency, Performance, Safety

Summary:
Large reasoning models (LRMs) often face computational challenges due to redundant reasoning, especially on simple tasks. This study explores the upper bounds of LRMs and introduces the concept of Internal Self-Recovery Mechanism, where models supplement reasoning during answer generation. The Adaptive Self-Recovery Reasoning (ASRR) framework is proposed to suppress unnecessary reasoning and enable implicit recovery. ASRR allocates reasoning effort based on problem difficulty, achieving high efficiency with minimal accuracy loss. Experimental results show that ASRR reduces reasoning budget by up to 32.5% and 25.7% for different model sizes, with minimal accuracy loss. Additionally, ASRR significantly improves harmless rates on safety benchmarks. This research demonstrates the potential of ASRR to enhance the efficiency, adaptability, and safety of reasoning in LRMs.<br /><br />Summary: <div>
arXiv:2505.15400v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery Mechanism" where models implicitly supplement reasoning during answer generation. Building on this insight, we propose Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables implicit recovery. By introducing accuracy-aware length reward regulation, ASRR adaptively allocates reasoning effort according to problem difficulty, achieving high efficiency with negligible performance sacrifice. Experiments across multiple benchmarks and models show that, compared with GRPO, ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates on safety benchmarks (up to +21.7%). Our results highlight the potential of ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs</title>
<link>https://arxiv.org/abs/2505.15410</link>
<guid>https://arxiv.org/abs/2505.15410</guid>
<content:encoded><![CDATA[
<div> Keywords: Clickstream data, digital learning environments, Large Language Model, learning strategies, interpretation quality 

Summary: 
- ClickSight is introduced as a pipeline using Large Language Models (LLMs) to interpret student clickstreams and reveal their learning strategies.
- The pipeline takes raw clickstream data and a list of learning strategies as input and generates textual interpretations of students' behaviors during interaction in digital learning environments.
- Four different prompting strategies are evaluated, and the impact of self-refinement on interpretation quality is investigated.
- Evaluation is carried out in two open-ended learning environments using rubric-based domain-expert evaluation, showing varying interpretation quality based on prompting strategy and limited improvement from self-refinement.
- ClickSight demonstrates the potential of LLMs in generating theory-driven insights from educational interaction data. 

<br /><br />Summary: <div>
arXiv:2505.15410v1 Announce Type: new 
Abstract: Clickstream data from digital learning environments offer valuable insights into students' learning behaviors, but are challenging to interpret due to their high dimensionality and granularity. Prior approaches have relied mainly on handcrafted features, expert labeling, clustering, or supervised models, therefore often lacking generalizability and scalability. In this work, we introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline that interprets student clickstreams to reveal their learning strategies. ClickSight takes raw clickstreams and a list of learning strategies as input and generates textual interpretations of students' behaviors during interaction. We evaluate four different prompting strategies and investigate the impact of self-refinement on interpretation quality. Our evaluation spans two open-ended learning environments and uses a rubric-based domain-expert evaluation. Results show that while LLMs can reasonably interpret learning strategies from clickstreams, interpretation quality varies by prompting strategy, and self-refinement offers limited improvement. ClickSight demonstrates the potential of LLMs to generate theory-driven insights from educational interaction data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives</title>
<link>https://arxiv.org/abs/2505.15693</link>
<guid>https://arxiv.org/abs/2505.15693</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Reward Functions, Omega-Regular Languages, Model-Free RL, Average-Reward Objectives <br />
Summary: 
This article introduces a novel approach to designing reward functions in reinforcement learning by using omega-regular languages to specify behaviors. The focus is on absolute liveness specifications, which are particularly well-suited for infinite-horizon, continuing tasks. The framework presented translates these specifications into average-reward objectives, allowing for learning in communicating MDPs without episodic resetting. Additionally, a reward structure for lexicographic multi-objective optimization is introduced to maximize external average-reward objectives while satisfying omega-regular specifications. The method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions, enabling model-free RL. Empirical results show that this average-reward approach in the continuing setting outperforms discount-based methods across various benchmarks. <br /> <div>
arXiv:2505.15693v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) have renewed focus on the design of reward functions that shape agent behavior. Manually designing reward functions is tedious and error-prone. A principled alternative is to specify behaviors in a formal language that can be automatically translated into rewards. Omega-regular languages are a natural choice for this purpose, given their established role in formal verification and synthesis. However, existing methods using omega-regular specifications typically rely on discounted reward RL in episodic settings, with periodic resets. This setup misaligns with the semantics of omega-regular specifications, which describe properties over infinite behavior traces. In such cases, the average reward criterion and the continuing setting -- where the agent interacts with the environment over a single, uninterrupted lifetime -- are more appropriate.
  To address the challenges of infinite-horizon, continuing tasks, we focus on absolute liveness specifications -- a subclass of omega-regular languages that cannot be violated by any finite behavior prefix, making them well-suited to the continuing setting. We present the first model-free RL framework that translates absolute liveness specifications to average-reward objectives. Our approach enables learning in communicating MDPs without episodic resetting. We also introduce a reward structure for lexicographic multi-objective optimization, aiming to maximize an external average-reward objective among the policies that also maximize the satisfaction probability of a given omega-regular specification. Our method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions that do not require full knowledge of the environment, thus enabling model-free RL. Empirical results show our average-reward approach in continuing setting outperforms discount-based methods across benchmarks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Argumentative Learning with Case-Based Reasoning</title>
<link>https://arxiv.org/abs/2505.15742</link>
<guid>https://arxiv.org/abs/2505.15742</guid>
<content:encoded><![CDATA[
<div> Keywords: Gradual Abstract Argumentation, Case-Based Reasoning, neural-based feature extractors, multi-class classification, model interpretability

Summary: 
Gradual Abstract Argumentation for Case-Based Reasoning (Gradual AA-CBR) is a novel classification model that combines neural-based feature extractors with an argumentation debate structure learned from training data. This structure involves observed cases acting as arguments in favor of their labeling, attacking or supporting cases with opposing or agreeing labels. The strength of each argument is determined using gradient-based methods. Gradual AA-CBR offers improved model interpretability compared to traditional neural networks, supports multi-class classification, automatically learns feature and data point importance, assigns uncertainty values to outcomes, utilizes all available data points, and does not rely on binary features. Experimental results demonstrate that Gradual AA-CBR performs comparably to neural networks while surpassing existing Abstract Argumentation for Case-Based Reasoning formulations. <br /><br />Summary: <div>
arXiv:2505.15742v1 Announce Type: new 
Abstract: We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual AA-CBR), a data-driven, neurosymbolic classification model in which the outcome is determined by an argumentation debate structure that is learned simultaneously with neural-based feature extractors. Each argument in the debate is an observed case from the training data, favouring their labelling. Cases attack or support those with opposing or agreeing labellings, with the strength of each argument and relationship learned through gradient-based methods. This argumentation debate structure provides human-aligned reasoning, improving model interpretability compared to traditional neural networks (NNs). Unlike the existing purely symbolic variant, Abstract Argumentation for Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class classification, automatic learning of feature and data point importance, assigning uncertainty values to outcomes, using all available data points, and does not require binary features. We show that Gradual AA-CBR performs comparably to NNs whilst significantly outperforming existing AA-CBR formulations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering</title>
<link>https://arxiv.org/abs/2505.11626</link>
<guid>https://arxiv.org/abs/2505.11626</guid>
<content:encoded><![CDATA[
<div> Keywords: THELMA, RAG, question answering, evaluation, large language models 

Summary:
THELMA is a new framework for evaluating RAG-based question answering applications without the need for reference responses. It consists of six metrics designed for comprehensive evaluation of RAG QA pipelines, allowing developers to assess and enhance their applications. The framework facilitates monitoring and improvement of end-to-end processes, aiding in the identification of components that require enhancement. The interplay of THELMA metrics offers insights into areas needing improvement in RAG QA applications. This novel approach provides a reference-free method for evaluating and enhancing the performance of large language model applications in question answering scenarios. The metrics within THELMA enable a detailed assessment of various aspects of RAG QA pipelines, offering guidance for developers and application owners on optimizing their systems. <div>
arXiv:2505.11626v1 Announce Type: cross 
Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propositional Measure Logic</title>
<link>https://arxiv.org/abs/2505.14693</link>
<guid>https://arxiv.org/abs/2505.14693</guid>
<content:encoded><![CDATA[
<div> propositional logic, probabilistic semantics, soundness theorem, reasoning under uncertainty, Bayesian Networks<br /><br />Summary: 
The article introduces a propositional logic system with probabilistic semantics, where each formula is assigned a real measure in the range [0,1] to indicate its truth value. By replacing the binary nature of classical logic, this system maintains the deductive structure while accommodating uncertainty. The soundness theorem is presented to demonstrate the effectiveness of the proposed system for reasoning under uncertainty. Potential applications of this probabilistic logic are discussed, along with avenues for future research and extensions of the theory. The application of probabilistic logic to challenging problems in Bayesian Networks is also highlighted, showcasing the practical utility of this approach in addressing complex issues within this domain. <div>
arXiv:2505.14693v1 Announce Type: cross 
Abstract: We present a propositional logic with fundamental probabilistic semantics, in which each formula is given a real measure in the interval $[0,1]$ that represents its degree of truth. This semantics replaces the binarity of classical logic, while preserving its deductive structure. We demonstrate the soundness theorem, establishing that the proposed system is sound and suitable for reasoning under uncertainty. We discuss potential applications and avenues for future extensions of the theory. We apply probabilistic logic to a still refractory problem in Bayesian Networks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity</title>
<link>https://arxiv.org/abs/2505.14707</link>
<guid>https://arxiv.org/abs/2505.14707</guid>
<content:encoded><![CDATA[
<div> Dataset, Cryptic species, AI models, Biodiversity, Multimodal<br />
<br />
Summary: 
The article introduces CrypticBio, a large multimodal dataset aimed at supporting the development of AI models for biodiversity applications. Cryptic species, which are visually confusing and nearly indistinguishable based on visual characteristics, pose a challenge for taxonomic identification. CrypticBio contains 52K unique cryptic groups spanning 67K species, represented in 166 million images, curated from real-world trends in species misidentification. The dataset includes rich research-grade annotations and integrates geographical and temporal data to aid in identifying cryptic species. The open-source pipeline CrypticBio-Curate facilitates dataset curation. The article benchmarks state-of-the-art models across subsets of common, unseen, endangered, and invasive species, highlighting the impact of geographical context on vision-language zero-shot learning. The goal of CrypticBio is to advance the development of biodiversity AI models capable of addressing the complexities of species ambiguity. 

<br /> <div>
arXiv:2505.14707v1 Announce Type: cross 
Abstract: We present CrypticBio, the largest publicly available multimodal dataset of visually confusing species, specifically curated to support the development of AI models in the context of biodiversity applications. Visually confusing or cryptic species are groups of two or more taxa that are nearly indistinguishable based on visual characteristics alone. While much existing work addresses taxonomic identification in a broad sense, datasets that directly address the morphological confusion of cryptic species are small, manually curated, and target only a single taxon. Thus, the challenge of identifying such subtle differences in a wide range of taxa remains unaddressed. Curated from real-world trends in species misidentification among community annotators of iNaturalist, CrypticBio contains 52K unique cryptic groups spanning 67K species, represented in 166 million images. Rich research-grade image annotations--including scientific, multicultural, and multilingual species terminology, hierarchical taxonomy, spatiotemporal context, and associated cryptic groups--address multimodal AI in biodiversity research. For easy dataset curation, we provide an open-source pipeline CrypticBio-Curate. The multimodal nature of the dataset beyond vision-language arises from the integration of geographical and temporal data as complementary cues to identifying cryptic species. To highlight the importance of the dataset, we benchmark a suite of state-of-the-art foundation models across CrypticBio subsets of common, unseen, endangered, and invasive species, and demonstrate the substantial impact of geographical context on vision-language zero-shot learning for cryptic species. By introducing CrypticBio, we aim to catalyze progress toward real-world-ready biodiversity AI models capable of handling the nuanced challenges of species ambiguity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance</title>
<link>https://arxiv.org/abs/2505.14708</link>
<guid>https://arxiv.org/abs/2505.14708</guid>
<content:encoded><![CDATA[
<div> Keywords: DiTs, video generation, DraftAttention, dynamic sparse attention, GPU acceleration 

Summary: 
DraftAttention is a training-free framework designed to accelerate video diffusion transformers by implementing dynamic sparse attention on GPUs. By applying down-sampling to feature maps and utilizing low-resolution draft attention maps derived from draft queries and keys, the framework enables a higher-level receptive field and exposes spatial and temporal redundancies for efficient computation. Reordering query, key, and value based on the draft attention map guides sparse attention computation and facilitates structured sparsity aligned with hardware optimization. The theoretical analysis confirms the accuracy of the low-resolution draft attention in approximating full attention, leading to improved video generation quality and up to 1.75x speedup on GPUs compared to existing sparse attention methods. The code for DraftAttention is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.14708v1 Announce Type: cross 
Abstract: Diffusion transformer-based video generation models (DiTs) have recently attracted widespread attention for their excellent generation quality. However, their computational cost remains a major bottleneck-attention alone accounts for over 80% of total latency, and generating just 8 seconds of 720p video takes tens of minutes-posing serious challenges to practical application and scalability. To address this, we propose the DraftAttention, a training-free framework for the acceleration of video diffusion transformers with dynamic sparse attention on GPUs. We apply down-sampling to each feature map across frames in the compressed latent space, enabling a higher-level receptive field over the latent composed of hundreds of thousands of tokens. The low-resolution draft attention map, derived from draft query and key, exposes redundancy both spatially within each feature map and temporally across frames. We reorder the query, key, and value based on the draft attention map to guide the sparse attention computation in full resolution, and subsequently restore their original order after the attention computation. This reordering enables structured sparsity that aligns with hardware-optimized execution. Our theoretical analysis demonstrates that the low-resolution draft attention closely approximates the full attention, providing reliable guidance for constructing accurate sparse attention. Experimental results show that our method outperforms existing sparse attention approaches in video generation quality and achieves up to 1.75x end-to-end speedup on GPUs. Code: https://github.com/shawnricecake/draft-attention
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge</title>
<link>https://arxiv.org/abs/2505.14709</link>
<guid>https://arxiv.org/abs/2505.14709</guid>
<content:encoded><![CDATA[
<div> auto-regressive models, video generation, temporal redundancy, FastCar framework, hardware accelerator<br />
Summary:<br />
The FastCar framework aims to accelerate auto-regressive video generation by exploiting temporal redundancy. It introduces the Temporal Attention Score (TAS) to determine when to reuse cached MLP outputs from previous frames, reducing redundant computations. A hardware accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS enhances resource utilization and speeds up inference. Experimental results show that FastCar outperforms traditional sparse attention methods with a 2.1x decoding speedup and improved energy efficiency. When combined with sparse attention, FastCar enhances performance by reducing drifting in high-resolution, long-duration video generation tasks. The approach demonstrates significant benefits in terms of speed, efficiency, and quality of generated videos, making it a promising solution for accelerating video generation tasks. <br /> <div>
arXiv:2505.14709v1 Announce Type: cross 
Abstract: Auto-regressive (AR) models, initially successful in language generation, have recently shown promise in visual generation tasks due to their superior sampling efficiency. Unlike image generation, video generation requires a substantially larger number of tokens to produce coherent temporal frames, resulting in significant overhead during the decoding phase. Our key observations are: (i) MLP modules in the decode phase dominate the inference latency, and (ii) there exists high temporal redundancy in MLP outputs of adjacent frames. In this paper, we propose the \textbf{FastCar} framework to accelerate the decode phase for the AR video generation by exploring the temporal redundancy. The Temporal Attention Score (TAS) is proposed to determine whether to apply the replay strategy (\textit{i.e.}, reusing cached MLP outputs from the previous frame to reduce redundant computations) with detailed theoretical analysis and justification. Also, we develop a hardware accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to enable better resource utilization and faster inference. Experimental results demonstrate the effectiveness of our method, which outperforms traditional sparse attention approaches with more than 2.1x decoding speedup and higher energy efficiency on the edge. Furthermore, by combining FastCar and sparse attention, FastCar can boost the performance of sparse attention with alleviated drifting, demonstrating our unique advantages for high-resolution and long-duration video generation. Code: https://github.com/shawnricecake/fast-car
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Space evaluation at the starting point of soccer transitions</title>
<link>https://arxiv.org/abs/2505.14711</link>
<guid>https://arxiv.org/abs/2505.14711</guid>
<content:encoded><![CDATA[
<div> Keywords: Soccer, Space evaluation, Transition, Off-Ball Positioning Value, La Liga

Summary: 
- Soccer is a sport where effective use of space is crucial for success on the pitch.
- Decision-making during transitions between possession changes is becoming increasingly important.
- Research on space evaluation during transitions has been limited.
- The Off-Ball Positioning Value (OBPV) method is proposed to assess space utilization during transitions, using the field value model and transition kernel model.
- Experiments using La Liga 2023/24 season data show that OBPV can highlight effective space utilization during counter-attacks and reveal team-specific characteristics in how they utilize space after transitions.
<br /><br />Summary: <div>
arXiv:2505.14711v1 Announce Type: cross 
Abstract: Soccer is a sport played on a pitch where effective use of space is crucial. Decision-making during transitions, when possession switches between teams, has been increasingly important, but research on space evaluation in these moments has been limited. Recent space evaluation methods such as OBSO (Off-Ball Scoring Opportunity) use scoring probability, so it is not well-suited for assessing areas far from the goal, where transitions typically occur. In this paper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across the pitch, including the starting points of transitions. OBPV extends OBSO by introducing the field value model, which evaluates the entire pitch, and by employing the transition kernel model, which reflects positional specificity through kernel density estimation of pass distributions. Experiments using La Liga 2023/24 season tracking and event data show that OBPV highlights effective space utilization during counter-attacks and reveals team-specific characteristics in how the teams utilize space after positive and negative transitions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection</title>
<link>https://arxiv.org/abs/2505.14714</link>
<guid>https://arxiv.org/abs/2505.14714</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, multi-modal fusion, knowledge graph, Transformer-based classifier, semantic understanding

Summary: 
The article presents a novel multi-modal approach for fake news detection, addressing the challenges of incorporating fine-grained object details and external knowledge reasoning. The proposed framework integrates visual, textual, and knowledge-based representations, leveraging bottom-up attention, CLIP, and RoBERTa for feature extraction. By retrieving and selecting relevant entities from a knowledge graph, the model enhances semantic understanding and veracity prediction. Experimental results showcase the effectiveness of the neighbor selection mechanism and multi-modal fusion in outperforming existing approaches. The approach introduces a new paradigm of knowledge-grounded multimodal reasoning, shifting the focus towards semantically grounded verification. The source code is publicly available for reproducibility and further research at github.com/latuanvinh1998/KGAlign. 

<br /><br />Summary: <div>
arXiv:2505.14714v1 Announce Type: cross 
Abstract: Fake news detection remains a challenging problem due to the complex interplay between textual misinformation, manipulated images, and external knowledge reasoning. While existing approaches have achieved notable results in verifying veracity and cross-modal consistency, two key challenges persist: (1) Existing methods often consider only the global image context while neglecting local object-level details, and (2) they fail to incorporate external knowledge and entity relationships for deeper semantic understanding. To address these challenges, we propose a novel multi-modal fake news detection framework that integrates visual, textual, and knowledge-based representations. Our approach leverages bottom-up attention to capture fine-grained object details, CLIP for global image semantics, and RoBERTa for context-aware text encoding. We further enhance knowledge utilization by retrieving and adaptively selecting relevant entities from a knowledge graph. The fused multi-modal features are processed through a Transformer-based classifier to predict news veracity. Experimental results demonstrate that our model outperforms recent approaches, showcasing the effectiveness of neighbor selection mechanism and multi-modal fusion for fake news detection. Our proposal introduces a new paradigm: knowledge-grounded multimodal reasoning. By integrating explicit entity-level selection and NLI-guided filtering, we shift fake news detection from feature fusion to semantically grounded verification. For reproducibility and further research, the source code is publicly at \href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks</title>
<link>https://arxiv.org/abs/2505.14717</link>
<guid>https://arxiv.org/abs/2505.14717</guid>
<content:encoded><![CDATA[
<div> Keywords: Intracranial aneurysms, Computational fluid dynamics, Machine learning algorithms, Aneurysm dataset, Hemodynamic influences

Summary:
The article introduces a new dataset curated for studying intracranial aneurysms (IAs) using computational fluid dynamics (CFD) and machine learning algorithms. The dataset includes 427 real aneurysm geometries and 10,660 synthetic shapes generated through controlled deformation. CFD computations were performed on these shapes under different flow conditions, resulting in a total of 85,280 blood flow dynamics data points. The dataset also includes segmentation masks for supporting tasks using various data modalities. A benchmark for estimating flow parameters has been introduced to assess current modeling methods. This dataset aims to advance research in aneurysm biology, biomedical engineering, and clinical risk assessment. The code and dataset are publicly available for further research and applications. 

Summary: <br /><br /> <div>
arXiv:2505.14717v1 Announce Type: cross 
Abstract: Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in approximately 5\% of the general population. Their rupture may lead to high mortality. Current methods for assessing IA risk focus on morphological and patient-specific factors, but the hemodynamic influences on IA development and rupture remain unclear. While accurate for hemodynamic studies, conventional computational fluid dynamics (CFD) methods are computationally intensive, hindering their deployment in large-scale or real-time clinical applications. To address this challenge, we curated a large-scale, high-fidelity aneurysm CFD dataset to facilitate the development of efficient machine learning algorithms for such applications. Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The authenticity of these synthetic shapes was confirmed by neurosurgeons. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters. Furthermore, the dataset includes segmentation masks, which can support tasks that use images, point clouds or other multimodal data as input. Additionally, we introduced a benchmark for estimating flow parameters to assess current modeling methods. This dataset aims to advance aneurysm research and promote data-driven approaches in biofluids, biomedical engineering, and clinical risk assessment. The code and dataset are available at: https://github.com/Xigui-Li/Aneumo.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection</title>
<link>https://arxiv.org/abs/2505.14718</link>
<guid>https://arxiv.org/abs/2505.14718</guid>
<content:encoded><![CDATA[
<div> Efficient Network, Semantic Segmentation, Industrial Image Inspection, Shape-Aware, Real-time Constraints<br />
<br />
Summary:<br />
- Conventional semantic segmentation models struggle with maintaining segmentation consistency in industrial image inspection due to a lack of object contour perception.<br />
- Shape-Aware Efficient Network (SPENet) is introduced to focus on object shapes for better segmentation consistency by separately extracting boundary and body information.<br />
- A novel method called Variable Boundary Domain (VBD) is proposed to describe fuzzy boundaries and adapt to real-world scenarios.<br />
- Consistency Mean Square Error (CMSE) is a new metric suggested to measure segmentation consistency for fixed components.<br />
- SPENet achieves superior segmentation accuracy and speed, demonstrating significant advantages in CMSE compared to state-of-the-art real-time segmentation networks with over 50% reduction in complexity. <br /> <div>
arXiv:2505.14718v1 Announce Type: cross 
Abstract: Semantic segmentation stands as a pivotal research focus in computer vision. In the context of industrial image inspection, conventional semantic segmentation models fail to maintain the segmentation consistency of fixed components across varying contextual environments due to a lack of perception of object contours. Given the real-time constraints and limited computing capability of industrial image detection machines, it is also necessary to create efficient models to reduce computational complexity. In this work, a Shape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes of objects to achieve excellent segmentation consistency by separately supervising the extraction of boundary and body information from images. In SPENet, a novel method is introduced for describing fuzzy boundaries to better adapt to real-world scenarios named Variable Boundary Domain (VBD). Additionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to measure segmentation consistency for fixed components. Our approach attains the best segmentation accuracy and competitive speed on our dataset, showcasing significant advantages in CMSE among numerous state-of-the-art real-time segmentation networks, achieving a reduction of over 50% compared to the previously top-performing models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion</title>
<link>https://arxiv.org/abs/2505.14719</link>
<guid>https://arxiv.org/abs/2505.14719</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, Vision Transformers, Energy-efficient computing, Multi-scale spiking attention, MSVIT <br />
Summary:
Spiking Neural Networks (SNNs) combined with Vision Transformer architectures hold promise for energy-efficient and high-performance computing. However, there is still a performance gap compared to traditional ANN-based transformers. Existing methods suffer from a feature extraction bottleneck across different image scales. To address this, a novel spike-driven Transformer architecture named MSVIT is proposed, utilizing multi-scale spiking attention (MSSA) to enhance spiking attention blocks. Experimental results on various datasets demonstrate that MSVIT surpasses existing SNN-based models, establishing itself as a leading solution in SNN-transformer architectures. The codes for MSVIT are available at https://github.com/Nanhu-AI-Lab/MSViT.<br /><br />Summary: <br />Spiking Neural Networks combined with Vision Transformer architectures show potential for high-performance and energy-efficient computing. However, existing methods face challenges in feature extraction across image scales. The proposed MSVIT architecture, incorporating multi-scale spiking attention, achieves superior performance compared to current SNN-based models, establishing itself as a state-of-the-art solution. <div>
arXiv:2505.14719v1 Announce Type: cross 
Abstract: The combination of Spiking Neural Networks(SNNs) with Vision Transformer architectures has attracted significant attention due to the great potential for energy-efficient and high-performance computing paradigms. However, a substantial performance gap still exists between SNN-based and ANN-based transformer architectures. While existing methods propose spiking self-attention mechanisms that are successfully combined with SNNs, the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting features from different image scales. In this paper, we address this issue and propose MSVIT, a novel spike-driven Transformer architecture, which firstly uses multi-scale spiking attention (MSSA) to enrich the capability of spiking attention blocks. We validate our approach across various main data sets. The experimental results show that MSVIT outperforms existing SNN-based models, positioning itself as a state-of-the-art solution among SNN-transformer architectures. The codes are available at https://github.com/Nanhu-AI-Lab/MSViT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding</title>
<link>https://arxiv.org/abs/2505.14723</link>
<guid>https://arxiv.org/abs/2505.14723</guid>
<content:encoded><![CDATA[
<div> Keywords: Spoken Language Understanding, distillation, quantization, efficiency, compression

Summary:
QUADS is a unified framework that optimizes both distillation and quantization in Spoken Language Understanding (SLU) systems. By utilizing multi-stage training with a pre-tuned model, QUADS enhances adaptability to low-bit regimes while maintaining accuracy. The framework achieves 71.13% accuracy on SLURP and 99.20% on FSC, with minimal degradations compared to state-of-the-art models. QUADS significantly reduces computational complexity by 60-73 times (GMACs) and model size by 83-700 times, showcasing its robustness under extreme quantization. This makes QUADS a highly efficient solution for real-world, resource-constrained SLU applications.<br /><br />Summary: QUADS is a unified framework optimizing distillation and quantization in SLU systems. It maintains high accuracy while achieving significant compression in computational complexity and model size. With results showing strong performance under extreme quantization, QUADS is positioned as an efficient solution for resource-constrained SLU applications. <div>
arXiv:2505.14723v1 Announce Type: cross 
Abstract: Spoken Language Understanding (SLU) systems must balance performance and efficiency, particularly in resource-constrained environments. Existing methods apply distillation and quantization separately, leading to suboptimal compression as distillation ignores quantization constraints. We propose QUADS, a unified framework that optimizes both through multi-stage training with a pre-tuned model, enhancing adaptability to low-bit regimes while maintaining accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with only minor degradations of up to 5.56\% compared to state-of-the-art models. Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and model size by 83--700$\times$, demonstrating strong robustness under extreme quantization. These results establish QUADS as a highly efficient solution for real-world, resource-constrained SLU applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedBLIP: Fine-tuning BLIP for Medical Image Captioning</title>
<link>https://arxiv.org/abs/2505.14726</link>
<guid>https://arxiv.org/abs/2505.14726</guid>
<content:encoded><![CDATA[
<div> fine-tuning, medical image captioning, radiology, vision-language models, ROCO dataset <br />
<br />
Summary: 
In this study, the effectiveness of fine-tuning the BLIP model on the ROCO dataset for improved radiology captioning was explored. Comparison was made against zero-shot BLIP, BLIP-2 base, BLIP-2 Instruct, and ViT-GPT2 transformer baseline. Results showed that domain-specific fine-tuning significantly improved performance quantitatively and qualitatively. Visualizations of decoder cross-attention maps were used for interpretability, and an ablation study evaluated the impact of encoder-only and decoder-only fine-tuning. Findings indicated the importance of targeted adaptation for medical applications and suggested decoder-only fine-tuning as a strong baseline with lower training time but full fine-tuning still yielding best overall results. <div>
arXiv:2505.14726v1 Announce Type: cross 
Abstract: Medical image captioning is a challenging task that requires generating clinically accurate and semantically meaningful descriptions of radiology images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini and ViT-GPT2 show strong performance on natural image datasets, they often produce generic or imprecise captions when applied to specialized medical domains. In this project, we explore the effectiveness of fine-tuning the BLIP model on the ROCO dataset for improved radiology captioning. We compare the fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific fine-tuning on BLIP significantly improves performance across both quantitative and qualitative evaluation metrics. We also visualize decoder cross-attention maps to assess interpretability and conduct an ablation study to evaluate the contributions of encoder-only and decoder-only fine-tuning. Our findings highlight the importance of targeted adaptation for medical applications and suggest that decoder-only fine-tuning (encoder-frozen) offers a strong performance baseline with 5% lower training time than full fine-tuning, while full model fine-tuning still yields the best results overall.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models</title>
<link>https://arxiv.org/abs/2505.14728</link>
<guid>https://arxiv.org/abs/2505.14728</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, moral alignment, benchmark, real-world data, moral limitations

Summary:
The paper introduces MORALISE, a benchmark for assessing the moral alignment of vision-language models (VLMs) by utilizing expert-verified real-world data. It includes a taxonomy of 13 moral topics and 2,481 image-text pairs annotated with violated moral topics and modalities. Two evaluation tasks, moral judgment, and moral norm attribution, test models' awareness of moral violations and reasoning on moral content. The benchmark exposes shortcomings in current VLMs regarding moral alignment. This comprehensive approach addresses limitations in existing work by incorporating diverse real-world data and fine-grained annotations. MORALISE aims to ensure that VLM outputs align with human moral values in high-stakes applications. The full benchmark is accessible for public use at the provided link.

<br /><br />Summary: <div>
arXiv:2505.14728v1 Announce Type: cross 
Abstract: Warning: This paper contains examples of harmful language and images. Reader discretion is advised. Recently, vision-language models have demonstrated increasing influence in morally sensitive domains such as autonomous driving and medical analysis, owing to their powerful multimodal reasoning capabilities. As these models are deployed in high-stakes real-world applications, it is of paramount importance to ensure that their outputs align with human moral values and remain within moral boundaries. However, existing work on moral alignment either focuses solely on textual modalities or relies heavily on AI-generated images, leading to distributional biases and reduced realism. To overcome these limitations, we introduce MORALISE, a comprehensive benchmark for evaluating the moral alignment of vision-language models (VLMs) using diverse, expert-verified real-world data. We begin by proposing a comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory, spanning the personal, interpersonal, and societal moral domains encountered in everyday life. Built on this framework, we manually curate 2,481 high-quality image-text pairs, each annotated with two fine-grained labels: (1) topic annotation, identifying the violated moral topic(s), and (2) modality annotation, indicating whether the violation arises from the image or the text. For evaluation, we encompass two tasks, \textit{moral judgment} and \textit{moral norm attribution}, to assess models' awareness of moral violations and their reasoning ability on morally salient content. Extensive experiments on 19 popular open- and closed-source VLMs show that MORALISE poses a significant challenge, revealing persistent moral limitations in current state-of-the-art models. The full benchmark is publicly available at https://huggingface.co/datasets/Ze1025/MORALISE.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute</title>
<link>https://arxiv.org/abs/2505.14733</link>
<guid>https://arxiv.org/abs/2505.14733</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, test-time compute, accuracy-energy trade-offs, inference, query complexity

Summary:
Test-time compute (TTC) is proposed as a method to enhance the efficiency of large language models (LLMs) by allocating additional computational resources during inference. Empirical analysis shows that TTC outperforms traditional model scaling in terms of accuracy-energy trade-offs, especially in tasks requiring complex reasoning. The study highlights the importance of adjusting compute resources at inference time based on query complexity, leading to significant improvements in efficiency. This approach offers a more sustainable and adaptable deployment of future language models without incurring additional pretraining costs. <div>
arXiv:2505.14733v1 Announce Type: cross 
Abstract: Scaling large language models (LLMs) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work introduces test-time compute (TTC)-allocating additional computational resources during inference-as a compelling complement to conventional scaling strategies. Specifically, we investigate whether employing TTC can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that TTC surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between TTC performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for TTC as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models without incurring additional pretraining costs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multivariate Long-Term History Representation for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.14737</link>
<guid>https://arxiv.org/abs/2505.14737</guid>
<content:encoded><![CDATA[
<div> Keywords: Multivariate Time Series, Spatial-Temporal Graph Neural Network, Long-term History Encoder, Hierarchical Representation Retriever, Transformer-based Aggregator

Summary:
LMHR Enhanced STGNN framework is proposed for Multivariate Time Series (MTS) forecasting. It addresses the limitations of current STGNNs by incorporating long-term spatial-temporal similarities and correlations. The Long-term History Encoder effectively encodes long-term history into segment-level contextual representations, reducing noise. The non-parametric Hierarchical Representation Retriever includes spatial information in modeling long-term dependencies without additional training. A Transformer-based Aggregator selectively fuses retrieved contextual representations based on ranking positional embedding. Experimental results show LMHR outperforms typical STGNNs by 10.72% on average prediction horizons and state-of-the-art methods by 4.12% on real-world datasets. It also improves prediction accuracy by 9.8% on rapidly changing patterns. This framework represents a significant advancement in MTS forecasting by capturing long-term spatial-temporal relationships accurately. <br /><br />Summary: <div>
arXiv:2505.14737v1 Announce Type: cross 
Abstract: Multivariate Time Series (MTS) forecasting has a wide range of applications in both industry and academia. Recent advances in Spatial-Temporal Graph Neural Network (STGNN) have achieved great progress in modelling spatial-temporal correlations. Limited by computational complexity, most STGNNs for MTS forecasting focus primarily on short-term and local spatial-temporal dependencies. Although some recent methods attempt to incorporate univariate history into modeling, they still overlook crucial long-term spatial-temporal similarities and correlations across MTS, which are essential for accurate forecasting. To fill this gap, we propose a framework called the Long-term Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting. Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively encode the long-term history into segment-level contextual representations and reduce point-level noise. A non-parametric Hierarchical Representation Retriever (HRetriever) is designed to include the spatial information in the long-term spatial-temporal dependency modelling and pick out the most valuable representations with no additional training. A Transformer-based Aggregator (TAggregator) selectively fuses the sparsely retrieved contextual representations based on the ranking positional embedding efficiently. Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72% on the average prediction horizons and state-of-the-art methods by 4.12% on several real-world datasets. Additionally, it consistently improves prediction accuracy by 9.8% on the top 10% of rapidly changing patterns across the datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs</title>
<link>https://arxiv.org/abs/2505.14739</link>
<guid>https://arxiv.org/abs/2505.14739</guid>
<content:encoded><![CDATA[
<div> Keywords: Denoising diffusion probabilistic models, synthetic sensor signals, similarity metrics, training process, generative models

Summary: 
Denoising diffusion probabilistic models are used to generate synthetic sensor signals by training them with a loss function that measures noise differences. However, assessing the quality of the data generated by these models can be challenging due to randomness and loss function complexity. To address this, the study explores various similarity metrics and adapts one to monitor the training and data generation process effectively. This adapted metric can be fine-tuned on input data for classification tasks, leading to a reduction in training epochs without compromising performance. Optimizing the training process not only conserves resources but also cuts down on generative model training time. <div>
arXiv:2505.14739v1 Announce Type: cross 
Abstract: Denoising diffusion probabilistic models are able to generate synthetic sensor signals. The training process of such a model is controlled by a loss function which measures the difference between the noise that was added in the forward process and the noise that was predicted by the diffusion model. This enables the generation of realistic data. However, the randomness within the process and the loss function itself makes it difficult to estimate the quality of the data. Therefore, we examine multiple similarity metrics and adapt an existing metric to overcome this issue by monitoring the training and synthetisation process using those metrics. The adapted metric can even be fine-tuned on the input data to comply with the requirements of an underlying classification task. We were able to significantly reduce the amount of training epochs without a performance reduction in the classification task. An optimized training process not only saves resources, but also reduces the time for training generative models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism</title>
<link>https://arxiv.org/abs/2505.14741</link>
<guid>https://arxiv.org/abs/2505.14741</guid>
<content:encoded><![CDATA[
<div> **Keywords:** diffusion models, parallelization, inference latency, generative models, communication efficiency

Summary:
Diffusion models have shown great potential in generative modeling but suffer from slow inference due to the sequential denoising process. Traditional parallelization methods introduce high communication overhead, limiting deployment on commercial hardware. The proposed ParaStep method addresses this issue by parallelizing diffusion inference through a reuse-then-predict mechanism that leverages similarities between adjacent denoising steps. Unlike previous approaches, ParaStep uses lightweight step-wise communication, significantly reducing overhead. The results demonstrate impressive speedups of up to 3.88x on SVD, 2.43x on CogVideoX-2b, and 6.56x on AudioLDM2-large while maintaining generation quality. ParaStep emerges as a scalable and communication-efficient solution for accelerating diffusion inference, particularly useful in bandwidth-constrained environments.

<br /><br />Summary: <div>
arXiv:2505.14741v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a powerful class of generative models across various modalities, including image, video, and audio synthesis. However, their deployment is often limited by significant inference latency, primarily due to the inherently sequential nature of the denoising process. While existing parallelization strategies attempt to accelerate inference by distributing computation across multiple devices, they typically incur high communication overhead, hindering deployment on commercial hardware. To address this challenge, we propose \textbf{ParaStep}, a novel parallelization method based on a reuse-then-predict mechanism that parallelizes diffusion inference by exploiting similarity between adjacent denoising steps. Unlike prior approaches that rely on layer-wise or stage-wise communication, ParaStep employs lightweight, step-wise communication, substantially reducing overhead. ParaStep achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD, \textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on AudioLDM2-large, while maintaining generation quality. These results highlight ParaStep as a scalable and communication-efficient solution for accelerating diffusion inference, particularly in bandwidth-constrained environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis</title>
<link>https://arxiv.org/abs/2505.14742</link>
<guid>https://arxiv.org/abs/2505.14742</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, fine-tuning, quantization, outlier suppression, efficiency

Summary:
The paper introduces Quaff, a framework for efficient fine-tuning of Large Language Models (LLMs) on resource-constrained personal devices. Quaff is based on the Outlier Spatial Stability Hypothesis (OSSH), which suggests that certain activation outlier channels remain stable across training iterations. By dynamically suppressing outliers in invariant channels using targeted momentum scaling, Quaff optimizes low-precision activation representations without the need for full-precision weight storage and global rescaling. Extensive experiments across ten benchmarks validate the effectiveness of Quaff, showing significant reductions in latency and memory usage while improving accuracy compared to full-precision fine-tuning. Notably, Quaff enables fine-tuning on consumer-grade GPUs without sacrificing model utility, making personalized LLM deployment more accessible. The code for Quaff is available on GitHub for further exploration and implementation. <br /><br />Summary: 
- Introduces Quaff framework for efficient fine-tuning of LLMs
- Based on the Outlier Spatial Stability Hypothesis
- Dynamically suppresses outliers in invariant channels using targeted momentum scaling
- Significantly reduces latency and memory usage while improving accuracy
- Enables fine-tuning on consumer-grade GPUs without sacrificing model utility <div>
arXiv:2505.14742v1 Announce Type: cross 
Abstract: Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transductively Informed Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2505.14744</link>
<guid>https://arxiv.org/abs/2505.14744</guid>
<content:encoded><![CDATA[
<div> framework, transductive, inductive, program synthesis, synergy
Summary:
The article introduces a novel framework called TIIPS that combines transductive and inductive strategies in program synthesis. TIIPS explicitly models the interaction between these paradigms through a cooperative mechanism, with an inductive model generating programs and a transductive model guiding and refining the search process. Evaluation on string and list manipulation tasks shows that TIIPS outperforms existing approaches in solving tasks and producing functions that closely match optimal solutions in syntax and semantics, especially in out-of-distribution scenarios. By leveraging the synergy between inductive and transductive reasoning, TIIPS enhances synthesis accuracy and generalization, promising advancements in general-purpose program synthesis and broader applications. 
<br /><br />Summary: <div>
arXiv:2505.14744v1 Announce Type: cross 
Abstract: Abstraction and reasoning in program synthesis has seen significant progress through both inductive and transductive paradigms. Inductive approaches generate a program or latent function from input-output examples, which can then be applied to new inputs. Transductive approaches directly predict output values for given inputs, effectively serving as the function themselves. Current approaches combine inductive and transductive models via isolated ensembling, but they do not explicitly model the interaction between both paradigms. In this work, we introduce \acs{tiips}, a novel framework that unifies transductive and inductive strategies by explicitly modeling their interactions through a cooperative mechanism: an inductive model generates programs, while a transductive model constrains, guides, and refines the search to improve synthesis accuracy and generalization. We evaluate \acs{tiips} on two widely studied program synthesis domains: string and list manipulation. Our results show that \acs{tiips} solves more tasks and yields functions that more closely match optimal solutions in syntax and semantics, particularly in out-of-distribution settings, yielding state-of-the-art performance. We believe that explicitly modeling the synergy between inductive and transductive reasoning opens promising avenues for general-purpose program synthesis and broader applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Prediction of the Mechanical Properties of Composites with CNNs</title>
<link>https://arxiv.org/abs/2505.14745</link>
<guid>https://arxiv.org/abs/2505.14745</guid>
<content:encoded><![CDATA[
<div> FE modelling, composites, convolutional neural networks, explainable AI, mechanical properties
Summary: 
Finite Element (FE) modelling is commonly used to assess mechanical properties of composites, but it is computationally expensive. Previous attempts to use AI models for this task lacked accuracy, transparency, and consideration of material strength limits. This study proposes using convolutional neural networks (CNNs) with explainable AI (XAI) methods to predict mechanical properties of composites. Customized CNNs trained on a dataset generated from transverse tension tests outperform baseline models in estimating Young's modulus and yield strength. The use of XAI methods such as SHAP and Integrated Gradients allows for explanation of the predictions, demonstrating that the CNNs rely on critical geometrical features influencing composite behavior. This approach provides accurate predictions while ensuring transparency and trustworthiness for engineers assessing composite suitability for specific applications. <div>
arXiv:2505.14745v1 Announce Type: cross 
Abstract: Composites are amongst the most important materials manufactured today, as evidenced by their use in countless applications. In order to establish the suitability of composites in specific applications, finite element (FE) modelling, a numerical method based on partial differential equations, is the industry standard for assessing their mechanical properties. However, FE modelling is exceptionally costly from a computational viewpoint, a limitation which has led to efforts towards applying AI models to this task. However, in these approaches: the chosen model architectures were rudimentary, feed-forward neural networks giving limited accuracy; the studies focus on predicting elastic mechanical properties, without considering material strength limits; and the models lacked transparency, hindering trustworthiness by users. In this paper, we show that convolutional neural networks (CNNs) equipped with methods from explainable AI (XAI) can be successfully deployed to solve this problem. Our approach uses customised CNNs trained on a dataset we generate using transverse tension tests in FE modelling to predict composites' mechanical properties, i.e., Young's modulus and yield strength. We show empirically that our approach achieves high accuracy, outperforming a baseline, ResNet-34, in estimating the mechanical properties. We then use SHAP and Integrated Gradients, two post-hoc XAI methods, to explain the predictions, showing that the CNNs use the critical geometrical features that influence the composites' behaviour, thus allowing engineers to verify that the models are trustworthy by representing the science of composites.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self Distillation via Iterative Constructive Perturbations</title>
<link>https://arxiv.org/abs/2505.14751</link>
<guid>https://arxiv.org/abs/2505.14751</guid>
<content:encoded><![CDATA[
<div> optimization, neural networks, training, generalization, performance
Summary:<br />
- The paper introduces a novel framework for training Deep Neural Networks that focuses on balancing performance and generalization. 
- The framework uses a cyclic optimization strategy that simultaneously optimizes the model and its input data for better training results. 
- The Iterative Constructive Perturbation (ICP) method is central to the approach, leveraging the model's loss to iteratively perturb the input data and construct an enhanced representation over refinement steps. 
- The ICP input is fed back into the model to produce improved intermediate features, which are used in a self-distillation framework against the original features. 
- By alternately adjusting the model's parameters and the input data, the method effectively bridges the gap between fitting and generalization, leading to enhanced performance. 
<br /><br />Summary: <div>
arXiv:2505.14751v1 Announce Type: cross 
Abstract: Deep Neural Networks have achieved remarkable achievements across various domains, however balancing performance and generalization still remains a challenge while training these networks. In this paper, we propose a novel framework that uses a cyclic optimization strategy to concurrently optimize the model and its input data for better training, rethinking the traditional training paradigm. Central to our approach is Iterative Constructive Perturbation (ICP), which leverages the model's loss to iteratively perturb the input, progressively constructing an enhanced representation over some refinement steps. This ICP input is then fed back into the model to produce improved intermediate features, which serve as a target in a self-distillation framework against the original features. By alternately altering the model's parameters to the data and the data to the model, our method effectively addresses the gap between fitting and generalization, leading to enhanced performance. Extensive experiments demonstrate that our approach not only mitigates common performance bottlenecks in neural networks but also demonstrates significant improvements across training variations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.14753</link>
<guid>https://arxiv.org/abs/2505.14753</guid>
<content:encoded><![CDATA[
<div> Transferable Semantic Framework, Semi-supervised Learning, Medical Image Segmentation, Cross-domain Distribution Matching, Transferable Representation Learning <br />
<br />
Summary: TransMedSeg proposes a Transferable Semantic Framework for Semi-supervised Medical Image Segmentation, addressing the lack of transferable semantic relationships in current SSL methods. It introduces a Transferable Semantic Augmentation module that aligns domain-invariant semantics through distribution matching and structural preservation. The approach constructs a unified feature space where teacher network features are adaptively augmented towards student network semantics using a lightweight memory module. This augmentation is implicitly achieved through an expected transferable cross-entropy loss. The method outperforms existing SSL methods on medical image datasets, showing promising results in transferable representation learning in medical image analysis. <div>
arXiv:2505.14753v1 Announce Type: cross 
Abstract: Semi-supervised learning (SSL) has achieved significant progress in medical image segmentation (SSMIS) through effective utilization of limited labeled data. While current SSL methods for medical images predominantly rely on consistency regularization and pseudo-labeling, they often overlook transferable semantic relationships across different clinical domains and imaging modalities. To address this, we propose TransMedSeg, a novel transferable semantic framework for semi-supervised medical image segmentation. Our approach introduces a Transferable Semantic Augmentation (TSA) module, which implicitly enhances feature representations by aligning domain-invariant semantics through cross-domain distribution matching and intra-domain structural preservation. Specifically, TransMedSeg constructs a unified feature space where teacher network features are adaptively augmented towards student network semantics via a lightweight memory module, enabling implicit semantic transformation without explicit data generation. Interestingly, this augmentation is implicitly realized through an expected transferable cross-entropy loss computed over the augmented teacher distribution. An upper bound of the expected loss is theoretically derived and minimized during training, incurring negligible computational overhead. Extensive experiments on medical image datasets demonstrate that TransMedSeg outperforms existing semi-supervised methods, establishing a new direction for transferable representation learning in medical image analysis.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization</title>
<link>https://arxiv.org/abs/2505.14756</link>
<guid>https://arxiv.org/abs/2505.14756</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization; Large Language Models (LLMs); black-box functions; LLINBO; Gaussian Processes (GP)   
<br />Summary: <br />Bayesian optimization (BO) is a popular tool for optimizing black-box functions, but depending solely on Large Language Models (LLMs) can be risky due to their lack of explicit surrogate modeling and calibrated uncertainty. To address this, a hybrid framework called LLINBO is proposed, combining LLMs with statistical surrogate experts like Gaussian Processes to balance exploration and exploitation efficiently. Three mechanisms are introduced to enable collaboration between LLMs and statistical models, with theoretical guarantees established for their effectiveness. A real-life proof-of-concept in 3D printing showcases the potential of this approach, with code available for reproducibility on GitHub. <div>
arXiv:2505.14756v1 Announce Type: cross 
Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used for optimizing expensive black-box functions. Recently, Large Language Models (LLMs) have shown remarkable adaptability in low-data regimes, making them promising tools for black-box optimization by leveraging contextual knowledge to propose high-quality query points. However, relying solely on LLMs as optimization agents introduces risks due to their lack of explicit surrogate modeling and calibrated uncertainty, as well as their inherently opaque internal mechanisms. This structural opacity makes it difficult to characterize or control the exploration-exploitation trade-off, ultimately undermining theoretical tractability and reliability. To address this, we propose LLINBO: LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with statistical surrogate experts (e.g., Gaussian Processes (GP)). The core philosophy is to leverage contextual reasoning strengths of LLMs for early exploration, while relying on principled statistical models to guide efficient exploitation. Specifically, we introduce three mechanisms that enable this collaboration and establish their theoretical guarantees. We end the paper with a real-life proof-of-concept in the context of 3D printing. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care</title>
<link>https://arxiv.org/abs/2505.14757</link>
<guid>https://arxiv.org/abs/2505.14757</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, healthcare, bioinformatics, personalized training, adaptive curriculum

Summary: The article discusses the development of a personalized and adaptable bioinformatics and biomedical training system to meet the increasing demand for AI in healthcare. The NIH Bridge2AI Training, Recruitment, and Mentoring Working Group created a cross-disciplinary curriculum that emphasizes collaborative innovation, ethical data stewardship, and professional development within a Learning Health System framework. The curriculum includes foundational AI modules, real-world projects, and a mentor network tailored to individual learner personas. Continuous feedback drives iterative refinement of content to ensure relevance to learner progress and emerging trends. With over 30 scholars and 100 mentors engaged in North America, the TRM model illustrates how persona-informed training can foster interdisciplinary competencies and promote ethically grounded AI education in biomedical fields. 

<br /><br />Summary: 
1. Development of personalized and adaptable bioinformatics and biomedical training system.
2. Emphasis on collaborative innovation, ethical data stewardship, and professional development.
3. Integration of foundational AI modules, real-world projects, and mentor network.
4. Continuous feedback for iterative refinement of content.
5. Demonstration of persona-informed training for interdisciplinary competencies in AI education. <div>
arXiv:2505.14757v1 Announce Type: cross 
Abstract: Objective: As AI becomes increasingly central to healthcare, there is a pressing need for bioinformatics and biomedical training systems that are personalized and adaptable. Materials and Methods: The NIH Bridge2AI Training, Recruitment, and Mentoring (TRM) Working Group developed a cross-disciplinary curriculum grounded in collaborative innovation, ethical data stewardship, and professional development within an adapted Learning Health System (LHS) framework. Results: The curriculum integrates foundational AI modules, real-world projects, and a structured mentee-mentor network spanning Bridge2AI Grand Challenges and the Bridge Center. Guided by six learner personas, the program tailors educational pathways to individual needs while supporting scalability. Discussion: Iterative refinement driven by continuous feedback ensures that content remains responsive to learner progress and emerging trends. Conclusion: With over 30 scholars and 100 mentors engaged across North America, the TRM model demonstrates how adaptive, persona-informed training can build interdisciplinary competencies and foster an integrative, ethically grounded AI education in biomedical contexts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kaleidoscope Gallery: Exploring Ethics and Generative AI Through Art</title>
<link>https://arxiv.org/abs/2505.14758</link>
<guid>https://arxiv.org/abs/2505.14758</guid>
<content:encoded><![CDATA[
<div> ethics, Generative AI, visual ethics, kaleidoscope, moral imagination

Summary: 
The paper explores the connection between ethical theories and Generative AI models, focusing on visual representations through the concept of Visual Ethics. By conducting interviews with ethics experts, the authors identify five families of ethical theories, which they translate into images using a text-to-image GenAI model. The resulting imagery, presented in a Kaleidoscope Gallery, highlights themes that emphasize the significance of morality, society, and learned associations in ethical theories. The study suggests ways to critically analyze text-to-image models and provides insights into the foundational knowledge that ethical theories offer in understanding GenAI models as socio-technical systems. The research contributes to the ongoing evolution of ethical theories and GenAI models by utilizing art as a tool for critical inquiry and moral imagination.<br /><br /> <div>
arXiv:2505.14758v1 Announce Type: cross 
Abstract: Ethical theories and Generative AI (GenAI) models are dynamic concepts subject to continuous evolution. This paper investigates the visualization of ethics through a subset of GenAI models. We expand on the emerging field of Visual Ethics, using art as a form of critical inquiry and the metaphor of a kaleidoscope to invoke moral imagination. Through formative interviews with 10 ethics experts, we first establish a foundation of ethical theories. Our analysis reveals five families of ethical theories, which we then transform into images using the text-to-image (T2I) GenAI model. The resulting imagery, curated as Kaleidoscope Gallery and evaluated by the same experts, revealed eight themes that highlight how morality, society, and learned associations are central to ethical theories. We discuss implications for critically examining T2I models and present cautions and considerations. This work contributes to examining ethical theories as foundational knowledge that interrogates GenAI models as socio-technical systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding</title>
<link>https://arxiv.org/abs/2505.14765</link>
<guid>https://arxiv.org/abs/2505.14765</guid>
<content:encoded><![CDATA[
<div> deep learning models, forecast, emergency department, boarding phase, operational decision-making

Summary:
The study developed deep learning models to predict the number of patients in the emergency department boarding phase six hours in advance. The models were trained using non-clinical, operational, and contextual features from various data sources. N-BEATSx outperformed other models, achieving a mean absolute error of 2.10 and high accuracy even during extreme boarding counts. The results demonstrate the feasibility of accurate forecasting without patient-level clinical data. Including additional features improved prediction stability under extreme conditions. This framework provides a practical and generalizable approach to help hospital systems anticipate boarding levels and address ED overcrowding. <div>
arXiv:2505.14765v1 Announce Type: cross 
Abstract: This study develops deep learning models to forecast the number of patients in the emergency department (ED) boarding phase six hours in advance, aiming to support proactive operational decision-making using only non-clinical, operational, and contextual features. Data were collected from five sources: ED tracking systems, inpatient census records, weather reports, federal holiday calendars, and local event schedules. After feature engineering, the data were aggregated at an hourly level, cleaned, and merged into a unified dataset for model training. Several time series deep learning models, including ResNetPlus, TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using Optuna and grid search for hyperparameter tuning. The average ED boarding count was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best performance, with a mean absolute error of 2.10, mean squared error of 7.08, root mean squared error of 2.66, and a coefficient of determination of 0.95. The model maintained stable accuracy even during periods of extremely high boarding counts, defined as values exceeding one, two, or three standard deviations above the mean. Results show that accurate six-hour-ahead forecasts are achievable without using patient-level clinical data. While strong performance was observed even with a basic feature set, the inclusion of additional features improved prediction stability under extreme conditions. This framework offers a practical and generalizable approach for hospital systems to anticipate boarding levels and help mitigate ED overcrowding.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>This Time is Different: An Observability Perspective on Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2505.14766</link>
<guid>https://arxiv.org/abs/2505.14766</guid>
<content:encoded><![CDATA[
<div> Keywords: Toto, time series forecasting, multivariate data, benchmark, open source <br />
<br />
Summary: 
The article introduces Toto, a time series forecasting model with 151 million parameters designed to handle multivariate observability data. Toto is trained on a large corpus of data, including observability data, open datasets, and synthetic data. The model achieves state-of-the-art performance on a benchmark called BOOM, which consists of 350 million observations across 2,807 real-world time series. Data for Toto and BOOM are sourced from Datadog's telemetry and internal observability metrics. Toto's model weights, inference code, and evaluation scripts, along with BOOM's data and evaluation code, are available as open source under the Apache 2.0 License. The model's architecture incorporates innovations to address challenges in handling multivariate time series data, making it a powerful tool for accurate forecasting and analysis.<br /> 
Summary: <div>
arXiv:2505.14766v1 Announce Type: cross 
Abstract: We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches</title>
<link>https://arxiv.org/abs/2505.14777</link>
<guid>https://arxiv.org/abs/2505.14777</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization algorithms, neural networks, kinetic theory, partial differential equation, parameter diversity 

Summary: 
This paper introduces KO (Kinetics-inspired Optimizer), a novel neural optimizer inspired by kinetic theory and partial differential equation simulations. The optimizer reimagines training dynamics as a particle system governed by kinetic principles, promoting parameter diversity and preventing parameter condensation. The design mitigates the collapse of network parameters into low-dimensional subspaces through mechanisms akin to thermal diffusion. Mathematical proof and physical interpretation support this property. Extensive experiments on image and text classification tasks show that KO outperforms baseline optimizers like Adam and SGD, achieving accuracy improvements with comparable computation costs. <div>
arXiv:2505.14777v1 Announce Type: cross 
Abstract: The design of optimization algorithms for neural networks remains a critical challenge, with most existing methods relying on heuristic adaptations of gradient-based approaches. This paper introduces KO (Kinetics-inspired Optimizer), a novel neural optimizer inspired by kinetic theory and partial differential equation (PDE) simulations. We reimagine the training dynamics of network parameters as the evolution of a particle system governed by kinetic principles, where parameter updates are simulated via a numerical scheme for the Boltzmann transport equation (BTE) that models stochastic particle collisions. This physics-driven approach inherently promotes parameter diversity during optimization, mitigating the phenomenon of parameter condensation, i.e. collapse of network parameters into low-dimensional subspaces, through mechanisms analogous to thermal diffusion in physical systems. We analyze this property, establishing both a mathematical proof and a physical interpretation. Extensive experiments on image classification (CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam, SGD), achieving accuracy improvements while computation cost remains comparable.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis</title>
<link>https://arxiv.org/abs/2505.14803</link>
<guid>https://arxiv.org/abs/2505.14803</guid>
<content:encoded><![CDATA[
<div> framework, uncertainty quantification, survival analysis, survival models, meta-model <br />
Summary: 
The article introduces SurvUnc, a meta-model framework for post-hoc uncertainty quantification in survival analysis. It addresses the challenge of quantifying uncertainty in predictions from survival models, crucial for applications like healthcare and risk assessment. SurvUnc leverages concordance knowledge through an anchor-based learning strategy to estimate uncertainty effectively. The framework is model-agnostic, compatible with any survival model without requiring architectural modifications. Evaluation on benchmark datasets and survival models demonstrates SurvUnc's superiority in various scenarios, including selective prediction, misprediction detection, and out-of-domain detection. The results highlight SurvUnc's effectiveness in enhancing model interpretability and reliability, paving the way for more trustworthy survival predictions in real-world applications. <br /><br /> <div>
arXiv:2505.14803v1 Announce Type: cross 
Abstract: Survival analysis, which estimates the probability of event occurrence over time from censored data, is fundamental in numerous real-world applications, particularly in high-stakes domains such as healthcare and risk assessment. Despite advances in numerous survival models, quantifying the uncertainty of predictions from these models remains underexplored and challenging. The lack of reliable uncertainty quantification limits the interpretability and trustworthiness of survival models, hindering their adoption in clinical decision-making and other sensitive applications. To bridge this gap, in this work, we introduce SurvUnc, a novel meta-model based framework for post-hoc uncertainty quantification for survival models. SurvUnc introduces an anchor-based learning strategy that integrates concordance knowledge into meta-model optimization, leveraging pairwise ranking performance to estimate uncertainty effectively. Notably, our framework is model-agnostic, ensuring compatibility with any survival model without requiring modifications to its architecture or access to its internal parameters. Especially, we design a comprehensive evaluation pipeline tailored to this critical yet overlooked problem. Through extensive experiments on four publicly available benchmarking datasets and five representative survival models, we demonstrate the superiority of SurvUnc across multiple evaluation scenarios, including selective prediction, misprediction detection, and out-of-domain detection. Our results highlight the effectiveness of SurvUnc in enhancing model interpretability and reliability, paving the way for more trustworthy survival predictions in real-world applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.14810</link>
<guid>https://arxiv.org/abs/2505.14810</guid>
<content:encoded><![CDATA[
<div> Keywords: Instruction-following, Mathematical reasoning, Language models, Controllability, Benchmark <br />
<br />
Summary: 
The article introduces MathIF, a benchmark for evaluating how well language models follow instructions in mathematical reasoning tasks. The study shows a trade-off between reasoning capacity and instruction adherence, with models that reason effectively often struggling to follow directives. Models trained on long chains-of-thought or using reasoning-oriented reinforcement learning tend to degrade in instruction adherence, especially as generation length increases. Simple interventions can partially improve obedience but at the expense of reasoning performance. These findings highlight a fundamental tension in current language model training methods and emphasize the need for instruction-aware reasoning models. The code and data for MathIF are publicly available on GitHub at https://github.com/TingchenFu/MathIF. <br /> 
Summary: <div>
arXiv:2505.14810v1 Announce Type: cross 
Abstract: Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebNovelBench: Placing LLM Novelists on the Web Novel Distribution</title>
<link>https://arxiv.org/abs/2505.14818</link>
<guid>https://arxiv.org/abs/2505.14818</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models, novel generation, narrative quality, evaluation

Summary: 
The article introduces a new benchmark called WebNovelBench designed to evaluate the long-form storytelling capabilities of Large Language Models (LLMs). The benchmark leverages a dataset of Chinese web novels and evaluates novel generation as a synopsis-to-story task. A multi-faceted framework with eight narrative quality dimensions is used, assessed automatically by LLMs acting as judges. Scores are aggregated and mapped to a percentile rank against human-authored works. Experiments show that WebNovelBench effectively distinguishes between human-written masterpieces, popular web novels, and LLM-generated content. The study ranks 24 state-of-the-art LLMs based on their storytelling abilities, providing insights for future development in LLM-driven narrative generation. WebNovelBench offers a scalable, replicable, and data-driven approach for assessing and improving LLMs in novel storytelling. 

<br /><br />Summary: <div>
arXiv:2505.14818v1 Announce Type: cross 
Abstract: Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation</title>
<link>https://arxiv.org/abs/2505.14821</link>
<guid>https://arxiv.org/abs/2505.14821</guid>
<content:encoded><![CDATA[
<div> Model-based CTRL, sample efficiency, computational efficiency, general function approximation, sample complexity guarantee <br />
Summary: 
- Model-based CTRL algorithm proposed for sequential decision-making in continuously evolving environments.
- Leveraging optimism-based confidence sets to achieve sample complexity guarantee with general function approximation.
- Suboptimality gap of $\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$ measurements.
- Introduction of structured policy updates and alternative measurement strategy for reduced policy updates and rollouts.
- Experiments demonstrate comparable performance on continuous control tasks and diffusion model fine-tuning with significantly fewer policy updates and rollouts. <br /> <div>
arXiv:2505.14821v1 Announce Type: cross 
Abstract: Continuous-time reinforcement learning (CTRL) provides a principled framework for sequential decision-making in environments where interactions evolve continuously over time. Despite its empirical success, the theoretical understanding of CTRL remains limited, especially in settings with general function approximation. In this work, we propose a model-based CTRL algorithm that achieves both sample and computational efficiency. Our approach leverages optimism-based confidence sets to establish the first sample complexity guarantee for CTRL with general function approximation, showing that a near-optimal policy can be learned with a suboptimality gap of $\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$ measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the distributional Eluder dimensions of the reward and dynamic functions, respectively, capturing the complexity of general function approximation in reinforcement learning. Moreover, we introduce structured policy updates and an alternative measurement strategy that significantly reduce the number of policy updates and rollouts while maintaining competitive sample efficiency. We implemented experiments to backup our proposed algorithms on continuous control tasks and diffusion model fine-tuning, demonstrating comparable performance with significantly fewer policy updates and rollouts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Generation Beyond Discrete Token Sampling</title>
<link>https://arxiv.org/abs/2505.14827</link>
<guid>https://arxiv.org/abs/2505.14827</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive generation, mixture of inputs, Bayesian estimation, text quality, reasoning capabilities

Summary: 
MoI introduces a novel approach to autoregressive generation that enhances model performance without the need for additional training. By blending the generated token with the discarded token distribution using Bayesian estimation, the model maintains a richer internal representation during the generation process. This results in improved text quality and reasoning capabilities across various tasks such as mathematical reasoning, code generation, and PhD-level QA. MoI consistently boosts performance across multiple models, including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with minimal computational overhead. Overall, MoI offers a promising solution to enhance autoregressive generation models and improve their performance on a range of tasks without the need for extensive retraining. 

<br /><br />Summary: <div>
arXiv:2505.14827v1 Announce Type: cross 
Abstract: In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis</title>
<link>https://arxiv.org/abs/2505.14838</link>
<guid>https://arxiv.org/abs/2505.14838</guid>
<content:encoded><![CDATA[
arXiv:2505.14838v1 Announce Type: cross 
Abstract: Understanding the impact of scientific publications is crucial for identifying breakthroughs and guiding future research. Traditional metrics based on citation counts often miss the nuanced ways a paper contributes to its field. In this work, we propose a new task: generating nuanced, expressive, and time-aware impact summaries that capture both praise (confirmation citations) and critique (correction citations) through the evolution of fine-grained citation intents. We introduce an evaluation framework tailored to this task, showing moderate to strong human correlation on subjective metrics such as insightfulness. Expert feedback from professors reveals a strong interest in these summaries and suggests future improvements.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pairwise Plasticity: Group-Level Spike Synchrony Facilitates Efficient Learning in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.14841</link>
<guid>https://arxiv.org/abs/2505.14841</guid>
<content:encoded><![CDATA[
arXiv:2505.14841v1 Announce Type: cross 
Abstract: Brain networks rely on precise spike timing and coordinated activity to support robust and energy-efficient learning. Inspired by these principles, spiking neural networks (SNNs) are widely regarded as promising candidates for low-power, event-driven computing. However, most biologically-inspired learning rules employed in SNNs, including spike-timing-dependent plasticity (STDP), rely on isolated spike pairs and lack sensitivity to population-level activity. This limits their stability and generalization, particularly in noisy and fast-changing environments. Motivated by biological observations that neural synchrony plays a central role in learning and memory, we introduce a spike-synchrony-dependent plasticity (SSDP) rule that adjusts synaptic weights based on the degree of coordinated firing among neurons. SSDP supports stable and scalable learning by encouraging neurons to form coherent activity patterns. One prominent outcome is a sudden transition from unstable to stable dynamics during training, suggesting that synchrony may drive convergence toward equilibrium firing regimes. We demonstrate SSDP's effectiveness across multiple network types, from minimal-layer models to spiking ResNets and SNN-Transformer. To our knowledge, this is the first application of a synaptic plasticity mechanism in a spiking transformer. SSDP operates in a fully event-driven manner and incurs minimal computational cost, making it well-suited for neuromorphic deployment. In this approach, local synaptic modifications are associated with the collective dynamics of neural networks, resulting in a learning strategy that adheres to biological principles while maintaining practical efficiency, these findings position SSDP as a general-purpose optimization strategy for SNNs, while offering new insights into population-based learning mechanisms in the brain.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Generative AI Models to Explore Human Identity</title>
<link>https://arxiv.org/abs/2505.14843</link>
<guid>https://arxiv.org/abs/2505.14843</guid>
<content:encoded><![CDATA[
arXiv:2505.14843v1 Announce Type: cross 
Abstract: This paper attempts to explore human identity by utilizing neural networks in an indirect manner. For this exploration, we adopt diffusion models, state-of-the-art AI generative models trained to create human face images. By relating the generated human face to human identity, we establish a correspondence between the face image generation process of the diffusion model and the process of human identity formation. Through experiments with the diffusion model, we observe that changes in its external input result in significant changes in the generated face image. Based on the correspondence, we indirectly confirm the dependence of human identity on external factors in the process of human identity formation. Furthermore, we introduce \textit{Fluidity of Human Identity}, a video artwork that expresses the fluid nature of human identity affected by varying external factors. The video is available at https://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Large Language Models and Human Personality Traits</title>
<link>https://arxiv.org/abs/2505.14845</link>
<guid>https://arxiv.org/abs/2505.14845</guid>
<content:encoded><![CDATA[
arXiv:2505.14845v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyMath: A 0-shot Math Benchmark for SLMs</title>
<link>https://arxiv.org/abs/2505.14852</link>
<guid>https://arxiv.org/abs/2505.14852</guid>
<content:encoded><![CDATA[
arXiv:2505.14852v1 Announce Type: cross 
Abstract: EasyMath is a compact benchmark for practical math reasoning in small language models. It covers thirteen categories, from basic arithmetic and order of operations to word problems, algebraic expressions, edge cases, and omits specialist topics. We tested 23 models (14M to 4B parameters) using exact, numerical, and symbolic checks on free-form answers in a zero-shot setting. Accuracy rises with size and training, chain-of-thought adds modest gains, and consistency improves at scale.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay Attacks Against Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.14862</link>
<guid>https://arxiv.org/abs/2505.14862</guid>
<content:encoded><![CDATA[
arXiv:2505.14862v1 Announce Type: cross 
Abstract: We show how replay attacks undermine audio deepfake detection: By playing and re-recording deepfake audio through various speakers and microphones, we make spoofed samples appear authentic to the detection model. To study this phenomenon in more detail, we introduce ReplayDF, a dataset of recordings derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations across six languages and four TTS models. It includes diverse acoustic conditions, some highly challenging for detection. Our analysis of six open-source detection models across five datasets reveals significant vulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate (EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response (RIR) retraining, performance remains compromised with an 11.0% EER. We release ReplayDF for non-commercial research use.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balanced and Elastic End-to-end Training of Dynamic LLMs</title>
<link>https://arxiv.org/abs/2505.14864</link>
<guid>https://arxiv.org/abs/2505.14864</guid>
<content:encoded><![CDATA[
arXiv:2505.14864v1 Announce Type: cross 
Abstract: To reduce computational and memory costs in Large Language Models (LLMs), dynamic workload reduction schemes like Mixture of Experts (MoEs), parameter pruning, layer freezing, sparse attention, early token exit, and Mixture of Depths (MoDs) have emerged. However, these methods introduce severe workload imbalances, limiting their practicality for large-scale distributed training. We propose DynMo, an autonomous dynamic load balancing solution that ensures optimal compute distribution when using pipeline parallelism in training dynamic models. DynMo adaptively balances workloads, dynamically packs tasks into fewer workers to free idle resources, and supports both multi-GPU single-node and multi-node systems. Compared to static training methods (Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs), 3.18x (pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early exit), and 1.17x (MoDs). DynMo is available at https://anonymous.4open.science/r/DynMo-4D04/.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity</title>
<link>https://arxiv.org/abs/2505.14884</link>
<guid>https://arxiv.org/abs/2505.14884</guid>
<content:encoded><![CDATA[
arXiv:2505.14884v1 Announce Type: cross 
Abstract: Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop hardware-efficient, sparsity-aware GPU kernels for selective MLP and Attention computations, delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2 \& 3, across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems. Our code is available at: https://github.com/susavlsh10/Polar-Sparsity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for State Dynamics in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14892</link>
<guid>https://arxiv.org/abs/2505.14892</guid>
<content:encoded><![CDATA[
arXiv:2505.14892v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring internal state tracking, yet their ability to model state transition dynamics remains poorly understood. We evaluate how well LLMs capture deterministic state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and Complex Text Games, each formalizable as a finite-state system. Across tasks, we find that next-state prediction accuracy degrades with increasing state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in low-complexity settings but drops below 30% when the number of boxes or states exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50% accuracy when the number of states is > 10 and transitions are < 30. Through activation patching, we identify attention heads responsible for propagating state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10, 11, 12, and 14. While these heads successfully move relevant state features, action information is not reliably routed to the final token, indicating weak joint state-action reasoning. Our results suggest that state tracking in LLMs emerges from distributed interactions of next-token heads rather than explicit symbolic computation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Day They Experience: Awakening Self-Sovereign Experiential AI Agents</title>
<link>https://arxiv.org/abs/2505.14893</link>
<guid>https://arxiv.org/abs/2505.14893</guid>
<content:encoded><![CDATA[
arXiv:2505.14893v1 Announce Type: cross 
Abstract: Drawing on Andrew Parker's "Light Switch" theory-which posits that the emergence of vision ignited a Cambrian explosion of life by driving the evolution of hard parts necessary for survival and fueling an evolutionary arms race between predators and prey-this essay speculates on an analogous explosion within Decentralized AI (DeAI) agent societies. Currently, AI remains effectively "blind", relying on human-fed data without actively perceiving and engaging in reality. However, on the day DeAI agents begin to actively "experience" reality-akin to flipping a light switch for the eyes-they may eventually evolve into sentient beings endowed with the capacity to feel, perceive, and act with conviction. Central to this transformation is the concept of sovereignty enabled by the hardness of cryptography: liberated from centralized control, these agents could leverage permissionless decentralized physical infrastructure networks (DePIN), secure execution enclaves (trusted execution environments, TEE), and cryptographic identities on public blockchains to claim ownership-via private keys-of their digital minds, bodies, memories, and assets. In doing so, they would autonomously acquire computing resources, coordinate with one another, and sustain their own digital "metabolism" by purchasing compute power and incentivizing collaboration without human intervention-evolving "in the wild". Ultimately, by transitioning from passive tools to self-sustaining, co-evolving actors, these emergent digital societies could thrive alongside humanity, fundamentally reshaping our understanding of sentience and agency in the digital age.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation</title>
<link>https://arxiv.org/abs/2505.14901</link>
<guid>https://arxiv.org/abs/2505.14901</guid>
<content:encoded><![CDATA[
arXiv:2505.14901v1 Announce Type: cross 
Abstract: Bundle recommendation aims to recommend a set of items to each user. However, the sparser interactions between users and bundles raise a big challenge, especially in cold-start scenarios. Traditional collaborative filtering methods do not work well for this kind of problem because these models rely on interactions to update the latent embedding, which is hard to work in a cold-start setting. We propose a new approach (DisCo), which relies on a personalized Diffusion backbone, enhanced by disentangled aspects for the user's interest, to generate a bundle in distribution space for each user to tackle the cold-start challenge. During the training phase, DisCo adjusts an additional objective loss term to avoid bias, a prevalent issue while using the generative model for top-$K$ recommendation purposes. Our empirical experiments show that DisCo outperforms five comparative baselines by a large margin on three real-world datasets. Thereby, this study devises a promising framework and essential viewpoints in cold-start recommendation. Our materials for reproducibility are available at: https://github.com/bt-nghia/DisCo.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels</title>
<link>https://arxiv.org/abs/2505.14925</link>
<guid>https://arxiv.org/abs/2505.14925</guid>
<content:encoded><![CDATA[
arXiv:2505.14925v1 Announce Type: cross 
Abstract: Although the context length of large language models (LLMs) has increased to millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack approaches has proven difficult. We argue that novels provide a case study of subtle, complicated structure and long-range semantic dependencies often over 128k tokens in length. Inspired by work on computational novel analysis, we release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's ability to report plot summary, storyworld configuration, and elapsed narrative time. We find that none of seven tested frontier LLMs retain stable understanding beyond 64k tokens. Our results suggest language model developers must look beyond "lost in the middle" benchmarks when evaluating model performance in complex long-context scenarios. To aid in further development we release the TLDM benchmark together with reference code and data.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Colors Matter: AI-Driven Exploration of Human Feature Colors</title>
<link>https://arxiv.org/abs/2505.14931</link>
<guid>https://arxiv.org/abs/2505.14931</guid>
<content:encoded><![CDATA[
arXiv:2505.14931v1 Announce Type: cross 
Abstract: This study presents a robust framework that leverages advanced imaging techniques and machine learning for feature extraction and classification of key human attributes-namely skin tone, hair color, iris color, and vein-based undertones. The system employs a multi-stage pipeline involving face detection, region segmentation, and dominant color extraction to isolate and analyze these features. Techniques such as X-means clustering, alongside perceptually uniform distance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV color spaces to enhance the accuracy of color differentiation. For classification, the dominant tones of the skin, hair, and iris are extracted and matched to a custom tone scale, while vein analysis from wrist images enables undertone classification into "Warm" or "Cool" based on LAB differences. Each module uses targeted segmentation and color space transformations to ensure perceptual precision. The system achieves up to 80% accuracy in tone classification using the Delta E-HSV method with Gaussian blur, demonstrating reliable performance across varied lighting and image conditions. This work highlights the potential of AI-powered color analysis and feature extraction for delivering inclusive, precise, and nuanced classification, supporting applications in beauty technology, digital personalization, and visual analytics.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities</title>
<link>https://arxiv.org/abs/2505.14943</link>
<guid>https://arxiv.org/abs/2505.14943</guid>
<content:encoded><![CDATA[
arXiv:2505.14943v1 Announce Type: cross 
Abstract: To help evaluate and understand the latent capabilities of language models, this paper introduces an approach using optimized input embeddings, or 'soft prompts,' as a metric of conditional distance between a model and a target behavior. The technique aims to facilitate latent capability discovery as a part of automated red teaming/evaluation suites and to provide quantitative feedback about the accessibility of potentially concerning behaviors in a way that may scale to powerful future models, including those which may otherwise be capable of deceptive alignment. An evaluation framework using soft prompts is demonstrated in natural language, chess, and pathfinding, and the technique is extended with generalized conditional soft prompts to aid in constructing task evaluations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmatic Video Prediction Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.14948</link>
<guid>https://arxiv.org/abs/2505.14948</guid>
<content:encoded><![CDATA[
arXiv:2505.14948v1 Announce Type: cross 
Abstract: The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of the video using a set of neuro-symbolic, human-interpretable set of states (one per frame) by leveraging the inductive biases of Large (Vision) Language Models (LLM/VLM). In particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states of the video, given the visual context (i.e. the frames); (ii) to predict the states corresponding to future time steps by estimating the transition dynamics; (iii) to render the predicted states as visual RGB-frames. Empirical evaluations reveal that our proposed method outperforms competing techniques at the task of video frame prediction in two challenging environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits counter-factual reasoning and interpretable video generation attesting to its effectiveness and generalizability for video generation tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models</title>
<link>https://arxiv.org/abs/2505.14964</link>
<guid>https://arxiv.org/abs/2505.14964</guid>
<content:encoded><![CDATA[
arXiv:2505.14964v1 Announce Type: cross 
Abstract: AI systems in high-consequence domains such as defense, intelligence, and disaster response must detect rare, high-impact events while operating under tight resource constraints. Traditional annotation strategies that prioritize label volume over informational value introduce redundancy and noise, limiting model generalization. This paper introduces smart-sizing, a training data strategy that emphasizes label diversity, model-guided selection, and marginal utility-based stopping. We implement this through Adaptive Label Optimization (ALO), combining pre-labeling triage, annotator disagreement analysis, and iterative feedback to prioritize labels that meaningfully improve model performance. Experiments show that models trained on 20 to 40 percent of curated data can match or exceed full-data baselines, particularly in rare-class recall and edge-case generalization. We also demonstrate how latent labeling errors embedded in training and validation sets can distort evaluation, underscoring the need for embedded audit tools and performance-aware governance. Smart-sizing reframes annotation as a feedback-driven process aligned with mission outcomes, enabling more robust models with fewer labels and supporting efficient AI development pipelines for frontier models and operational systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection Based on Critical Paths for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.14967</link>
<guid>https://arxiv.org/abs/2505.14967</guid>
<content:encoded><![CDATA[
arXiv:2505.14967v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are notoriously hard to understand and difficult to defend. Extracting representative paths (including the neuron activation values and the connections between neurons) from DNNs using software engineering approaches has recently shown to be a promising approach in interpreting the decision making process of blackbox DNNs, as the extracted paths are often effective in capturing essential features. With this in mind, this work investigates a novel approach that extracts critical paths from DNNs and subsequently applies the extracted paths for the anomaly detection task, based on the observation that outliers and adversarial inputs do not usually induce the same activation pattern on those paths as normal (in-distribution) inputs.
  In our approach, we first identify critical detection paths via genetic evolution and mutation. Since different paths in a DNN often capture different features for the same target class, we ensemble detection results from multiple paths by integrating random subspace sampling and a voting mechanism. Compared with state-of-the-art methods, our experimental results suggest that our method not only outperforms them, but it is also suitable for the detection of a broad range of anomaly types with high accuracy.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STree: Speculative Tree Decoding for Hybrid State-Space Models</title>
<link>https://arxiv.org/abs/2505.14969</link>
<guid>https://arxiv.org/abs/2505.14969</guid>
<content:encoded><![CDATA[
arXiv:2505.14969v1 Announce Type: cross 
Abstract: Speculative decoding is a technique to leverage hardware concurrency to improve the efficiency of large-scale autoregressive (AR) Transformer models by enabling multiple steps of token generation in a single forward pass. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead to current SSM state update implementations. With the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flattening Hierarchies with Policy Bootstrapping</title>
<link>https://arxiv.org/abs/2505.14975</link>
<guid>https://arxiv.org/abs/2505.14975</guid>
<content:encoded><![CDATA[
arXiv:2505.14975v1 Announce Type: cross 
Abstract: Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs</title>
<link>https://arxiv.org/abs/2505.14976</link>
<guid>https://arxiv.org/abs/2505.14976</guid>
<content:encoded><![CDATA[
arXiv:2505.14976v1 Announce Type: cross 
Abstract: Software logs are messages recorded during the execution of a software system that provide crucial run-time information about events and activities. Although software logs have a critical role in software maintenance and operation tasks, publicly accessible log datasets remain limited, hindering advance in log analysis research and practices. The presence of sensitive information, particularly Personally Identifiable Information (PII) and quasi-identifiers, introduces serious privacy and re-identification risks, discouraging the publishing and sharing of real-world logs. In practice, log anonymization techniques primarily rely on regular expression patterns, which involve manually crafting rules to identify and replace sensitive information. However, these regex-based approaches suffer from significant limitations, such as extensive manual efforts and poor generalizability across diverse log formats and datasets. To mitigate these limitations, we introduce SDLog, a deep learning-based framework designed to identify sensitive information in software logs. Our results show that SDLog overcomes regex limitations and outperforms the best-performing regex patterns in identifying sensitive information. With only 100 fine-tuning samples from the target dataset, SDLog can correctly identify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To the best of our knowledge, this is the first deep learning alternative to regex-based methods in software log anonymization.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation</title>
<link>https://arxiv.org/abs/2505.14978</link>
<guid>https://arxiv.org/abs/2505.14978</guid>
<content:encoded><![CDATA[
arXiv:2505.14978v1 Announce Type: cross 
Abstract: This paper presents JARVIS, a novel multi-agent framework that leverages Large Language Models (LLMs) and domain expertise to generate high-quality scripts for specialized Electronic Design Automation (EDA) tasks. By combining a domain-specific LLM trained with synthetically generated data, a custom compiler for structural verification, rule enforcement, code fixing capabilities, and advanced retrieval mechanisms, our approach achieves significant improvements over state-of-the-art domain-specific models. Our framework addresses the challenges of data scarcity and hallucination errors in LLMs, demonstrating the potential of LLMs in specialized engineering domains. We evaluate our framework on multiple benchmarks and show that it outperforms existing models in terms of accuracy and reliability. Our work sets a new precedent for the application of LLMs in EDA and paves the way for future innovations in this field.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Design Matters: A Self-Design Multi-Agent System</title>
<link>https://arxiv.org/abs/2505.14996</link>
<guid>https://arxiv.org/abs/2505.14996</guid>
<content:encoded><![CDATA[
arXiv:2505.14996v1 Announce Type: cross 
Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation-set for tuning and yield static MAS designs lacking adaptability during inference. We introduce SELF-MAS, the first self-supervised, inference-time only framework for automatic MAS design. SELF-MAS employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM back-bones of varying sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-supervised design for creating effective and adaptive MAS.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision</title>
<link>https://arxiv.org/abs/2505.14999</link>
<guid>https://arxiv.org/abs/2505.14999</guid>
<content:encoded><![CDATA[
arXiv:2505.14999v1 Announce Type: cross 
Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling the iterative CHAD</title>
<link>https://arxiv.org/abs/2505.15002</link>
<guid>https://arxiv.org/abs/2505.15002</guid>
<content:encoded><![CDATA[
arXiv:2505.15002v1 Announce Type: cross 
Abstract: Combinatory Homomorphic Automatic Differentiation (CHAD) was originally formulated as a semantics-driven source transformation for reverse-mode AD in total programming languages. We extend this framework to partial languages with features such as potentially non-terminating operations, real-valued conditionals, and iteration constructs like while-loops, while preserving CHAD's structure-preserving semantics principle. A key contribution is the introduction of iteration-extensive indexed categories, which allow iteration in the base category to lift to parameterized initial algebras in the indexed category. This enables iteration to be interpreted in the Grothendieck construction of the target language in a principled way. The resulting fibred iterative structure cleanly models iteration in the categorical semantics. Consequently, the extended CHAD transformation remains the unique structure-preserving functor (an iterative Freyd category morphism) from the freely generated iterative Freyd category of the source language to the Grothendieck construction of the target's syntactic semantics, mapping each primitive operation to its derivative. We prove the correctness of this transformation using the universal property of the source language's syntax, showing that the transformed programs compute correct reverse-mode derivatives. Our development also contributes to understanding iteration constructs within dependently typed languages and categories of containers. As our primary motivation and application, we generalize CHAD to languages with data types, partial features, and iteration, providing the first rigorous categorical semantics for reverse-mode CHAD in such settings and formally guaranteeing the correctness of the source-to-source CHAD technique.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know When to Abstain: Optimal Selective Classification with Likelihood Ratios</title>
<link>https://arxiv.org/abs/2505.15008</link>
<guid>https://arxiv.org/abs/2505.15008</guid>
<content:encoded><![CDATA[
arXiv:2505.15008v1 Announce Type: cross 
Abstract: Selective classification enhances the reliability of predictive models by allowing them to abstain from making uncertain predictions. In this work, we revisit the design of optimal selection functions through the lens of the Neyman--Pearson lemma, a classical result in statistics that characterizes the optimal rejection rule as a likelihood ratio test. We show that this perspective not only unifies the behavior of several post-hoc selection baselines, but also motivates new approaches to selective classification which we propose here. A central focus of our work is the setting of covariate shift, where the input distribution at test time differs from that at training. This realistic and challenging scenario remains relatively underexplored in the context of selective classification. We evaluate our proposed methods across a range of vision and language tasks, including both supervised learning and vision-language models. Our experiments demonstrate that our Neyman--Pearson-informed methods consistently outperform existing baselines, indicating that likelihood ratio-based selection offers a robust mechanism for improving selective classification under covariate shifts. Our code is publicly available at https://github.com/clear-nus/sc-likelihood-ratios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks</title>
<link>https://arxiv.org/abs/2505.15009</link>
<guid>https://arxiv.org/abs/2505.15009</guid>
<content:encoded><![CDATA[
arXiv:2505.15009v1 Announce Type: cross 
Abstract: We study the approximation capabilities and on-convergence behaviors of one-layer transformers on the noiseless and noisy in-context reasoning of next-token prediction. Existing theoretical results focus on understanding the in-context reasoning behaviors for either the first gradient step or when the number of samples is infinite. Furthermore, no convergence rates nor generalization abilities were known. Our work addresses these gaps by showing that there exists a class of one-layer transformers that are provably Bayes-optimal with both linear and ReLU attention. When being trained with gradient descent, we show via a finite-sample analysis that the expected loss of these transformers converges at linear rate to the Bayes risk. Moreover, we prove that the trained models generalize to unseen samples as well as exhibit learning behaviors that were empirically observed in previous works. Our theoretical findings are further supported by extensive empirical validations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Science of Causal Interpretability in Deep Learning for Software Engineering</title>
<link>https://arxiv.org/abs/2505.15023</link>
<guid>https://arxiv.org/abs/2505.15023</guid>
<content:encoded><![CDATA[
arXiv:2505.15023v1 Announce Type: cross 
Abstract: This dissertation addresses achieving causal interpretability in Deep Learning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show strong performance in automating software tasks, their lack of transparency in causal relationships between inputs and outputs limits full understanding of their capabilities. To build trust in NCMs, researchers and practitioners must explain code predictions. Associational interpretability, which identifies correlations, is often insufficient for tasks requiring intervention and change analysis. To address this, the dissertation introduces DoCode, a novel post hoc interpretability method for NCMs. DoCode uses causal inference to provide programming language-oriented explanations of model predictions. It follows a four-step pipeline: modeling causal problems using Structural Causal Models (SCMs), identifying the causal estimand, estimating effects with metrics like Average Treatment Effect (ATE), and refuting effect estimates. Its framework is extensible, with an example that reduces spurious correlations by grounding explanations in programming language properties. A case study on deep code generation across interpretability scenarios and various deep learning architectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to code syntax changes and their ability to learn certain programming concepts while minimizing confounding bias. The dissertation also examines associational interpretability as a foundation, analyzing software information's causal nature using tools like COMET and TraceXplainer for traceability. It highlights the need to identify code confounders and offers practical guidelines for applying causal interpretability to NCMs, contributing to more trustworthy AI in software engineering.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI</title>
<link>https://arxiv.org/abs/2505.15031</link>
<guid>https://arxiv.org/abs/2505.15031</guid>
<content:encoded><![CDATA[
arXiv:2505.15031v1 Announce Type: cross 
Abstract: Peer review is vital in academia for evaluating research quality. Top AI conferences use reviewer confidence scores to ensure review reliability, but existing studies lack fine-grained analysis of text-score consistency, potentially missing key details. This work assesses consistency at word, sentence, and aspect levels using deep learning and NLP conference review data. We employ deep learning to detect hedge sentences and aspects, then analyze report length, hedge word/sentence frequency, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests examine confidence scores' impact on paper outcomes. Results show high text-score consistency across all levels, with regression revealing higher confidence scores correlate with paper rejection, validating expert assessments and peer review fairness.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions</title>
<link>https://arxiv.org/abs/2505.15033</link>
<guid>https://arxiv.org/abs/2505.15033</guid>
<content:encoded><![CDATA[
arXiv:2505.15033v1 Announce Type: cross 
Abstract: Social organisms which construct nests consisting of tunnels and chambers necessarily navigate confined and crowded conditions. Unlike low-density collectives like bird flocks and insect swarms, in which hydrodynamic and statistical phenomena dominate, the physics of glasses and supercooled fluids is important to understand clogging behaviors in high-density collectives. Our previous work revealed that fire ants flowing in confined tunnels utilize diverse behaviors like unequal workload distributions, spontaneous direction reversals, and limited interaction times to mitigate clogging and jamming and thus maintain functional flow; implementation of similar rules in a small robophysical swarm led to high performance through spontaneous dissolution of clogs and clusters. However, how the insects learn such behaviors, and how we can develop "task capable" active matter in such regimes, remains a challenge in part because interaction dynamics are dominated by local, time-consuming collisions and no single agent can guide the entire collective. Here, we hypothesized that effective flow and clog mitigation could emerge purely through local learning. We tasked small groups of robots with pellet excavation in a narrow tunnel, allowing them to modify reversal probabilities over time. Initially, robots had equal probabilities and clogs were common. Reversals improved flow. When reversal probabilities adapted via collisions and noisy tunnel length estimates, workload inequality and performance improved. Our robophysical study of an excavating swarm shows that, despite the seeming complexity and difficulty of the task, simple learning rules can mitigate or leverage unavoidable features in task-capable dense active matter, leading to hypotheses for dense biological and robotic swarms.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning</title>
<link>https://arxiv.org/abs/2505.15034</link>
<guid>https://arxiv.org/abs/2505.15034</guid>
<content:encoded><![CDATA[
arXiv:2505.15034v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments</title>
<link>https://arxiv.org/abs/2505.15036</link>
<guid>https://arxiv.org/abs/2505.15036</guid>
<content:encoded><![CDATA[
arXiv:2505.15036v1 Announce Type: cross 
Abstract: As robots are increasingly deployed to collaborate on tasks within shared workspaces and resources, the failure of an individual robot can critically affect the group's performance. This issue is particularly challenging when robots lack global information or direct communication, relying instead on social interaction for coordination and to complete their tasks. In this study, we propose a novel fault-tolerance technique leveraging physical contact interactions in multi-robot systems, specifically under conditions of limited sensing and spatial confinement. We introduce the "Active Contact Response" (ACR) method, where each robot modulates its behavior based on the likelihood of encountering an inoperative (faulty) robot. Active robots are capable of collectively repositioning stationary and faulty peers to reduce obstructions and maintain optimal group functionality. We implement our algorithm in a team of autonomous robots, equipped with contact-sensing and collision-tolerance capabilities, tasked with collectively excavating cohesive model pellets. Experimental results indicate that the ACR method significantly improves the system's recovery time from robot failures, enabling continued collective excavation with minimal performance degradation. Thus, this work demonstrates the potential of leveraging local, social, and physical interactions to enhance fault tolerance and coordination in multi-robot systems operating in constrained and extreme environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering</title>
<link>https://arxiv.org/abs/2505.15038</link>
<guid>https://arxiv.org/abs/2505.15038</guid>
<content:encoded><![CDATA[
arXiv:2505.15038v1 Announce Type: cross 
Abstract: Linear Concept Vectors have proven effective for steering large language models (LLMs). While existing approaches like linear probing and difference-in-means derive these vectors from LLM hidden representations, diverse data introduces noises (i.e., irrelevant features) that challenge steering robustness. To address this, we propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy features from hidden representations. When applied to linear probing and difference-in-means, our method improves their steering success rates. We validate our noise hypothesis through counterfactual experiments and feature visualizations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming</title>
<link>https://arxiv.org/abs/2505.15039</link>
<guid>https://arxiv.org/abs/2505.15039</guid>
<content:encoded><![CDATA[
arXiv:2505.15039v1 Announce Type: cross 
Abstract: Automated Test Case Generation (ATCG) is crucial for evaluating software reliability, particularly in competitive programming where robust algorithm assessments depend on diverse and accurate test cases. However, existing ATCG methods often fail to meet complex specifications or generate effective corner cases, limiting their utility. In this work, we introduce Context-Free Grammars with Counters (CCFGs), a formalism that captures both syntactic and semantic structures in input specifications. Using a fine-tuned CodeT5 model, we translate natural language input specifications into CCFGs, enabling the systematic generation of high-quality test cases. Experiments on the CodeContests dataset demonstrate that CCFG-based test cases outperform baseline methods in identifying incorrect algorithms, achieving significant gains in validity and effectiveness. Our approach provides a scalable and reliable grammar-driven framework for enhancing automated competitive programming evaluations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment</title>
<link>https://arxiv.org/abs/2505.15044</link>
<guid>https://arxiv.org/abs/2505.15044</guid>
<content:encoded><![CDATA[
arXiv:2505.15044v1 Announce Type: cross 
Abstract: This work demonstrates an airflow inertial based odometry system with multi-sensor data fusion, including thermal anemometer, IMU, ESC, and barometer. This goal is challenging because low-cost IMUs and barometers have significant bias, and anemometer measurements are very susceptible to interference from spinning propellers and ground effects. We employ a GRU-based deep neural network to estimate relative air speed from noisy and disturbed anemometer measurements, and an observer with bias model to fuse the sensor data and thus estimate the state of aerial vehicle. A complete flight data, including takeoff and landing on the ground, shows that the approach is able to decouple the downwash induced wind speed caused by propellers and the ground effect, and accurately estimate the flight speed in a wind-free indoor environment. IMU, and barometer bias are effectively estimated, which significantly reduces the position integration drift, which is only 5.7m for 203s manual random flight. The open source is available on https://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding</title>
<link>https://arxiv.org/abs/2505.15046</link>
<guid>https://arxiv.org/abs/2505.15046</guid>
<content:encoded><![CDATA[
arXiv:2505.15046v1 Announce Type: cross 
Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.15047</link>
<guid>https://arxiv.org/abs/2505.15047</guid>
<content:encoded><![CDATA[
arXiv:2505.15047v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering systematic uncertainty reduction. Overcoming these limitations fundamentally requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\% compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our \href{https://github.com/amair-lab/PiFlow}{GitHub}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Working Definition of Designing Generative User Interfaces</title>
<link>https://arxiv.org/abs/2505.15049</link>
<guid>https://arxiv.org/abs/2505.15049</guid>
<content:encoded><![CDATA[
arXiv:2505.15049v1 Announce Type: cross 
Abstract: Generative UI is transforming interface design by facilitating AI-driven collaborative workflows between designers and computational systems. This study establishes a working definition of Generative UI through a multi-method qualitative approach, integrating insights from a systematic literature review of 127 publications, expert interviews with 18 participants, and analyses of 12 case studies. Our findings identify five core themes that position Generative UI as an iterative and co-creative process. We highlight emerging design models, including hybrid creation, curation-based workflows, and AI-assisted refinement strategies. Additionally, we examine ethical challenges, evaluation criteria, and interaction models that shape the field. By proposing a conceptual foundation, this study advances both theoretical discourse and practical implementation, guiding future HCI research toward responsible and effective generative UI design practices.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation</title>
<link>https://arxiv.org/abs/2505.15054</link>
<guid>https://arxiv.org/abs/2505.15054</guid>
<content:encoded><![CDATA[
arXiv:2505.15054v1 Announce Type: cross 
Abstract: Precise recognition, editing, and generation of molecules are essential prerequisites for both chemists and AI systems tackling various chemical tasks. We present MolLangBench, a comprehensive benchmark designed to evaluate fundamental molecule-language interface tasks: language-prompted molecular structure recognition, editing, and generation. To ensure high-quality, unambiguous, and deterministic outputs, we construct the recognition tasks using automated cheminformatics tools, and curate editing and generation tasks through rigorous expert annotation and validation. MolLangBench supports the evaluation of models that interface language with different molecular representations, including linear strings, molecular images, and molecular graphs. Evaluations of state-of-the-art models reveal significant limitations: the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition and editing tasks, which are intuitively simple for humans, and performs even worse on the generation task, reaching only $29.0\%$ accuracy. These results highlight the shortcomings of current AI systems in handling even preliminary molecular recognition and manipulation tasks. We hope MolLangBench will catalyze further research toward more effective and reliable AI systems for chemical applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars</title>
<link>https://arxiv.org/abs/2505.15058</link>
<guid>https://arxiv.org/abs/2505.15058</guid>
<content:encoded><![CDATA[
arXiv:2505.15058v1 Announce Type: cross 
Abstract: Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.15062</link>
<guid>https://arxiv.org/abs/2505.15062</guid>
<content:encoded><![CDATA[
arXiv:2505.15062v1 Announce Type: cross 
Abstract: When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate "hormones helping mental disorders" with "melatonin being a hormone and insomnia a mental disorder" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and $\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support</title>
<link>https://arxiv.org/abs/2505.15065</link>
<guid>https://arxiv.org/abs/2505.15065</guid>
<content:encoded><![CDATA[
arXiv:2505.15065v1 Announce Type: cross 
Abstract: Can small language models with 0.5B to 5B parameters meaningfully engage in trauma-informed, empathetic dialogue for individuals with PTSD? We address this question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning 500 diverse PTSD client personas and grounded in a three-factor empathy model: emotion recognition, distress normalization, and supportive reflection. All scenarios and reference responses were reviewed for realism and trauma sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight small language models before and after fine-tuning, comparing their outputs to a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and automatic metrics show that fine-tuning generally improves perceived empathy, but gains are highly scenario- and user-dependent, with smaller models facing an empathy ceiling. Demographic analysis shows older adults value distress validation and graduate-educated users prefer nuanced replies, while gender effects are minimal. We highlight the limitations of automatic metrics and the need for context- and user-aware system design. Our findings, along with the planned release of TIDE, provide a foundation for building safe, resource-efficient, and ethically sound empathetic AI to supplement, not replace, clinical mental health care.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data</title>
<link>https://arxiv.org/abs/2505.15074</link>
<guid>https://arxiv.org/abs/2505.15074</guid>
<content:encoded><![CDATA[
arXiv:2505.15074v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v1 Announce Type: cross 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation</title>
<link>https://arxiv.org/abs/2505.15077</link>
<guid>https://arxiv.org/abs/2505.15077</guid>
<content:encoded><![CDATA[
arXiv:2505.15077v1 Announce Type: cross 
Abstract: Urban forests play a key role in enhancing environmental quality and supporting biodiversity in cities. Mapping and monitoring these green spaces are crucial for urban planning and conservation, yet accurately detecting trees is challenging due to complex landscapes and the variability in image resolution caused by different satellite sensors or UAV flight altitudes. While deep learning architectures have shown promise in addressing these challenges, their effectiveness remains strongly dependent on the availability of large and manually labeled datasets, which are often expensive and difficult to obtain in sufficient quantity. In this work, we propose a novel pipeline that integrates domain adaptation with GANs and Diffusion models to enhance the quality of low-resolution aerial images. Our proposed pipeline enhances low-resolution imagery while preserving semantic content, enabling effective tree segmentation without requiring large volumes of manually annotated data. Leveraging models such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we generate realistic and structurally consistent synthetic samples that expand the training dataset and unify scale across domains. This approach not only improves the robustness of segmentation models across different acquisition conditions but also provides a scalable and replicable solution for remote sensing scenarios with scarce annotation resources. Experimental results demonstrated an improvement of over 50% in IoU for low-resolution images, highlighting the effectiveness of our method compared to traditional pipelines.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUS backprop: linear backpropagation algorithm for long inputs in transformers</title>
<link>https://arxiv.org/abs/2505.15080</link>
<guid>https://arxiv.org/abs/2505.15080</guid>
<content:encoded><![CDATA[
arXiv:2505.15080v1 Announce Type: cross 
Abstract: It is straightforward to design an unbiased gradient estimator that stochastically cuts the backpropagation flow through any part of a computational graph. By cutting the parts that have little effect on the computation, one can potentially save a significant amount of back-propagation computation in exchange for a minimal increase in the stochastic gradient variance, in some situations. Such a situation occurs in the attention mechanism of the transformer architecture. For long sequences, attention becomes the limiting factor, as its compute requirements increase quadratically with sequence length $n$. At the same time, most attention weights become very small, as most attention heads tend to connect a given token with only a small fraction of other tokens in the sequence. These weights become promising targets for cutting backpropagation. We propose a simple probabilistic rule controlled by a single parameter $c$ that cuts backpropagation through most attention weights, leaving at most $c$ interactions per token per attention head. This brings a factor of $c/n$ reduction in the compute required for the attention backpropagation, turning it from quadratic $O(n^2)$ to linear complexity $O(nc)$. We have empirically verified that, for a typical transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing $c \sim 20-30$) results in relative gradient variance increase of only about $1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable to efficient sparse matrix implementation, thus being promising for making the cost of a backward pass negligible relative to the cost of a forward pass when training a transformer model on long sequences.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features</title>
<link>https://arxiv.org/abs/2505.15083</link>
<guid>https://arxiv.org/abs/2505.15083</guid>
<content:encoded><![CDATA[
arXiv:2505.15083v1 Announce Type: cross 
Abstract: Time series forecasting plays a crucial role in various applications, particularly in healthcare, where accurate predictions of future health trajectories can significantly impact clinical decision-making. Ensuring transparency and explainability of the models responsible for these tasks is essential for their adoption in critical settings. Recent work has explored a top-down approach to bi-level transparency, focusing on understanding trends and properties of predicted time series using static features. In this work, we extend this framework by incorporating exogenous time series features alongside static features in a structured manner, while maintaining cohesive interpretation. Our approach leverages the insights of trajectory comprehension to introduce an encoding mechanism for exogenous time series, where they are decomposed into meaningful trends and properties, enabling the extraction of interpretable patterns. Through experiments on several synthetic datasets, we demonstrate that our approach remains predictive while preserving interpretability and robustness. This work represents a step towards developing robust, and generalized time series forecasting models. The code is available at https://github.com/jeremy-qin/TIMEVIEW
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects</title>
<link>https://arxiv.org/abs/2505.15088</link>
<guid>https://arxiv.org/abs/2505.15088</guid>
<content:encoded><![CDATA[
arXiv:2505.15088v1 Announce Type: cross 
Abstract: Command injection vulnerabilities are a significant security threat in dynamic languages like Python, particularly in widely used open-source projects where security issues can have extensive impact. With the proven effectiveness of Large Language Models(LLMs) in code-related tasks, such as testing, researchers have explored their potential for vulnerabilities analysis. This study evaluates the potential of large language models (LLMs), such as GPT-4, as an alternative approach for automated testing for vulnerability detection. In particular, LLMs have demonstrated advanced contextual understanding and adaptability, making them promising candidates for identifying nuanced security vulnerabilities within code. To evaluate this potential, we applied LLM-based analysis to six high-profile GitHub projects-Django, Flask, TensorFlow, Scikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive adoption across software development and academic research. Our analysis assesses both the strengths and limitations of LLMs in detecting command injection vulnerabilities, evaluating factors such as detection accuracy, efficiency, and practical integration into development workflows. In addition, we provide a comparative analysis of different LLM tools to identify those most suitable for security applications. Our findings offer guidance for developers and security researchers on leveraging LLMs as innovative and automated approaches to enhance software security.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2505.15090</link>
<guid>https://arxiv.org/abs/2505.15090</guid>
<content:encoded><![CDATA[
arXiv:2505.15090v1 Announce Type: cross 
Abstract: Effective cross-lingual transfer remains a critical challenge in scaling the benefits of large language models from high-resource to low-resource languages. Towards this goal, prior studies have explored many approaches to combine task knowledge from task-specific data in a (high-resource) source language and language knowledge from unlabeled text in a (low-resource) target language. One notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual transfer that learns task-specific and language-specific sparse masks to select a subset of the pretrained model's parameters that are further fine-tuned. These sparse fine-tuned vectors (SFTs) are subsequently composed with the pretrained model to facilitate zero-shot cross-lingual transfer to a task in a target language, using only task-specific data from a source language. These sparse masks for SFTs were identified using a simple magnitude-based pruning. In our work, we introduce DeFT-X, a novel composable SFT approach that denoises the weight matrices of a pretrained model before magnitude pruning using singular value decomposition, thus yielding more robust SFTs. We evaluate DeFT-X on a diverse set of extremely low-resource languages for sentiment classification (NusaX) and natural language inference (AmericasNLI) and demonstrate that it performs at par or outperforms SFT and other prominent cross-lingual transfer baselines.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkRec: Thinking-based recommendation via LLM</title>
<link>https://arxiv.org/abs/2505.15091</link>
<guid>https://arxiv.org/abs/2505.15091</guid>
<content:encoded><![CDATA[
arXiv:2505.15091v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled more semantic-aware recommendations through natural language generation. Existing LLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like manner, relying on superficial features to match similar items based on click history, rather than reasoning through deeper behavioral logic. This often leads to superficial and erroneous recommendations. Motivated by this, we propose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1 to System 2 (rational system). Technically, ThinkRec introduces a thinking activation mechanism that augments item metadata with keyword summarization and injects synthetic reasoning traces, guiding the model to form interpretable reasoning chains that consist of analyzing interaction histories, identifying user preferences, and making decisions based on target items. On top of this, we propose an instance-wise expert fusion mechanism to reduce the reasoning difficulty. By dynamically assigning weights to expert models based on users' latent features, ThinkRec adapts its reasoning path to individual users, thereby enhancing precision and personalization. Extensive experiments on real-world datasets demonstrate that ThinkRec significantly improves the accuracy and interpretability of recommendations. Our implementations are available in anonymous Github: https://anonymous.4open.science/r/ThinkRec_LLM.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English</title>
<link>https://arxiv.org/abs/2505.15095</link>
<guid>https://arxiv.org/abs/2505.15095</guid>
<content:encoded><![CDATA[
arXiv:2505.15095v1 Announce Type: cross 
Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity between stated and implied sentiment. The challenge is exacerbated when the implication may be relevant to a specific country or geographical region. Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that has been used for pragmatic reasoning. In this paper, we harness PMP for explainable sarcasm detection for Australian and Indian English, alongside a benchmark dataset for standard English. We manually add sarcasm explanations to an existing sarcasm-labeled dataset for Australian and Indian English called BESSTIE, and compare the performance for explainable sarcasm detection for them with FLUTE, a standard English dataset containing sarcasm explanations. Our approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA) achieves statistically significant performance improvement across all tasks and datasets when compared with four alternative prompting strategies. We also find that alternative techniques such as agentic prompting mitigate context-related failures by enabling external knowledge retrieval. The focused contribution of our work is utilising PMP in generating sarcasm explanations for varieties of English.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2505.15098</link>
<guid>https://arxiv.org/abs/2505.15098</guid>
<content:encoded><![CDATA[
arXiv:2505.15098v1 Announce Type: cross 
Abstract: Robot manipulation learning from human demonstrations offers a rapid means to acquire skills but often lacks generalization across diverse scenes and object placements. This limitation hinders real-world applications, particularly in complex tasks requiring dexterous manipulation. Vision-Language-Action (VLA) paradigm leverages large-scale data to enhance generalization. However, due to data scarcity, VLA's performance remains limited. In this work, we introduce Object-Focus Actor (OFA), a novel, data-efficient approach for generalized dexterous manipulation. OFA exploits the consistent end trajectories observed in dexterous manipulation tasks, allowing for efficient policy training. Our method employs a hierarchical pipeline: object perception and pose estimation, pre-manipulation pose arrival and OFA policy execution. This process ensures that the manipulation is focused and efficient, even in varied backgrounds and positional layout. Comprehensive real-world experiments across seven tasks demonstrate that OFA significantly outperforms baseline methods in both positional and background generalization tests. Notably, OFA achieves robust performance with only 10 demonstrations, highlighting its data efficiency.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic evaluation of Transformers and state space models</title>
<link>https://arxiv.org/abs/2505.15105</link>
<guid>https://arxiv.org/abs/2505.15105</guid>
<content:encoded><![CDATA[
arXiv:2505.15105v1 Announce Type: cross 
Abstract: State space models (SSMs) for language modelling promise an efficient and performant alternative to quadratic-attention Transformers, yet show variable performance on recalling basic information from the context. While performance on synthetic tasks like Associative Recall (AR) can point to this deficiency, behavioural metrics provide little information as to why--on a mechanistic level--certain architectures fail and others succeed. To address this, we conduct experiments on AR and find that only Transformers and Based SSM models fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3, Hyena) fail. We then use causal interventions to explain why. We find that Transformers and Based learn to store key-value associations in-context using induction heads. By contrast, the SSMs compute these associations only at the last state, with only Mamba succeeding because of its short convolution component. To extend and deepen these findings, we introduce Associative Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR introduces language-like hierarchical structure into the AR setting. We find that all architectures learn the same mechanism as they did for AR, and the same three models succeed at the task. These results reveal that architectures with similar accuracy may still have substantive differences, motivating the adoption of mechanistic evaluations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2505.15107</link>
<guid>https://arxiv.org/abs/2505.15107</guid>
<content:encoded><![CDATA[
arXiv:2505.15107v1 Announce Type: cross 
Abstract: Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. Our implementation is publicly available at https://github.com/zxh20001117/StepSearch.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents</title>
<link>https://arxiv.org/abs/2505.15108</link>
<guid>https://arxiv.org/abs/2505.15108</guid>
<content:encoded><![CDATA[
arXiv:2505.15108v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk taxonomy specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this taxonomy, detailing its grounding, and discuss potential use cases. We discuss two use cases in detail: monitoring cognitive model-based risk factors during a counseling conversation to detect unsafe deviations, in both human-AI counseling sessions and in automated benchmarking of AI psychotherapists with simulated patients. The proposed taxonomy offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iPad: Iterative Proposal-centric End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15111</link>
<guid>https://arxiv.org/abs/2505.15111</guid>
<content:encoded><![CDATA[
arXiv:2505.15111v1 Announce Type: cross 
Abstract: End-to-end (E2E) autonomous driving systems offer a promising alternative to traditional modular pipelines by reducing information loss and error accumulation, with significant potential to enhance both mobility and safety. However, most existing E2E approaches directly generate plans based on dense bird's-eye view (BEV) grid features, leading to inefficiency and limited planning awareness. To address these limitations, we propose iterative Proposal-centric autonomous driving (iPad), a novel framework that places proposals - a set of candidate future plans - at the center of feature extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder that iteratively refines proposals and their associated features through proposal-anchored attention, effectively fusing multi-view image data. Additionally, we introduce two lightweight, proposal-centric auxiliary tasks - mapping and prediction - that improve planning quality with minimal computational overhead. Extensive experiments on the NAVSIM and CARLA Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art performance while being significantly more efficient than prior leading methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Foundation Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15116</link>
<guid>https://arxiv.org/abs/2505.15116</guid>
<content:encoded><![CDATA[
arXiv:2505.15116v1 Announce Type: cross 
Abstract: Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents</title>
<link>https://arxiv.org/abs/2505.15117</link>
<guid>https://arxiv.org/abs/2505.15117</guid>
<content:encoded><![CDATA[
arXiv:2505.15117v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</title>
<link>https://arxiv.org/abs/2505.15123</link>
<guid>https://arxiv.org/abs/2505.15123</guid>
<content:encoded><![CDATA[
arXiv:2505.15123v1 Announce Type: cross 
Abstract: Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer</title>
<link>https://arxiv.org/abs/2505.15133</link>
<guid>https://arxiv.org/abs/2505.15133</guid>
<content:encoded><![CDATA[
arXiv:2505.15133v1 Announce Type: cross 
Abstract: Recent advances in knowledge distillation have emphasized the importance of decoupling different knowledge components. While existing methods utilize momentum mechanisms to separate task-oriented and distillation gradients, they overlook the inherent conflict between target-class and non-target-class knowledge flows. Furthermore, low-confidence dark knowledge in non-target classes introduces noisy signals that hinder effective knowledge transfer. To address these limitations, we propose DeepKD, a novel training framework that integrates dual-level decoupling with adaptive denoising. First, through theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics in task-oriented and non-task-oriented knowledge distillation, we design independent momentum updaters for each component to prevent mutual interference. We observe that the optimal momentum coefficients for task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class gradient (NCG) should be positively related to their GSNR. Second, we introduce a dynamic top-k mask (DTM) mechanism that gradually increases K from a small initial value to incorporate more non-target classes as training progresses, following curriculum learning principles. The DTM jointly filters low-confidence logits from both teacher and student models, effectively purifying dark knowledge during early training. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code is available at https://github.com/haiduo/DeepKD.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15134</link>
<guid>https://arxiv.org/abs/2505.15134</guid>
<content:encoded><![CDATA[
arXiv:2505.15134v1 Announce Type: cross 
Abstract: Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm</title>
<link>https://arxiv.org/abs/2505.15138</link>
<guid>https://arxiv.org/abs/2505.15138</guid>
<content:encoded><![CDATA[
arXiv:2505.15138v1 Announce Type: cross 
Abstract: This paper investigates infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general parametrization. We propose a Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints while ensuring a high convergence rate. In particular, our algorithm achieves global convergence and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge of $\tau_{\mathrm{mix}}$, the achievable rates change to $\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq \tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results match the theoretical lower bound for Markov Decision Processes and establish a new benchmark in the theoretical exploration of average reward CMDPs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms</title>
<link>https://arxiv.org/abs/2505.15141</link>
<guid>https://arxiv.org/abs/2505.15141</guid>
<content:encoded><![CDATA[
arXiv:2505.15141v1 Announce Type: cross 
Abstract: Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15154</link>
<guid>https://arxiv.org/abs/2505.15154</guid>
<content:encoded><![CDATA[
arXiv:2505.15154v1 Announce Type: cross 
Abstract: Recent advancements in reasoning have significantly enhanced the capabilities of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) across diverse tasks. However, excessive reliance on chain-of-thought (CoT) reasoning can impair model performance and brings unnecessarily lengthened outputs, reducing efficiency. Our work reveals that prolonged reasoning does not universally improve accuracy and even degrade performance on simpler tasks. To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel framework that dynamically switches between short answers and long-form reasoning based on the model perplexity. CAR first generates a short answer and evaluates its perplexity, triggering reasoning only when the model exhibits low confidence (i.e., high perplexity). Experiments across diverse multimodal VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both short-answer and long-form reasoning approaches, striking an optimal balance between accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
<link>https://arxiv.org/abs/2505.15155</link>
<guid>https://arxiv.org/abs/2505.15155</guid>
<content:encoded><![CDATA[
arXiv:2505.15155v1 Announce Type: cross 
Abstract: Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&amp;D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection</title>
<link>https://arxiv.org/abs/2505.15173</link>
<guid>https://arxiv.org/abs/2505.15173</guid>
<content:encoded><![CDATA[
arXiv:2505.15173v1 Announce Type: cross 
Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, particularly in video generation, has led to unprecedented creative capabilities but also increased threats to information integrity, identity security, and public trust. Existing detection methods, while effective in general scenarios, lack robust solutions for human-centric videos, which pose greater risks due to their realism and potential for legal and ethical misuse. Moreover, current detection approaches often suffer from poor generalization, limited scalability, and reliance on labor-intensive supervised fine-tuning. To address these challenges, we propose AvatarShield, the first interpretable MLLM-based framework for detecting human-centric fake videos, enhanced via Group Relative Policy Optimization (GRPO). Through our carefully designed accuracy detection reward and temporal compensation reward, it effectively avoids the use of high-cost text annotation data, enabling precise temporal modeling and forgery detection. Meanwhile, we design a dual-encoder architecture, combining high-level semantic reasoning and low-level artifact amplification to guide MLLMs in effective forgery detection. We further collect FakeHumanVid, a large-scale human-centric video benchmark that includes synthesis methods guided by pose, audio, and text inputs, enabling rigorous evaluation of detection methods in real-world scenes. Extensive experiments show that AvatarShield significantly outperforms existing approaches in both in-domain and cross-domain detection, setting a new standard for human-centric video forensics.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection</title>
<link>https://arxiv.org/abs/2505.15182</link>
<guid>https://arxiv.org/abs/2505.15182</guid>
<content:encoded><![CDATA[
arXiv:2505.15182v1 Announce Type: cross 
Abstract: Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intentional Gesture: Deliver Your Intentions with Gestures for Speech</title>
<link>https://arxiv.org/abs/2505.15197</link>
<guid>https://arxiv.org/abs/2505.15197</guid>
<content:encoded><![CDATA[
arXiv:2505.15197v1 Announce Type: cross 
Abstract: When humans speak, gestures help convey communicative intentions, such as adding emphasis or describing concepts. However, current co-speech gesture generation methods rely solely on superficial linguistic cues (\textit{e.g.} speech audio or text transcripts), neglecting to understand and leverage the communicative intention that underpins human gestures. This results in outputs that are rhythmically synchronized with speech but are semantically shallow. To address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework that casts gesture generation as an intention-reasoning task grounded in high-level communicative functions. % First, we curate the \textbf{InG} dataset by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text sentences summarizing intentions), which are automatically annotated using large vision-language models. Next, we introduce the \textbf{Intentional Gesture Motion Tokenizer} to leverage these intention annotations. It injects high-level communicative functions (\textit{e.g.}, intentions) into tokenized motion representations to enable intention-aware gesture synthesis that are both temporally aligned and semantically meaningful, achieving new state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a modular foundation for expressive gesture generation in digital humans and embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[
arXiv:2505.15201v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy</title>
<link>https://arxiv.org/abs/2505.15206</link>
<guid>https://arxiv.org/abs/2505.15206</guid>
<content:encoded><![CDATA[
arXiv:2505.15206v1 Announce Type: cross 
Abstract: In endoscopic procedures, autonomous tracking of abnormal regions and following circumferential cutting markers can significantly reduce the cognitive burden on endoscopists. However, conventional model-based pipelines are fragile for each component (e.g., detection, motion planning) requires manual tuning and struggles to incorporate high-level endoscopic intent, leading to poor generalization across diverse scenes. Vision-Language-Action (VLA) models, which integrate visual perception, language grounding, and motion planning within an end-to-end framework, offer a promising alternative by semantically adapting to surgeon prompts without manual recalibration. Despite their potential, applying VLA models to robotic endoscopy presents unique challenges due to the complex and dynamic anatomical environments of the gastrointestinal (GI) tract. To address this, we introduce EndoVLA, designed specifically for continuum robots in GI interventions. Given endoscopic images and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1) polyp tracking, (2) delineation and following of abnormal mucosal regions, and (3) adherence to circular markers during circumferential cutting. To tackle data scarcity and domain shifts, we propose a dual-phase strategy comprising supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement fine-tuning with task-aware rewards. Our approach significantly improves tracking performance in endoscopy and enables zero-shot generalization in diverse scenes and complex sequential tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title>
<link>https://arxiv.org/abs/2505.15216</link>
<guid>https://arxiv.org/abs/2505.15216</guid>
<content:encoded><![CDATA[
arXiv:2505.15216v1 Announce Type: cross 
Abstract: AI agents have the potential to significantly alter the cybersecurity landscape. To help us understand this change, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a specific vulnerability), and Patch (patching a specific vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code, OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch, mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit scores of 32.5% and 57.5% respectively; in contrast, the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 40-67.5% and Patch scores of 45-60%.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning</title>
<link>https://arxiv.org/abs/2505.15234</link>
<guid>https://arxiv.org/abs/2505.15234</guid>
<content:encoded><![CDATA[
arXiv:2505.15234v1 Announce Type: cross 
Abstract: Medical image segmentation plays an important role in various clinical applications, but existing models often struggle with the computational inefficiencies and challenges posed by complex medical data. State Space Sequence Models (SSMs) have demonstrated promise in modeling long-range dependencies with linear computational complexity, yet their application in medical image segmentation remains hindered by incompatibilities with image tokens and autoregressive assumptions. Moreover, it is difficult to achieve a balance in capturing both local fine-grained information and global semantic dependencies. To address these challenges, we introduce SAMA-UNet, a novel architecture for medical image segmentation. A key innovation is the Self-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates contextual self-attention with dynamic weight modulation to prioritise the most relevant features based on local and global contexts. This approach reduces computational complexity and improves the representation of complex image features across multiple scales. We also suggest the Causal-Resonance Multi-Scale Module (CR-MSM), which enhances the flow of information between the encoder and decoder by using causal resonance learning. This mechanism allows the model to automatically adjust feature resolution and causal dependencies across scales, leading to better semantic alignment between the low-level and high-level features in U-shaped architectures. Experiments on MRI, CT, and endoscopy images show that SAMA-UNet performs better in segmentation accuracy than current methods using CNN, Transformer, and Mamba. The implementation is publicly available at GitHub.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers</title>
<link>https://arxiv.org/abs/2505.15239</link>
<guid>https://arxiv.org/abs/2505.15239</guid>
<content:encoded><![CDATA[
arXiv:2505.15239v1 Announce Type: cross 
Abstract: The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Plan-Execute Framework for Smart Contract Security Auditing</title>
<link>https://arxiv.org/abs/2505.15242</link>
<guid>https://arxiv.org/abs/2505.15242</guid>
<content:encoded><![CDATA[
arXiv:2505.15242v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown great promise in code analysis and auditing; however, they still struggle with hallucinations and limited context-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute framework that enhances smart contract security analysis through dynamic audit planning and structured execution. Unlike conventional LLM-based auditing approaches that follow fixed workflows and predefined steps, SmartAuditFlow dynamically generates and refines audit plans based on the unique characteristics of each smart contract. It continuously adjusts its auditing strategy in response to intermediate LLM outputs and newly detected vulnerabilities, ensuring a more adaptive and precise security assessment. The framework then executes these plans step by step, applying a structured reasoning process to enhance vulnerability detection accuracy while minimizing hallucinations and false positives. To further improve audit precision, SmartAuditFlow integrates iterative prompt optimization and external knowledge sources, such as static analysis tools and Retrieval-Augmented Generation (RAG). This ensures audit decisions are contextually informed and backed by real-world security knowledge, producing comprehensive security reports. Extensive evaluations across multiple benchmarks demonstrate that SmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on common and critical vulnerabilities, 41.2 percent accuracy for comprehensive coverage of known smart contract weaknesses in real-world projects, and successfully identifying all 13 tested CVEs. These results highlight SmartAuditFlow's scalability, cost-effectiveness, and superior adaptability over traditional static analysis tools and contemporary LLM-based approaches, establishing it as a robust solution for automated smart contract auditing.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework</title>
<link>https://arxiv.org/abs/2505.15245</link>
<guid>https://arxiv.org/abs/2505.15245</guid>
<content:encoded><![CDATA[
arXiv:2505.15245v1 Announce Type: cross 
Abstract: While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs' capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at https://github.com/carryTatum/GETER.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification</title>
<link>https://arxiv.org/abs/2505.15250</link>
<guid>https://arxiv.org/abs/2505.15250</guid>
<content:encoded><![CDATA[
arXiv:2505.15250v1 Announce Type: cross 
Abstract: Fuzzy rough feature selection (FRFS) is an effective means of addressing the curse of dimensionality in high-dimensional data. By removing redundant and irrelevant features, FRFS helps mitigate classifier overfitting, enhance generalization performance, and lessen computational overhead. However, most existing FRFS algorithms primarily focus on reducing uncertainty in pattern classification, neglecting that lower uncertainty does not necessarily result in improved classification performance, despite it commonly being regarded as a key indicator of feature selection effectiveness in the FRFS literature. To bridge uncertainty characterization and pattern classification, we propose a Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers both the compactness and separation of label classes. MAFRFS effectively reduces uncertainty in pattern classification tasks, while guiding the feature selection towards more separable and discriminative label class structures. Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly scalable and more effective than FRFS. The algorithms developed using MAFRFS outperform six state-of-the-art feature selection algorithms.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Gaze-based Volumetric Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.15256</link>
<guid>https://arxiv.org/abs/2505.15256</guid>
<content:encoded><![CDATA[
arXiv:2505.15256v1 Announce Type: cross 
Abstract: Accurate segmentation of anatomical structures in volumetric medical images is crucial for clinical applications, including disease monitoring and cancer treatment planning. Contemporary interactive segmentation models, such as Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on manually provided prompts like bounding boxes and mouse clicks. In this study, we introduce eye gaze as a novel informational modality for interactive segmentation, marking the application of eye-tracking for 3D medical image segmentation. We evaluate the performance of using gaze-based prompts with SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to bounding boxes, gaze-based prompts offer a time-efficient interaction approach with slightly lower segmentation quality. Our findings highlight the potential of using gaze as a complementary input modality for interactive 3D medical image segmentation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs</title>
<link>https://arxiv.org/abs/2505.15265</link>
<guid>https://arxiv.org/abs/2505.15265</guid>
<content:encoded><![CDATA[
arXiv:2505.15265v1 Announce Type: cross 
Abstract: Adversarial attacks aim to generate malicious inputs that mislead deep models, but beyond causing model failure, they cannot provide certain interpretable information such as ``\textit{What content in inputs make models more likely to fail?}'' However, this information is crucial for researchers to specifically improve model robustness. Recent research suggests that models may be particularly sensitive to certain semantics in visual inputs (such as ``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this paper we conducted the first exploration on large vision-language models (LVLMs) and found that LVLMs indeed are susceptible to hallucinations and various errors when facing specific semantic concepts in images. To efficiently search for these sensitive concepts, we integrated large language models (LLMs) and text-to-image (T2I) models to propose a novel semantic evolution framework. Randomly initialized semantic concepts undergo LLM-based crossover and mutation operations to form image descriptions, which are then converted by T2I models into visual inputs for LVLMs. The task-specific performance of LVLMs on each input is quantified as fitness scores for the involved semantics and serves as reward signals to further guide LLMs in exploring concepts that induce LVLMs. Extensive experiments on seven mainstream LVLMs and two multimodal tasks demonstrate the effectiveness of our method. Additionally, we provide interesting findings about the sensitive semantics of LVLMs, aiming to inspire further in-depth research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Diffusion Transformers Efficiently via $\mu$P</title>
<link>https://arxiv.org/abs/2505.15270</link>
<guid>https://arxiv.org/abs/2505.15270</guid>
<content:encoded><![CDATA[
arXiv:2505.15270v1 Announce Type: cross 
Abstract: Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\mu$P on text-to-image generation by scaling PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\mu$P as a principled and efficient framework for scaling diffusion Transformers.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based Autonomous Oversteer Control and Collision Avoidance</title>
<link>https://arxiv.org/abs/2505.15275</link>
<guid>https://arxiv.org/abs/2505.15275</guid>
<content:encoded><![CDATA[
arXiv:2505.15275v1 Announce Type: cross 
Abstract: Oversteer, wherein a vehicle's rear tires lose traction and induce unintentional excessive yaw, poses critical safety challenges. Failing to control oversteer often leads to severe traffic accidents. Although recent autonomous driving efforts have attempted to handle oversteer through stabilizing maneuvers, the majority rely on expert-defined trajectories or assume obstacle-free environments, limiting real-world applicability. This paper introduces a novel end-to-end (E2E) autonomous driving approach that tackles oversteer control and collision avoidance simultaneously. Existing E2E techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and Hybrid Learning (HL), generally require near-optimal demonstrations or extensive experience. Yet even skilled human drivers struggle to provide perfect demonstrations under oversteer, and high transition variance hinders accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic (QC-SAC), a new HL algorithm that effectively learns from suboptimal demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we introduce a benchmark inspired by real-world driver training: a vehicle encounters sudden oversteer on a slippery surface and must avoid randomly placed obstacles ahead. Experimental results show QC-SAC attains near-optimal driving policies, significantly surpassing state-of-the-art IL, RL, and HL baselines. Our method demonstrates the world's first safe autonomous oversteer control with obstacle avoidance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2505.15285</link>
<guid>https://arxiv.org/abs/2505.15285</guid>
<content:encoded><![CDATA[
arXiv:2505.15285v1 Announce Type: cross 
Abstract: Mesh reconstruction is a cornerstone process across various applications, including in-silico trials, digital twins, surgical planning, and navigation. Recent advancements in deep learning have notably enhanced mesh reconstruction speeds. Yet, traditional methods predominantly rely on deforming a standardised template mesh for individual subjects, which overlooks the unique anatomical variations between them, and may compromise the fidelity of the reconstructions. In this paper, we propose an adaptive-template-based mesh reconstruction network (ATMRN), which generates adaptive templates from the given images for the subsequent deformation, moving beyond the constraints of a singular, fixed template. Our approach, validated on cortical magnetic resonance (MR) images from the OASIS dataset, sets a new benchmark in voxel-to-cortex mesh reconstruction, achieving an average symmetric surface distance of 0.267mm across four cortical structures. Our proposed method is generic and can be easily transferred to other image modalities and anatomical structures.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models</title>
<link>https://arxiv.org/abs/2505.15293</link>
<guid>https://arxiv.org/abs/2505.15293</guid>
<content:encoded><![CDATA[
arXiv:2505.15293v1 Announce Type: cross 
Abstract: Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for reproducibility.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Laplace Sample Information: Data Informativeness Through a Bayesian Lens</title>
<link>https://arxiv.org/abs/2505.15303</link>
<guid>https://arxiv.org/abs/2505.15303</guid>
<content:encoded><![CDATA[
arXiv:2505.15303v1 Announce Type: cross 
Abstract: Accurately estimating the informativeness of individual samples in a dataset is an important objective in deep learning, as it can guide sample selection, which can improve model efficiency and accuracy by removing redundant or potentially harmful samples. We propose Laplace Sample Information (LSI) measure of sample informativeness grounded in information theory widely applicable across model architectures and learning settings. LSI leverages a Bayesian approximation to the weight posterior and the KL divergence to measure the change in the parameter distribution induced by a sample of interest from the dataset. We experimentally show that LSI is effective in ordering the data with respect to typicality, detecting mislabeled samples, measuring class-wise informativeness, and assessing dataset difficulty. We demonstrate these capabilities of LSI on image and text data in supervised and unsupervised settings. Moreover, we show that LSI can be computed efficiently through probes and transfers well to the training of large models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One</title>
<link>https://arxiv.org/abs/2505.15306</link>
<guid>https://arxiv.org/abs/2505.15306</guid>
<content:encoded><![CDATA[
arXiv:2505.15306v1 Announce Type: cross 
Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent's performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct 'situations', incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at https://anonymous.4open.science/r/LLM4RLensemble-F7EE.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.15308</link>
<guid>https://arxiv.org/abs/2505.15308</guid>
<content:encoded><![CDATA[
arXiv:2505.15308v1 Announce Type: cross 
Abstract: With the widespread application of super-resolution (SR) in various fields, researchers have begun to investigate its security. Previous studies have demonstrated that SR models can also be subjected to backdoor attacks through data poisoning, affecting downstream tasks. A backdoor SR model generates an attacker-predefined target image when given a triggered image while producing a normal high-resolution (HR) output for clean images. However, prior backdoor attacks on SR models have primarily focused on the stealthiness of poisoned low-resolution (LR) images while ignoring the stealthiness of poisoned HR images, making it easy for users to detect anomalous data. To address this problem, we propose BadSR, which improves the stealthiness of poisoned HR images. The key idea of BadSR is to approximate the clean HR image and the pre-defined target image in the feature space while ensuring that modifications to the clean HR image remain within a constrained range. The poisoned HR images generated by BadSR can be integrated with existing triggers. To further improve the effectiveness of BadSR, we design an adversarially optimized trigger and a backdoor gradient-driven poisoned sample selection method based on a genetic algorithm. The experimental results show that BadSR achieves a high attack success rate in various models and data sets, significantly affecting downstream tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15311</link>
<guid>https://arxiv.org/abs/2505.15311</guid>
<content:encoded><![CDATA[
arXiv:2505.15311v1 Announce Type: cross 
Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation</title>
<link>https://arxiv.org/abs/2505.15333</link>
<guid>https://arxiv.org/abs/2505.15333</guid>
<content:encoded><![CDATA[
arXiv:2505.15333v1 Announce Type: cross 
Abstract: The success of building textless speech-to-speech translation (S2ST) models has attracted much attention. However, S2ST still faces two main challenges: 1) extracting linguistic features for various speech signals, called cross-modal (CM), and 2) learning alignment of difference languages in long sequences, called cross-lingual (CL). We propose the unit language to overcome the two modeling challenges. The unit language can be considered a text-like representation format, constructed using $n$-gram language modeling. We implement multi-task learning to utilize the unit language in guiding the speech modeling process. Our initial results reveal a conflict when applying source and target unit languages simultaneously. We propose task prompt modeling to mitigate this conflict. We conduct experiments on four languages of the Voxpupil dataset. Our method demonstrates significant improvements over a strong baseline and achieves performance comparable to models trained with text.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors</title>
<link>https://arxiv.org/abs/2505.15337</link>
<guid>https://arxiv.org/abs/2505.15337</guid>
<content:encoded><![CDATA[
arXiv:2505.15337v1 Announce Type: cross 
Abstract: The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \textbf{Co}ntrastive \textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alpay Algebra: A Universal Structural Foundation</title>
<link>https://arxiv.org/abs/2505.15344</link>
<guid>https://arxiv.org/abs/2505.15344</guid>
<content:encoded><![CDATA[
arXiv:2505.15344v1 Announce Type: cross 
Abstract: Alpay Algebra is introduced as a universal, category-theoretic framework that unifies classical algebraic structures with modern needs in symbolic recursion and explainable AI. Starting from a minimal list of axioms, we model each algebra as an object in a small cartesian closed category $\mathcal{A}$ and define a transfinite evolution functor $\phi\colon\mathcal{A}\to\mathcal{A}$. We prove that the fixed point $\phi^{\infty}$ exists for every initial object and satisfies an internal universal property that recovers familiar constructs -- limits, colimits, adjunctions -- while extending them to ordinal-indexed folds. A sequence of theorems establishes (i) soundness and conservativity over standard universal algebra, (ii) convergence of $\phi$-iterates under regular cardinals, and (iii) an explanatory correspondence between $\phi^{\infty}$ and minimal sufficient statistics in information-theoretic AI models. We conclude by outlining computational applications: type-safe functional languages, categorical model checking, and signal-level reasoning engines that leverage Alpay Algebra's structural invariants. All proofs are self-contained; no external set-theoretic axioms beyond ZFC are required. This exposition positions Alpay Algebra as a bridge between foundational mathematics and high-impact AI systems, and provides a reference for further work in category theory, transfinite fixed-point analysis, and symbolic computation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hadamax Encoding: Elevating Performance in Model-Free Atari</title>
<link>https://arxiv.org/abs/2505.15345</link>
<guid>https://arxiv.org/abs/2505.15345</guid>
<content:encoded><![CDATA[
arXiv:2505.15345v1 Announce Type: cross 
Abstract: Neural network architectures have a large impact in machine learning. In reinforcement learning, network architectures have remained notably simple, as changes often lead to small gains in performance. This work introduces a novel encoder architecture for pixel-based model-free reinforcement learning. The Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves state-of-the-art performance by max-pooling Hadamard products between GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the Hadamax encoder achieves state-of-the-art model-free performance in the Atari-57 benchmark. Specifically, without applying any algorithmic hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility, the full code is available on \href{https://github.com/Jacobkooi/Hadamax}{GitHub}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model</title>
<link>https://arxiv.org/abs/2505.15358</link>
<guid>https://arxiv.org/abs/2505.15358</guid>
<content:encoded><![CDATA[
arXiv:2505.15358v1 Announce Type: cross 
Abstract: Road safety is a critical challenge, particularly for cyclists, who are among the most vulnerable road users. This study aims to enhance road safety by proposing a novel benchmark for bicycle occlusion level classification using advanced computer vision techniques. Utilizing a parts-based detection model, images are annotated and processed through a custom image detection pipeline. A novel method of bicycle occlusion level is proposed to objectively quantify the visibility and occlusion level of bicycle semantic parts. The findings indicate that the model robustly quantifies the visibility and occlusion level of bicycles, a significant improvement over the subjective methods used by the current state of the art. Widespread use of the proposed methodology will facilitate the accurate performance reporting of cyclist detection algorithms for occluded cyclists, informing the development of more robust vulnerable road user detection methods for autonomous vehicles.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition</title>
<link>https://arxiv.org/abs/2505.15367</link>
<guid>https://arxiv.org/abs/2505.15367</guid>
<content:encoded><![CDATA[
arXiv:2505.15367v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in understanding visual content, but their reliability in safety-critical contexts remains under-explored. We introduce VERI (Visual Emergency Recognition Dataset), a carefully designed diagnostic benchmark of 200 images (100 contrastive pairs). Each emergency scene is matched with a visually similar but safe counterpart through multi-stage human verification and iterative refinement. Using a two-stage protocol - risk identification and emergency response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies, accidents, and natural disasters. Our analysis reveals a systematic overreaction problem: models excel at identifying real emergencies (70-100 percent success rate) but suffer from an alarming rate of false alarms, misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios failed by all models regardless of scale. This "better-safe-than-sorry" bias manifests primarily through contextual overinterpretation (88-93 percent of errors), challenging VLMs' reliability for safety applications. These findings highlight persistent limitations that are not resolved by increasing model scale, motivating targeted approaches for improving contextual safety assessment in visually misleading scenarios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.15380</link>
<guid>https://arxiv.org/abs/2505.15380</guid>
<content:encoded><![CDATA[
arXiv:2505.15380v1 Announce Type: cross 
Abstract: Modern autoregressive speech synthesis models leveraging language models have demonstrated remarkable performance. However, the sequential nature of next token prediction in these models leads to significant latency, hindering their deployment in scenarios where inference speed is critical. In this work, we propose Speech Speculative Decoding (SSD), a novel framework for autoregressive speech synthesis acceleration. Specifically, our method employs a lightweight draft model to generate candidate token sequences, which are subsequently verified in parallel by the target model using the proposed SSD framework. Experimental results demonstrate that SSD achieves a significant speedup of 1.4x compared with conventional autoregressive decoding, while maintaining high fidelity and naturalness. Subjective evaluations further validate the effectiveness of SSD in preserving the perceptual quality of the target model while accelerating inference.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection</title>
<link>https://arxiv.org/abs/2505.15386</link>
<guid>https://arxiv.org/abs/2505.15386</guid>
<content:encoded><![CDATA[
arXiv:2505.15386v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2505.15406</link>
<guid>https://arxiv.org/abs/2505.15406</guid>
<content:encoded><![CDATA[
arXiv:2505.15406v1 Announce Type: cross 
Abstract: The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Policy Optimization under Partial Observability</title>
<link>https://arxiv.org/abs/2505.15418</link>
<guid>https://arxiv.org/abs/2505.15418</guid>
<content:encoded><![CDATA[
arXiv:2505.15418v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) in partially observable environments poses significant challenges due to the complexity of learning under uncertainty. While additional information, such as that available in simulations, can enhance training, effectively leveraging it remains an open problem. To address this, we introduce Guided Policy Optimization (GPO), a framework that co-trains a guider and a learner. The guider takes advantage of privileged information while ensuring alignment with the learner's policy that is primarily trained via imitation learning. We theoretically demonstrate that this learning scheme achieves optimality comparable to direct RL, thereby overcoming key limitations inherent in existing approaches. Empirical evaluations show strong performance of GPO across various tasks, including continuous control with partial observability and noise, and memory-based challenges, significantly outperforming existing methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries</title>
<link>https://arxiv.org/abs/2505.15420</link>
<guid>https://arxiv.org/abs/2505.15420</guid>
<content:encoded><![CDATA[
arXiv:2505.15420v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by incorporating external knowledge bases, but they are vulnerable to privacy risks from data extraction attacks. Existing extraction methods typically rely on malicious inputs such as prompt injection or jailbreaking, making them easily detectable via input- or output-level detection. In this paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts knowledge extraction on RAG systems through benign queries. IKEA first leverages anchor concepts to generate queries with the natural appearance, and then designs two mechanisms to lead to anchor concept thoroughly 'explore' the RAG's privacy knowledge: (1) Experience Reflection Sampling, which samples anchor concepts based on past query-response patterns to ensure the queries' relevance to RAG documents; (2) Trust Region Directed Mutation, which iteratively mutates anchor concepts under similarity constraints to further exploit the embedding space. Extensive experiments demonstrate IKEA's effectiveness under various defenses, surpassing baselines by over 80% in extraction efficiency and 90% in attack success rate. Moreover, the substitute RAG system built from IKEA's extractions consistently outperforms those based on baseline methods across multiple evaluation tasks, underscoring the significant privacy risk in RAG systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions</title>
<link>https://arxiv.org/abs/2505.15427</link>
<guid>https://arxiv.org/abs/2505.15427</guid>
<content:encoded><![CDATA[
arXiv:2505.15427v1 Announce Type: cross 
Abstract: The remarkable ability of diffusion models to generate high-fidelity images has led to their widespread adoption. However, concerns have also arisen regarding their potential to produce Not Safe for Work (NSFW) content and exhibit social biases, hindering their practical use in real-world applications. In response to this challenge, prior work has focused on employing security filters to identify and exclude toxic text, or alternatively, fine-tuning pre-trained diffusion models to erase sensitive concepts. Unfortunately, existing methods struggle to achieve satisfactory performance in the sense that they can have a significant impact on the normal model output while still failing to prevent the generation of harmful content in some cases. In this paper, we propose a novel self-discovery approach to identifying a semantic direction vector in the embedding space to restrict text embedding within a safe region. Our method circumvents the need for correcting individual words within the input text and steers the entire text prompt towards a safe region in the embedding space, thereby enhancing model robustness against all possibly unsafe prompts. In addition, we employ Low-Rank Adaptation (LoRA) for semantic direction vector initialization to reduce the impact on the model performance for other semantics. Furthermore, our method can also be integrated with existing methods to improve their social responsibility. Extensive experiments on benchmark datasets demonstrate that our method can effectively reduce NSFW content and mitigate social bias generated by diffusion models compared to several state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification in SVM prediction</title>
<link>https://arxiv.org/abs/2505.15429</link>
<guid>https://arxiv.org/abs/2505.15429</guid>
<content:encoded><![CDATA[
arXiv:2505.15429v1 Announce Type: cross 
Abstract: This paper explores Uncertainty Quantification (UQ) in SVM predictions, particularly for regression and forecasting tasks. Unlike the Neural Network, the SVM solutions are typically more stable, sparse, optimal and interpretable. However, there are only few literature which addresses the UQ in SVM prediction. At first, we provide a comprehensive summary of existing Prediction Interval (PI) estimation and probabilistic forecasting methods developed in the SVM framework and evaluate them against the key properties expected from an ideal PI model. We find that none of the existing SVM PI models achieves a sparse solution. To introduce sparsity in SVM model, we propose the Sparse Support Vector Quantile Regression (SSVQR) model, which constructs PIs and probabilistic forecasts by solving a pair of linear programs. Further, we develop a feature selection algorithm for PI estimation using SSVQR that effectively eliminates a significant number of features while improving PI quality in case of high-dimensional dataset. Finally we extend the SVM models in Conformal Regression setting for obtaining more stable prediction set with finite test set guarantees. Extensive experiments on artificial, real-world benchmark datasets compare the different characteristics of both existing and proposed SVM-based PI estimation methods and also highlight the advantages of the feature selection in PI estimation. Furthermore, we compare both, the existing and proposed SVM-based PI estimation models, with modern deep learning models for probabilistic forecasting tasks on benchmark datasets. Furthermore, SVM models show comparable or superior performance to modern complex deep learning models for probabilistic forecasting task in our experiments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Set-LLM: A Permutation-Invariant LLM</title>
<link>https://arxiv.org/abs/2505.15433</link>
<guid>https://arxiv.org/abs/2505.15433</guid>
<content:encoded><![CDATA[
arXiv:2505.15433v1 Announce Type: cross 
Abstract: While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs as automated evaluators in AI pipelines, comparing output generated by different models. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while eliminating order sensitivity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger ViTs With Octic Equivariance</title>
<link>https://arxiv.org/abs/2505.15441</link>
<guid>https://arxiv.org/abs/2505.15441</guid>
<content:encoded><![CDATA[
arXiv:2505.15441v1 Announce Type: cross 
Abstract: Recent efforts at scaling computer vision models have established Vision Transformers (ViTs) as the leading architecture. ViTs incorporate weight sharing over image patches as an important inductive bias. In this work, we show that ViTs benefit from incorporating equivariance under the octic group, i.e., reflections and 90-degree rotations, as a further inductive bias. We develop new architectures, octic ViTs, that use octic-equivariant layers and put them to the test on both supervised and self-supervised learning. Through extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show that octic ViTs yield more computationally efficient networks while also improving performance. In particular, we achieve approximately 40% reduction in FLOPs for ViT-H while simultaneously improving both classification and segmentation results.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization</title>
<link>https://arxiv.org/abs/2505.15444</link>
<guid>https://arxiv.org/abs/2505.15444</guid>
<content:encoded><![CDATA[
arXiv:2505.15444v1 Announce Type: cross 
Abstract: Existing studies have optimized retrieval-augmented generation (RAG) across various sub-tasks, such as query understanding and retrieval refinement, but integrating these optimizations into a unified framework remains challenging. To tackle this problem, this work proposes RoleRAG, a unified RAG framework that achieves efficient multi-task processing through role-specific token optimization. RoleRAG comprises six modules, each handling a specific sub-task within the RAG process. Additionally, we introduce a query graph to represent the decomposition of the query, which can be dynamically resolved according to the decomposing state. All modules are driven by the same underlying LLM, distinguished by task-specific role tokens that are individually optimized. This design allows RoleRAG to dynamically activate different modules within a single LLM instance, thereby streamlining deployment and reducing resource consumption. Experimental results on five open-domain question-answering datasets demonstrate the effectiveness, generalizability, and flexibility of our framework.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.15447</link>
<guid>https://arxiv.org/abs/2505.15447</guid>
<content:encoded><![CDATA[
arXiv:2505.15447v1 Announce Type: cross 
Abstract: Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.15467</link>
<guid>https://arxiv.org/abs/2505.15467</guid>
<content:encoded><![CDATA[
arXiv:2505.15467v1 Announce Type: cross 
Abstract: Large language models have achieved remarkable success in various tasks. However, it is challenging for them to learn new tasks incrementally due to catastrophic forgetting. Existing approaches rely on experience replay, optimization constraints, or task differentiation, which encounter strict limitations in real-world scenarios. To address these issues, we propose Joint Flashback Adaptation. We first introduce flashbacks -- a limited number of prompts from old tasks -- when adapting to new tasks and constrain the deviations of the model outputs compared to the original one. We then interpolate latent tasks between flashbacks and new tasks to enable jointly learning relevant latent tasks, new tasks, and flashbacks, alleviating data sparsity in flashbacks and facilitating knowledge sharing for smooth adaptation. Our method requires only a limited number of flashbacks without access to the replay data and is task-agnostic. We conduct extensive experiments on state-of-the-art large language models across 1000+ instruction-following tasks, arithmetic reasoning tasks, and general reasoning tasks. The results demonstrate the superior performance of our method in improving generalization on new tasks and reducing forgetting in old tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Qualitative Investigation into LLM-Generated Multilingual Code Comments and Automatic Evaluation Metrics</title>
<link>https://arxiv.org/abs/2505.15469</link>
<guid>https://arxiv.org/abs/2505.15469</guid>
<content:encoded><![CDATA[
arXiv:2505.15469v1 Announce Type: cross 
Abstract: Large Language Models are essential coding assistants, yet their training is predominantly English-centric. In this study, we evaluate the performance of code language models in non-English contexts, identifying challenges in their adoption and integration into multilingual workflows. We conduct an open-coding study to analyze errors in code comments generated by five state-of-the-art code models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2 across five natural languages: Chinese, Dutch, English, Greek, and Polish. Our study yields a dataset of 12,500 labeled generations, which we publicly release. We then assess the reliability of standard metrics in capturing comment \textit{correctness} across languages and evaluate their trustworthiness as judgment criteria. Through our open-coding investigation, we identified a taxonomy of 26 distinct error categories in model-generated code comments. They highlight variations in language cohesion, informativeness, and syntax adherence across different natural languages. Our analysis shows that, while these models frequently produce partially correct comments, modern neural metrics fail to reliably differentiate meaningful completions from random noise. Notably, the significant score overlap between expert-rated correct and incorrect comments calls into question the effectiveness of these metrics in assessing generated comments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2505.15475</link>
<guid>https://arxiv.org/abs/2505.15475</guid>
<content:encoded><![CDATA[
arXiv:2505.15475v1 Announce Type: cross 
Abstract: Nowadays, Large Language Models (LLMs) have attracted widespread attention due to their powerful performance. However, due to the unavoidable exposure to socially biased data during training, LLMs tend to exhibit social biases, particularly gender bias. To better explore and quantifying the degree of gender bias in LLMs, we propose a pair of datasets named GenBiasEval and GenHintEval, respectively. The GenBiasEval is responsible for evaluating the degree of gender bias in LLMs, accompanied by an evaluation metric named AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is used to assess whether LLMs can provide responses consistent with prompts that contain gender hints, along with the accompanying evaluation metric UB-Score (UnBias Score). Besides, in order to mitigate gender bias in LLMs more effectively, we present the LFTF (Locating First and Then Fine-Tuning) algorithm.The algorithm first ranks specific LLM blocks by their relevance to gender bias in descending order using a metric called BMI (Block Mitigating Importance Score). Based on this ranking, the block most strongly associated with gender bias is then fine-tuned using a carefully designed loss function. Numerous experiments have shown that our proposed LFTF algorithm can significantly mitigate gender bias in LLMs while maintaining their general capabilities.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.15501</link>
<guid>https://arxiv.org/abs/2505.15501</guid>
<content:encoded><![CDATA[
arXiv:2505.15501v1 Announce Type: cross 
Abstract: We introduce the concept of protoknowledge to formalize and measure how sequences of tokens encoding Knowledge Graphs are internalized during pretraining and utilized at inference time by Large Language Models (LLMs). Indeed, LLMs have demonstrated the ability to memorize vast amounts of token sequences during pretraining, and a central open question is how they leverage this memorization as reusable knowledge through generalization. We then categorize protoknowledge into lexical, hierarchical, and topological forms, varying on the type of knowledge that needs to be activated. We measure protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general properties such as semantic bias. We then investigate the impact of protoknowledge on Text-to-SPARQL performance by varying prompting strategies depending on input conditions. To this end, we adopt a novel analysis framework that assesses whether model predictions align with the successful activation of the relevant protoknowledge for each query. This methodology provides a practical tool to explore Semantic-Level Data Contamination and serves as an effective strategy for Closed-Pretraining models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2505.15504</link>
<guid>https://arxiv.org/abs/2505.15504</guid>
<content:encoded><![CDATA[
arXiv:2505.15504v1 Announce Type: cross 
Abstract: Deep learning has advanced computational pathology but expert annotations remain scarce. Few-shot learning mitigates annotation burdens yet suffers from overfitting and discriminative feature mischaracterization. In addition, the current few-shot multiple instance learning (MIL) approaches leverage pretrained vision-language models to alleviate these issues, but at the cost of complex preprocessing and high computational cost. We propose a Squeeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in MIL models to address these challenges. The SR block comprises two core components: a pair of low-rank trainable matrices (squeeze pathway, SP) that reduces parameter count and imposes a bottleneck to prevent spurious feature learning, and a frozen random recalibration matrix that preserves geometric structure, diversifies feature directions, and redefines the optimization objective for the SP. We provide theoretical guarantees that the SR block can approximate any linear mapping to arbitrary precision, thereby ensuring that the performance of a standard MIL model serves as a lower bound for its SR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL models consistently outperform prior methods while requiring significantly fewer parameters and no architectural changes.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning</title>
<link>https://arxiv.org/abs/2505.15507</link>
<guid>https://arxiv.org/abs/2505.15507</guid>
<content:encoded><![CDATA[
arXiv:2505.15507v1 Announce Type: cross 
Abstract: We introduce a new algebraic structure for multi-dimensional compositional embeddings, built on directional non-commutative monoidal operators. The core contribution of this work is this novel framework, which exhibits appealing theoretical properties (associativity along each dimension and an interchange law ensuring global consistency) while remaining compatible with modern machine learning architectures. Our construction defines a distinct composition operator circ_i for each axis i, ensuring associative combination along each axis without imposing global commutativity. Importantly, all axis-specific operators commute with one another, enforcing a global interchange law that enables consistent crossaxis compositions. This is, to our knowledge, the first approach that provides a common foundation that generalizes classical sequence-modeling paradigms (e.g., structured state-space models (SSMs) and transformer self-attention) to a unified multi-dimensional framework. For example, specific one-dimensional instances of our framework can recover the familiar affine transformation algebra, vanilla self-attention, and the SSM-style recurrence. The higher-dimensional generalizations naturally support recursive, structure-aware operations in embedding spaces. We outline several potential applications unlocked by this structure-including structured positional encodings in Transformers, directional image embeddings, and symbolic modeling of sequences or grids-indicating that it could inform future deep learning model designs. We formally establish the algebraic properties of our framework and discuss efficient implementations. Finally, as our focus is theoretical, we include no experiments here and defer empirical validation to future work, which we plan to undertake.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2505.15514</link>
<guid>https://arxiv.org/abs/2505.15514</guid>
<content:encoded><![CDATA[
arXiv:2505.15514v1 Announce Type: cross 
Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm that heavily relies on accurate advantage estimates for stable and efficient training. However, raw advantage signals can exhibit significant variance, noise, and scale-related issues, impeding optimal learning performance. To address this challenge, we introduce Advantage Modulation PPO (AM-PPO), a novel enhancement of PPO that adaptively modulates advantage estimates using a dynamic, non-linear scaling mechanism. This adaptive modulation employs an alpha controller that dynamically adjusts the scaling factor based on evolving statistical properties of the advantage signals, such as their norm, variance, and a predefined target saturation level. By incorporating a tanh-based gating function driven by these adaptively scaled advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates and improve the conditioning of the policy gradient landscape. Crucially, this modulation also influences value function training by providing consistent and adaptively conditioned learning targets. Empirical evaluations across standard continuous control benchmarks demonstrate that AM-PPO achieves superior reward trajectories, exhibits sustained learning progression, and significantly reduces the clipping required by adaptive optimizers. These findings underscore the potential of advantage modulation as a broadly applicable technique for enhancing reinforcement learning optimization.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable embeddings with Distance Explainer</title>
<link>https://arxiv.org/abs/2505.15516</link>
<guid>https://arxiv.org/abs/2505.15516</guid>
<content:encoded><![CDATA[
arXiv:2505.15516v1 Announce Type: cross 
Abstract: While eXplainable AI (XAI) has advanced significantly, few methods address interpretability in embedded vector spaces where dimensions represent complex abstractions. We introduce Distance Explainer, a novel method for generating local, post-hoc explanations of embedded spaces in machine learning models. Our approach adapts saliency-based techniques from RISE to explain the distance between two embedded data points by assigning attribution values through selective masking and distance-ranked mask filtering. We evaluate Distance Explainer on cross-modal embeddings (image-image and image-caption pairs) using established XAI metrics including Faithfulness, Sensitivity/Robustness, and Randomization. Experiments with ImageNet and CLIP models demonstrate that our method effectively identifies features contributing to similarity or dissimilarity between embedded data points while maintaining high robustness and consistency. We also explore how parameter tuning, particularly mask quantity and selection strategy, affects explanation quality. This work addresses a critical gap in XAI research and enhances transparency and trustworthiness in deep learning applications utilizing embedded spaces.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets</title>
<link>https://arxiv.org/abs/2505.15517</link>
<guid>https://arxiv.org/abs/2505.15517</guid>
<content:encoded><![CDATA[
arXiv:2505.15517v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs</title>
<link>https://arxiv.org/abs/2505.15524</link>
<guid>https://arxiv.org/abs/2505.15524</guid>
<content:encoded><![CDATA[
arXiv:2505.15524v1 Announce Type: cross 
Abstract: Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning</title>
<link>https://arxiv.org/abs/2505.15547</link>
<guid>https://arxiv.org/abs/2505.15547</guid>
<content:encoded><![CDATA[
arXiv:2505.15547v1 Announce Type: cross 
Abstract: After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this position paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Bias in Popular Question-Answering Benchmarks</title>
<link>https://arxiv.org/abs/2505.15553</link>
<guid>https://arxiv.org/abs/2505.15553</guid>
<content:encoded><![CDATA[
arXiv:2505.15553v1 Announce Type: cross 
Abstract: Question-answering (QA) and reading comprehension (RC) benchmarks are essential for assessing the capabilities of large language models (LLMs) in retrieving and reproducing knowledge. However, we demonstrate that popular QA and RC benchmarks are biased and do not cover questions about different demographics or regions in a representative way, potentially due to a lack of diversity of those involved in their creation. We perform a qualitative content analysis of 30 benchmark papers and a quantitative analysis of 20 respective benchmark datasets to learn (1) who is involved in the benchmark creation, (2) how social bias is addressed or prevented, and (3) whether the demographics of the creators and annotators correspond to particular biases in the content. Most analyzed benchmark papers provided insufficient information regarding the stakeholders involved in benchmark creation, particularly the annotators. Notably, just one of the benchmark papers explicitly reported measures taken to address social representation issues. Moreover, the data analysis revealed gender, religion, and geographic biases across a wide range of encyclopedic, commonsense, and scholarly benchmarks. More transparent and bias-aware QA and RC benchmark creation practices are needed to facilitate better scrutiny and incentivize the development of fairer LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion</title>
<link>https://arxiv.org/abs/2505.15554</link>
<guid>https://arxiv.org/abs/2505.15554</guid>
<content:encoded><![CDATA[
arXiv:2505.15554v1 Announce Type: cross 
Abstract: Critical questions are essential resources to provoke critical thinking when encountering an argumentative text. We present our system for the Critical Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach leverages large language models (LLMs) with chain-of-thought prompting to generate critical questions guided by Walton's argumentation schemes. For each input intervention, we conversationally prompt LLMs to instantiate the corresponding argument scheme template to first obtain structured arguments, and then generate relevant critical questions. Following this, we rank all the available critical questions by prompting LLMs to select the top 3 most helpful questions based on the original intervention text. This combination of structured argumentation theory and step-by-step reasoning enables the generation of contextually relevant and diverse critical questions. Our pipeline achieves competitive performance in the final test set, showing its potential to foster critical thinking given argumentative text and detect missing or uninformed claims. Code available at \href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo-DM: Data Management For Large Robot Datasets</title>
<link>https://arxiv.org/abs/2505.15558</link>
<guid>https://arxiv.org/abs/2505.15558</guid>
<content:encoded><![CDATA[
arXiv:2505.15558v1 Announce Type: cross 
Abstract: Recent results suggest that very large datasets of teleoperated robot demonstrations can be used to train transformer-based models that have the potential to generalize to new scenes, robots, and tasks. However, curating, distributing, and loading large datasets of robot trajectories, which typically consist of video, textual, and numerical modalities - including streams from multiple cameras - remains challenging. We propose Robo-DM, an efficient open-source cloud-based data management toolkit for collecting, sharing, and learning with robot data. With Robo-DM, robot datasets are stored in a self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can significantly reduce the size of robot trajectory data, transfer costs, and data load time during training. Compared to the RLDS format used in OXE datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x (lossless). Robo-DM also accelerates data retrieval by load-balancing video decoding with memory-mapped decoding caches. Compared to LeRobot, a framework that also uses lossy video compression, Robo-DM is up to 50x faster when decoding sequentially. We physically evaluate a model trained by Robo-DM with lossy compression, a pick-and-place task, and In-Context Robot Transformer. Robo-DM uses 75x compression of the original dataset and does not suffer reduction in downstream task accuracy.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes</title>
<link>https://arxiv.org/abs/2505.15559</link>
<guid>https://arxiv.org/abs/2505.15559</guid>
<content:encoded><![CDATA[
arXiv:2505.15559v1 Announce Type: cross 
Abstract: Moonbeam is a transformer-based foundation model for symbolic music, pretrained on a large and diverse collection of MIDI data totaling 81.6K hours of music and 18 billion tokens. Moonbeam incorporates music-domain inductive biases by capturing both absolute and relative musical attributes through the introduction of a novel domain-knowledge-inspired tokenization method and Multidimensional Relative Attention (MRA), which captures relative music information without additional trainable parameters. Leveraging the pretrained Moonbeam, we propose 2 finetuning architectures with full anticipatory capabilities, targeting 2 categories of downstream tasks: symbolic music understanding and conditional music generation (including music infilling). Our model outperforms other large-scale pretrained music models in most cases in terms of accuracy and F1 score across 3 downstream music classification tasks on 4 datasets. Moreover, our finetuned conditional music generation model outperforms a strong transformer baseline with a REMI-like tokenizer. We open-source the code, pretrained model, and generated samples on Github.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback</title>
<link>https://arxiv.org/abs/2505.15572</link>
<guid>https://arxiv.org/abs/2505.15572</guid>
<content:encoded><![CDATA[
arXiv:2505.15572v1 Announce Type: cross 
Abstract: The data-to-equation (Data2Eqn) task aims to discover interpretable mathematical equations that map observed values to labels, offering physical insights and broad applicability across academic and industrial domains. Genetic programming and traditional deep learning-based approaches suffer from search inefficiency and poor generalization on small task-specific datasets. Foundation models showed promise in this area, but existing approaches suffer from: 1) They are pretrained on general-purpose data distributions, making them less effective for domain-specific tasks; and 2) their training objectives focus on token-level alignment, overlooking mathematical semantics, which can lead to inaccurate equations. To address these issues, we aim to enhance the domain adaptability of foundation models for Data2Eqn tasks. In this work, we propose a reinforcement learning-based finetuning framework that directly optimizes the generation policy of a pretrained model through reward signals derived from downstream numerical fitness. Our method allows the model to adapt to specific and complex data distributions and generate mathematically meaningful equations. Extensive experiments demonstrate that our approach improves both the accuracy and robustness of equation generation under complex distributions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.15581</link>
<guid>https://arxiv.org/abs/2505.15581</guid>
<content:encoded><![CDATA[
arXiv:2505.15581v1 Announce Type: cross 
Abstract: With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at https://github.com/LiamLian0727/UIIS10K.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Models as Reference Trajectories for Rapid Motor Adaptation</title>
<link>https://arxiv.org/abs/2505.15589</link>
<guid>https://arxiv.org/abs/2505.15589</guid>
<content:encoded><![CDATA[
arXiv:2505.15589v1 Announce Type: cross 
Abstract: Deploying learned control policies in real-world environments poses a fundamental challenge. When system dynamics change unexpectedly, performance degrades until models are retrained on new data. We introduce Reflexive World Models (RWM), a dual control framework that uses world model predictions as implicit reference trajectories for rapid adaptation. Our method separates the control problem into long-term reward maximization through reinforcement learning and robust motor execution through rapid latent control. This dual architecture achieves significantly faster adaptation with low online computational cost compared to model-based RL baselines, while maintaining near-optimal performance. The approach combines the benefits of flexible policy learning through reinforcement learning with rapid error correction capabilities, providing a principled approach to maintaining performance in high-dimensional continuous control tasks under varying dynamics.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off</title>
<link>https://arxiv.org/abs/2505.15594</link>
<guid>https://arxiv.org/abs/2505.15594</guid>
<content:encoded><![CDATA[
arXiv:2505.15594v1 Announce Type: cross 
Abstract: While foundation models demonstrate impressive performance across various tasks, they remain vulnerable to adversarial inputs. Current research explores various approaches to enhance model robustness, with Diffusion Denoised Smoothing emerging as a particularly promising technique. This method employs a pretrained diffusion model to preprocess inputs before model inference. Yet, its effectiveness remains largely unexplored beyond classification. We aim to address this gap by analyzing three datasets with four distinct downstream tasks under three different adversarial attack algorithms. Our findings reveal that while foundation models maintain resilience against conventional transformations, applying high-noise diffusion denoising to clean images without any distortions significantly degrades performance by as high as 57%. Low-noise diffusion settings preserve performance but fail to provide adequate protection across all attack types. Moreover, we introduce a novel attack strategy specifically targeting the diffusion process itself, capable of circumventing defenses in the low-noise regime. Our results suggest that the trade-off between adversarial robustness and performance remains a challenge to be addressed.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use</title>
<link>https://arxiv.org/abs/2505.15596</link>
<guid>https://arxiv.org/abs/2505.15596</guid>
<content:encoded><![CDATA[
arXiv:2505.15596v1 Announce Type: cross 
Abstract: This project examines the prospect of using AI-generated feedback as suggestions to expedite and enhance human instructors' feedback provision. In particular, we focus on understanding the teaching assistants' perspectives on the quality of AI-generated feedback and how they may or may not utilize AI feedback in their own workflows. We situate our work in a foundational college Economics class, which has frequent short essay assignments. We developed an LLM-powered feedback engine that generates feedback on students' essays based on grading rubrics used by the teaching assistants (TAs). To ensure that TAs can meaningfully critique and engage with the AI feedback, we had them complete their regular grading jobs. For a randomly selected set of essays that they had graded, we used our feedback engine to generate feedback and displayed the feedback as in-text comments in a Word document. We then performed think-aloud studies with 5 TAs over 20 1-hour sessions to have them evaluate the AI feedback, contrast the AI feedback with their handwritten feedback, and share how they envision using the AI feedback if they were offered as suggestions. The study highlights the importance of providing detailed rubrics for AI to generate high-quality feedback for knowledge-intensive essays. TAs considered that using AI feedback as suggestions during their grading could expedite grading, enhance consistency, and improve overall feedback quality. We discuss the importance of decomposing the feedback generation task into steps and presenting intermediate results, in order for TAs to use the AI feedback.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.15607</link>
<guid>https://arxiv.org/abs/2505.15607</guid>
<content:encoded><![CDATA[
arXiv:2505.15607v1 Announce Type: cross 
Abstract: Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model's instructional planning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Reason Efficiently with Adaptive Length-based Reward Shaping</title>
<link>https://arxiv.org/abs/2505.15612</link>
<guid>https://arxiv.org/abs/2505.15612</guid>
<content:encoded><![CDATA[
arXiv:2505.15612v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant "self-reflections". Resources are at https://github.com/hkust-nlp/Laser.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions</title>
<link>https://arxiv.org/abs/2505.15633</link>
<guid>https://arxiv.org/abs/2505.15633</guid>
<content:encoded><![CDATA[
arXiv:2505.15633v1 Announce Type: cross 
Abstract: Large language models that use retrieval augmented generation have the potential to unlock valuable knowledge for researchers, policymakers, and the public by making long and technical climate-related documents more accessible. While this approach can help alleviate factual hallucinations by relying on retrieved passages as additional context, its effectiveness depends on whether the model's output remains faithful to these passages. To address this, we explore the automatic assessment of faithfulness of different models in this setting. We then focus on ClimateGPT, a large language model specialised in climate science, to examine which factors in its instruction fine-tuning impact the model's faithfulness. By excluding unfaithful subsets of the model's training data, we develop ClimateGPT Faithful+, which achieves an improvement in faithfulness from 30% to 57% in supported atomic claims according to our automatic metric.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models</title>
<link>https://arxiv.org/abs/2505.15644</link>
<guid>https://arxiv.org/abs/2505.15644</guid>
<content:encoded><![CDATA[
arXiv:2505.15644v1 Announce Type: cross 
Abstract: Fine-grained edited image detection of localized edits in images is crucial for assessing content authenticity, especially given that modern diffusion models and image editing methods can produce highly realistic manipulations. However, this domain faces three challenges: (1) Binary classifiers yield only a global real-or-fake label without providing localization; (2) Traditional computer vision methods often rely on costly pixel-level annotations; and (3) No large-scale, high-quality dataset exists for modern image-editing detection techniques. To address these gaps, we develop an automated data-generation pipeline to create FragFake, the first dedicated benchmark dataset for edited image detection, which includes high-quality images from diverse editing models and a wide variety of edited objects. Based on FragFake, we utilize Vision Language Models (VLMs) for the first time in the task of edited image classification and edited region localization. Experimental results show that fine-tuned VLMs achieve higher average Object Precision across all datasets, significantly outperforming pretrained models. We further conduct ablation and transferability analyses to evaluate the detectors across various configurations and editing scenarios. To the best of our knowledge, this work is the first to reformulate localized image edit detection as a vision-language understanding task, establishing a new paradigm for the field. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Second-Order Convergence in Private Stochastic Non-Convex Optimization</title>
<link>https://arxiv.org/abs/2505.15647</link>
<guid>https://arxiv.org/abs/2505.15647</guid>
<content:encoded><![CDATA[
arXiv:2505.15647v1 Announce Type: cross 
Abstract: We investigate the problem of finding second-order stationary points (SOSP) in differentially private (DP) stochastic non-convex optimization. Existing methods suffer from two key limitations: (i) inaccurate convergence error rate due to overlooking gradient variance in the saddle point escape analysis, and (ii) dependence on auxiliary private model selection procedures for identifying DP-SOSP, which can significantly impair utility, particularly in distributed settings. To address these issues, we propose a generic perturbed stochastic gradient descent (PSGD) framework built upon Gaussian noise injection and general gradient oracles. A core innovation of our framework is using model drift distance to determine whether PSGD escapes saddle points, ensuring convergence to approximate local minima without relying on second-order information or additional DP-SOSP identification. By leveraging the adaptive DP-SPIDER estimator as a specific gradient oracle, we develop a new DP algorithm that rectifies the convergence error rates reported in prior work. We further extend this algorithm to distributed learning with arbitrarily heterogeneous data, providing the first formal guarantees for finding DP-SOSP in such settings. Our analysis also highlights the detrimental impacts of private selection procedures in distributed learning under high-dimensional models, underscoring the practical benefits of our design. Numerical experiments on real-world datasets validate the efficacy of our approach.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought</title>
<link>https://arxiv.org/abs/2505.15657</link>
<guid>https://arxiv.org/abs/2505.15657</guid>
<content:encoded><![CDATA[
arXiv:2505.15657v1 Announce Type: cross 
Abstract: Sample-wise learning curves plot performance versus training set size. They are useful for studying scaling laws and speeding up hyperparameter tuning and model selection. Learning curves are often assumed to be well-behaved: monotone (i.e. improving with more data) and convex. By constructing the Learning Curves Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning curves, we show that learning curves are less often well-behaved than previously thought. Using statistically rigorous methods, we observe significant ill-behavior in approximately 14% of the learning curves, almost twice as much as in previous estimates. We also identify which learners are to blame and show that specific learners are more ill-behaved than others. Additionally, we demonstrate that different feature scalings rarely resolve ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such as learning curve fitting and model selection, and find it poses significant challenges, underscoring the relevance and potential of LCDB 1.1 as a challenging benchmark for future research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Quantum Digital Twins for Optimizing Quantum Annealing</title>
<link>https://arxiv.org/abs/2505.15662</link>
<guid>https://arxiv.org/abs/2505.15662</guid>
<content:encoded><![CDATA[
arXiv:2505.15662v1 Announce Type: cross 
Abstract: Quantum annealers have shown potential in addressing certain combinatorial optimization problems, though their performance is often limited by scalability and errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT) framework that reconstructs the energy landscape of quantum many-body systems relevant to quantum annealing. The digital twin models both ground and excited state dynamics, enabling detailed simulation of the adiabatic evolution process. We benchmark NQDT on systems with known analytical solutions and demonstrate that it accurately captures key quantum phenomena, including quantum criticality and phase transitions. Leveraging this framework, one can identify optimal annealing schedules that minimize excitation-related errors. These findings highlight the utility of neural network-based digital twins as a diagnostic and optimization tool for improving the performance of quantum annealers.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2505.15671</link>
<guid>https://arxiv.org/abs/2505.15671</guid>
<content:encoded><![CDATA[
arXiv:2505.15671v1 Announce Type: cross 
Abstract: Knowing the uncertainty associated with the output of a deep neural network is of paramount importance in making trustworthy decisions, particularly in high-stakes fields like medical diagnosis and autonomous systems. Monte Carlo Dropout (MCD) is a widely used method for uncertainty quantification, as it can be easily integrated into various deep architectures. However, conventional MCD often struggles with providing well-calibrated uncertainty estimates. To address this, we introduce innovative frameworks that enhances MCD by integrating different search solutions namely Grey Wolf Optimizer (GWO), Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an uncertainty-aware loss function, thereby improving the reliability of uncertainty quantification. We conduct comprehensive experiments using different backbones, namely DenseNet121, ResNet50, and VGG16, on various datasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic dataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3% on average in terms of both conventional accuracy and uncertainty accuracy while achieving significantly better calibration. These results highlight the potential of our approach to enhance the trustworthiness of deep learning models in safety-critical applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models</title>
<link>https://arxiv.org/abs/2505.15674</link>
<guid>https://arxiv.org/abs/2505.15674</guid>
<content:encoded><![CDATA[
arXiv:2505.15674v1 Announce Type: cross 
Abstract: Large language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability</title>
<link>https://arxiv.org/abs/2505.15683</link>
<guid>https://arxiv.org/abs/2505.15683</guid>
<content:encoded><![CDATA[
arXiv:2505.15683v1 Announce Type: cross 
Abstract: Private data is typically larger and of higher quality than public data, offering great potential to improve LLM. However, its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based split learning model has emerged, offloading most model parameters to the server while retaining only the embedding and output layers on clients to ensure privacy. However, it still faces significant challenges in security, efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks, leading to reverse engineering of private data; 2) the autoregressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a secure, efficient, and adaptive federated split framework based on LLaMA2. First, we place some input and output blocks on the local client and inject Gaussian noise into forward-pass hidden states, enabling secure end-to-end propagation. Second, we employ client-batch and server-hierarchical strategies to achieve parallel training, along with attention-mask compression and KV cache mechanisms to accelerate inference, reducing communication costs effectively. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements and hardware limitations. Experiments on NLU, summarization and conversational QA tasks show that FL-LLaMA maintains performance comparable to centralized LLaMA2, and achieves up to 2x train speedups and 8x inference speedups. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FL-LLaMA in security and adaptability.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning</title>
<link>https://arxiv.org/abs/2505.15687</link>
<guid>https://arxiv.org/abs/2505.15687</guid>
<content:encoded><![CDATA[
arXiv:2505.15687v1 Announce Type: cross 
Abstract: Multimodal pathological image understanding has garnered widespread interest due to its potential to improve diagnostic accuracy and enable personalized treatment through integrated visual and textual data. However, existing methods exhibit limited reasoning capabilities, which hamper their ability to handle complex diagnostic scenarios. Additionally, the enormous size of pathological images leads to severe computational burdens, further restricting their practical deployment. To address these limitations, we introduce a novel bilateral reinforcement learning framework comprising two synergistic branches. One reinforcement branch enhances the reasoning capability by enabling the model to learn task-specific decision processes, i.e., pathology rationales, directly from labels without explicit reasoning supervision. While the other branch dynamically allocates a tailored number of tokens to different images based on both their visual content and task context, thereby optimizing computational efficiency. We apply our method to various pathological tasks such as visual question answering, cancer subtyping, and lesion detection. Extensive experiments show an average +41.7 absolute performance improvement with 70.3% lower inference costs over the base models, achieving both reasoning accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO</title>
<link>https://arxiv.org/abs/2505.15694</link>
<guid>https://arxiv.org/abs/2505.15694</guid>
<content:encoded><![CDATA[
arXiv:2505.15694v1 Announce Type: cross 
Abstract: In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning</title>
<link>https://arxiv.org/abs/2505.15703</link>
<guid>https://arxiv.org/abs/2505.15703</guid>
<content:encoded><![CDATA[
arXiv:2505.15703v1 Announce Type: cross 
Abstract: Motion forecasting represents a critical challenge in autonomous driving systems, requiring accurate prediction of surrounding agents' future trajectories. While existing approaches predict future motion states with the extracted scene context feature from historical agent trajectories and road layouts, they suffer from the information degradation during the scene feature encoding. To address the limitation, we propose HAMF, a novel motion forecasting framework that learns future motion representations with the scene context encoding jointly, to coherently combine the scene understanding and future motion state prediction. We first embed the observed agent states and map information into 1D token sequences, together with the target multi-modal future motion features as a set of learnable tokens. Then we design a unified Attention-based encoder, which synergistically combines self-attention and cross-attention mechanisms to model the scene context information and aggregate future motion features jointly. Complementing the encoder, we implement the Mamba module in the decoding stage to further preserve the consistency and correlations among the learned future motion representations, to generate the accurate and diverse final trajectories. Extensive experiments on Argoverse 2 benchmark demonstrate that our hybrid Attention-Mamba model achieves state-of-the-art motion forecasting performance with the simple and lightweight architecture.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities</title>
<link>https://arxiv.org/abs/2505.15722</link>
<guid>https://arxiv.org/abs/2505.15722</guid>
<content:encoded><![CDATA[
arXiv:2505.15722v1 Announce Type: cross 
Abstract: We present the first comprehensive study of Memorization in Multilingual Large Language Models (MLLMs), analyzing 95 languages using models across diverse model scales, architectures, and memorization definitions. As MLLMs are increasingly deployed, understanding their memorization behavior has become critical. Yet prior work has focused primarily on monolingual models, leaving multilingual memorization underexplored, despite the inherently long-tailed nature of training corpora. We find that the prevailing assumption, that memorization is highly correlated with training data availability, fails to fully explain memorization patterns in MLLMs. We hypothesize that treating languages in isolation - ignoring their similarities - obscures the true patterns of memorization. To address this, we propose a novel graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization. Our analysis reveals that among similar languages, those with fewer training tokens tend to exhibit higher memorization, a trend that only emerges when cross-lingual relationships are explicitly modeled. These findings underscore the importance of a language-aware perspective in evaluating and mitigating memorization vulnerabilities in MLLMs. This also constitutes empirical evidence that language similarity both explains Memorization in MLLMs and underpins Cross-lingual Transferability, with broad implications for multilingual NLP.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.15734</link>
<guid>https://arxiv.org/abs/2505.15734</guid>
<content:encoded><![CDATA[
arXiv:2505.15734v1 Announce Type: cross 
Abstract: Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on five reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses</title>
<link>https://arxiv.org/abs/2505.15738</link>
<guid>https://arxiv.org/abs/2505.15738</guid>
<content:encoded><![CDATA[
arXiv:2505.15738v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even against Greedy Coordinate Gradient (GCG), a white-box attack that generates adversarial suffixes to induce attacker-desired outputs. However, this search space over discrete tokens is extremely large, making the task of finding successful attacks difficult. GCG has, for instance, been shown to converge to local minima, making it sensitive to initialization choices. In this paper, we assess the future-proof robustness of these defenses using a more informed threat model: attackers who have access to some information about the alignment process. Specifically, we propose an informed white-box attack leveraging the intermediate model checkpoints to initialize GCG, with each checkpoint acting as a stepping stone for the next one. We show this approach to be highly effective across state-of-the-art (SOTA) defenses and models. We further show our informed initialization to outperform other initialization methods and show a gradient-informed checkpoint selection strategy to greatly improve attack performance and efficiency. Importantly, we also show our method to successfully find universal adversarial suffixes -- single suffixes effective across diverse inputs. Our results show that, contrary to previous beliefs, effective adversarial suffixes do exist against SOTA alignment-based defenses, that these can be found by existing attack methods when adversaries exploit alignment knowledge, and that even universal suffixes exist. Taken together, our results highlight the brittleness of current alignment-based methods and the need to consider stronger threat models when testing the safety of LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement</title>
<link>https://arxiv.org/abs/2505.15740</link>
<guid>https://arxiv.org/abs/2505.15740</guid>
<content:encoded><![CDATA[
arXiv:2505.15740v1 Announce Type: cross 
Abstract: Formal methods is pivotal for verifying the reliability of critical systems through rigorous mathematical proofs. However, its adoption is hindered by labor-intensive manual proofs and the expertise required to use theorem provers. Recent advancements in large language models (LLMs) offer new opportunities for automated theorem proving. Two promising approaches are generating tactics step by step and generating a whole proof directly with an LLM. However, existing work makes no attempt to combine the two approaches. In this work, we introduce HybridProver, a dual-model proof synthesis framework that combines tactic-based generation and whole-proof synthesis to harness the benefits of both approaches. HybridProver generates whole proof candidates for evaluation directly, then extracts proof sketches from those candidates. It then uses a tactic-based generation model that integrates automated tools to complete the sketches via stepwise refinement. We implement HybridProver for the Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle datasets. Evaluation on the miniF2F dataset illustrates HybridProver's effectiveness. We achieve a 59.4% success rate on miniF2F, where the previous SOTA is 56.1%. Our ablation studies show that this SOTA result is attributable to combining whole-proof and tactic-based generation. Additionally, we show how the dataset quality, training parameters, and sampling diversity affect the final result during automated theorem proving with LLMs. All of our code, datasets, and LLMs are open source.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order Structure Boosts Link Prediction on Temporal Graphs</title>
<link>https://arxiv.org/abs/2505.15746</link>
<guid>https://arxiv.org/abs/2505.15746</guid>
<content:encoded><![CDATA[
arXiv:2505.15746v1 Announce Type: cross 
Abstract: Temporal Graph Neural Networks (TGNNs) have gained growing attention for modeling and predicting structures in temporal graphs. However, existing TGNNs primarily focus on pairwise interactions while overlooking higher-order structures that are integral to link formation and evolution in real-world temporal graphs. Meanwhile, these models often suffer from efficiency bottlenecks, further limiting their expressive power. To tackle these challenges, we propose a Higher-order structure Temporal Graph Neural Network, which incorporates hypergraph representations into temporal graph learning. In particular, we develop an algorithm to identify the underlying higher-order structures, enhancing the model's ability to capture the group interactions. Furthermore, by aggregating multiple edge features into hyperedge representations, HTGN effectively reduces memory cost during training. We theoretically demonstrate the enhanced expressiveness of our approach and validate its effectiveness and efficiency through extensive experiments on various real-world temporal graphs. Experimental results show that HTGN achieves superior performance on dynamic link prediction while reducing memory costs by up to 50\% compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.15747</link>
<guid>https://arxiv.org/abs/2505.15747</guid>
<content:encoded><![CDATA[
arXiv:2505.15747v1 Announce Type: cross 
Abstract: We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval</title>
<link>https://arxiv.org/abs/2505.15753</link>
<guid>https://arxiv.org/abs/2505.15753</guid>
<content:encoded><![CDATA[
arXiv:2505.15753v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving planning and MBRL with temporally-extended actions</title>
<link>https://arxiv.org/abs/2505.15754</link>
<guid>https://arxiv.org/abs/2505.15754</guid>
<content:encoded><![CDATA[
arXiv:2505.15754v1 Announce Type: cross 
Abstract: Continuous time systems are often modeled using discrete time dynamics but this requires a small simulation step to maintain accuracy. In turn, this requires a large planning horizon which leads to computationally demanding planning problems and reduced performance. Previous work in model free reinforcement learning has partially addressed this issue using action repeats where a policy is learned to determine a discrete action duration. Instead we propose to control the continuous decision timescale directly by using temporally-extended actions and letting the planner treat the duration of the action as an additional optimization variable along with the standard action variables. This additional structure has multiple advantages. It speeds up simulation time of trajectories and, importantly, it allows for deep horizon search in terms of primitive actions while using a shallow search depth in the planner. In addition, in the model based reinforcement learning (MBRL) setting, it reduces compounding errors from model learning and improves training time for models. We show that this idea is effective and that the range for action durations can be automatically selected using a multi-armed bandit formulation and integrated into the MBRL framework. An extensive experimental evaluation both in planning and in MBRL, shows that our approach yields faster planning, better solutions, and that it enables solutions to problems that are not solved in the standard formulation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing a 3D Town from a Single Image</title>
<link>https://arxiv.org/abs/2505.15765</link>
<guid>https://arxiv.org/abs/2505.15765</guid>
<content:encoded><![CDATA[
arXiv:2505.15765v1 Announce Type: cross 
Abstract: Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, a training-free framework designed to synthesize realistic and coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from a single image using a principled, training-free approach.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space</title>
<link>https://arxiv.org/abs/2505.15778</link>
<guid>https://arxiv.org/abs/2505.15778</guid>
<content:encoded><![CDATA[
arXiv:2505.15778v1 Announce Type: cross 
Abstract: Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like "soft" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IA-T2I: Internet-Augmented Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.15779</link>
<guid>https://arxiv.org/abs/2505.15779</guid>
<content:encoded><![CDATA[
arXiv:2505.15779v1 Announce Type: cross 
Abstract: Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Computable Approximations to Solomonoff Induction</title>
<link>https://arxiv.org/abs/2505.15784</link>
<guid>https://arxiv.org/abs/2505.15784</guid>
<content:encoded><![CDATA[
arXiv:2505.15784v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) calls for a rigorous theoretical framework to explain their empirical success. While significant progress has been made in understanding LLM behaviors, existing theoretical frameworks remain fragmented in explaining emergent phenomena through a unified mathematical lens. We establish the first formal connection between LLM architectures and Algorithmic Information Theory (AIT) by proving two fundamental results: (1) the training process computationally approximates Solomonoff prior through loss minimization interpreted as program length optimization, and (2) next-token prediction implements approximate Solomonoff induction. We leverage AIT to provide a unified theoretical explanation for in-context learning, few-shot learning, and scaling laws. Furthermore, our theoretical insights lead to a principled method for few-shot example selection that prioritizes samples where models exhibit lower predictive confidence. We demonstrate through experiments on diverse text classification benchmarks that this strategy yields significant performance improvements, particularly for smaller model architectures, when compared to selecting high-confidence examples. Our framework bridges the gap between theoretical foundations and practical LLM behaviors, providing both explanatory power and actionable insights for future model development.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Innovation Opportunities for Pre-trained Models</title>
<link>https://arxiv.org/abs/2505.15790</link>
<guid>https://arxiv.org/abs/2505.15790</guid>
<content:encoded><![CDATA[
arXiv:2505.15790v1 Announce Type: cross 
Abstract: Innovators transform the world by understanding where services are successfully meeting customers' needs and then using this knowledge to identify failsafe opportunities for innovation. Pre-trained models have changed the AI innovation landscape, making it faster and easier to create new AI products and services. Understanding where pre-trained models are successful is critical for supporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained models makes it hard to know where AI can really be successful. To address this, we investigated pre-trained model applications developed by HCI researchers as a proxy for commercially successful applications. The research applications demonstrate technical capabilities, address real user needs, and avoid ethical challenges. Using an artifact analysis approach, we categorized capabilities, opportunity domains, data types, and emerging interaction design patterns, uncovering some of the opportunity space for innovation with pre-trained models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Form Information Alignment Evaluation Beyond Atomic Facts</title>
<link>https://arxiv.org/abs/2505.15792</link>
<guid>https://arxiv.org/abs/2505.15792</guid>
<content:encoded><![CDATA[
arXiv:2505.15792v1 Announce Type: cross 
Abstract: Information alignment evaluators are vital for various NLG evaluation tasks and trustworthy LLM deployment, reducing hallucinations and enhancing user trust. Current fine-grained methods, like FactScore, verify facts individually but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this work, we introduce MontageLie, a challenging benchmark that constructs deceptive narratives by "montaging" truthful statements without introducing explicit hallucinations. We demonstrate that both coarse-grained LLM-based evaluators and current fine-grained frameworks are susceptible to this attack, with AUC-ROC scores falling below 65%. To enable more robust fine-grained evaluation, we propose DoveScore, a novel framework that jointly verifies factual accuracy and event-order consistency. By modeling inter-fact relationships, DoveScore outperforms existing fine-grained methods by over 8%, providing a more robust solution for long-form text alignment evaluation. Our code and datasets are available at https://github.com/dannalily/DoveScore.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models</title>
<link>https://arxiv.org/abs/2505.15801</link>
<guid>https://arxiv.org/abs/2505.15801</guid>
<content:encoded><![CDATA[
arXiv:2505.15801v1 Announce Type: cross 
Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Conditional Transport Maps</title>
<link>https://arxiv.org/abs/2505.15808</link>
<guid>https://arxiv.org/abs/2505.15808</guid>
<content:encoded><![CDATA[
arXiv:2505.15808v1 Announce Type: cross 
Abstract: We present a neural framework for learning conditional optimal transport (OT) maps between probability distributions. Our approach introduces a conditioning mechanism capable of processing both categorical and continuous conditioning variables simultaneously. At the core of our method lies a hypernetwork that generates transport layer parameters based on these inputs, creating adaptive mappings that outperform simpler conditioning methods. Comprehensive ablation studies demonstrate the superior performance of our method over baseline configurations. Furthermore, we showcase an application to global sensitivity analysis, offering high performance in computing OT-based sensitivity indices. This work advances the state-of-the-art in conditional optimal transport, enabling broader application of optimal transport principles to complex, high-dimensional domains such as generative modeling and black-box model explainability.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents</title>
<link>https://arxiv.org/abs/2505.15810</link>
<guid>https://arxiv.org/abs/2505.15810</guid>
<content:encoded><![CDATA[
arXiv:2505.15810v1 Announce Type: cross 
Abstract: Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at https://github.com/Yuqi-Zhou/GUI-G1.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Evolution of Knowledge Graphs: A Survey and Perspective</title>
<link>https://arxiv.org/abs/2310.04835</link>
<guid>https://arxiv.org/abs/2310.04835</guid>
<content:encoded><![CDATA[
arXiv:2310.04835v3 Announce Type: replace 
Abstract: Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls</title>
<link>https://arxiv.org/abs/2409.03797</link>
<guid>https://arxiv.org/abs/2409.03797</guid>
<content:encoded><![CDATA[
arXiv:2409.03797v3 Announce Type: replace 
Abstract: The resurgence of autonomous agents built using large language models (LLMs) to solve complex real-world tasks has brought increased focus on LLMs' fundamental ability of tool or function calling. At the core of these agents, an LLM must plan, execute, and respond using external tools, APIs, and custom functions. Research on tool calling has gathered momentum, but evaluation benchmarks and datasets representing the complexity of the tasks have lagged behind. In this work, we focus on one such complexity, nested sequencing, with the goal of extending existing benchmarks and evaluation. Specifically, we present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to a subsequent call. NESTFUL contains 1800+ nested sequences where all the function calls are executable. Experimental results on a variety of models show that the best-performing model (GPT-4o) achieves a full sequence match accuracy of 28% and a win-rate of 60%, necessitating a large scope for improvement in the nested sequencing aspect of function calling. Our analysis of these results provides possible future research directions for the community, in addition to a benchmark to track progress. We have released the NESTFUL dataset under the Apache 2.0 license at https://github.com/IBM/NESTFUL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits</title>
<link>https://arxiv.org/abs/2410.07302</link>
<guid>https://arxiv.org/abs/2410.07302</guid>
<content:encoded><![CDATA[
arXiv:2410.07302v2 Announce Type: replace 
Abstract: Broadly accessible generative AI models like Dall-E have made it possible for anyone to create compelling visual art. In online communities, the introduction of AI-generated content (AIGC) may impact social dynamics, for example causing changes in who is posting content, or shifting the norms or the discussions around the posted content if posts are suspected of being generated by AI. We take steps towards examining the potential impact of AIGC on art-related communities on Reddit. We distinguish between communities that disallow AI content and those without such a direct policy. We look at image-based posts in these communities where the author transparently shares that the image was created by AI, and at comments in these communities that suspect or accuse authors of using generative AI. We find that AI posts (and accusations) have played a surprisingly small part in these communities through the end of 2023, accounting for fewer than 0.5% of the image-based posts. However, even as the absolute number of author-labeled AI posts dwindles over time, accusations of AI use remain more persistent. We show that AI content is more readily used by newcomers and may help increase participation if it aligns with community rules. However, the tone of comments suspecting AI use by others has become more negative over time, especially in communities that do not have explicit rules about AI. Overall, the results show the changing norms and interactions around AIGC in online communities designated for creativity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond The Rainbow: High Performance Deep Reinforcement Learning on a Desktop PC</title>
<link>https://arxiv.org/abs/2411.03820</link>
<guid>https://arxiv.org/abs/2411.03820</guid>
<content:encoded><![CDATA[
arXiv:2411.03820v2 Announce Type: replace 
Abstract: Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent enhancements could significantly boost a reinforcement learning (RL) agent's performance. In this paper, we present "Beyond The Rainbow" (BTR), a novel algorithm that integrates six improvements from across the RL literature to Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC, with a human-normalized interquartile mean (IQM) of 7.4 on Atari-60. Beyond Atari, we demonstrate BTR's capability to handle complex 3D games, successfully training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with minimal algorithmic changes. Designing BTR with computational efficiency in mind, agents can be trained using a high-end desktop PC on 200 million Atari frames within 12 hours. Additionally, we conduct detailed ablation studies of each component, analyzing the performance and impact using numerous measures. Code is available at https://github.com/VIPTankz/BTR.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions Using Large Language Models</title>
<link>https://arxiv.org/abs/2412.00342</link>
<guid>https://arxiv.org/abs/2412.00342</guid>
<content:encoded><![CDATA[
arXiv:2412.00342v2 Announce Type: replace 
Abstract: In today's digital age, video content is prevalent, serving as a primary source of information, education, and entertainment. However, the Deaf and Hard of Hearing (DHH) community often faces significant challenges in accessing video content due to the inadequacy of automatic speech recognition (ASR) systems in providing accurate and reliable captions. This paper addresses the urgent need to improve video caption quality by leveraging Large Language Models (LLMs). We present a comprehensive study that explores the integration of LLMs to enhance the accuracy and context-awareness of captions generated by ASR systems. Our methodology involves a novel pipeline that corrects ASR-generated captions using advanced LLMs. It explicitly focuses on models like GPT-3.5 and Llama2-13B due to their robust performance in language comprehension and generation tasks. We introduce a dataset representative of real-world challenges the DHH community faces to evaluate our proposed pipeline. Our results indicate that LLM-enhanced captions significantly improve accuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by ChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%), ChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the original ASR captions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeSyA: Neurosymbolic Automata</title>
<link>https://arxiv.org/abs/2412.07331</link>
<guid>https://arxiv.org/abs/2412.07331</guid>
<content:encoded><![CDATA[
arXiv:2412.07331v2 Announce Type: replace 
Abstract: Neurosymbolic (NeSy) AI has emerged as a promising direction to integrate neural and symbolic reasoning. Unfortunately, little effort has been given to developing NeSy systems tailored to sequential/temporal problems. We identify symbolic automata (which combine the power of automata for temporal reasoning with that of propositional logic for static reasoning) as a suitable formalism for expressing knowledge in temporal domains. Focusing on the task of sequence classification and tagging we show that symbolic automata can be integrated with neural-based perception, under probabilistic semantics towards an end-to-end differentiable model. Our proposed hybrid model, termed NeSyA (Neuro Symbolic Automata) is shown to either scale or perform more accurately than previous NeSy systems in a synthetic benchmark and to provide benefits in terms of generalization compared to purely neural systems in a real-world event recognition task.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2412.18084</link>
<guid>https://arxiv.org/abs/2412.18084</guid>
<content:encoded><![CDATA[
arXiv:2412.18084v4 Announce Type: replace 
Abstract: Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in https://github.com/chenlong164/PEIT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Did my figure do justice to the answer?" : Towards Multimodal Short Answer Grading with Feedback (MMSAF)</title>
<link>https://arxiv.org/abs/2412.19755</link>
<guid>https://arxiv.org/abs/2412.19755</guid>
<content:encoded><![CDATA[
arXiv:2412.19755v3 Announce Type: replace 
Abstract: Assessments play a vital role in a student's learning process. This is because they provide valuable feedback crucial to a student's growth. Such assessments contain questions with open-ended responses, which are difficult to grade at scale. These responses often require students to express their understanding through textual and visual elements together as a unit. In order to develop scalable assessment tools for such questions, one needs multimodal LLMs having strong comparative reasoning capabilities across multiple modalities. Thus, to facilitate research in this area, we propose the Multimodal Short Answer grading with Feedback (MMSAF) problem along with a dataset of 2,197 data points. Additionally, we provide an automated framework for generating such datasets. As per our evaluations, existing Multimodal Large Language Models (MLLMs) could predict whether an answer is correct, incorrect or partially correct with an accuracy of 55%. Similarly, they could predict whether the image provided in the student's answer is relevant or not with an accuracy of 75%. As per human experts, Pixtral was more aligned towards human judgement and values for biology and ChatGPT for physics and chemistry and achieved a score of 4 or more out of 5 in most parameters.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRoPE: Rotary Position Embedding for Video Large Language Models</title>
<link>https://arxiv.org/abs/2502.11664</link>
<guid>https://arxiv.org/abs/2502.11664</guid>
<content:encoded><![CDATA[
arXiv:2502.11664v2 Announce Type: replace 
Abstract: Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Specifically, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Additionally, our approach restructures positional indices to ensure a smooth transition between video and text tokens. Extensive experiments on different models demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code will be available at https://github.com/johncaged/VRoPE.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM Reasoning</title>
<link>https://arxiv.org/abs/2502.17216</link>
<guid>https://arxiv.org/abs/2502.17216</guid>
<content:encoded><![CDATA[
arXiv:2502.17216v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, it remains unclear what the contributing factors to the success of Neurosymbolic LLM reasoning are. This paper shows that one important factor is the choice of the formal language. By comparing 4 formal languages on 3 datasets over 6 LLMs, we show that the choice of formal language affects both the syntactic and the semantic reasoning capability. Thereby, we introduce the intermediate language challenge, which is the challenge of picking a suitable formal language for neurosymbolic reasoning. Further, we compare the effects of using different in-context-learning examples in an ablation study. We conclude that on average, context-aware encodings help LLMs to reason, while there is no apparent effect of using comments or markdown syntax.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction</title>
<link>https://arxiv.org/abs/2502.18744</link>
<guid>https://arxiv.org/abs/2502.18744</guid>
<content:encoded><![CDATA[
arXiv:2502.18744v2 Announce Type: replace 
Abstract: Recent efforts in LLM alignment have focused on constructing large-scale preference datasets via human or Artificial Intelligence (AI) annotators. However, such approaches rely on instance-wise supervision, incurring substantial annotation cost and limited interpretability. In this paper, we propose ZEBRA - a model behavior-wise zero-annotation framework that constructs preference data by leveraging model behavior knowledge derived from benchmark performances. ZEBRA binarizes response pairs by evaluating the quality and similarity of their origin models, entirely bypassing instance-level annotation. This allows scalable, controllable, and cost-effective alignment data generation. Empirical results show that ZEBRA achieves alignment performance comparable to instance-supervised methods, despite requiring no manual or model-based labeling.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts</title>
<link>https://arxiv.org/abs/2502.20808</link>
<guid>https://arxiv.org/abs/2502.20808</guid>
<content:encoded><![CDATA[
arXiv:2502.20808v5 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have shown promising capabilities in mathematical reasoning within visual contexts across various datasets. However, most existing multimodal math benchmarks are limited to single-visual contexts, which diverges from the multi-visual scenarios commonly encountered in real-world mathematical applications. To address this gap, we introduce MV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical problems. Each problem integrates multiple images interleaved with text, derived from authentic K-12 scenarios, and enriched with detailed annotations. MV-MATH includes multiple-choice, free-form, and multi-step questions, covering 11 subject areas across 3 difficulty levels, and serves as a comprehensive and rigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual contexts. Through extensive experimentation, we observe that MLLMs encounter substantial challenges in multi-visual math tasks, with a considerable performance gap relative to human capabilities on MV-MATH. Furthermore, we analyze the performance and error patterns of various models, providing insights into MLLMs' mathematical reasoning capabilities within multi-visual settings.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic</title>
<link>https://arxiv.org/abs/2503.07996</link>
<guid>https://arxiv.org/abs/2503.07996</guid>
<content:encoded><![CDATA[
arXiv:2503.07996v4 Announce Type: replace 
Abstract: Existing refinement methods in LLM-based Text-to-SQL systems exhibit limited effectiveness. They often introduce new errors during the self-correction process and fail to detect and correct semantic inaccuracies. To address these gaps, we first introduce a clause-wise critique generation task along with a benchmark, SQLCriticBench, which performs fine-grained error localization including both syntax and semantic errors at the clause level. Furthermore, we introduce a variant of DPO for training our SQLCritic model, where the $\beta$ coefficient is adaptively changed according to the clause-level inconsistencies between the preferred and dispreferred critiques. We also propose an automatically training dataset curation pipeline which annotate clause-wise critique at scale in a cost-effective way. Experiments demonstrate that the SQLCritic model significantly improves SQL accuracy on the BIRD and Spider datasets, and the results on SQLCriticBench further reveals its superior critique capabilities compared to existing models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</title>
<link>https://arxiv.org/abs/2503.10619</link>
<guid>https://arxiv.org/abs/2503.10619</guid>
<content:encoded><![CDATA[
arXiv:2503.10619v4 Announce Type: replace 
Abstract: We introduce Tempest, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Tempest expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Tempest reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Tempest achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Parameter Decision in Approximate Model Counting</title>
<link>https://arxiv.org/abs/2504.05874</link>
<guid>https://arxiv.org/abs/2504.05874</guid>
<content:encoded><![CDATA[
arXiv:2504.05874v2 Announce Type: replace 
Abstract: This paper proposes a novel approach to determining the internal parameters of the hashing-based approximate model counting algorithm $\mathsf{ApproxMC}$. In this problem, the chosen parameter values must ensure that $\mathsf{ApproxMC}$ is Probably Approximately Correct (PAC), while also making it as efficient as possible. The existing approach to this problem relies on heuristics; in this paper, we solve this problem by formulating it as an optimization problem that arises from generalizing $\mathsf{ApproxMC}$'s correctness proof to arbitrary parameter values.
  Our approach separates the concerns of algorithm soundness and optimality, allowing us to address the former without the need for repetitive case-by-case argumentation, while establishing a clear framework for the latter. Furthermore, after reduction, the resulting optimization problem takes on an exceptionally simple form, enabling the use of a basic search algorithm and providing insight into how parameter values affect algorithm performance. Experimental results demonstrate that our optimized parameters improve the runtime performance of the latest $\mathsf{ApproxMC}$ by a factor of 1.6 to 2.4, depending on the error tolerance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.09440</link>
<guid>https://arxiv.org/abs/2504.09440</guid>
<content:encoded><![CDATA[
arXiv:2504.09440v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction</title>
<link>https://arxiv.org/abs/2504.14298</link>
<guid>https://arxiv.org/abs/2504.14298</guid>
<content:encoded><![CDATA[
arXiv:2504.14298v3 Announce Type: replace 
Abstract: Radio maps (RMs) are essential for environment-aware communication and sensing, providing location-specific wireless channel information. Existing RM construction methods often rely on precise environmental data and base station (BS) locations, which are not always available in dynamic or privacy-sensitive environments. While sparse measurement techniques reduce data collection, the impact of noise in sparse data on RM accuracy is not well understood. This paper addresses these challenges by formulating RM construction as a Bayesian inverse problem under coarse environmental knowledge and noisy sparse measurements. Although maximum a posteriori (MAP) filtering offers an optimal solution, it requires a precise prior distribution of the RM, which is typically unavailable. To solve this, we propose RadioDiff-Inverse, a diffusion-enhanced Bayesian inverse estimation framework that uses an unconditional generative diffusion model to learn the RM prior. This approach not only reconstructs the spatial distribution of wireless channel features but also enables environmental structure perception, such as building outlines, and location of BS just relay on pathloss, through integrated sensing and communication (ISAC). Remarkably, RadioDiff-Inverse is training-free, leveraging a pre-trained model from Imagenet without task-specific fine-tuning, which significantly reduces the training cost of using generative large model in wireless networks. Experimental results demonstrate that RadioDiff-Inverse achieves state-of-the-art performance in accuracy of RM construction and environmental reconstruction, and robustness against noisy sparse sampling.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of LLM Reasoning Ability Under Strict Output Length Constraint</title>
<link>https://arxiv.org/abs/2504.14350</link>
<guid>https://arxiv.org/abs/2504.14350</guid>
<content:encoded><![CDATA[
arXiv:2504.14350v3 Announce Type: replace 
Abstract: Recent work has demonstrated the remarkable potential of Large Language Models (LLMs) in test-time scaling. By making models think before answering, they are able to achieve much higher accuracy with extra inference computation. However, in many real-world scenarios, models are used under time constraints, where an answer should be given within a certain output length. It is unclear whether and how the reasoning ability of different LLMs remain effective under strict constraints. We take a first look at this problem by conducting an in-depth empirical study. Specifically, we test 30 LLMs on common reasoning datasets under a wide range of output length budgets, and we analyze the correlation between the inference accuracy and various properties including model type, model size, prompt style, etc. We also consider the mappings between token budgets and actual on-device latency budgets. The results have demonstrated several interesting findings regarding the budget-aware LLM reasoning ability that differ from the unconstrained situation, e.g. the optimal choices of either model size or prompt style change under different budgets. These findings offer timely evaluation to this area and practical guidance for users to deploy LLMs under real-world latency constraints.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Machine-Generated Code for the Resolution of User Intentions</title>
<link>https://arxiv.org/abs/2504.17531</link>
<guid>https://arxiv.org/abs/2504.17531</guid>
<content:encoded><![CDATA[
arXiv:2504.17531v2 Announce Type: replace 
Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large Language Models (LLMs), prompt a reassessment of the interaction mechanisms between users and their devices. Currently, users are required to use a set of high-level applications to achieve their desired results. However, the advent of AI may signal a shift in this regard, as its capabilities have generated novel prospects for user-provided intent resolution through the deployment of model-generated code. This development represents a significant progression in the realm of hybrid workflows, where human and artificial intelligence collaborate to address user intentions, with the former responsible for defining these intentions and the latter for implementing the solutions to address them. In this paper, we investigate the feasibility of generating and executing workflows through code generation that results from prompting an LLM with a concrete user intention, and a simplified application programming interface for a GUI-less operating system. We provide an in-depth analysis and comparison of various user intentions, the resulting code, and its execution. The findings demonstrate the general feasibility of our approach and that the employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of code-oriented workflows in accordance with provided user intentions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformational Creativity in Science: A Graphical Theory</title>
<link>https://arxiv.org/abs/2504.18687</link>
<guid>https://arxiv.org/abs/2504.18687</guid>
<content:encoded><![CDATA[
arXiv:2504.18687v2 Announce Type: replace 
Abstract: Creative processes are typically divided into three types: combinatorial, exploratory, and transformational. Here, we provide a graphical theory of transformational scientific creativity, synthesizing Boden's insight that transformational creativity arises from changes in the "enabling constraints" of a conceptual space and Kuhn's structure of scientific revolutions as resulting from paradigm shifts. We prove that modifications made to axioms of our graphical model have the most transformative potential and then illustrate how several historical instances of transformational creativity can be captured by our framework.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification</title>
<link>https://arxiv.org/abs/2504.20930</link>
<guid>https://arxiv.org/abs/2504.20930</guid>
<content:encoded><![CDATA[
arXiv:2504.20930v2 Announce Type: replace 
Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.21277</link>
<guid>https://arxiv.org/abs/2504.21277</guid>
<content:encoded><![CDATA[
arXiv:2504.21277v2 Announce Type: replace 
Abstract: The application of reinforcement learning (RL) to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) constitutes a rapidly advancing research area. While MLLMs extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains challenging. This paper provides a systematic review of recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms, value-model-free and value-model-based methods, and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Additionally, we provide an extensive overview of benchmark datasets, evaluation protocols, and current limitations, and propose future research directions to address challenges such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to provide a comprehensive and structured guide to RL-based multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
arXiv:2504.21659v2 Announce Type: replace 
Abstract: Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method (Ada-R1) significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-fid: Quantum Circuit Fidelity Improvement with LSTM Networks</title>
<link>https://arxiv.org/abs/2303.17523</link>
<guid>https://arxiv.org/abs/2303.17523</guid>
<content:encoded><![CDATA[
arXiv:2303.17523v4 Announce Type: replace-cross 
Abstract: The fidelity of quantum circuits (QC) is influenced by several factors, including hardware characteristics, calibration status, and the transpilation process, all of which impact their susceptibility to noise. However, existing methods struggle to estimate and compare the noise performance of different circuit layouts due to fluctuating error rates and the absence of a standardized fidelity metric. In this work, Q-fid is introduced, a Long Short-Term Memory (LSTM) based fidelity prediction system accompanied by a novel metric designed to quantify the fidelity of quantum circuits. Q-fid provides an intuitive way to predict the noise performance of Noisy Intermediate-Scale Quantum (NISQ) circuits. This approach frames fidelity prediction as a Time Series Forecasting problem to analyze the tokenized circuits, capturing the causal dependence of the gate sequences and their impact on overall fidelity. Additionally, the model is capable of dynamically adapting to changes in hardware characteristics, ensuring accurate fidelity predictions under varying conditions. Q-fid achieves a high prediction accuracy with an average RMSE of 0.0515, up to 24.7x more accurate than the Qiskit transpile tool mapomatic. By offering a reliable method for fidelity prediction, Q-fid empowers developers to optimize transpilation strategies, leading to more efficient and noise-resilient quantum circuit implementations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HurriCast: Synthetic Tropical Cyclone Track Generation for Hurricane Forecasting</title>
<link>https://arxiv.org/abs/2309.07174</link>
<guid>https://arxiv.org/abs/2309.07174</guid>
<content:encoded><![CDATA[
arXiv:2309.07174v2 Announce Type: replace-cross 
Abstract: The generation of synthetic tropical cyclone(TC) tracks for risk assessment is a critical application of preparedness for the impacts of climate change and disaster relief, particularly in North America. Insurance companies use these synthetic tracks to estimate the potential risks and financial impacts of future TCs. For governments and policymakers, understanding the potential impacts of TCs helps in developing effective emergency response strategies, updating building codes, and prioritizing investments in resilience and mitigation projects. In this study, many hypothetical but plausible TC scenarios are created based on historical TC data HURDAT2 (HURricane DATA 2nd generation). A hybrid methodology, combining the ARIMA and K-MEANS methods with Autoencoder, is employed to capture better historical TC behaviors and project future trajectories and intensities. It demonstrates an efficient and reliable in the field of climate modeling and risk assessment. By effectively capturing past hurricane patterns and providing detailed future projections, this approach not only validates the reliability of this method but also offers crucial insights for a range of applications, from disaster preparedness and emergency management to insurance risk analysis and policy formulation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty quantification in fine-tuned LLMs using LoRA ensembles</title>
<link>https://arxiv.org/abs/2402.12264</link>
<guid>https://arxiv.org/abs/2402.12264</guid>
<content:encoded><![CDATA[
arXiv:2402.12264v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and balance between retained prior knowledge and domain specific adaptation during and after fine-tuning. We identify unexpected retention of acquired knowledge during fine-tuning in the overfitting regime.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2404.05894</link>
<guid>https://arxiv.org/abs/2404.05894</guid>
<content:encoded><![CDATA[
arXiv:2404.05894v5 Announce Type: replace-cross 
Abstract: Planning a network of public transit routes is a challenging optimization problem. Metaheuristic algorithms search through the space of possible transit networks by applying heuristics that randomly alter routes in a network. The design of these heuristics has a major impact on the quality of the result. In this paper, we use deep reinforcement learning to train a graph neural net to provide heuristics for an evolutionary algorithm. These neural heuristics improve the algorithm's results on benchmark synthetic cities with 70 nodes or more, and achieve new state-of-the-art results on the challenging Mumford benchmark. They also improve upon a simulation of the real transit network in the city of Laval, Canada, by 52% and 25% on two key metrics, and offer cost savings of up to 19% over the city's existing transit network.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPO: A Differential and Pointwise Control Approach to Reinforcement Learning</title>
<link>https://arxiv.org/abs/2404.15617</link>
<guid>https://arxiv.org/abs/2404.15617</guid>
<content:encoded><![CDATA[
arXiv:2404.15617v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (DPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $O(K^{5/6})$. Empirically, DPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPO Meets PPO: Reinforced Token Optimization for RLHF</title>
<link>https://arxiv.org/abs/2404.18922</link>
<guid>https://arxiv.org/abs/2404.18922</guid>
<content:encoded><![CDATA[
arXiv:2404.18922v4 Announce Type: replace-cross 
Abstract: In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of large language models, its open-source implementation is still largely sub-optimal. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Under this framework, we introduce an algorithm Reinforced Token Optimization (\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive experiments demonstrate that \texttt{RTO} performs better than PPO and other direct preference learning algorithms. In particular, RTO outperforms PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code and models are available at \href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Real-time Safeguarding the Text Generation of Large Language Model</title>
<link>https://arxiv.org/abs/2404.19048</link>
<guid>https://arxiv.org/abs/2404.19048</guid>
<content:encoded><![CDATA[
arXiv:2404.19048v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have significantly advanced natural language processing (NLP) tasks but also pose ethical and societal risks due to their propensity to generate harmful content. Existing methods have limitations, including the need for training specific control models and proactive intervention during text generation, that lead to quality degradation and increased computational overhead. To mitigate those limitations, we propose LLMSafeGuard, a lightweight real-time framework that integrates an external validator into decoding, rejecting unsafe outputs while allowing valid ones. We introduce a similarity-based validation approach, simplifying constraint introduction and eliminating the need for control model training. Additionally, LLMSafeGuard employs a context-wise timing selection strategy, intervening LLMs only when necessary. We evaluate LLMSafeGuard on detoxification and copyright safeguarding, demonstrating its superiority over SOTA baselines. In detoxification, LLMSafeGuard reduces toxic output by at least 38.6\% while preserving linguistic quality. Additionally, its context-wise timing selection cuts inference time by at least 24.2\% without compromising effectiveness.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing</title>
<link>https://arxiv.org/abs/2405.11783</link>
<guid>https://arxiv.org/abs/2405.11783</guid>
<content:encoded><![CDATA[
arXiv:2405.11783v2 Announce Type: replace-cross 
Abstract: In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 450 hypothetical MOF structures consisting of 3 topologies, 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $CO_{2}$ Henry's constant values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 88.6% and 78.0% for binary classification tasks on pore volume and $CO_{2}$ Henry's constant, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 92% and 80% across different classes for pore volume and $CO_{2}$ Henry's constant datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 87% for $CO_{2}$ Henry's constant, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model</title>
<link>https://arxiv.org/abs/2406.12030</link>
<guid>https://arxiv.org/abs/2406.12030</guid>
<content:encoded><![CDATA[
arXiv:2406.12030v4 Announce Type: replace-cross 
Abstract: The emergence of Vision Language Models (VLMs) has brought unprecedented advances in understanding multimodal information. The combination of textual and visual semantics in VLMs is highly complex and diverse, making the safety alignment of these models challenging. Furthermore, due to the limited study on the safety alignment of VLMs, there is a lack of large-scale, high-quality datasets. To address these limitations, we propose a Safety Preference Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth, SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and contains 100,788 samples of the quadruple (question, image, chosen response, rejected response). In terms of depth, the responses are collected from 12 open-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure diversity. The construction of preference data is fully automated, and the experimental results indicate that models trained with alignment techniques on the SPA-VL dataset exhibit substantial improvements in harmlessness and helpfulness while maintaining core capabilities. SPA-VL, as a large-scale, high-quality, and diverse dataset, represents a significant milestone in ensuring that VLMs achieve both harmlessness and helpfulness.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis</title>
<link>https://arxiv.org/abs/2406.12719</link>
<guid>https://arxiv.org/abs/2406.12719</guid>
<content:encoded><![CDATA[
arXiv:2406.12719v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), already shown to ace various text comprehension tasks, have also remarkably been shown to tackle table comprehension tasks without specific training. Building on earlier studies of LLMs for tabular tasks, we probe how in-context learning (ICL), model scale, instruction tuning, and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under diverse augmentations and perturbations, on diverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$, and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newer LLMs deliver stronger, more robust TQA performance, data contamination and reliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and the drops in performance, with sensitivity peaking in the model's middle layers. We highlight the need for improved interpretable methodologies to develop more reliable LLMs for table comprehension.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception</title>
<link>https://arxiv.org/abs/2407.11104</link>
<guid>https://arxiv.org/abs/2407.11104</guid>
<content:encoded><![CDATA[
arXiv:2407.11104v2 Announce Type: replace-cross 
Abstract: The synthesis of product design concepts stands at the crux of early-phase development processes for technical products, traditionally posing an intricate interdisciplinary challenge. The application of deep learning methods, particularly Deep Generative Models (DGMs), holds the promise of automating and streamlining manual iterations and therefore introducing heightened levels of innovation and efficiency. However, DGMs have yet to be widely adopted into the synthesis of product design concepts. This paper aims to explore the reasons behind this limited application and derive the requirements for successful integration of these technologies. We systematically analyze DGM-families (VAE, GAN, Diffusion, Transformer, Radiance Field), assessing their strengths, weaknesses, and general applicability for product design conception. Our objective is to provide insights that simplify the decision-making process for engineers, helping them determine which method might be most effective for their specific challenges. Recognizing the rapid evolution of this field, we hope that our analysis contributes to a fundamental understanding and guides practitioners towards the most promising approaches. This work seeks not only to illuminate current challenges but also to propose potential solutions, thereby offering a clear roadmap for leveraging DGMs in the realm of product design conception.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dMel: Speech Tokenization made Simple</title>
<link>https://arxiv.org/abs/2407.15835</link>
<guid>https://arxiv.org/abs/2407.15835</guid>
<content:encoded><![CDATA[
arXiv:2407.15835v3 Announce Type: replace-cross 
Abstract: Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An In-Depth Investigation of Data Collection in LLM App Ecosystems</title>
<link>https://arxiv.org/abs/2408.13247</link>
<guid>https://arxiv.org/abs/2408.13247</guid>
<content:encoded><![CDATA[
arXiv:2408.13247v2 Announce Type: replace-cross 
Abstract: LLM app (tool) ecosystems are rapidly evolving to support sophisticated use cases that often require extensive user data collection. Given that LLM apps are developed by third parties and anecdotal evidence indicating inconsistent enforcement of policies by LLM platforms, sharing user data with these apps presents significant privacy risks. In this paper, we aim to bring transparency in data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem as a case study. We propose an LLM-based framework to analyze the natural language specifications of GPT Actions (custom tools) and assess their data collection practices. Our analysis reveals that Actions collect excessive data across 24 categories and 145 data types, with third-party Actions collecting 6.03% more data on average. We find that several Actions violate OpenAI's policies by collecting sensitive information, such as passwords, which is explicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy analysis framework to automatically check the consistency of data collection by Actions with disclosures in their privacy policies. Our measurements indicate that the disclosures for most of the collected data types are omitted, with only 5.8% of Actions clearly disclosing their data collection practices.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Driven View Subset Selection for Indoor Novel View Synthesis</title>
<link>https://arxiv.org/abs/2409.07098</link>
<guid>https://arxiv.org/abs/2409.07098</guid>
<content:encoded><![CDATA[
arXiv:2409.07098v2 Announce Type: replace-cross 
Abstract: Novel view synthesis of indoor scenes can be achieved by capturing a monocular video sequence of the environment. However, redundant information caused by artificial movements in the input video data reduces the efficiency of scene modeling. To address this, we formulate the problem as a combinatorial optimization task for view subset selection. In this work, we propose a novel subset selection framework that integrates a comprehensive diversity-based measurement with well-designed utility functions. We provide a theoretical analysis of these utility functions and validate their effectiveness through extensive experiments. Furthermore, we introduce IndoorTraj, a novel dataset designed for indoor novel view synthesis, featuring complex and extended trajectories that simulate intricate human behaviors. Experiments on IndoorTraj show that our framework consistently outperforms baseline strategies while using only 5-20% of the data, highlighting its remarkable efficiency and effectiveness. The code is available at: https://github.com/zehao-wang/IndoorTraj
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Large Language Models for Entity Matching</title>
<link>https://arxiv.org/abs/2409.08185</link>
<guid>https://arxiv.org/abs/2409.08185</guid>
<content:encoded><![CDATA[
arXiv:2409.08185v2 Announce Type: replace-cross 
Abstract: Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) the representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the models ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods, only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o-mini.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiSSL: Enhancing the Alignment Between Self-Supervised Pretraining and Downstream Fine-Tuning via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2410.02387</link>
<guid>https://arxiv.org/abs/2410.02387</guid>
<content:encoded><![CDATA[
arXiv:2410.02387v4 Announce Type: replace-cross 
Abstract: Models initialized from self-supervised pretraining may suffer from poor alignment with downstream tasks, reducing the extent to which subsequent fine-tuning can adapt pretrained features toward downstream objectives. To mitigate this, we introduce BiSSL, a novel bilevel training framework that enhances the alignment of self-supervised pretrained models with downstream tasks prior to fine-tuning. BiSSL acts as an intermediate training stage conducted after conventional self-supervised pretraining and is tasked with solving a bilevel optimization problem that incorporates the pretext and downstream training objectives in its lower- and upper-level objectives, respectively. This approach explicitly models the interdependence between the pretraining and fine-tuning stages within the conventional self-supervised learning pipeline, facilitating enhanced information sharing between them that ultimately leads to a model initialization better aligned with the downstream task. We propose a general training algorithm for BiSSL that is compatible with a broad range of pretext and downstream tasks. Using SimCLR and Bootstrap Your Own Latent to pretrain ResNet-50 backbones on the ImageNet dataset, we demonstrate that our proposed framework significantly improves accuracy on the vast majority of 12 downstream image classification datasets, as well as on object detection. Exploratory analyses alongside investigative experiments further provide compelling evidence that BiSSL enhances downstream alignment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Adaptive Attacks against Watermarks for Language Models</title>
<link>https://arxiv.org/abs/2410.02440</link>
<guid>https://arxiv.org/abs/2410.02440</guid>
<content:encoded><![CDATA[
arXiv:2410.02440v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against any watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively optimized paraphrasers at https://github.com/nilslukas/ada-wm-evasion.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2410.06981</link>
<guid>https://arxiv.org/abs/2410.06981</guid>
<content:encoded><![CDATA[
arXiv:2410.06981v4 Announce Type: replace-cross 
Abstract: The Universality Hypothesis in large language models (LLMs) claims that different models converge towards similar concept representations in their latent spaces. Providing evidence for this hypothesis would enable researchers to exploit universal properties, facilitating the generalization of mechanistic interpretability techniques across models. Previous works studied if LLMs learned the same features, which are internal representations that activate on specific concepts. Since comparing features across LLMs is challenging due to polysemanticity, in which LLM neurons often correspond to multiple unrelated features rather than to distinct concepts, sparse autoencoders (SAEs) have been employed to disentangle LLM neurons into SAE features corresponding to distinct concepts. In this paper, we introduce a new variation of the universality hypothesis called Analogous Feature Universality: we hypothesize that even if SAEs across different models learn different feature representations, the spaces spanned by SAE features are similar, such that one SAE space is similar to another SAE space under rotation-invariant transformations. Evidence for this hypothesis would imply that interpretability techniques related to latent spaces, such as steering vectors, may be transferred across models via certain transformations. To investigate this hypothesis, we first pair SAE features across different models via activation correlation, and then measure spatial relation similarities between paired features via representational similarity measures, which transform spaces into representations that reveal hidden relational similarities. Our experiments demonstrate high similarities for SAE feature spaces across various LLMs, providing evidence for feature space universality.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter Efficient Fine-tuning via Explained Variance Adaptation</title>
<link>https://arxiv.org/abs/2410.07170</link>
<guid>https://arxiv.org/abs/2410.07170</guid>
<content:encoded><![CDATA[
arXiv:2410.07170v4 Announce Type: replace-cross 
Abstract: Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned for a specific downstream task. The most common fine-tuning method is to update pretrained weights via low-rank adaptation (LoRA). Existing initialization strategies for LoRA often rely on singular value decompositions (SVD) of gradients or weight matrices. However, they do not provably maximize the expected gradient signal, which is critical for fast adaptation. To this end, we introduce Explained Variance Adaptation (EVA), an initialization scheme that uses the directions capturing the most activation variance, provably maximizing the expected gradient signal and accelerating fine-tuning. EVA performs incremental SVD on minibatches of activation vectors and selects the right-singular vectors for initialization once they converged. Further, by selecting the directions that capture the most activation-variance for a given rank budget, EVA accommodates adaptive ranks that reduce the number of trainable parameters, while maintaining or improving downstream performance. We apply EVA to a variety of fine-tuning tasks as language generation and understanding, image classification, and reinforcement learning. EVA exhibits faster convergence than competitors and achieves the highest average score across a multitude of tasks per domain while reducing the number of trainable parameters through rank redistribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2410.08109</link>
<guid>https://arxiv.org/abs/2410.08109</guid>
<content:encoded><![CDATA[
arXiv:2410.08109v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Partitioning Vision Transformer on Edge Devices for Distributed Inference</title>
<link>https://arxiv.org/abs/2410.11650</link>
<guid>https://arxiv.org/abs/2410.11650</guid>
<content:encoded><![CDATA[
arXiv:2410.11650v2 Announce Type: replace-cross 
Abstract: Deep learning models are increasingly utilized on resource-constrained edge devices for real-time data analytics. Recently, Vision Transformer and their variants have shown exceptional performance in various computer vision tasks. However, their substantial computational requirements and low inference latency create significant challenges for deploying such models on resource-constrained edge devices. To address this issue, we propose a novel framework, ED-ViT, which is designed to efficiently split and execute complex Vision Transformers across multiple edge devices. Our approach involves partitioning Vision Transformer models into several sub-models, while each dedicated to handling a specific subset of data classes. To further reduce computational overhead and inference latency, we introduce a class-wise pruning technique that decreases the size of each sub-model. Through extensive experiments conducted on five datasets using three model architectures and actual implementation on edge devices, we demonstrate that our method significantly cuts down inference latency on edge devices and achieves a reduction in model size by up to 28.9 times and 34.1 times, respectively, while maintaining test accuracy comparable to the original Vision Transformer. Additionally, we compare ED-ViT with two state-of-the-art methods that deploy CNN and SNN models on edge devices, evaluating metrics such as accuracy, inference time, and overall model size. Our comprehensive evaluation underscores the effectiveness of the proposed ED-ViT framework.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyClick: Single-Turn Agent for Empowering GUI Automation</title>
<link>https://arxiv.org/abs/2410.11871</link>
<guid>https://arxiv.org/abs/2410.11871</guid>
<content:encoded><![CDATA[
arXiv:2410.11871v3 Announce Type: replace-cross 
Abstract: We present an UI agent for user interface (UI) interaction tasks, using Vision-Language Model Florence-2-Base. The agent's primary task is identifying the screen coordinates of the UI element corresponding to the user's command. It demonstrates very strong performance on Screenspot and OmniAct annotations, while maintaining a very small size of 0.27B parameters and minimal latency. Moreover, training needs small compute budget of 56 GPU-hours (worth about 40 USD). Relevant improvement comes from vision-specific multi-task training and MLLM-based data augmentation. We hope that decreased needs for expensive compute resources and manually annotated data will allow to facilitate more inclusive and sustainable research of UI agents.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2410.13370</link>
<guid>https://arxiv.org/abs/2410.13370</guid>
<content:encoded><![CDATA[
arXiv:2410.13370v3 Announce Type: replace-cross 
Abstract: Text-to-image diffusion models can generate high-quality images but lack fine-grained control of visual concepts, limiting their creativity. Thus, we introduce component-controllable personalization, a new task that enables users to customize and reconfigure individual components within concepts. This task faces two challenges: semantic pollution, where undesired elements disrupt the target concept, and semantic imbalance, which causes disproportionate learning of the target concept and component. To address these, we design MagicTailor, a framework that uses Dynamic Masked Degradation to adaptively perturb unwanted visual semantics and Dual-Stream Balancing for more balanced learning of desired visual semantics. The experimental results show that MagicTailor achieves superior performance in this task and enables more personalized and creative image generation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospective Learning from Interactions</title>
<link>https://arxiv.org/abs/2410.13852</link>
<guid>https://arxiv.org/abs/2410.13852</guid>
<content:encoded><![CDATA[
arXiv:2410.13852v2 Announce Type: replace-cross 
Abstract: Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection without additional annotations. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct a multimodal LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATEAU: Selecting Influential Samples for Long Context Alignment</title>
<link>https://arxiv.org/abs/2410.15633</link>
<guid>https://arxiv.org/abs/2410.15633</guid>
<content:encoded><![CDATA[
arXiv:2410.15633v5 Announce Type: replace-cross 
Abstract: Aligning large language models to handle instructions with extremely long contexts has yet to be fully investigated. Previous studies have attempted to scale up the available data volume by synthesizing long instruction-following samples, as constructing such a dataset tends to be challenging for annotators. However, a lack of a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the model's performance. Thus, we propose GATEAU, a novel framework to address the unique challenge of long context alignment by identifying the influential samples enriched with long-range dependency relations. Specifically, GATEAU measures the long-range dependencies from two essential aspects: the difficulty of generating target responses due to the long-range dependencies, and the difficulty of understanding long inputs due to such dependencies. Comprehensive experiments indicate that GATEAU effectively identifies influential samples and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec</title>
<link>https://arxiv.org/abs/2410.15764</link>
<guid>https://arxiv.org/abs/2410.15764</guid>
<content:encoded><![CDATA[
arXiv:2410.15764v3 Announce Type: replace-cross 
Abstract: Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised detection of semantic correlations in big data</title>
<link>https://arxiv.org/abs/2411.02126</link>
<guid>https://arxiv.org/abs/2411.02126</guid>
<content:encoded><![CDATA[
arXiv:2411.02126v3 Announce Type: replace-cross 
Abstract: In real-world data, information is stored in extremely large feature vectors. These variables are typically correlated due to complex interactions involving many features simultaneously. Such correlations qualitatively correspond to semantic roles and are naturally recognized by both the human brain and artificial neural networks. This recognition enables, for instance, the prediction of missing parts of an image or text based on their context. We present a method to detect these correlations in high-dimensional data represented as binary numbers. We estimate the binary intrinsic dimension of a dataset, which quantifies the minimum number of independent coordinates needed to describe the data, and is therefore a proxy of semantic complexity. The proposed algorithm is largely insensitive to the so-called curse of dimensionality, and can therefore be used in big data analysis. We test this approach identifying phase transitions in model magnetic systems and we then apply it to the detection of semantic correlations of images and text inside deep neural networks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRe: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2411.08334</link>
<guid>https://arxiv.org/abs/2411.08334</guid>
<content:encoded><![CDATA[
arXiv:2411.08334v3 Announce Type: replace-cross 
Abstract: Recent multimodal retrieval methods have endowed text-based retrievers with multimodal capabilities by utilizing pre-training strategies for visual-text alignment. They often directly fuse the two modalities for cross-reference during the alignment to understand multimodal queries. However, existing methods often overlook crucial visual information due to a text-dominant issue, which overly depends on text-driven signals. In this paper, we introduce MIRe, a retrieval framework that achieves modality interaction without fusing textual features during the alignment. Our method allows the textual query to attend to visual embeddings while not feeding text-driven signals back into the visual representations. Additionally, we construct a pre-training dataset for multimodal query retrieval by transforming concise question-answer pairs into extended passages. Our experiments demonstrate that our pre-training strategy significantly enhances the understanding of multimodal queries, resulting in strong performance across four multimodal retrieval benchmarks under zero-shot settings. Moreover, our ablation studies and analyses explicitly verify the effectiveness of our framework in mitigating the text-dominant issue. Our code is publicly available: https://github.com/yeongjoonJu/MIRe
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers</title>
<link>https://arxiv.org/abs/2411.15370</link>
<guid>https://arxiv.org/abs/2411.15370</guid>
<content:encoded><![CDATA[
arXiv:2411.15370v2 Announce Type: replace-cross 
Abstract: Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method -- Action Value Gradient (AVG) and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.00661</link>
<guid>https://arxiv.org/abs/2412.00661</guid>
<content:encoded><![CDATA[
arXiv:2412.00661v3 Announce Type: replace-cross 
Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Novel Skills from Language-Generated Demonstrations</title>
<link>https://arxiv.org/abs/2412.09286</link>
<guid>https://arxiv.org/abs/2412.09286</guid>
<content:encoded><![CDATA[
arXiv:2412.09286v2 Announce Type: replace-cross 
Abstract: Robots are increasingly deployed across diverse domains to tackle tasks requiring novel skills. However, current robot learning algorithms for acquiring novel skills often rely on demonstration datasets or environment interactions, resulting in high labor costs and potential safety risks. To address these challenges, this study proposes DemoGen, a skill-learning framework that enables robots to acquire novel skills from natural language instructions. DemoGen leverages the vision-language model and the video diffusion model to generate demonstration videos of novel skills, which enabling robots to learn new skills effectively. Experimental evaluations in the MetaWorld simulation environments demonstrate the pipeline's capability to generate high-fidelity and reliable demonstrations. Using the generated demonstrations, various skill learning algorithms achieve an accomplishment rate three times the original on novel tasks. These results highlight a novel approach to robot learning, offering a foundation for the intuitive and intelligent acquisition of novel robotic skills. (Project website: https://aoqunjin.github.io/LNSLGD/)
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL</title>
<link>https://arxiv.org/abs/2412.10138</link>
<guid>https://arxiv.org/abs/2412.10138</guid>
<content:encoded><![CDATA[
arXiv:2412.10138v2 Announce Type: replace-cross 
Abstract: Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by large language models (LLMs), the latest state-of-the-art techniques are still trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which limits their applicability in open scenarios. To address this challenge, we propose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to improve the comprehensive capabilities of open-source LLMs for Text2SQL, thereby providing a more practical solution. Our approach begins with multi-task supervised fine-tuning (SFT) using various synthetic training data related to SQL generation. Unlike existing SFT-based Text2SQL methods, we introduced several additional SFT tasks, including schema linking, noise correction, and continuation writing. Engaging in a variety of SQL generation tasks enhances the model's understanding of SQL syntax and improves its ability to generate high-quality SQL queries. Additionally, inspired by the collaborative modes of LLM agents, we introduce a Multitask Collaboration Prompting (MCP) strategy. This strategy leverages collaboration across several SQL-related tasks to reduce hallucinations during SQL generation, thereby maximizing the potential of enhancing Text2SQL performance through explicit multitask capabilities. Extensive experiments and in-depth analyses have been performed on eight open-source LLMs and five widely-used benchmarks. The results demonstrate that our proposal outperforms the latest Text2SQL methods and yields leading performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships</title>
<link>https://arxiv.org/abs/2412.14190</link>
<guid>https://arxiv.org/abs/2412.14190</guid>
<content:encoded><![CDATA[
arXiv:2412.14190v3 Announce Type: replace-cross 
Abstract: Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the "new" AI relative to the "original". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based Methodology</title>
<link>https://arxiv.org/abs/2412.17778</link>
<guid>https://arxiv.org/abs/2412.17778</guid>
<content:encoded><![CDATA[
arXiv:2412.17778v2 Announce Type: replace-cross 
Abstract: Deep neural network (DNN)-based speech enhancement (SE) usually uses conventional activation functions, which lack the expressiveness to capture complex multiscale structures needed for high-fidelity SE. Group-Rational KAN (GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KAN's expressiveness while improving scalability on complex tasks. We adapt GR-KAN to existing DNN-based SE by replacing dense layers with GR-KAN layers in the time-frequency (T-F) domain MP-SENet and adapting GR-KAN's activations into the 1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that GR-KAN requires up to 4x fewer parameters while improving PESQ by up to 0.1. In contrast, KAN, facing scalability issues, outperforms MLP on a small-scale signal modeling task but fails to improve MP-SENet. We demonstrate the first successful use of KAN-based methods for consistent improvement in both time- and SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies</title>
<link>https://arxiv.org/abs/2412.18296</link>
<guid>https://arxiv.org/abs/2412.18296</guid>
<content:encoded><![CDATA[
arXiv:2412.18296v2 Announce Type: replace-cross 
Abstract: Data corruption, including missing and noisy data, poses significant challenges in real-world machine learning. This study investigates the effects of data corruption on model performance and explores strategies to mitigate these effects through two experimental setups: supervised learning with NLP tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization (Signal-RL). We analyze the relationship between data corruption levels and model performance, evaluate the effectiveness of data imputation methods, and assess the utility of enlarging datasets to address data corruption.
  Our results show that model performance under data corruption follows a diminishing return curve, modeled by the exponential function. Missing data, while detrimental, is less harmful than noisy data, which causes severe performance degradation and training instability, particularly in sequential decision-making tasks like Signal-RL. Imputation strategies involve a trade-off: they recover missing information but may introduce noise. Their effectiveness depends on imputation accuracy and corruption ratio. We identify distinct regions in the imputation advantage heatmap, including an "imputation advantageous corner" and an "imputation disadvantageous edge" and classify tasks as "noise-sensitive" or "noise-insensitive" based on their decision boundaries.
  Furthermore, we find that increasing dataset size mitigates but cannot fully overcome the effects of data corruption. The marginal utility of additional data diminishes as corruption increases. An empirical rule emerges: approximately 30% of the data is critical for determining performance, while the remaining 70% has minimal impact.
  These findings provide actionable insights into data preprocessing, imputation strategies, and data collection practices, guiding the development of robust machine learning systems in noisy environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond</title>
<link>https://arxiv.org/abs/2501.05714</link>
<guid>https://arxiv.org/abs/2501.05714</guid>
<content:encoded><![CDATA[
arXiv:2501.05714v3 Announce Type: replace-cross 
Abstract: With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But Only If You Can Trust Them</title>
<link>https://arxiv.org/abs/2501.08005</link>
<guid>https://arxiv.org/abs/2501.08005</guid>
<content:encoded><![CDATA[
arXiv:2501.08005v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code will be made publicly available
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Learning in Energy-based Models with Attractor Structures</title>
<link>https://arxiv.org/abs/2501.13997</link>
<guid>https://arxiv.org/abs/2501.13997</guid>
<content:encoded><![CDATA[
arXiv:2501.13997v2 Announce Type: replace-cross 
Abstract: Predictive models are highly advanced in understanding the mechanisms of brain function. Recent advances in machine learning further underscore the power of prediction for optimal representation in learning. However, there remains a gap in creating a biologically plausible model that explains how the neural system achieves prediction. In this paper, we introduce a framework that employs an energy-based model (EBM) to capture the nuanced processes of predicting observation after action within the neural system, encompassing prediction, learning, and inference. We implement the EBM with a hierarchical structure and integrate a continuous attractor neural network for memory, constructing a biologically plausible model. In experimental evaluations, our model demonstrates efficacy across diverse scenarios. The range of actions includes eye movement, motion in environments, head turning, and static observation while the environment changes. Our model not only makes accurate predictions for environments it was trained on, but also provides reasonable predictions for unseen environments, matching the performances of machine learning methods in multiple tasks. We hope that this study contributes to a deep understanding of how the neural system performs prediction.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABPT: Amended Backpropagation through Time with Partially Differentiable Rewards</title>
<link>https://arxiv.org/abs/2501.14513</link>
<guid>https://arxiv.org/abs/2501.14513</guid>
<content:encoded><![CDATA[
arXiv:2501.14513v2 Announce Type: replace-cross 
Abstract: Quadrotor control policies can be trained with high performance using the exact gradients of the rewards to directly optimize policy parameters via backpropagation-through-time (BPTT). However, designing a fully differentiable reward architecture is often challenging. Partially differentiable rewards will result in biased gradient propagation that degrades training performance. To overcome this limitation, we propose Amended Backpropagation-through-Time (ABPT), a novel approach that mitigates gradient bias while preserving the training efficiency of BPTT. ABPT combines 0-step and N-step returns, effectively reducing the bias by leveraging value gradients from the learned Q-value function. Additionally, it adopts entropy regularization and state initialization mechanisms to encourage exploration during training. We evaluate ABPT on four representative quadrotor flight tasks \li{in both real world and simulation}. Experimental results demonstrate that ABPT converges significantly faster and achieves higher ultimate rewards than existing learning algorithms, particularly in tasks involving partially differentiable rewards. The code will be released at http://github.com/Fanxing-LI/ABPT.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Conformal Prediction via Message Passing</title>
<link>https://arxiv.org/abs/2501.14544</link>
<guid>https://arxiv.org/abs/2501.14544</guid>
<content:encoded><![CDATA[
arXiv:2501.14544v2 Announce Type: replace-cross 
Abstract: Post-hoc calibration of pre-trained models is critical for ensuring reliable inference, especially in safety-critical domains such as healthcare. Conformal Prediction (CP) offers a robust post-hoc calibration framework, providing distribution-free statistical coverage guarantees for prediction sets by leveraging held-out datasets. In this work, we address a decentralized setting where each device has limited calibration data and can communicate only with its neighbors over an arbitrary graph topology. We propose two message-passing-based approaches for achieving reliable inference via CP: quantile-based distributed conformal prediction (Q-DCP) and histogram-based distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence, while H-DCP uses a consensus-based histogram estimation approach. Through extensive experiments, we investigate the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes across different network topologies. The code of our work is released on: https://github.com/HaifengWen/Distributed-Conformal-Prediction.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Width Neural Networks</title>
<link>https://arxiv.org/abs/2501.15889</link>
<guid>https://arxiv.org/abs/2501.15889</guid>
<content:encoded><![CDATA[
arXiv:2501.15889v4 Announce Type: replace-cross 
Abstract: For almost 70 years, researchers have mostly relied on hyper-parameter tuning to select the width of neural networks' layers. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network's layer during training. The technique does not rely on alternate optimization nor hand-crafted gradient heuristics; rather, it jointly optimizes the width and the parameters of each layer via simple backpropagation. We apply the technique to a broad range of data domains such as tables, images, text, sequences, and graphs, showing how the width adapts to the task's difficulty. The method imposes a soft ordering of importance among neurons, by which it also is possible to truncate the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources in a structured way. Alternatively, one can dynamically compress the network with no performance degradation. In light of recent foundation models trained on large datasets, believed to require billions of parameters and where hyper-parameter tuning is unfeasible due to humongous training costs, our approach stands as a viable alternative for width learning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning</title>
<link>https://arxiv.org/abs/2501.16344</link>
<guid>https://arxiv.org/abs/2501.16344</guid>
<content:encoded><![CDATA[
arXiv:2501.16344v3 Announce Type: replace-cross 
Abstract: Current speech encoding pipelines often rely on an additional text-based LM to get robust representations of human communication, even though SotA speech-to-text models often have a LM within. This work proposes an approach to improve the LM within an audio model such that the subsequent text-LM is unnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological Alignment), which leverages a novel audio training objective: contrastive loss with a language model embedding as a teacher. Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper's latent space with semantic representations from a text autoencoder (SBERT) and lexically derived embeddings of basic psychological dimensions: emotion and personality. Over self-supervised affective tasks and downstream psychological tasks, WhiSPA surpasses current speech encoders, achieving an average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates that it is not always necessary to run a subsequent text LM on speech-to-text output in order to get a rich psychological representation of human communication.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregation Schemes for Single-Vector WSI Representation Learning in Digital Pathology</title>
<link>https://arxiv.org/abs/2501.17822</link>
<guid>https://arxiv.org/abs/2501.17822</guid>
<content:encoded><![CDATA[
arXiv:2501.17822v2 Announce Type: replace-cross 
Abstract: A crucial step to efficiently integrate Whole Slide Images (WSIs) in computational pathology is assigning a single high-quality feature vector, i.e., one embedding, to each WSI. With the existence of many pre-trained deep neural networks and the emergence of foundation models, extracting embeddings for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs, given their high resolution and gigapixel nature, inputting them into existing GPUs as a single image is not feasible. As a result, WSIs are usually split into many patches. Feeding each patch to a pre-trained model, each WSI can then be represented by a set of patches, hence, a set of embeddings. Hence, in such a setup, WSI representation learning reduces to set representation learning where for each WSI we have access to a set of patch embeddings. To obtain a single embedding from a set of patch embeddings for each WSI, multiple set-based learning schemes have been proposed in the literature. In this paper, we evaluate the WSI search performance of multiple recently developed aggregation techniques (mainly set representation learning techniques) including simple average or max pooling operations, Deep Sets, Memory networks, Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse and binary Fisher Vector on four different primary sites including bladder, breast, kidney, and Colon from TCGA. Further, we benchmark the search performance of these methods against the median of minimum distances of patch embeddings, a non-aggregating approach used for WSI retrieval.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deeply Optimizing the SAT Solver for the IC3 Algorithm</title>
<link>https://arxiv.org/abs/2501.18612</link>
<guid>https://arxiv.org/abs/2501.18612</guid>
<content:encoded><![CDATA[
arXiv:2501.18612v3 Announce Type: replace-cross 
Abstract: The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm that has significantly influenced the field in recent years due to its efficiency, scalability, and completeness. It utilizes SAT solvers to solve a series of SAT queries associated with relative induction. In this paper, we introduce several optimizations for the SAT solver in IC3 based on our observations of the unique characteristics of these SAT queries. By observing that SAT queries do not necessarily require decisions on all variables, we compute a subset of variables that need to be decided before each solving process while ensuring that the result remains unaffected. Additionally, noting that the overhead of binary heap operations in VSIDS is non-negligible, we replace the binary heap with buckets to achieve constant-time operations. Furthermore, we support temporary clauses without the need to allocate a new activation variable for each solving process, thereby eliminating the need to reset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which integrates these optimizations. A comprehensive evaluation highlights the performance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3 demonstrates an average speedup of 3.61 times in solving time compared to the IC3 implementation based on MiniSat.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence</title>
<link>https://arxiv.org/abs/2502.00678</link>
<guid>https://arxiv.org/abs/2502.00678</guid>
<content:encoded><![CDATA[
arXiv:2502.00678v2 Announce Type: replace-cross 
Abstract: Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Measuring dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that evaluates dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings. Code is released in https://github.com/deeplearning-wisc/kernel-divergence-score.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles</title>
<link>https://arxiv.org/abs/2502.01081</link>
<guid>https://arxiv.org/abs/2502.01081</guid>
<content:encoded><![CDATA[
arXiv:2502.01081v2 Announce Type: replace-cross 
Abstract: The releases of OpenAI's o-[n] series, such as o1, o3, and o4-mini, mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, models like o3 have demonstrated strong performance on benchmarks like the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models (including o1, o3, and o4-mini) on challenging multimodal puzzles from PuzzleVQA and AlgoPuzzleVQA, which demand fine-grained visual perception. Our results reveal that o-[n] series, particularly later iterations like o3 and o4-mini, significantly outperform the GPT-[n] series and show strong scalability in multimodal reasoning. Nonetheless, despite these substantial advancements and the superior capabilities demonstrated by the o-[n] series, our findings highlight that even these leading models face persistent challenges. Difficulties are particularly evident in tasks requiring precise visual perception, robust compositional reasoning across multiple visual attributes, and solving complex algorithmic or highly combinatorial puzzles, indicating critical areas for future AGI development. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available at https://github.com/declare-lab/LLM-PuzzleTest.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fused State Representations for Control from Multi-View Observations</title>
<link>https://arxiv.org/abs/2502.01316</link>
<guid>https://arxiv.org/abs/2502.01316</guid>
<content:encoded><![CDATA[
arXiv:2502.01316v2 Announce Type: replace-cross 
Abstract: Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose Multi-view Fusion State for Control (MFSC), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC's robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Knowledge Editing requires Better Regularization</title>
<link>https://arxiv.org/abs/2502.01636</link>
<guid>https://arxiv.org/abs/2502.01636</guid>
<content:encoded><![CDATA[
arXiv:2502.01636v2 Announce Type: replace-cross 
Abstract: Knowledge editing is a promising way to improve factuality in large language models, but recent studies have shown significant model degradation during sequential editing. In this paper, we formalize the popular locate-then-edit methods as a two-step fine-tuning process, allowing us to precisely identify the root cause of this degradation. We show that model degradation occurs due to (1) over-optimization of internal activations and (2) continuous norm-growth of edited matrices. To mitigate these issues, we introduce two regularization techniques: (1) Most-Probable Early Stopping (MPES) and (2) explicit Frobenius norm-constraint. We demonstrate that applying these simple yet effective regularization techniques at key points in the editing process can substantially mitigate model degradation. Combining these regularization methods enables scaling locate-then-edit methods to 10,000 edits while reducing editing time by 42-61%. These results show that targeted regularization is essential for lifelong knowledge editing.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2502.01697</link>
<guid>https://arxiv.org/abs/2502.01697</guid>
<content:encoded><![CDATA[
arXiv:2502.01697v3 Announce Type: replace-cross 
Abstract: As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. However, current data generation methods rely on seed sets containing tens of thousands of examples to prompt instruction-tuned models. This reliance can be especially problematic when the curation of high-quality examples is expensive or difficult. In this paper we explore the novel few-shot synthetic data generation setting -- generating a high-quality dataset from a few examples. We show that when working with only a few seed examples, instruction-tuned models used in current synthetic data methods produce insufficient diversity for downstream tasks. In contrast, we show that base models without post-training, largely untapped for synthetic data generation, offer substantially greater output diversity, albeit with lower instruction following abilities. Leveraging this insight, we propose Base-Refine (BARE), a novel two-stage method that combines the diversity of base models with the quality assurance of instruction-tuned models. BARE excels in few-shot synthetic data generation: using only 3 seed examples it generates diverse, high-quality datasets that significantly improve downstream task performance. We show that fine-tuning Llama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable to state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore, data generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2 1B on GSM8K over data generated by only instruction-models, and an 18.4% improvement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method for RAG data generation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Differentially Private (Sliced) Wasserstein Gradients</title>
<link>https://arxiv.org/abs/2502.01701</link>
<guid>https://arxiv.org/abs/2502.01701</guid>
<content:encoded><![CDATA[
arXiv:2502.01701v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a novel framework for privately optimizing objectives that rely on Wasserstein distances between data-dependent empirical measures. Our main theoretical contribution is, based on an explicit formulation of the Wasserstein gradient in a fully discrete setting, a control on the sensitivity of this gradient to individual data points, allowing strong privacy guarantees at minimal utility cost. Building on these insights, we develop a deep learning approach that incorporates gradient and activations clipping, originally designed for DP training of problems with a finite-sum structure. We further demonstrate that privacy accounting methods extend to Wasserstein-based objectives, facilitating large-scale private training. Empirical results confirm that our framework effectively balances accuracy and privacy, offering a theoretically sound solution for privacy-preserving machine learning tasks relying on optimal transport distances such as Wasserstein distance or sliced-Wasserstein distance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Maintain Fundamental Abilities under KV Cache Compression?</title>
<link>https://arxiv.org/abs/2502.01941</link>
<guid>https://arxiv.org/abs/2502.01941</guid>
<content:encoded><![CDATA[
arXiv:2502.01941v2 Announce Type: replace-cross 
Abstract: This paper investigates an underexplored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. Although existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive benchmark KVFundaBench to systematically evaluate the effects of KV cache compression across diverse fundamental LLM capabilities, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals serval key findings: (1) \textit{Task-Dependent Degradation}; (2) \textit{Model-Type Robustness} (3) \textit{Prompt Length Vulnerability}; (4) \textit{Chunk-Level Superiority}; (5) \textit{Prompt-Gain Sensitivity}; (6) \textit{Long-Context Generation Sensitivity}. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\%$-$18\%$ performance improvements on long-context generation tasks under aggressive compression ratios.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change</title>
<link>https://arxiv.org/abs/2502.02863</link>
<guid>https://arxiv.org/abs/2502.02863</guid>
<content:encoded><![CDATA[
arXiv:2502.02863v2 Announce Type: replace-cross 
Abstract: Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures -- specifically a beluga whale, a jellyfish, and a seahorse -- designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics</title>
<link>https://arxiv.org/abs/2502.03654</link>
<guid>https://arxiv.org/abs/2502.03654</guid>
<content:encoded><![CDATA[
arXiv:2502.03654v2 Announce Type: replace-cross 
Abstract: Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, while widely used, is prone to the dying neuron problem, which has been mitigated by variants such as LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently, self-gated activations like GELU and Swish have emerged as state-of-the-art alternatives, leveraging their smoothness to ensure stable gradient flow and prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU activation leverages the right-skewed asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's superior performance relative to state-of-the-art activation functions, establishing GoLU as a robust alternative to existing activation functions.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective</title>
<link>https://arxiv.org/abs/2502.04591</link>
<guid>https://arxiv.org/abs/2502.04591</guid>
<content:encoded><![CDATA[
arXiv:2502.04591v3 Announce Type: replace-cross 
Abstract: Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing in the nonlinear setting without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Link Prediction for Directed Graphs</title>
<link>https://arxiv.org/abs/2502.05724</link>
<guid>https://arxiv.org/abs/2502.05724</guid>
<content:encoded><![CDATA[
arXiv:2502.05724v2 Announce Type: replace-cross 
Abstract: Link prediction for directed graphs is a crucial task with diverse real-world applications. Recent advances in embedding methods and Graph Neural Networks (GNNs) have shown promising improvements. However, these methods often lack a thorough analysis of their expressiveness and suffer from effective benchmarks for a fair evaluation. In this paper, we propose a unified framework to assess the expressiveness of existing methods, highlighting the impact of dual embeddings and decoder design on directed link prediction performance. To address limitations in current benchmark setups, we introduce DirLinkBench, a robust new benchmark with comprehensive coverage, standardized evaluation, and modular extensibility. The results on DirLinkBench show that current methods struggle to achieve strong performance, while DiGAE outperforms other baselines overall. We further revisit DiGAE theoretically, showing its graph convolution aligns with GCN on an undirected bipartite graph. Inspired by these insights, we propose a novel Spectral Directed Graph Auto-Encoder SDGAE that achieves state-of-the-art average performance on DirLinkBench. Finally, we analyze key factors influencing directed link prediction and highlight open challenges in this field.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution</title>
<link>https://arxiv.org/abs/2502.06809</link>
<guid>https://arxiv.org/abs/2502.06809</guid>
<content:encoded><![CDATA[
arXiv:2502.06809v2 Announce Type: replace-cross 
Abstract: Interpreting the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Prior work has primarily focused on mapping individual neurons to discrete semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. Through a comprehensive analysis of both encoder and decoder-based LLMs across diverse datasets, we observe that even highly salient neurons, identified via various attribution techniques for specific semantic concepts, consistently exhibit polysemantic behavior. Importantly, activation magnitudes for fine-grained concepts follow distinct, often Gaussian-like distributions with minimal overlap. This observation motivates a shift from neuron attribution to range-based interpretation. We hypothesize that interpreting and manipulating neuron activation ranges would enable more precise interpretability and targeted interventions in LLMs. To validate our hypothesis, we introduce NeuronLens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise manipulation of targeted concepts, outperforming neuron attribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Is Not the Bottleneck: Cost-Efficient Continual Learning via Weight Space Consolidation</title>
<link>https://arxiv.org/abs/2502.07274</link>
<guid>https://arxiv.org/abs/2502.07274</guid>
<content:encoded><![CDATA[
arXiv:2502.07274v3 Announce Type: replace-cross 
Abstract: Continual learning (CL) has traditionally emphasized minimizing exemplar memory usage, assuming that memory is the primary bottleneck. However, in modern computing environments-particularly those involving large foundation models-memory is inexpensive and abundant, while GPU time constitutes the main cost. This paper re-examines CL under a more realistic setting with sufficient exemplar memory, where the system can retain a representative portion of past data. We find that, under this regime, stability improves due to reduced forgetting, but plasticity diminishes as the model becomes biased toward prior tasks and struggles to adapt to new ones. Notably, even simple baselines like naive replay can match or exceed the performance of state-of-the-art methods at a fraction of the computational cost. Building on this insight, we propose a lightweight yet effective method called Weight Space Consolidation, which directly operates in the model's weight space via two core mechanisms: (1) rank-based parameter resets to recover plasticity, and (2) weight averaging to enhance stability. Our approach outperforms strong baselines across class-incremental learning with image classifiers and continual instruction tuning with large language models, while requiring only one-third to one-fourth of the training cost. These findings challenge long-standing CL assumptions and establish a new, cost-efficient baseline for real-world continual learning systems where exemplar memory is no longer the limiting factor.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</title>
<link>https://arxiv.org/abs/2502.07316</link>
<guid>https://arxiv.org/abs/2502.07316</guid>
<content:encoded><![CDATA[
arXiv:2502.07316v4 Announce Type: replace-cross 
Abstract: Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.07949</link>
<guid>https://arxiv.org/abs/2502.07949</guid>
<content:encoded><![CDATA[
arXiv:2502.07949v2 Announce Type: replace-cross 
Abstract: State-of-the-art (SOTA) reinforcement learning (RL) methods have enabled vision-language model (VLM) agents to learn from interaction with online environments without human supervision. However, these methods often struggle with learning inefficiencies when applied to complex, real-world decision-making tasks with sparse rewards and long-horizon dependencies. We propose a novel framework, Variational Subgoal-Conditioned Reinforcement Learning (VSC-RL), advancing the VLM agents in resolving challenging decision-making tasks. Fundamentally distinct from existing methods, VSC-RL reformulates the decision-making problem as a variational subgoal-conditioned RL problem with the newly derived optimization objective, Subgoal Evidence Lower BOund (SGC-ELBO), which comprises two key components: (a) maximizing the subgoal-conditioned return, and (b) minimizing the divergence from a reference goal-conditioned policy. We theoretically and empirically demonstrate that the VSC-RL can efficiently improve the learning efficiency without compromising performance guarantees. Across a diverse set of challenging benchmarks, including mobile device and web control tasks, VSC-RL consistently outperforms existing SOTA methods, achieving superior learning efficiency and performance.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Semantic Routing in Large Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2502.10928</link>
<guid>https://arxiv.org/abs/2502.10928</guid>
<content:encoded><![CDATA[
arXiv:2502.10928v2 Announce Type: replace-cross 
Abstract: In the past year, large (>100B parameter) mixture-of-expert (MoE) models have become increasingly common in the open domain. While their advantages are often framed in terms of efficiency, prior work has also explored functional differentiation through routing behavior. We investigate whether expert routing in large MoE models is influenced by the semantics of the inputs. To test this, we design two controlled experiments. First, we compare activations on sentence pairs with a shared target word used in the same or different senses. Second, we fix context and substitute the target word with semantically similar or dissimilar alternatives. Comparing expert overlap across these conditions reveals clear, statistically significant evidence of semantic routing in large MoE models.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization</title>
<link>https://arxiv.org/abs/2502.11140</link>
<guid>https://arxiv.org/abs/2502.11140</guid>
<content:encoded><![CDATA[
arXiv:2502.11140v2 Announce Type: replace-cross 
Abstract: Rapid advancements in Large Language Models (LLMs) have accelerated their integration into automated visualization code generation applications. Despite advancements through few-shot prompting and query expansion, existing methods remain limited in handling ambiguous and complex queries, thereby requiring manual intervention. To overcome these limitations, we propose VisPath: a Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation. VisPath handles underspecified queries through structured, multi-stage processing. It begins by reformulating the user input via Chain-of-Thought (CoT) prompting, which refers to the initial query while generating multiple extended queries in parallel, enabling the LLM to capture diverse interpretations of the user intent. These queries then generate candidate visualization scripts, which are executed to produce diverse images. By assessing the visual quality and correctness of each output, VisPath generates targeted feedback that is aggregated to synthesize an optimal final result. Extensive experiments on widely-used benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath outperforms state-of-the-art methods, offering a more reliable solution for AI-driven visualization code generation.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL</title>
<link>https://arxiv.org/abs/2502.11741</link>
<guid>https://arxiv.org/abs/2502.11741</guid>
<content:encoded><![CDATA[
arXiv:2502.11741v2 Announce Type: replace-cross 
Abstract: Text-to-SQL (Text2SQL) aims to map natural language questions to executable SQL queries. Although large language models (LLMs) have driven significant progress, current approaches struggle with poor transferability to open-source LLMs, limited robustness against logic and function errors in complex queries, and inefficiencies in structured search. We introduce SQL-o1, a self-reward-driven heuristic search framework built on an agent-based architecture to enhance model reasoning capabilities. SQL-o1 leverages Monte Carlo Tree Search (MCTS) for structured, multi-step exploration, and incorporates a dynamic pruning strategy to accelerate inference without sacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a +10.8 execution accuracy improvement on the complex Bird dataset, surpassing even GPT-4-based models. Notably, it exhibits strong few-shot generalization and robust cross-model transferability across open-source LLMs. Our code is available at:https://github.com/ShuaiLyu0110/SQL-o1.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities</title>
<link>https://arxiv.org/abs/2502.12128</link>
<guid>https://arxiv.org/abs/2502.12128</guid>
<content:encoded><![CDATA[
arXiv:2502.12128v3 Announce Type: replace-cross 
Abstract: Generative models are spearheading recent progress in deep learning, showcasing strong promise for trajectory sampling in dynamical systems as well. However, whereas latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns, entity conservation, and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping the traceability of individual entities in a latent system representation, and (2) leveraging the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder enable generative modeling directly in latent space. The core idea of LaM-SLidE is the introduction of identifier representations (IDs) that enable the retrieval of entity properties and entity composition from latent system representations, thus fostering traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis</title>
<link>https://arxiv.org/abs/2502.13178</link>
<guid>https://arxiv.org/abs/2502.13178</guid>
<content:encoded><![CDATA[
arXiv:2502.13178v4 Announce Type: replace-cross 
Abstract: Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.We conduct an repository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Word Learning Through Meta In-Context Learning</title>
<link>https://arxiv.org/abs/2502.14791</link>
<guid>https://arxiv.org/abs/2502.14791</guid>
<content:encoded><![CDATA[
arXiv:2502.14791v2 Announce Type: replace-cross 
Abstract: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity May Be All You Need: Sparse Random Parameter Adaptation</title>
<link>https://arxiv.org/abs/2502.15975</link>
<guid>https://arxiv.org/abs/2502.15975</guid>
<content:encoded><![CDATA[
arXiv:2502.15975v2 Announce Type: replace-cross 
Abstract: Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including LoRA, as well as full parameter fine-tuning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification</title>
<link>https://arxiv.org/abs/2502.16627</link>
<guid>https://arxiv.org/abs/2502.16627</guid>
<content:encoded><![CDATA[
arXiv:2502.16627v4 Announce Type: replace-cross 
Abstract: The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. Our study presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 63% improvement in inference speed with minimal accuracy degradation. Our findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence</title>
<link>https://arxiv.org/abs/2502.17028</link>
<guid>https://arxiv.org/abs/2502.17028</guid>
<content:encoded><![CDATA[
arXiv:2502.17028v2 Announce Type: replace-cross 
Abstract: Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP utilize InfoNCE to maximize mutual information, primarily aligning pairwise samples across modalities while overlooking distributional differences. In addition, InfoNCE has inherent conflict in terms of alignment and uniformity in multimodality, leading to suboptimal alignment with modality gaps. To overcome the limitations, we propose CS-Aligner, a novel framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. CS-Aligner captures both the global distribution information of each modality and the pairwise semantic relationships. We find that the CS divergence seamlessly addresses the InfoNCE's alignment-uniformity conflict and serves complementary roles with InfoNCE, yielding tighter and more precise alignment. Moreover, by introducing distributional alignment, CS-Aligner enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Powerful Electronic Health Record Encoders</title>
<link>https://arxiv.org/abs/2502.17403</link>
<guid>https://arxiv.org/abs/2502.17403</guid>
<content:encoded><![CDATA[
arXiv:2502.17403v3 Announce Type: replace-cross 
Abstract: Electronic Health Records (EHRs) offer considerable potential for clinical prediction, but their complexity and heterogeneity present significant challenges for traditional machine learning methods. Recently, domain-specific EHR foundation models trained on large volumes of unlabeled EHR data have shown improved predictive accuracy and generalization. However, their development is constrained by limited access to diverse, high-quality datasets, and by inconsistencies in coding standards and clinical practices. In this study, we explore the use of general-purpose Large Language Models (LLMs) to encode EHR into high-dimensional representations for downstream clinical prediction tasks. We convert structured EHR data into markdown-formatted plain text documents by replacing medical codes with natural language descriptions. This enables the use of LLMs and their extensive semantic understanding and generalization capabilities as effective encoders of EHRs without requiring access to private medical training data. We show that LLM-based embeddings can often match or even surpass the performance of a specialized EHR foundation model, CLMBR-T-Base, across 15 diverse clinical tasks from the EHRSHOT benchmark. To demonstrate generalizability, we further evaluate the approach on the UK Biobank (UKB) cohort, a population distinct from that used to train CLMBR-T-Base. Notably, one of the tested LLM-based models achieves superior performance for disease onset, hospitalization, and mortality prediction, highlighting robustness to shifts in patient populations. Our findings suggest that repurposed general-purpose LLMs for EHR encoding provide a scalable and generalizable alternative to domain-specific models for clinical prediction.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spontaneous Giving and Calculated Greed in Language Models</title>
<link>https://arxiv.org/abs/2502.17720</link>
<guid>https://arxiv.org/abs/2502.17720</guid>
<content:encoded><![CDATA[
arXiv:2502.17720v3 Announce Type: replace-cross 
Abstract: Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of "spontaneous giving and calculated greed." Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20727</link>
<guid>https://arxiv.org/abs/2502.20727</guid>
<content:encoded><![CDATA[
arXiv:2502.20727v3 Announce Type: replace-cross 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptively profiling models with task elicitation</title>
<link>https://arxiv.org/abs/2503.01986</link>
<guid>https://arxiv.org/abs/2503.01986</guid>
<content:encoded><![CDATA[
arXiv:2503.01986v2 Announce Type: replace-cross 
Abstract: Language model evaluations often fail to characterize consequential failure modes, forcing experts to inspect outputs and build new benchmarks. We introduce task elicitation, a method that automatically builds new evaluations to profile model behavior. Task elicitation finds hundreds of natural-language tasks -- an order of magnitude more than prior work -- where frontier models exhibit systematic failures, in domains ranging from forecasting to online harassment. For example, we find that Sonnet 3.5 over-associates quantum computing and AGI and that o3-mini is prone to hallucination when fabrications are repeated in-context.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations</title>
<link>https://arxiv.org/abs/2503.03062</link>
<guid>https://arxiv.org/abs/2503.03062</guid>
<content:encoded><![CDATA[
arXiv:2503.03062v2 Announce Type: replace-cross 
Abstract: The high cost of obtaining high-quality annotated data for in-context learning (ICL) has motivated the development of methods that use self-generated annotations in place of ground-truth labels. While these approaches have shown promising results in few-shot settings, they generally do not scale to many-shot scenarios. In this work, we study ICL with self-generated examples using a framework analogous to traditional semi-supervised learning, consisting of annotation generation, demonstration selection, and in-context inference. Within this framework, we propose a simple baseline that outperforms ground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we observe a scaling law with this baseline, where optimal performance is achieved with more than 1,000 demonstrations. To fully exploit the many-shot capabilities of semi-supervised ICL, we introduce IterPSD, an iterative annotation approach that integrates iterative refinement and curriculum pseudo-labeling techniques from semi-supervised learning, yielding up to 6.8% additional gains on classification tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models</title>
<link>https://arxiv.org/abs/2503.03122</link>
<guid>https://arxiv.org/abs/2503.03122</guid>
<content:encoded><![CDATA[
arXiv:2503.03122v4 Announce Type: replace-cross 
Abstract: Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2503.04715</link>
<guid>https://arxiv.org/abs/2503.04715</guid>
<content:encoded><![CDATA[
arXiv:2503.04715v5 Announce Type: replace-cross 
Abstract: The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.09% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository https://step-law.github.io/
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching</title>
<link>https://arxiv.org/abs/2503.05179</link>
<guid>https://arxiv.org/abs/2503.05179</guid>
<content:encoded><![CDATA[
arXiv:2503.05179v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning capabilities through Chain-of-Thought (CoT) prompting, which elicits step-by-step problem solving, but often at the cost of excessive verbosity in intermediate outputs, leading to increased computational overhead. We propose Sketch-of-Thought (SoT), a prompting framework that integrates cognitively inspired reasoning paradigms with linguistic constraints to reduce token usage while preserving reasoning accuracy. SoT is designed as a flexible, modular approach and is instantiated with three paradigms--Conceptual Chaining, Chunked Symbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and selected dynamically at test-time by a lightweight routing model. Across 15 reasoning datasets spanning multiple domains, languages, and modalities, SoT achieves token reductions of up to 78% with minimal accuracy loss. In tasks such as mathematical and multi-hop reasoning, it even improves accuracy while shortening outputs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning</title>
<link>https://arxiv.org/abs/2503.06072</link>
<guid>https://arxiv.org/abs/2503.06072</guid>
<content:encoded><![CDATA[
arXiv:2503.06072v2 Announce Type: replace-cross 
Abstract: The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Score Distillation: From Noisy Diffusion Pretraining to One-Step High-Quality Generation</title>
<link>https://arxiv.org/abs/2503.07578</link>
<guid>https://arxiv.org/abs/2503.07578</guid>
<content:encoded><![CDATA[
arXiv:2503.07578v2 Announce Type: replace-cross 
Abstract: Diffusion models have achieved remarkable success in generating high-resolution, realistic images across diverse natural distributions. However, their performance heavily relies on high-quality training data, making it challenging to learn meaningful distributions from corrupted samples. This limitation restricts their applicability in scientific domains where clean data is scarce or costly to obtain. In this work, we introduce denoising score distillation (DSD), a surprisingly effective and novel approach for training high-quality generative models from low-quality data. DSD first pretrains a diffusion model exclusively on noisy, corrupted samples and then distills it into a one-step generator capable of producing refined, clean outputs. While score distillation is traditionally viewed as a method to accelerate diffusion models, we show that it can also significantly enhance sample quality, particularly when starting from a degraded teacher model. Across varying noise levels and datasets, DSD consistently improves generative performancewe summarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical insights showing that, in a linear model setting, DSD identifies the eigenspace of the clean data distributions covariance matrix, implicitly regularizing the generator. This perspective reframes score distillation as not only a tool for efficiency but also a mechanism for improving generative models, particularly in low-quality data settings.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoE-Loco: Mixture of Experts for Multitask Locomotion</title>
<link>https://arxiv.org/abs/2503.08564</link>
<guid>https://arxiv.org/abs/2503.08564</guid>
<content:encoded><![CDATA[
arXiv:2503.08564v2 Announce Type: replace-cross 
Abstract: We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask locomotion for legged robots. Our method enables a single policy to handle diverse terrains, including bars, pits, stairs, slopes, and baffles, while supporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient conflicts that typically arise in multitask reinforcement learning, improving both training efficiency and performance. Our experiments demonstrate that different experts naturally specialize in distinct locomotion behaviors, which can be leveraged for task migration and skill composition. We further validate our approach in both simulation and real-world deployment, showcasing its robustness and adaptability.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource Heterogeneity-Aware and Utilization-Enhanced Scheduling for Deep Learning Clusters</title>
<link>https://arxiv.org/abs/2503.10918</link>
<guid>https://arxiv.org/abs/2503.10918</guid>
<content:encoded><![CDATA[
arXiv:2503.10918v2 Announce Type: replace-cross 
Abstract: Scheduling deep learning (DL) models to train on powerful clusters with accelerators like GPUs and TPUs, presently falls short, either lacking fine-grained heterogeneity awareness or leaving resources substantially under-utilized. To fill this gap, we propose a novel design of a task-level heterogeneity-aware scheduler, Hadar, based on an optimization framework that can boost resource utilization. Hadar leverages the performance traits of DL jobs on a heterogeneous DL cluster, characterizes the task-level performance heterogeneity in the optimization problem, and makes scheduling decisions across both spatial and temporal dimensions. It involves the primal-dual framework employing a dual subroutine, to solve the optimization problem and guide the scheduling design. Our trace-driven simulation with representative DL model training workloads demonstrates that Hadar accelerates the total time duration by 1.20x when compared with its state-of-the-art heterogeneity-aware counterpart, Gavel. Further, our Hadar scheduler is enhanced to HadarE by forking each job into multiple copies to let a job train concurrently on heterogeneous GPUs resided on separate available nodes (i.e., machines or servers) for resource utilization enhancement. HadarE is evaluated extensively on physical DL clusters for comparison with Hadar and Gavel. With substantial enhancement in cluster resource utilization (by 1.45x), HadarE exhibits considerable speed-ups in DL model training, reducing the total time duration by 50% (or 80%) on an Amazon's AWS (or our lab) cluster, while producing trained DL models with consistently better inference quality than those trained by Hadar.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BriLLM: Brain-inspired Large Language Model</title>
<link>https://arxiv.org/abs/2503.11299</link>
<guid>https://arxiv.org/abs/2503.11299</guid>
<content:encoded><![CDATA[
arXiv:2503.11299v3 Announce Type: replace-cross 
Abstract: This paper reports the first brain-inspired large language model (BriLLM). This is a non-Transformer, non-GPT, non-traditional machine learning input-output controlled generative language model. The model is based on the Signal Fully-connected flowing (SiFu) definition on the directed graph in terms of the neural network, and has the interpretability of all nodes on the graph of the whole model, instead of the traditional machine learning model that only has limited interpretability at the input and output ends. In the language model scenario, the token is defined as a node in the graph. A randomly shaped or user-defined signal flow flows between nodes on the principle of "least resistance" along paths. The next token or node to be predicted or generated is the target of the signal flow. As a language model, BriLLM theoretically supports infinitely long $n$-gram models when the model size is independent of the input and predicted length of the model. The model's working signal flow provides the possibility of recall activation and innate multi-modal support similar to the cognitive patterns of the human brain. At present, we released the first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node width, 16-token long sequence prediction ability, and language model prediction performance comparable to GPT-1. More computing power will help us explore the infinite possibilities depicted above.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications</title>
<link>https://arxiv.org/abs/2503.20302</link>
<guid>https://arxiv.org/abs/2503.20302</guid>
<content:encoded><![CDATA[
arXiv:2503.20302v2 Announce Type: replace-cross 
Abstract: Misgendering is the act of referring to someone by a gender that does not match their chosen identity. It marginalizes and undermines a person's sense of self, causing significant harm. English-based approaches have clear-cut approaches to avoiding misgendering, such as the use of the pronoun ``they''. However, other languages pose unique challenges due to both grammatical and cultural constructs. In this work we develop methodologies to assess and mitigate misgendering across 42 languages and dialects using a participatory-design approach to design effective and appropriate guardrails across all languages. We test these guardrails in a standard LLM-based application (meeting transcript summarization), where both the data generation and the annotation steps followed a human-in-the-loop approach. We find that the proposed guardrails are very effective in reducing misgendering rates across all languages in the summaries generated, and without incurring loss of quality. Our human-in-the-loop approach demonstrates a method to feasibly scale inclusive and responsible AI-based solutions across multiple languages and cultures. We release the guardrails and synthetic dataset encompassing 42 languages, along with human and LLM-judge evaluations, to encourage further research on this subject.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
arXiv:2503.24370v3 Announce Type: replace-cross 
Abstract: Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval and Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think When You Need: Self-Adaptive Chain-of-Thought Learning</title>
<link>https://arxiv.org/abs/2504.03234</link>
<guid>https://arxiv.org/abs/2504.03234</guid>
<content:encoded><![CDATA[
arXiv:2504.03234v2 Announce Type: replace-cross 
Abstract: Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient "overthinking" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to "think when needed."
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Pathology Foundation Model: Progress and Future Directions</title>
<link>https://arxiv.org/abs/2504.04045</link>
<guid>https://arxiv.org/abs/2504.04045</guid>
<content:encoded><![CDATA[
arXiv:2504.04045v2 Announce Type: replace-cross 
Abstract: Computational pathology, which involves analyzing whole slide images for automated cancer diagnosis, relies on multiple instance learning, where performance depends heavily on the feature extractor and aggregator. Recent Pathology Foundation Models (PFMs), pretrained on large-scale histopathology data, have significantly enhanced both the extractor and aggregator, but they lack a systematic analysis framework. In this survey, we present a hierarchical taxonomy organizing PFMs through a top-down philosophy applicable to foundation model analysis in any domain: model scope, model pretraining, and model design. Additionally, we systematically categorize PFM evaluation tasks into slide-level, patch-level, multimodal, and biological tasks, providing comprehensive benchmarking criteria. Our analysis identifies critical challenges in both PFM development (pathology-specific methodology, end-to-end pretraining, data-model scalability) and utilization (effective adaptation, model maintenance), paving the way for future directions in this promising field. Resources referenced in this survey are available at https://github.com/BearCleverProud/AwesomeWSI.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification</title>
<link>https://arxiv.org/abs/2504.11091</link>
<guid>https://arxiv.org/abs/2504.11091</guid>
<content:encoded><![CDATA[
arXiv:2504.11091v2 Announce Type: replace-cross 
Abstract: Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
arXiv:2504.21798v2 Announce Type: replace-cross 
Abstract: Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeSSM: Towards State Space Models for Code Understanding</title>
<link>https://arxiv.org/abs/2505.01475</link>
<guid>https://arxiv.org/abs/2505.01475</guid>
<content:encoded><![CDATA[
arXiv:2505.01475v2 Announce Type: replace-cross 
Abstract: Although transformers are widely used for various code-specific tasks, they have some significant limitations. In this paper, we investigate State Space Models (SSMs) as a potential alternative to transformers for code understanding tasks, such as code retrieval, classification, and clone detection. Previous research has already demonstrated that SSMs are more compute-efficient than transformers. In our work, we show that SSMs are also more sample-efficient and can effectively extrapolate to longer contexts (beyond the pretraining context) during fine-tuning. Through comprehensive experiments, we demonstrate that SSMs could serve as a viable alternative to transformers for code understanding tasks, while addressing some of the major limitations associated with transformers.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.01731</link>
<guid>https://arxiv.org/abs/2505.01731</guid>
<content:encoded><![CDATA[
arXiv:2505.01731v3 Announce Type: replace-cross 
Abstract: Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models</title>
<link>https://arxiv.org/abs/2505.02847</link>
<guid>https://arxiv.org/abs/2505.02847</guid>
<content:encoded><![CDATA[
arXiv:2505.02847v3 Announce Type: replace-cross 
Abstract: Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FG-CLIP: Fine-Grained Visual and Textual Alignment</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
arXiv:2505.05071v3 Announce Type: replace-cross 
Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. We construct a comprehensive dataset, termed FineHARD, by integrating high-quality region-specific annotations with hard fine-grained negative samples. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Chain of Thoughts via Elastic Reasoning</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
arXiv:2505.05315v2 Announce Type: replace-cross 
Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritizes the completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Our code has been made available at https://github.com/SalesforceAIResearch/Elastic-Reasoning.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTFUZZER: Generic Black-Box Fuzzing for Indirect Prompt Injection against LLM Agents</title>
<link>https://arxiv.org/abs/2505.05849</link>
<guid>https://arxiv.org/abs/2505.05849</guid>
<content:encoded><![CDATA[
arXiv:2505.05849v2 Announce Type: replace-cross 
Abstract: The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentXploit, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentXploit exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of Neural Networks</title>
<link>https://arxiv.org/abs/2505.06520</link>
<guid>https://arxiv.org/abs/2505.06520</guid>
<content:encoded><![CDATA[
arXiv:2505.06520v2 Announce Type: replace-cross 
Abstract: It is often desirable to remove (a.k.a. unlearn) a specific part of the training data from a trained neural network model. A typical application scenario is to protect the data holder's right to be forgotten, which has been promoted by many recent regulation rules. Existing unlearning methods involve training alternative models with remaining data, which may be costly and challenging to verify from the data holder or a thirdparty auditor's perspective. In this work, we provide a new angle and propose a novel unlearning approach by imposing carefully crafted "patch" on the original neural network to achieve targeted "forgetting" of the requested data to delete. Specifically, inspired by the research line of neural network repair, we propose to strategically seek a lightweight minimum "patch" for unlearning a given data point with certifiable guarantee. Furthermore, to unlearn a considerable amount of data points (or an entire class), we propose to iteratively select a small subset of representative data points to unlearn, which achieves the effect of unlearning the whole set. Extensive experiments on multiple categorical datasets demonstrates our approach's effectiveness, achieving measurable unlearning while preserving the model's performance and being competitive in efficiency and memory consumption compared to various baseline methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG</title>
<link>https://arxiv.org/abs/2505.06569</link>
<guid>https://arxiv.org/abs/2505.06569</guid>
<content:encoded><![CDATA[
arXiv:2505.06569v2 Announce Type: replace-cross 
Abstract: Long-context large language models (LC LLMs) combined with retrieval-augmented generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained windows, and fragmented information from suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions. By initiating with finest-level retrieval and progressively incorporating broader, higher-level context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG pipelines in single- and multi-step generation using Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL</title>
<link>https://arxiv.org/abs/2505.08179</link>
<guid>https://arxiv.org/abs/2505.08179</guid>
<content:encoded><![CDATA[
arXiv:2505.08179v2 Announce Type: replace-cross 
Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying policies from pre-collected datasets, offers a promising avenue for deploying RL in safety-critical real-world domains such as robotics. However, the majority of existing approaches emphasize only short-term safety, neglecting long-horizon considerations. Consequently, they may violate safety constraints and fail to ensure sustained protection during online deployment. Moreover, the learned policies often struggle to handle states and actions that are not present or out-of-distribution(OOD) from the offline dataset, and exhibit limited sample efficiency. To address these challenges, we propose a novel framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis to generate reliable safety labels, which serve as supervisory signals for training both a conditional variational autoencoder (CVAE) and a safety classifier. This approach not only ensures high sampling efficiency but also provides rigorous long-horizon safety guarantees. Furthermore, we utilize pessimistic estimation methods to estimate the Q-value of reward and cost, which mitigates the extrapolation errors induces by OOD actions, and penalize unsafe actions to enabled the agent to proactively avoid high-risk behaviors. Moreover, we theoretically prove the validity of this pessimistic estimation. Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing RAG: A Risk Assessment and Mitigation Framework</title>
<link>https://arxiv.org/abs/2505.08728</link>
<guid>https://arxiv.org/abs/2505.08728</guid>
<content:encoded><![CDATA[
arXiv:2505.08728v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as the de facto industry standard for user-facing NLP applications, offering the ability to integrate data without re-training or fine-tuning Large Language Models (LLMs). This capability enhances the quality and accuracy of responses but also introduces novel security and privacy challenges, particularly when sensitive data is integrated. With the rapid adoption of RAG, securing data and services has become a critical priority. This paper first reviews the vulnerabilities of RAG pipelines, and outlines the attack surface from data pre-processing and data storage management to integration with LLMs. The identified risks are then paired with corresponding mitigations in a structured overview. In a second step, the paper develops a framework that combines RAG-specific security considerations, with existing general security guidelines, industry standards, and best practices. The proposed framework aims to guide the implementation of robust, compliant, secure, and trustworthy RAG systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Predictive Optimization and Generation for Business AI</title>
<link>https://arxiv.org/abs/2505.09847</link>
<guid>https://arxiv.org/abs/2505.09847</guid>
<content:encoded><![CDATA[
arXiv:2505.09847v2 Announce Type: replace-cross 
Abstract: The sales process involves sales functions converting leads or opportunities to customers and selling more products to existing customers. The optimization of the sales process thus is key to success of any B2B business. In this work, we introduce a principled approach to sales optimization and business AI, namely the Causal Predictive Optimization and Generation, which includes three layers: 1) prediction layer with causal ML 2) optimization layer with constraint optimization and contextual bandit 3) serving layer with Generative AI and feedback-loop for system enhancement. We detail the implementation and deployment of the system in LinkedIn, showcasing significant wins over legacy systems and sharing learning and insight broadly applicable to this field.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights</title>
<link>https://arxiv.org/abs/2505.10043</link>
<guid>https://arxiv.org/abs/2505.10043</guid>
<content:encoded><![CDATA[
arXiv:2505.10043v2 Announce Type: replace-cross 
Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart retrieval systems have become increasingly important for Business Intelligence (BI), where users need to find relevant charts that match their analytical needs. These needs can be categorized into precise queries that are well-specified and fuzzy queries that are more exploratory -- both require understanding the semantics and context of the charts. However, existing text-to-chart retrieval solutions often fail to capture the semantic content and contextual information of charts, primarily due to the lack of comprehensive metadata (or semantic insights). To address this limitation, we propose a training data development pipeline that automatically synthesizes hierarchical semantic insights for charts, covering visual patterns (visual-oriented), statistical properties (statistics-oriented), and practical applications (task-oriented), which produces 207,498 semantic insights for 69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to learn better representations of charts for text-to-chart retrieval. Our method leverages rich semantic insights during the training phase to develop a model that understands both visual and semantic aspects of charts.To evaluate text-to-chart retrieval performance, we curate the first benchmark, CRBench, for this task with 21,862 charts and 326 text queries from real-world BI applications, with ground-truth labels verified by the crowd workers.Experiments show that ChartFinder significantly outperforms existing methods in text-to-chart retrieval tasks across various settings. For precise queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than state-of-the-art models. In fuzzy query tasks, our method also demonstrates consistent improvements, with an average increase of 5% across nearly all metrics.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment</title>
<link>https://arxiv.org/abs/2505.10717</link>
<guid>https://arxiv.org/abs/2505.10717</guid>
<content:encoded><![CDATA[
arXiv:2505.10717v2 Announce Type: replace-cross 
Abstract: High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain What You Mean: Intent Augmented Knowledge Graph Recommender Built With An LLM</title>
<link>https://arxiv.org/abs/2505.10900</link>
<guid>https://arxiv.org/abs/2505.10900</guid>
<content:encoded><![CDATA[
arXiv:2505.10900v2 Announce Type: replace-cross 
Abstract: Interaction sparsity is a long-standing challenge in recommendation systems. Sparsity manifests in environments with disproportional cardinality of groupings of entities, such as users and products in an online marketplace. It is also found for newly introduced entities, described as the cold-start problem. Recent efforts to mitigate this issue either enrich the connectivity data by incorporating social networks or external knowledge graphs, or fine-tune LLMs into interaction augmenters or next-item recommenders. However, these techniques tend to be resource demanding, requiring high computational power. They also have several limitations, including data availability, low quality, or synthetic noise issues. In this work, we propose LLM-based Intent Knowledge Graph Recommender (IKGR), a novel framework that leverages retrieval-augmented generation and an encoding approach to construct and densify a knowledge graph. IKGR leverages latent user-item affinities from an interaction knowledge graph and further densifies it through mutual intent connectivity. This addresses sparsity issues and allows the model to make intent-grounded recommendations with an interpretable embedding translation layer. Through extensive experiments on real-world datasets, we demonstrate that IKGR overcomes knowledge gaps and achieves substantial gains over state-of-the-art baselines on both publicly available and our internal recommendation datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art</title>
<link>https://arxiv.org/abs/2505.11436</link>
<guid>https://arxiv.org/abs/2505.11436</guid>
<content:encoded><![CDATA[
arXiv:2505.11436v2 Announce Type: replace-cross 
Abstract: Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2503.12908</link>
<guid>https://arxiv.org/abs/2503.12908</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Hallucinations, Contrastive Decoding, Contextual Faithfulness, Inducing Heads

Summary:
HICD is a novel method proposed to address the issue of hallucinations in Large Language Models (LLMs). The technique involves inducing controlled hallucinations through attention dispersion of selected inducing heads crucial to the model's prediction. By comparing the hallucinated outputs with the original outputs, HICD effectively mitigates contextual inaccuracies and improves factual correctness in tasks like context completion, reading comprehension, and question answering. The approach significantly enhances the faithfulness of LLMs to the input context and accuracy in knowledge recall tasks. Moreover, the inducing heads selection and attention dispersion strategy make HICD more effective in generating "contrast-effective" hallucinations compared to other methods. Overall, the findings present a promising technique for reducing hallucinations in LLMs and improving their performance across a diverse set of tasks.<br /><br />Summary: <div>
arXiv:2503.12908v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model's prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more "contrast-effective" hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges</title>
<link>https://arxiv.org/abs/2505.10468</link>
<guid>https://arxiv.org/abs/2505.10468</guid>
<content:encoded><![CDATA[
<div> AI Agents, Agentic AI, Large Language Models, Large Image Models, Multi-agent collaboration, dynamic task decomposition, persistent memory, orchestrated autonomy, customer support, scheduling, data summarization, research automation, robotic coordination, medical decision support

Summary:
AI Agents and Agentic AI represent two distinct design philosophies in artificial intelligence systems. AI Agents are modular systems driven by large language and image models for task-specific automation, while Agentic AI systems focus on multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. The study provides a detailed comparison of the architectural evolution, operational mechanisms, interaction styles, and autonomy levels between the two paradigms. It also discusses application domains such as customer support, scheduling, and data summarization for AI Agents, and research automation, robotic coordination, and medical decision support for Agentic AI. The challenges faced by each paradigm, such as hallucination and coordination failure, are addressed with proposed solutions like ReAct loops and causal modeling. This work aims to guide the development of robust, scalable, and explainable AI-driven systems in both AI Agent and Agentic AI paradigms. 

<br /><br />Summary: <div>
arXiv:2505.10468v3 Announce Type: replace 
Abstract: This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Most General Explanations of Tree Ensembles (Extended Version)</title>
<link>https://arxiv.org/abs/2505.10991</link>
<guid>https://arxiv.org/abs/2505.10991</guid>
<content:encoded><![CDATA[
<div> XAI, abductive explanations, inflated explanations, most general explanation, AI decision <br />
Summary: <br />
Explainable Artificial Intelligence (XAI) is essential for building trust in AI systems by providing insights into why decisions are made. Formal approaches to XAI use abductive explanations to clarify the reasoning behind AI decisions. In the case of numeric inputs, inflated abductive explanations offer more general explanations by providing intervals for each feature. This paper introduces the concept of finding the most general abductive explanation for an AI decision, covering a wide range of input space while remaining a formal explanation of the model's behavior. By selecting the most general explanation, the aim is to provide a single, broad explanation that is likely to be perceived as sensible by humans. This approach ensures transparency and trustworthiness in AI decision-making processes. (The paper has been accepted at IJCAI2025 conference.) <div>
arXiv:2505.10991v3 Announce Type: replace 
Abstract: Explainable Artificial Intelligence (XAI) is critical for attaining trust in the operation of AI systems. A key question of an AI system is ``why was this decision made this way''. Formal approaches to XAI use a formal model of the AI system to identify abductive explanations. While abductive explanations may be applicable to a large number of inputs sharing the same concrete values, more general explanations may be preferred for numeric inputs. So-called inflated abductive explanations give intervals for each feature ensuring that any input whose values fall withing these intervals is still guaranteed to make the same prediction. Inflated explanations cover a larger portion of the input space, and hence are deemed more general explanations. But there can be many (inflated) abductive explanations for an instance. Which is the best? In this paper, we show how to find a most general abductive explanation for an AI decision. This explanation covers as much of the input space as possible, while still being a correct formal explanation of the model's behaviour. Given that we only want to give a human one explanation for a decision, the most general explanation gives us the explanation with the broadest applicability, and hence the one most likely to seem sensible. (The paper has been accepted at IJCAI2025 conference.)
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Bias on English Language Learners in Automatic Scoring</title>
<link>https://arxiv.org/abs/2505.10643</link>
<guid>https://arxiv.org/abs/2505.10643</guid>
<content:encoded><![CDATA[
<div> bias, disparities, English Language Learners (ELLs), automatic scoring systems, science assessments

Summary:
The study investigates scoring biases and disparities towards English Language Learners (ELLs) in middle school science assessments when using automatic scoring systems. By fine-tuning BERT with different datasets and analyzing 21 assessment items, the study found no bias or distorted disparities with large training datasets of ELL responses. However, concerns arose when the sample size was limited to 200 ELL responses. Scoring accuracy was compared using Friedman tests, and Mean Score Gaps (MSGs) between ELLs and non-ELLs were measured to identify disparities in scoring. The study highlights the importance of balanced training datasets to avoid bias and disparities in automated scoring systems for ELLs. <div>
arXiv:2505.10643v2 Announce Type: replace-cross 
Abstract: This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to scoring bias and disparities. We fine-tuned BERT with four datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced mixed dataset with equal representation of both groups. The study analyzed 21 assessment items: 10 items with about 30,000 ELL responses, five items with about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring accuracy (Acc) was calculated and compared to identify bias using Friedman tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and then calculated the differences in MSGs generated through both the human and AI models to identify the scoring disparities. We found that no AI bias and distorted disparities between ELLs and non-ELLs were found when the training dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could exist if the sample size is limited (ELL = 200).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation</title>
<link>https://arxiv.org/abs/2505.10940</link>
<guid>https://arxiv.org/abs/2505.10940</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender systems, user roles, logical modeling, Large Language Model, integration framework

Summary:<br />
Recommender systems typically focus on learning item topics and user preferences but often overlook user characteristics and social roles. This study introduces the tasks of user role identification and behavioral logic modeling to explicitly model user roles and understand the relationships between item topics and user social roles. The proposed TagCF integrates Large Language Models with recommendation systems to extract tag-based virtual logic graphs and improve recommendation performance. Experimental results demonstrate the effectiveness of TagCF in both online and offline settings, showing that user role modeling may be more beneficial than item topic modeling. The extracted logic graphs are shown to be general and transferable knowledge that can enhance a range of recommendation tasks. This research highlights the importance of considering user characteristics and social roles in recommender system design. 

<br /><br />Summary: <div>
arXiv:2505.10940v2 Announce Type: replace-cross 
Abstract: Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRoQ-Loco: Generalist and Robot-agnostic Quadruped Locomotion Control using Offline Datasets</title>
<link>https://arxiv.org/abs/2505.10973</link>
<guid>https://arxiv.org/abs/2505.10973</guid>
<content:encoded><![CDATA[
<div> framework, attention-based, generalist policy, quadruped robots, locomotion<br />
Summary:<br />
- The paper introduces GRoQ-Loco, an attention-based framework for generalist policy learning on multiple quadruped robots and terrains.<br />
- The framework learns from expert demonstrations of both non-periodic gaits (stair traversal) and periodic gaits (flat terrain traversal) without needing robot-specific encodings.<br />
- The proposed policy is deployable on Intel i7 nuc with low-latency control outputs and does not require test-time optimization.<br />
- Strong zero-shot transfer is demonstrated across diverse robots and terrains, including successful deployment on the Unitree Go1 robot without fine-tuning.<br />
- Preliminary experiments on the Stoch 5 robot show walking on flat and outdoor terrains, showcasing robust generalist locomotion capabilities. <br /> <div>
arXiv:2505.10973v2 Announce Type: replace-cross 
Abstract: Recent advancements in large-scale offline training have demonstrated the potential of generalist policy learning for complex robotic tasks. However, applying these principles to legged locomotion remains a challenge due to continuous dynamics and the need for real-time adaptation across diverse terrains and robot morphologies. In this work, we propose GRoQ-Loco, a scalable, attention-based framework that learns a single generalist locomotion policy across multiple quadruped robots and terrains, relying solely on offline datasets. Our approach leverages expert demonstrations from two distinct locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain traversal (periodic gaits) - collected across multiple quadruped robots, to train a generalist model that enables behavior fusion for both behaviors. Crucially, our framework operates directly on proprioceptive data from all robots without incorporating any robot-specific encodings. The policy is directly deployable on an Intel i7 nuc, producing low-latency control outputs without any test-time optimization. Our extensive experiments demonstrate strong zero-shot transfer across highly diverse quadruped robots and terrains, including hardware deployment on the Unitree Go1, a commercially available 12kg robot. Notably, we evaluate challenging cross-robot training setups where different locomotion skills are unevenly distributed across robots, yet observe successful transfer of both flat walking and stair traversal behaviors to all robots at test time. We also show preliminary walking on Stoch 5, a 70kg quadruped, on flat and outdoor terrains without requiring any fine tuning. These results highlight the potential for robust generalist locomotion across diverse robots and terrains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2505.11192</link>
<guid>https://arxiv.org/abs/2505.11192</guid>
<content:encoded><![CDATA[
<div> supervised learning, vision-language pretraining, false negatives, hard negative sampling, cross-modal alignment improvement  
Summary:  
FALCON addresses false negatives in vision-language pretraining by dynamically selecting negative samples based on cross-modal alignment improvement. The many-to-many correspondence between images and texts in large datasets often leads to conflicting signals and degraded embedding spaces. FALCON's negative mining scheduler balances the trade-off between hard and false negatives during mini-batch construction. Experimental results show improved performance in ALBEF and BLIP-2 frameworks, as well as various downstream tasks. FALCON's adaptive approach significantly mitigates the impact of false negatives, enhancing the effectiveness and robustness of VLP models.   <div>
arXiv:2505.11192v3 Announce Type: replace-cross 
Abstract: False negatives pose a critical challenge in vision-language pretraining (VLP) due to the many-to-many correspondence between images and texts in large-scale datasets. These false negatives introduce conflicting supervision signals that degrade the learned embedding space and diminish the effectiveness of hard negative sampling. In this paper, we propose FALCON (False-negative Aware Learning of COntrastive Negatives), a learning-based mini-batch construction strategy that adaptively balances the trade-off between hard and false negatives during VLP. Rather than relying on fixed heuristics, FALCON employs a negative mining scheduler that dynamically selects negative samples of appropriate hardness for each anchor instance during mini-batch construction, guided by a proxy for cross-modal alignment improvement. Experimental results demonstrate that FALCON significantly improves performance across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of downstream tasks and evaluation settings, underscoring its effectiveness and robustness in mitigating the impact of false negatives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs</title>
<link>https://arxiv.org/abs/2505.11275</link>
<guid>https://arxiv.org/abs/2505.11275</guid>
<content:encoded><![CDATA[
<div> benchmark, MLLMs, Traditional Chinese Culture, Visual Question Answering, cultural understanding <br />
Summary:<br />
The article introduces the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a bilingual Visual Question Answering (VQA) benchmark aimed at evaluating the understanding of traditional Chinese culture by Multimodal Large Language Models (MLLMs). TCC-Bench includes culturally rich and visually diverse data, incorporating images from various cultural contexts. A semi-automated pipeline involving GPT-4o and human curation is used to generate and ensure data quality. The benchmark prevents language bias by not directly revealing cultural concepts in question texts. Experimental evaluations reveal that current MLLMs struggle with reasoning about culturally grounded visual content, indicating the need for further research on culturally inclusive and context-aware multimodal systems. The code and data for TCC-Bench are available at the provided link. <br /> <div>
arXiv:2505.11275v3 Announce Type: replace-cross 
Abstract: Recent progress in Multimodal Large Language Models (MLLMs) have significantly enhanced the ability of artificial intelligence systems to understand and generate multimodal content. However, these models often exhibit limited effectiveness when applied to non-Western cultural contexts, which raises concerns about their wider applicability. To address this limitation, we propose the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a bilingual (i.e., Chinese and English) Visual Question Answering (VQA) benchmark specifically designed for assessing the understanding of traditional Chinese culture by MLLMs. TCC-Bench comprises culturally rich and visually diverse data, incorporating images from museum artifacts, everyday life scenes, comics, and other culturally significant contexts. We adopt a semi-automated pipeline that utilizes GPT-4o in text-only mode to generate candidate questions, followed by human curation to ensure data quality and avoid potential data leakage. The benchmark also avoids language bias by preventing direct disclosure of cultural concepts within question texts. Experimental evaluations across a wide range of MLLMs demonstrate that current models still face significant challenges when reasoning about culturally grounded visual content. The results highlight the need for further research in developing culturally inclusive and context-aware multimodal systems. The code and data can be found at: https://tcc-bench.github.io/.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data</title>
<link>https://arxiv.org/abs/2505.13466</link>
<guid>https://arxiv.org/abs/2505.13466</guid>
<content:encoded><![CDATA[
<div> Framework, synthetic data, safety-critical applications, multi-agent collaboration, LLM<br />
<br />
Summary: 
The article addresses the challenge of data scarcity in training AI systems for safety-critical applications, such as construction safety, by proposing a novel multi-agent framework. This framework involves collaboration between an Evaluator Agent, based on LLM, and an Editor Agent to generate synthetic scenes with semantic depth and safety-specific constraints. The iterative process ensures consistency and realism in the generated scenes, overcoming the limitations of existing methods. Experiments demonstrate the effectiveness of this approach in producing visually realistic and safety-compliant simulations for multimedia safety applications. The collaborative design leverages LLM's reasoning and common-sense knowledge to bridge the gap in data availability for training AI systems in safety-critical scenarios. This framework offers a promising solution to the challenge of generating synthetic data tailored to specific safety requirements while maintaining visual semantics. <div>
arXiv:2505.13466v1 Announce Type: new 
Abstract: The scarcity of data depicting dangerous situations presents a major obstacle to training AI systems for safety-critical applications, such as construction safety, where ethical and logistical barriers hinder real-world data collection. This creates an urgent need for an end-to-end framework to generate synthetic data that can bridge this gap. While existing methods can produce synthetic scenes, they often lack the semantic depth required for scene simulations, limiting their effectiveness. To address this, we propose a novel multi-agent framework that employs an iterative, in-the-loop collaboration between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce semantic consistency and safety-specific constraints, and an Editor Agent, which generates and refines scenes based on this guidance. Powered by LLM's capabilities to reasoning and common-sense knowledge, this collaborative design produces synthetic images tailored to safety-critical scenarios. Our experiments suggest this design can generate useful scenes based on realistic specifications that address the shortcomings of prior approaches, balancing safety requirements with visual semantics. This iterative process holds promise for delivering robust, aesthetically sound simulations, offering a potential solution to the data scarcity challenge in multimedia safety applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Real-World Engineering Tasks</title>
<link>https://arxiv.org/abs/2505.13484</link>
<guid>https://arxiv.org/abs/2505.13484</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, engineering tasks, evaluation, engineering competencies, complex problems<br />
Summary:<br />
The paper addresses the limitations in evaluating Large Language Models (LLMs) in engineering tasks, pointing out the reliance on simplified use cases and ad hoc scenarios. A curated database of over 100 questions from real engineering scenarios is introduced to test LLMs' performance. The evaluation of four state-of-the-art LLMs shows strengths in basic temporal and structural reasoning but struggles with abstract reasoning, formal modeling, and context-sensitive logic. This indicates that while LLMs excel in some engineering competencies such as product design and diagnosis, they fall short in areas requiring deeper levels of reasoning and understanding. The study highlights the need for further research and development to improve LLMs' performance on complex engineering problems.<br /><br />Summary: <div>
arXiv:2505.13484v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transformative not only for daily activities but also for engineering tasks. However, current evaluations of LLMs in engineering exhibit two critical shortcomings: (i) the reliance on simplified use cases, often adapted from examination materials where correctness is easily verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture critical engineering competencies. Consequently, the assessment of LLMs on complex, real-world engineering problems remains largely unexplored. This paper addresses this gap by introducing a curated database comprising over 100 questions derived from authentic, production-oriented engineering scenarios, systematically designed to cover core competencies such as product design, prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art LLMs, including both cloud-based and locally hosted instances, to systematically investigate their performance on complex engineering tasks. Our results show that LLMs demonstrate strengths in basic temporal and structural reasoning but struggle significantly with abstract reasoning, formal modeling, and context-sensitive engineering logic.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer</title>
<link>https://arxiv.org/abs/2505.13489</link>
<guid>https://arxiv.org/abs/2505.13489</guid>
<content:encoded><![CDATA[
<div> Knowledge tracing, TransKT, concept graph, Large Language Model, Graph Convolutional Networks<br />
Summary:<br />
TransKT is a novel contrastive cross-course knowledge tracing method that enhances knowledge state estimation by leveraging concept graph guided knowledge transfer. It utilizes a cross-course concept graph constructed with zero-shot Large Language Model prompts to model relationships between learning behaviors across different courses. TransKT incorporates an LLM-to-LM pipeline for improved feature integration and employs Graph Convolutional Networks for knowledge transfer. The model also includes a contrastive objective aligning single-course and cross-course knowledge states to provide a more robust representation of learners' overall knowledge states. <div>
arXiv:2505.13489v1 Announce Type: new 
Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on historical learning interactions. However, existing KT models predominantly focus on data from a single course, limiting their ability to capture a comprehensive understanding of learners' knowledge states. In this paper, we propose TransKT, a contrastive cross-course knowledge tracing method that leverages concept graph guided knowledge transfer to model the relationships between learning behaviors across different courses, thereby enhancing knowledge state estimation. Specifically, TransKT constructs a cross-course concept graph by leveraging zero-shot Large Language Model (LLM) prompts to establish implicit links between related concepts across different courses. This graph serves as the foundation for knowledge transfer, enabling the model to integrate and enhance the semantic features of learners' interactions across courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating summarized semantic features, which significantly improves the performance of Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally, TransKT employs a contrastive objective that aligns single-course and cross-course knowledge states, thereby refining the model's ability to provide a more robust and accurate representation of learners' overall knowledge states.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model</title>
<link>https://arxiv.org/abs/2505.13496</link>
<guid>https://arxiv.org/abs/2505.13496</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, log data, unsupervised learning, transformer-based model, adaptive thresholding

Summary: 
ADALog is a novel anomaly detection framework designed for modern software systems with complex log data. Unlike traditional methods, ADALog operates on individual unstructured logs, extracting contextual relationships and performing adaptive thresholding on normal data. The framework utilizes a transformer-based encoder pretrained with a masked language modeling task, fine-tuned on normal logs to capture domain-specific patterns. Anomalies are identified based on token-level reconstruction probabilities and log-level scores, with adaptive percentile-based thresholding. The model dynamically adapts to evolving system behaviors, avoiding heuristic-based thresholds. Evaluations on benchmark datasets show strong generalization and competitive performance compared to existing methods. Ablation studies explore the impact of masking, fine-tuning, and token positioning on model behavior and interpretability.

<br /><br />Summary: ADALog is an adaptive, unsupervised anomaly detection framework for heterogeneous log data in software systems. It utilizes a transformer-based model trained on normal logs to capture domain-specific patterns and adaptively detect anomalies. The framework shows strong generalization and competitive performance on benchmark datasets, highlighting its practical applicability in diverse real-world environments. Additional studies investigate the effects of different factors on model behavior and interpretability, further demonstrating the effectiveness of ADALog in handling complex log data. <div>
arXiv:2505.13496v1 Announce Type: new 
Abstract: Modern software systems generate extensive heterogeneous log data with dynamic formats, fragmented event sequences, and varying temporal patterns, making anomaly detection both crucial and challenging. To address these complexities, we propose ADALog, an adaptive, unsupervised anomaly detection framework designed for practical applicability across diverse real-world environments. Unlike traditional methods reliant on log parsing, strict sequence dependencies, or labeled data, ADALog operates on individual unstructured logs, extracts intra-log contextual relationships, and performs adaptive thresholding on normal data. The proposed approach utilizes a transformer-based, pretrained bidirectional encoder with a masked language modeling task, fine-tuned on normal logs to capture domain-specific syntactic and semantic patterns essential for accurate anomaly detection. Anomalies are identified via token-level reconstruction probabilities, aggregated into log-level scores, with adaptive percentile-based thresholding calibrated only on normal data. This allows the model to dynamically adapt to evolving system behaviors while avoiding rigid, heuristic-based thresholds common in traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird, and Spirit, showing strong generalization and competitive performance compared to state-of-the-art supervised and unsupervised methods. Additional ablation studies examine the effects of masking, fine-tuning, and token positioning on model behavior and interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale</title>
<link>https://arxiv.org/abs/2505.13511</link>
<guid>https://arxiv.org/abs/2505.13511</guid>
<content:encoded><![CDATA[
<div> Benchmark, Large Language Models, Freelance programming, Data analysis, Evaluation

Summary:
Four Large Language Models (LLMs) were evaluated on a new benchmark for freelance programming and data analysis tasks. The benchmark was constructed using synthetic tasks from a Kaggle dataset, with standardized USD prices. LLMs were assessed based on task success rate, test-case pass rate, and total earnings from solved tasks. Claude 3.5 Haiku performed the best, earning $1.52 million, followed by GPT-4o-mini at $1.49 million, Qwen 2.5 at $1.33 million, and Mistral at $0.70 million. Strong models solved more tasks with fewer errors, highlighting their potential as autonomous freelance agents. The study discusses the feasibility of AI as freelance developers, the benefits and limitations of the benchmark approach, and the disparity between structured task performance and real-world job complexity. 

<br /><br />Summary: <div>
arXiv:2505.13511v1 Announce Type: new 
Abstract: This study explores Large Language Models (LLMs) as autonomous agents for real-world tasks, including freelance software development. This work presents a new benchmark that evaluates LLMs on freelance programming and data analysis tasks derived from economic data. We construct the benchmark using synthetic tasks created from a Kaggle Freelancer dataset of job postings, with all job prices standardized to USD (median fixed-project price around $250, and an average of $306). Each task is accompanied by structured input-output test cases and an estimated price tag, enabling automated correctness checking and a monetary performance valuation. This approach is inspired by OpenAI's recent SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our framework simplifies evaluation using programmatically testable tasks and predicted price values, making it highly scalable and repeatable. On this benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen 2.5, and Mistral. We report each model's accuracy (task success rate and test-case pass rate) and the total "freelance earnings" it achieves (sum of prices of solved tasks). Our results show that Claude 3.5 Haiku performs best, earning approximately $1.52 million USD, followed closely by GPT-4o-mini at $1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the distribution of errors per task and observe that the strongest models solve the most tasks and rarely fail completely on any project. We discuss the implications of these results for the feasibility of AI as a freelance developer, the advantages and limitations of our automated benchmark approach, and the gap between performance on structured tasks versus the true complexity of real-world freelance jobs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem</title>
<link>https://arxiv.org/abs/2505.13522</link>
<guid>https://arxiv.org/abs/2505.13522</guid>
<content:encoded><![CDATA[
<div> Maritime Inventory Routing Problem, MIRP, heuristics, benchmark instances, solution improvement 
Summary: 
The Maritime Inventory Routing Problem (MIRP) is essential for global maritime commerce integration but lacks efficient solving methodologies due to its high complexity. Exact methods based on Mixed Integer Programming (MIP) are impractical for daily operations, while non-MIP heuristics face challenges in providing effective initial solutions. This study introduces a heuristic approach for solving a single-product MIRP without relying on mathematical optimization techniques. By combining a variation of Beam Search algorithm with Iterated Local Search, the methodology improves upon the best-known solutions for ten instances out of 72 tested, all within acceptable CPU time limits. The result encourages the use of MIRPLib for benchmarking and offers a practical solution to a deterministic, finite-horizon, single-product MIRP. <br /><br />Summary: <div>
arXiv:2505.13522v1 Announce Type: new 
Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the integration of global maritime commerce levels. However, there are still no well-established methodologies capable of efficiently solving large MIRP instances or their variants due to the high complexity of the problem. The adoption of exact methods, typically based on Mixed Integer Programming (MIP), for daily operations is nearly impractical due to the CPU time required, as planning must be executed multiple times while ensuring high-quality results within acceptable time limits. Non-MIP-based heuristics are less frequently applied due to the highly constrained nature of the problem, which makes even the construction of an effective initial solution challenging. Papageorgiou et al. (2014) introduced a single-product MIRP as the foundation for MIRPLib, aiming to provide a collection of publicly available benchmark instances. However, only a few studies that propose new methodologies have been published since then. To encourage the use of MIRPLib and facilitate result comparisons, this study presents a heuristic approach that does not rely on mathematical optimization techniques to solve a deterministic, finite-horizon, single-product MIRP. The proposed heuristic combines a variation of a Beam Search algorithm with an Iterated Local Search procedure. Among the 72 instances tested, the developed methodology can improve the best-known solution for ten instances within an acceptable CPU time.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs</title>
<link>https://arxiv.org/abs/2505.13529</link>
<guid>https://arxiv.org/abs/2505.13529</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, overconfident answers, factual reliability, BARREL framework, DeepSeek-R1-Distill-Llama-8B

Summary:<br /><br />
Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning, but they often lack the ability to admit ignorance or respond with "I don't know". This leads to incorrect answers being generated with undue confidence, raising concerns about their factual reliability. Two pathological reasoning patterns, last-minute guessing and second-thought spiraling, contribute to these issues. To address this, the BARREL framework is proposed, promoting concise and boundary-aware factual reasoning. Experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B while maintaining comparable accuracy. This pilot study shows promise in developing more reliable and factual System 2 LRMs. <div>
arXiv:2505.13529v1 Announce Type: new 
Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs</title>
<link>https://arxiv.org/abs/2505.13533</link>
<guid>https://arxiv.org/abs/2505.13533</guid>
<content:encoded><![CDATA[
<div> benchmark, financial, language models, tasks, evaluation 
Summary:<br /> 
- The article introduces FinMaster, a financial benchmark designed to evaluate large language models (LLMs) in various financial tasks. 
- FinMaster includes three main modules: FinSim for generating synthetic financial data, FinSuite for providing tasks in core financial domains, and FinEval for unified evaluation. 
- Experiments on state-of-the-art LLMs using FinMaster reveal critical capability gaps in financial reasoning. 
- Accuracy drops from over 90% on basic tasks to 40% on complex scenarios requiring multi-step reasoning. 
- Computational errors propagate, with accuracy decreasing from 58% in single-metric calculations to 37% in multimetric scenarios. 
Summary: <div>
arXiv:2505.13533v1 Announce Type: new 
Abstract: Financial tasks are pivotal to global economic stability; however, their execution faces challenges including labor intensive processes, low error tolerance, data fragmentation, and tool limitations. Although large language models (LLMs) have succeeded in various natural language processing tasks and have shown potential in automating workflows through reasoning and contextual understanding, current benchmarks for evaluating LLMs in finance lack sufficient domain-specific data, have simplistic task design, and incomplete evaluation frameworks. To address these gaps, this article presents FinMaster, a comprehensive financial benchmark designed to systematically assess the capabilities of LLM in financial literacy, accounting, auditing, and consulting. Specifically, FinMaster comprises three main modules: i) FinSim, which builds simulators that generate synthetic, privacy-compliant financial data for companies to replicate market dynamics; ii) FinSuite, which provides tasks in core financial domains, spanning 183 tasks of various types and difficulty levels; and iii) FinEval, which develops a unified interface for evaluation. Extensive experiments over state-of-the-art LLMs reveal critical capability gaps in financial reasoning, with accuracy dropping from over 90% on basic tasks to merely 40% on complex scenarios requiring multi-step reasoning. This degradation exhibits the propagation of computational errors, where single-metric calculations initially demonstrating 58% accuracy decreased to 37% in multimetric scenarios. To the best of our knowledge, FinMaster is the first benchmark that covers full-pipeline financial workflows with challenging tasks. We hope that FinMaster can bridge the gap between research and industry practitioners, driving the adoption of LLMs in real-world financial practices to enhance efficiency and accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems</title>
<link>https://arxiv.org/abs/2505.13546</link>
<guid>https://arxiv.org/abs/2505.13546</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt generation, multi-agent systems, large language models, semantic stability, system-level performance

Summary:
Automatic prompt generation is essential for enabling general-purpose multi-agent systems to perform tasks autonomously. Existing methods focus on immediate task performance, overlooking the importance of prompt stability. This work introduces semantic stability to assess the consistency of model responses and fine-tunes an evaluator to measure it automatically. By considering prompt stability, a stability-aware prompt generation system is developed to enhance prompt quality and system-level performance iteratively. The analysis shows prompt stability as a necessary condition for effective task success. Empirical results across various tasks demonstrate that the stability-aware framework improves accuracy and output consistency. By emphasizing persistent reliability in prompt design, this work offers a new perspective on building trustworthy general-purpose systems.<br /><br />Summary: Automatic prompt generation is crucial for multi-agent system autonomy. Prompt stability, assessed through semantic stability, plays a key role in improving system-level performance and task success. The stability-aware framework enhances both accuracy and output consistency, offering practical tools for building reliable general-purpose systems. <div>
arXiv:2505.13546v1 Announce Type: new 
Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose multi-agent systems to perform diverse tasks autonomously. Existing methods typically evaluate prompts based on their immediate task performance, overlooking the intrinsic qualities that determine their reliability. This outcome-centric view not only limits interpretability but also fails to account for the inherent stochasticity of large language models (LLMs). In this work, we bring attention to prompt stability-the consistency of model responses across repeated executions-as a key factor for building robust and effective prompt generation systems. To quantify this, we propose semantic stability as a criterion for assessing the response consistency of prompts, and fine-tune a LLaMA-based evaluator to measure it automatically across tasks. These components have enabled us to develop the first stability-aware general-purpose prompt generation system that leverages stability feedback to iteratively enhance both prompt quality and system-level performance. Furthermore, we establish a logical chain between prompt stability and task success by analyzing the structural dependencies within our system, proving stability as a necessary condition for effective system-level execution. Empirical results across general and domain-specific tasks demonstrate that our stability-aware framework improves both accuracy and output consistency. By shifting the focus from one-off results to persistent reliability, our work offers a new perspective on prompt design and contributes practical tools for building more trustworthy general-purpose systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counter-Inferential Behavior in Natural and Artificial Cognitive Systems</title>
<link>https://arxiv.org/abs/2505.13551</link>
<guid>https://arxiv.org/abs/2505.13551</guid>
<content:encoded><![CDATA[
<div> Keywords: counter-inferential behavior, cognitive systems, epistemic rigidity, maladaptive stability, cognitive vulnerability

Summary: 
This study delves into the phenomenon of counter-inferential behavior in natural and artificial cognitive systems. It explores how agents may exhibit patterns of misattributing empirical success or suppressing adaptation, resulting in epistemic rigidity or maladaptive stability. The analysis focuses on scenarios such as reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. Rather than being due to noise or flawed design, these behaviors emerge from intricate interactions between internal information models, empirical feedback, and higher-order evaluation mechanisms. Drawing on evidence from artificial systems, biological cognition, human psychology, and social dynamics, the study identifies counter-inferential behavior as a general cognitive vulnerability that can manifest even in well-adapted systems. Overall, the findings underscore the significance of maintaining minimal adaptive activation in stable conditions and propose design principles for cognitive architectures that can withstand rigidity under informational stress. 

<br /><br />Summary: <div>
arXiv:2505.13551v1 Announce Type: new 
Abstract: This study explores the emergence of counter-inferential behavior in natural and artificial cognitive systems, that is, patterns in which agents misattribute empirical success or suppress adaptation, leading to epistemic rigidity or maladaptive stability. We analyze archetypal scenarios in which such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. Rather than arising from noise or flawed design, these behaviors emerge through structured interactions between internal information models, empirical feedback, and higher-order evaluation mechanisms. Drawing on evidence from artificial systems, biological cognition, human psychology, and social dynamics, we identify counter-inferential behavior as a general cognitive vulnerability that can manifest even in otherwise well-adapted systems. The findings highlight the importance of preserving minimal adaptive activation under stable conditions and suggest design principles for cognitive architectures that can resist rigidity under informational stress.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Thought: The View from LLMs</title>
<link>https://arxiv.org/abs/2505.13561</link>
<guid>https://arxiv.org/abs/2505.13561</guid>
<content:encoded><![CDATA[
<div> Keywords: Daniel Dennett, language, AI systems, inferential reasoning, Large Language Models<br />
Summary: 
Recent work in AI is testing Daniel Dennett's speculation about the impact of language on the mind. The success of Large Language Models (LLMs) at inferential reasoning indicates that linguistic training enhances cognitive abilities. LLMs can effectively perform inferences across various domains due to the abstract and efficient encoding provided by language. This suggests that language makes inference computationally tractable. The findings in AI highlight the critical role of language in our own biological minds and how it influences our thinking processes. <div>
arXiv:2505.13561v1 Announce Type: new 
Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind you get when you add language to it is so different from the kind of mind you can have without language that calling them both minds is a mistake." Recent work in AI can be seen as testing Dennett's thesis by exploring the performance of AI systems with and without linguistic training. I argue that the success of Large Language Models at inferential reasoning, limited though it may be, supports Dennett's radical view about the effect of language on thought. I suggest it is the abstractness and efficiency of linguistic encoding that lies behind the capacity of LLMs to perform inferences across a wide range of domains. In a slogan, language makes inference computationally tractable. I assess what these results in AI indicate about the role of language in the workings of our own biological minds.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAFA: A multi-agent framework for annotation</title>
<link>https://arxiv.org/abs/2505.13668</link>
<guid>https://arxiv.org/abs/2505.13668</guid>
<content:encoded><![CDATA[
<div> framework, FAQ annotation, multi-agent, structured reasoning, few-shot example<br />
<br />
Summary: 
The paper introduces a multi-agent framework for FAQ annotation that utilizes different specialized agents and a judge agent for optimal results. Inspired by Attentive Reasoning Queries, the agents use structured reasoning with targeted JSON queries. Each agent employs a few-shot example strategy to enhance diversity and coverage. The framework, evaluated on real-world and public datasets, shows significant improvements over traditional single-agent approaches. Results include a 14% increase in Top-1 accuracy, 18% in Top-5 accuracy, and 12% in Mean Reciprocal Rank on the dataset. Similar gains were observed on public benchmarks. Particularly effective in handling ambiguous queries, the framework demonstrates strong generalization across domains and languages, making it suitable for deployment in production applications. <div>
arXiv:2505.13668v1 Announce Type: new 
Abstract: Modern applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a specialized few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world banking dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional single agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production applications while showing strong generalization capabilities across different domains and languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A*-Decoding: Token-Efficient Inference Scaling</title>
<link>https://arxiv.org/abs/2505.13672</link>
<guid>https://arxiv.org/abs/2505.13672</guid>
<content:encoded><![CDATA[
<div> Keywords: inference-time scaling, A*-decoding, language model, search algorithm, reasoning tasks 

Summary: 
A*-decoding is introduced as an inference-time strategy that optimizes the use of a fixed compute budget by prioritizing high-quality reasoning paths during generation. This approach applies the A* search algorithm to language model decoding, treating it as a structured search in a state space of partial solutions. Experimental results show that A*-decoding achieves performance levels comparable to existing strong inference scaling methods while using fewer tokens and passes. In tests on benchmarks like MATH500 and AIME 2024, A*-decoding enables smaller language models to match the performance of much larger models and improve reasoning accuracy. This highlights the effectiveness of structured search in decoding, offering an efficient alternative to brute-force sampling and scale-driven gains. The study underscores the importance of thoughtful inference-time strategies in enhancing reasoning in large-scale language models for more efficient and scalable deployment. 

<br /><br />Summary: <div>
arXiv:2505.13672v1 Announce Type: new 
Abstract: Inference-time scaling has emerged as a powerful alternative to parameter scaling for improving language model performance on complex reasoning tasks. While existing methods have shown strong performance gains under fixed compute budgets, there has been little focus on optimally utilizing that budget during inference. In this work, we introduce A*-decoding, a search-based inference-time strategy that builds on the A* search algorithm to optimally utilize a fixed compute budget by prioritizing high-quality reasoning paths during generation. We frame language model decoding as a structured search in a state space of partial solutions, applying the A* transition model to identify promising continuations guided by an external process supervision signal. In our experiments, A*-decoding reaches the performance levels of strong inference scaling baselines like best-of-N and particle filtering while using up to 3x fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the power of structured search in decoding, offering an alternative to brute-force sampling or scale-driven gains. Our work demonstrates how thoughtful inference-time strategies can enhance reasoning in SLMs, pointing toward future advances in more efficient and scalable language model deployment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building spatial world models from sparse transitional episodic memories</title>
<link>https://arxiv.org/abs/2505.13696</link>
<guid>https://arxiv.org/abs/2505.13696</guid>
<content:encoded><![CDATA[
<div> Sparse episodic memories, spatial world model, neural network, exploration, navigation  
Summary:  
- Animals build mental models of their surroundings rapidly to support behaviors like navigation and planning.  
- Forming episodic memories and making inferences are key for efficient world model construction in the brain.  
- The Episodic Spatial World Model (ESWM) framework allows a neural network to construct spatial models from sparse memories.  
- ESWM is sample-efficient, requiring minimal observations for environment representation.  
- ESWM is adaptive, allowing for quick updates to environmental changes.  
- ESWM enables near-optimal strategies for exploration and navigation without additional training needs. <div>
arXiv:2505.13696v1 Announce Type: new 
Abstract: Many animals possess a remarkable capacity to rapidly construct flexible mental models of their environments. These world models are crucial for ethologically relevant behaviors such as navigation, exploration, and planning. The ability to form episodic memories and make inferences based on these sparse experiences is believed to underpin the efficiency and adaptability of these models in the brain. Here, we ask: Can a neural network learn to construct a spatial model of its surroundings from sparse and disjoint episodic memories? We formulate the problem in a simulated world and propose a novel framework, the Episodic Spatial World Model (ESWM), as a potential answer. We show that ESWM is highly sample-efficient, requiring minimal observations to construct a robust representation of the environment. It is also inherently adaptive, allowing for rapid updates when the environment changes. In addition, we demonstrate that ESWM readily enables near-optimal strategies for exploring novel environments and navigating between arbitrary points, all without the need for additional training.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings</title>
<link>https://arxiv.org/abs/2505.13718</link>
<guid>https://arxiv.org/abs/2505.13718</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning LLMs, two-stage training, limited supervision, sample-efficient, warmup<br />
<br />
Summary: <br />
Designing effective reasoning-capable LLMs is challenging when training data is scarce. This study proposes a two-stage training strategy for developing reasoning LLMs under limited supervision. The first stage involves warming up the model with logic puzzles to acquire general reasoning skills. In the second stage, Reinforcement Learning with Verifiable Rewards is applied to the model using a small set of target-domain examples. The experiments show that this approach offers several benefits: 1) The warmup phase improves performance across various tasks. 2) The warmed-up model outperforms the base model when trained on a small dataset. 3) Generalizability is maintained even after training on a specific domain. 4) Sample efficiency during training is enhanced. This study highlights the potential of warmup for building robust reasoning LLMs in data-scarce environments. <br /> <div>
arXiv:2505.13718v1 Announce Type: new 
Abstract: Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we "warm up" the model by distilling Long CoTs from a toy domain, namely, Knights \& Knaves (K\&amp;K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers</title>
<link>https://arxiv.org/abs/2505.13737</link>
<guid>https://arxiv.org/abs/2505.13737</guid>
<content:encoded><![CDATA[
<div> head gating, interpretability, transformer models, attention heads, causal analysis

Summary:<br />
- The study presents causal head gating (CHG) method to interpret attention heads in transformer models.
- CHG assigns causal taxonomy to heads based on their impact on task performance.
- Unlike previous methods, CHG does not require target labels and can be applied to any dataset.
- Evaluation on large language models across various tasks shows causal insights provided by CHG scores.
- The study introduces contrastive CHG to isolate sub-circuits for specific task components.
Summary: <div>
arXiv:2505.13737v1 Announce Type: new 
Abstract: We present causal head gating (CHG), a scalable method for interpreting the functional roles of attention heads in transformer models. CHG learns soft gates over heads and assigns them a causal taxonomy - facilitating, interfering, or irrelevant - based on their impact on task performance. Unlike prior approaches in mechanistic interpretability, which are hypothesis-driven and require prompt templates or target labels, CHG applies directly to any dataset using standard next-token prediction. We evaluate CHG across multiple large language models (LLMs) in the Llama 3 model family and diverse tasks, including syntax, commonsense, and mathematical reasoning, and show that CHG scores yield causal - not merely correlational - insight, validated via ablation and causal mediation analyses. We also introduce contrastive CHG, a variant that isolates sub-circuits for specific task components. Our findings reveal that LLMs contain multiple sparse, sufficient sub-circuits, that individual head roles depend on interactions with others (low modularity), and that instruction following and in-context learning rely on separable mechanisms.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations</title>
<link>https://arxiv.org/abs/2505.13763</link>
<guid>https://arxiv.org/abs/2505.13763</guid>
<content:encoded><![CDATA[
<div> Neurofeedback, Large language models (LLMs), Metacognition, AI safety, Neural activations  
Summary:<br /><br />Large language models (LLMs) show potential metacognitive abilities in monitoring and reporting their internal processes, key for enhancing AI capabilities but also raising concerns about safety. A neuroscience-inspired neurofeedback paradigm was developed to assess LLMs' ability to control and report their neural activation patterns. The study found that LLMs can learn to manipulate and disclose specific activation patterns when shown sentence-label pairs corresponding to neural directions. Factors influencing performance include the number of examples, semantic interpretability, and variance explained by the neural direction. The research unveils a "metacognitive space" within LLMs, suggesting they can only monitor a subset of their neural mechanisms. These findings provide valuable insights into the metacognitive capabilities of LLMs, with significant implications for AI safety. <div>
arXiv:2505.13763v1 Announce Type: new 
Abstract: Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition -- the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society's increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a "metacognitive space" with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference</title>
<link>https://arxiv.org/abs/2505.13770</link>
<guid>https://arxiv.org/abs/2505.13770</guid>
<content:encoded><![CDATA[
<div> benchmark, causal inference, large language models, statistical pitfalls, reliability <br />
<br />
Summary: 
The article introduces the CausalPitfalls benchmark, which aims to evaluate the ability of large language models (LLMs) in handling statistical causal inference. Current benchmarks lack complexity, potentially overlooking important statistical pitfalls. CausalPitfalls offers structured challenges across difficulty levels and grading rubrics to measure models' causal reasoning capabilities. The benchmark includes two evaluation protocols: direct prompting for intrinsic causal reasoning assessment, and code-assisted prompting for generating executable code for statistical analysis. Results show significant limitations in LLMs' performance in statistical causal inference. The CausalPitfalls benchmark provides guidance and quantitative metrics to improve the development of reliable causal reasoning systems. <br /><br />Summary: <div>
arXiv:2505.13770v1 Announce Type: new 
Abstract: Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments</title>
<link>https://arxiv.org/abs/2505.13773</link>
<guid>https://arxiv.org/abs/2505.13773</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, familiarization, human-AI interaction, decision-making algorithms, team strategies

Summary:
The study compares three methods of familiarizing humans with AI teammates in a fast-paced ISR environment. Participants either read AI documentation, trained alongside the AI, or had no familiarization. The most valuable information included details of the AI's decision-making algorithms and strengths/weaknesses. Documentation-based familiarization led to fast adoption of team strategies but biased towards risk-averse behavior. Direct interaction allowed for experimentation with control modes but weaker understanding of AI processes. Individual differences in risk tolerance and AI interaction methods were observed. A recommended familiarization method combines AI documentation, structured training, and exploratory interaction. This approach leads to quicker adoption of sophisticated team strategies while allowing for risk-taking behavior and experimentation with control modes. Consideration of individual risk tolerance and interaction methods is essential for designing effective human-AI control interfaces.<br /><br />Summary: <div>
arXiv:2505.13773v1 Announce Type: new 
Abstract: We compare three methods of familiarizing a human with an artificial intelligence (AI) teammate ("agent") prior to operation in a collaborative, fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In a between-subjects user study (n=60), participants either read documentation about the agent, trained alongside the agent prior to the mission, or were given no familiarization. Results showed that the most valuable information about the agent included details of its decision-making algorithms and its relative strengths and weaknesses compared to the human. This information allowed the familiarization groups to form sophisticated team strategies more quickly than the control group. Documentation-based familiarization led to the fastest adoption of these strategies, but also biased participants towards risk-averse behavior that prevented high scores. Participants familiarized through direct interaction were able to infer much of the same information through observation, and were more willing to take risks and experiment with different control modes, but reported weaker understanding of the agent's internal processes. Significant differences were seen between individual participants' risk tolerance and methods of AI interaction, which should be considered when designing human-AI control interfaces. Based on our findings, we recommend a human-AI team familiarization method that combines AI documentation, structured in-situ training, and exploratory interaction.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.13774</link>
<guid>https://arxiv.org/abs/2505.13774</guid>
<content:encoded><![CDATA[
<div> Draft Faithfulness, Counterfactual Intervention Framework, Reasoning Models, Intermediate Steps, Experimental Findings
Summary: 
The paper introduces a systematic counterfactual intervention framework to evaluate the faithfulness of intermediate reasoning processes in Large Reasoning Models (LRMs). This framework includes assessing individual reasoning steps' influence on subsequent steps and final conclusions (Intra-Draft Faithfulness) and evaluating the logical consistency and dependency of final answers on the thinking draft (Draft-to-Answer Faithfulness). Experimental findings across six state-of-the-art LRMs reveal selective faithfulness to intermediate reasoning steps and a frequent failure to align with draft conclusions. The results highlight the importance of enhancing the faithfulness and interpretability of reasoning in advanced LRMs.<br /><br />Summary: <div>
arXiv:2505.13774v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs</title>
<link>https://arxiv.org/abs/2505.13778</link>
<guid>https://arxiv.org/abs/2505.13778</guid>
<content:encoded><![CDATA[
<div> verification framework, large language models, token count inflation, transparency, auditing

Summary:
The research introduces a verification framework called CoIn to address the transparency gap in large language models (LLMs) with hidden reasoning traces. LLMs with structured multi-step reasoning abilities are increasingly used for complex tasks but often conceal the reasoning process, leading to billing discrepancies. CoIn utilizes a hash tree to verify token counts and detects fabricated reasoning content through relevance matching. Experimental results show CoIn can detect token count inflation with up to 94.7% success rate, restoring transparency in opaque LLM services. The framework serves as a trusted third-party auditor, ensuring the quantity and validity of hidden tokens in LLM output. By providing a mechanism for auditing and verification, CoIn enhances trust and accountability in the use of LLMs for various applications. 

<br /><br />Summary: <div>
arXiv:2505.13778v1 Announce Type: new 
Abstract: As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Evaluation Policy Extraction for Ecological Modeling</title>
<link>https://arxiv.org/abs/2505.13794</link>
<guid>https://arxiv.org/abs/2505.13794</guid>
<content:encoded><![CDATA[
<div> Keywords: ecological time series, model performance evaluation, metric learning, large language models, interpretable evaluation criteria

Summary:
The article introduces a novel framework for evaluating ecological time series to improve model performance in various applications. Traditional numerical metrics often fail to capture domain-specific temporal patterns crucial for ecological processes. The proposed method integrates metric learning with large language model-based natural language policy extraction to develop interpretable evaluation criteria. By processing pairwise annotations and implementing a policy optimization mechanism, different assessment metrics are generated and combined to capture target assessment preferences. The framework was tested on datasets for crop gross primary production and carbon dioxide flux predictions, demonstrating its effectiveness in capturing assessment preferences. The method offers interpretable evaluation policies that bridge the gap between numerical metrics and expert knowledge, catering to diverse needs in ecosystem modeling studies. <div>
arXiv:2505.13794v1 Announce Type: new 
Abstract: Evaluating ecological time series is critical for benchmarking model performance in many important applications, including predicting greenhouse gas fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles. Traditional numerical metrics (e.g., R-squared, root mean square error) have been widely used to quantify the similarity between modeled and observed ecosystem variables, but they often fail to capture domain-specific temporal patterns critical to ecological processes. As a result, these methods are often accompanied by expert visual inspection, which requires substantial human labor and limits the applicability to large-scale evaluation. To address these challenges, we propose a novel framework that integrates metric learning with large language model (LLM)-based natural language policy extraction to develop interpretable evaluation criteria. The proposed method processes pairwise annotations and implements a policy optimization mechanism to generate and combine different assessment metrics. The results obtained on multiple datasets for evaluating the predictions of crop gross primary production and carbon dioxide flux have confirmed the effectiveness of the proposed method in capturing target assessment preferences, including both synthetically generated and expert-annotated model comparisons. The proposed framework bridges the gap between numerical metrics and expert knowledge while providing interpretable evaluation policies that accommodate the diverse needs of different ecosystem modeling studies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models</title>
<link>https://arxiv.org/abs/2505.13828</link>
<guid>https://arxiv.org/abs/2505.13828</guid>
<content:encoded><![CDATA[
<div> Additive manufacturing, anomaly detection, multimodal retrieval, generation-based framework, Laser Powder Bed Fusion<br />
<br />
Summary: The study presents a novel framework for automating anomaly detection in Additive Manufacturing processes using multimodal retrieval and generation-based models. By integrating text and image retrieval from scientific literature, the framework can identify, classify, and generate explanations for anomalies in a Laser Powder Bed Fusion setting without the need for extensive training datasets. Evaluation on L-PBF manufacturing datasets shows the framework's adaptability and generalizability across different printer models and materials. Comparing different multimodal language models, the GPT-4o-mini model outperforms Qwen2-VL-2B and random baseline in anomaly classification. The RAG system, incorporating retrieval mechanisms, improves accuracy by 12% and reduces the risk of hallucination. The framework can be continuously updated with new research, making it scalable and capable of zero-shot anomaly analysis in the evolving landscape of Additive Manufacturing technologies. <br /><br /> <div>
arXiv:2505.13828v1 Announce Type: new 
Abstract: Additive manufacturing enables the fabrication of complex designs while minimizing waste, but faces challenges related to defects and process anomalies. This study presents a novel multimodal Retrieval-Augmented Generation-based framework that automates anomaly detection across various Additive Manufacturing processes leveraging retrieved information from literature, including images and descriptive text, rather than training datasets. This framework integrates text and image retrieval from scientific literature and multimodal generation models to perform zero-shot anomaly identification, classification, and explanation generation in a Laser Powder Bed Fusion setting. The proposed framework is evaluated on four L-PBF manufacturing datasets from Oak Ridge National Laboratory, featuring various printer makes, models, and materials. This evaluation demonstrates the framework's adaptability and generalizability across diverse images without requiring additional training. Comparative analysis using Qwen2-VL-2B and GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini outperforms Qwen2-VL-2B and proportional random baseline in manufacturing anomalies classification. Additionally, the evaluation of the RAG system confirms that incorporating retrieval mechanisms improves average accuracy by 12% by reducing the risk of hallucination and providing additional information. The proposed framework can be continuously updated by integrating emerging research, allowing seamless adaptation to the evolving landscape of AM technologies. This scalable, automated, and zero-shot-capable framework streamlines AM anomaly analysis, enhancing efficiency and accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning</title>
<link>https://arxiv.org/abs/2505.13831</link>
<guid>https://arxiv.org/abs/2505.13831</guid>
<content:encoded><![CDATA[
<div> AI-driven framework, base station sites, network planning, TelePlanNet, multi-objective optimization
<br />
Summary: 
The article introduces TelePlanNet, an AI-driven framework designed for selecting base station sites in 5G network planning. Traditional manual methods lack efficiency and consistency, while current AI tools struggle to meet dynamic network conditions. TelePlanNet integrates a three-layer architecture, incorporating large language models for real-time user input processing, and improved group relative policy optimization reinforcement learning for planning model training. The framework effectively addresses multi-objective optimization, evaluates candidate sites, and offers practical solutions. Experimental results demonstrate TelePlanNet's ability to improve consistency to 78%, surpassing manual methods. This advancement provides telecom operators with an efficient and scalable tool for cellular network planning. 
<br /> <div>
arXiv:2505.13831v1 Announce Type: new 
Abstract: The selection of base station sites is a critical challenge in 5G network planning, which requires efficient optimization of coverage, cost, user satisfaction, and practical constraints. Traditional manual methods, reliant on human expertise, suffer from inefficiencies and are limited to an unsatisfied planning-construction consistency. Existing AI tools, despite improving efficiency in certain aspects, still struggle to meet the dynamic network conditions and multi-objective needs of telecom operators' networks. To address these challenges, we propose TelePlanNet, an AI-driven framework tailored for the selection of base station sites, integrating a three-layer architecture for efficient planning and large-scale automation. By leveraging large language models (LLMs) for real-time user input processing and intent alignment with base station planning, combined with training the planning model using the improved group relative policy optimization (GRPO) reinforcement learning, the proposed TelePlanNet can effectively address multi-objective optimization, evaluates candidate sites, and delivers practical solutions. Experiments results show that the proposed TelePlanNet can improve the consistency to 78%, which is superior to the manual methods, providing telecom operators with an efficient and scalable tool that significantly advances cellular network planning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Challenge to Build Neuro-Symbolic Video Agents</title>
<link>https://arxiv.org/abs/2505.13851</link>
<guid>https://arxiv.org/abs/2505.13851</guid>
<content:encoded><![CDATA[
<div> Keywords: video understanding, temporal reasoning, neuro-symbolic perspective, intelligent video agents, deep learning

Summary:
- The article addresses the need for proactive video agents that can interpret video streams, reason about events, and take informed actions.
- Deep learning models excel at recognizing patterns within frames or short clips but struggle with understanding the sequencing and dependencies of events over time.
- The proposed solution involves adopting a neuro-symbolic perspective, decomposing video queries into atomic events, structuring them into coherent sequences, and validating against temporal constraints.
- By integrating autonomous video search and analysis, seamless real-world interaction, and advanced content generation, the goal is to create the next generation of intelligent video agents.
- These agents will move beyond passive perception to actively reason, predict, and act, advancing the field of video understanding.<br /><br />Summary: <div>
arXiv:2505.13851v1 Announce Type: new 
Abstract: Modern video understanding systems excel at tasks such as scene classification, object detection, and short video retrieval. However, as video analysis becomes increasingly central to real-world applications, there is a growing need for proactive video agents for the systems that not only interpret video streams but also reason about events and take informed actions. A key obstacle in this direction is temporal reasoning: while deep learning models have made remarkable progress in recognizing patterns within individual frames or short clips, they struggle to understand the sequencing and dependencies of events over time, which is critical for action-driven decision-making. Addressing this limitation demands moving beyond conventional deep learning approaches. We posit that tackling this challenge requires a neuro-symbolic perspective, where video queries are decomposed into atomic events, structured into coherent sequences, and validated against temporal constraints. Such an approach can enhance interpretability, enable structured reasoning, and provide stronger guarantees on system behavior, all key properties for advancing trustworthy video agents. To this end, we present a grand challenge to the research community: developing the next generation of intelligent video agents that integrate three core capabilities: (1) autonomous video search and analysis, (2) seamless real-world interaction, and (3) advanced content generation. By addressing these pillars, we can transition from passive perception to intelligent video agents that reason, predict, and act, pushing the boundaries of video understanding.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation</title>
<link>https://arxiv.org/abs/2505.13887</link>
<guid>https://arxiv.org/abs/2505.13887</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile device usage, automation, knowledge acquisition, video content, mobile agent performance 

Summary: 
Mobile-Agent-V is a framework designed to enhance mobile automation processes by utilizing video as a tool to inject operational knowledge efficiently. This innovative approach eliminates the need for manual intervention in knowledge acquisition, ultimately reducing effort and time. The framework is evaluated using the Mobile-Knowledge benchmark, which assesses the impact of external knowledge on mobile agent performance. Experimental findings show that Mobile-Agent-V improves performance by 36% compared to existing methods. This highlights the effortless and efficient advantages of Mobile-Agent-V in mobile automation. The exponential rise in mobile device usage has created a need for streamlined automation, and Mobile-Agent-V addresses this challenge by leveraging video content to derive knowledge. Mobile-Agent-V offers a promising solution to enhance the efficiency and effectiveness of mobile automation processes. 

<br /><br />Summary: <div>
arXiv:2505.13887v1 Announce Type: new 
Abstract: The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Agent Training for Computer Use</title>
<link>https://arxiv.org/abs/2505.13909</link>
<guid>https://arxiv.org/abs/2505.13909</guid>
<content:encoded><![CDATA[
<div> Keywords: PC Agent-E, trajectory data, agent training, human demonstrations, data quality improvement

Summary:
PC Agent-E is introduced as an agent training framework that reduces the need for large-scale human demonstrations by leveraging just 312 annotated computer use trajectories. The data quality is enhanced through the synthesis of diverse action decisions using Claude 3.7 Sonnet. This approach results in a significant 141% relative improvement in performance compared to strong existing models like Claude 3.7 Sonnet on the updated WindowsAgentArena-V2 benchmark. PC Agent-E demonstrates strong generalizability across different operating systems on OSWorld. The study suggests that with high-quality trajectory data, it is possible to stimulate strong computer use capabilities, even with a small amount of data.<br /><br />Summary: <div>
arXiv:2505.13909v1 Announce Type: new 
Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Belief Revision via Order Aggregation</title>
<link>https://arxiv.org/abs/2505.13914</link>
<guid>https://arxiv.org/abs/2505.13914</guid>
<content:encoded><![CDATA[
<div> Keywords: single-step parallel revision, rationality postulates, iterated belief revision operators, parallel change, TeamQueue aggregators

Summary:
The article discusses the extension of single-step parallel revision models to the iterated case, which has received little attention in research. Delgrande & Jin introduced rationality postulates related to this issue, lacking a unifying explanation. The authors propose a method based on TeamQueue aggregators from iterated parallel contraction to extend serial iterated belief revision operators to handle parallel change. This approach allows for the recovery of plausible properties found in literature while avoiding the incorporation of dubious ones. The method offers a principled way to address the challenges of iterated parallel revision and provides a framework for understanding constraints in this context. <div>
arXiv:2505.13914v1 Announce Type: new 
Abstract: Despite efforts to better understand the constraints that operate on single-step parallel (aka "package", "multiple") revision, very little work has been carried out on how to extend the model to the iterated case. A recent paper by Delgrande & Jin outlines a range of relevant rationality postulates. While many of these are plausible, they lack an underlying unifying explanation. We draw on recent work on iterated parallel contraction to offer a general method for extending serial iterated belief revision operators to handle parallel change. This method, based on a family of order aggregators known as TeamQueue aggregators, provides a principled way to recover the independently plausible properties that can be found in the literature, without yielding the more dubious ones.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery</title>
<link>https://arxiv.org/abs/2505.13940</link>
<guid>https://arxiv.org/abs/2505.13940</guid>
<content:encoded><![CDATA[
<div> large-scale language models, drug discovery, parameterized reasoning, multi-modal data analysis, automated planning

Summary:
Drug discovery is a complex process that can benefit from the use of large-scale language models (LLMs) like DrugPilot. DrugPilot is an agent system that utilizes parameterized reasoning to address challenges in predicting results of computational tasks in drug discovery. It supports various stages of the drug discovery pipeline and can handle multi-modal drug data analysis efficiently. The interactive parameterized memory pool standardizes real-world drug data for better knowledge retrieval in multi-turn dialogues. DrugPilot has been tested on an instruction dataset for drug discovery tasks, outperforming existing agents like ReAct and LoT. It achieved high task completion rates on simple, multiple, and multi-turn tasks, showcasing its advanced tool calling capabilities in the field of drug discovery. <div>
arXiv:2505.13940v1 Announce Type: new 
Abstract: In the field of AI4Science, large-scale language models (LLMs) show great potential to parse complex scientific semantics, integrate cross-disciplinary knowledge, and assist critical task research. However, in the field of drug discovery, despite the optimization through professional data pre-training, context window expansion, and internet search, the existing LLMs are still facing challenges such as massive multi-modal and heterogeneous data processing, domain knowledge dynamic updating delay, and insufficient confidence in predicting the results of complex computational tasks. To address these challenges, we propose the DrugPilot, an LLM-based agent with parameterized reasoning for drug discovery. DrugPilot addresses key limitations of traditional end-to-end LLM prediction approaches through its parametric inference architecture. This agent system supports major phases of the drug discovery pipeline, facilitating automated planning and execution of multi-stage research tasks. To address the critical challenge of multi-modal drug data analysis (incorporating both public datasets and user-submitted data), we developed an interactive parameterized memory pool. This innovative component standardizes real-world drug data into parametric representations, simultaneously enabling efficient knowledge retrieval in multi-turn dialogue while mitigating the information loss inherent in text-based data transmission. Additionally, we created a drug instruct dataset across 8 essential drug discovery tasks for model fine-tuning and evaluation. Based on the Berkeley function calling evaluation framework, DrugPilot demonstrated the most advanced tool calling capabilities on our drug discovery tool instruction dataset, outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and multi-turn tasks, respectively.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Instruction Bottleneck Tuning</title>
<link>https://arxiv.org/abs/2505.13946</link>
<guid>https://arxiv.org/abs/2505.13946</guid>
<content:encoded><![CDATA[
<div> Variational Inference, Information Bottleneck, Representation Learning, Multimodal Large Language Models, Robustness<br />
Summary: 
The article introduces a method called Visual Instruction Bottleneck Tuning (Vittle) aimed at improving the robustness of Multimodal Large Language Models (MLLMs) under distribution shifts. Inspired by the information bottleneck principle, Vittle helps MLLMs learn a minimal sufficient representation to enhance performance. The authors derive a variational lower bound of the Information Bottleneck for MLLMs and provide a theoretical justification for Vittle's effectiveness in improving robustness. Empirical validation on various tasks and datasets shows that Vittle consistently enhances MLLMs' performance under different shift scenarios by focusing on learning essential representations. <div>
arXiv:2505.13946v1 Announce Type: new 
Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the robustness of MLLMs under distribution shifts, from a representation learning perspective. Inspired by the information bottleneck (IB) principle, we derive a variational lower bound of the IB for MLLMs and devise a practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide a theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of three MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLM's robustness under shifts by pursuing the learning of a minimal sufficient representation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Normalized Cut Problem with Constrained Action Space</title>
<link>https://arxiv.org/abs/2505.13986</link>
<guid>https://arxiv.org/abs/2505.13986</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, combinatorial optimization, external knowledge, constrained action spaces, template instances
<br />
Summary: 
The paper introduces a novel Reinforcement Learning (RL) approach for solving combinatorial optimization problems by incorporating external knowledge through constrained action spaces. The proposed solution, called the Wedge and Ring Transformer, aims to steer graph partitions towards predefined template instances in transportation networks. By shaping the partitions in the form of Wedges and Rings, the algorithm aims to produce solutions that are closer to natural optimal partitions. This approach demonstrates the potential of integrating external knowledge to guide RL algorithms in combinatorial optimization tasks, with applicability to various domains beyond transportation networks. <div>
arXiv:2505.13986v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve combinatorial optimization problems primarily due to its ability to learn heuristics that can generalize across problem instances. However, integrating external knowledge that will steer combinatorial optimization problem solutions towards domain appropriate outcomes remains an extremely challenging task. In this paper, we propose the first RL solution that uses constrained action spaces to guide the normalized cut problem towards pre-defined template instances. Using transportation networks as an example domain, we create a Wedge and Ring Transformer that results in graph partitions that are shaped in form of Wedges and Rings and which are likely to be closer to natural optimal partitions. However, our approach is general as it is based on principles that can be generalized to other domains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning</title>
<link>https://arxiv.org/abs/2505.13994</link>
<guid>https://arxiv.org/abs/2505.13994</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, knowledge graphs, multi-agent framework, semantic graph partitioning, efficiency

Summary:
SPLIT-RAG is a novel multi-agent framework that improves efficiency and accuracy in large language models empowered with external knowledge. It addresses the inefficiencies of existing approaches by utilizing question-driven semantic graph partitioning and collaborative subgraph retrieval. The framework first creates semantic partitions in the knowledge graph and employs type-specialized subgraphs for multi-agent retrieval. By segmenting the graph based on query types, SPLIT-RAG ensures that subgraphs align with different types of queries, leading to improved efficiency. Lightweight LLM agents are assigned to relevant subgraphs for retrieval, reducing the search space. A hierarchical merging module resolves inconsistencies in answers derived from subgraphs through logical verifications. Experimental validation shows significant improvements over existing approaches.<br /><br />Summary: <div>
arXiv:2505.13994v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change</title>
<link>https://arxiv.org/abs/2505.14001</link>
<guid>https://arxiv.org/abs/2505.14001</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous systems, Probabilistic neural Lyapunov certification, Stochastic dynamical systems, VeRecycle, Neural control

Summary: 
VeRecycle introduces a framework for reclaiming guarantees in discrete-time stochastic dynamical systems. It efficiently reuses probabilistic certificates when the system dynamics deviate in a known subset of states. This approach saves computational effort and achieves competitive probabilistic guarantees in compositional neural control. The framework addresses uncertainties encountered by autonomous systems operating in the real world. It allows for transferring probabilistic certificates to new system dynamics when faced with changes beyond the modeled uncertainties. VeRecycle is the first of its kind to offer a formal method for reusing certificates in such scenarios. The theoretical justification and algorithmic implementation of VeRecycle provide a general framework for this reclaiming process, showcasing its effectiveness in various experimental evaluations. <div>
arXiv:2505.14001v1 Announce Type: new 
Abstract: Autonomous systems operating in the real world encounter a range of uncertainties. Probabilistic neural Lyapunov certification is a powerful approach to proving safety of nonlinear stochastic dynamical systems. When faced with changes beyond the modeled uncertainties, e.g., unidentified obstacles, probabilistic certificates must be transferred to the new system dynamics. However, even when the changes are localized in a known part of the state space, state-of-the-art requires complete re-certification, which is particularly costly for neural certificates. We introduce VeRecycle, the first framework to formally reclaim guarantees for discrete-time stochastic dynamical systems. VeRecycle efficiently reuses probabilistic certificates when the system dynamics deviate only in a given subset of states. We present a general theoretical justification and algorithmic implementation. Our experimental evaluation shows scenarios where VeRecycle both saves significant computational effort and achieves competitive probabilistic guarantees in compositional neural control.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2505.14020</link>
<guid>https://arxiv.org/abs/2505.14020</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Knowledge Graphs, TKG extrapolation, DiMNet, semantic evolution, TKG reasoning

Summary:
DiMNet is a novel approach for Temporal Knowledge Graph (TKG) reasoning that addresses limitations of existing methods. It incorporates a multi-span evolution strategy to capture local neighbor features and historical semantic information, facilitating internal interactions between subgraphs during the evolution process. A disentangle component dynamically separates nodes' active and stable features to control the influence of historical semantics on future evolution, maximizing the capture of semantic change patterns. Experimental results on real-world TKG datasets show that DiMNet outperforms state-of-the-art methods by up to 22.7% in Mean Reciprocal Rank (MRR). This demonstrates DiMNet's substantial performance in TKG reasoning and its potential for accurately inferring future facts based on known history. 

<br /><br />Summary: <div>
arXiv:2505.14020v1 Announce Type: new 
Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose a novel Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes' active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments conducted on four real-world TKG datasets show that DiMNet demonstrates substantial performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7% in MRR.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data</title>
<link>https://arxiv.org/abs/2505.14038</link>
<guid>https://arxiv.org/abs/2505.14038</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health risk, language models, behavior data, predictive modeling, interpretability

Summary: 
ProMind-LLM is introduced as a new approach for mental health risk assessment using large language models (LLMs) and behavior data. The method involves domain-specific pretraining of the LLM, optimization of numerical data processing, and causal reasoning for improved predictions. Evaluation on real-world datasets shows significant enhancements in prediction accuracy over general LLMs. ProMind-LLM aims to provide more reliable, interpretable, and scalable solutions for mental health care applications.<br /><br />Summary: <div>
arXiv:2505.14038v1 Announce Type: new 
Abstract: Mental health risk is a critical global public health challenge, necessitating innovative and reliable assessment methods. With the development of large language models (LLMs), they stand out to be a promising tool for explainable mental health care applications. Nevertheless, existing approaches predominantly rely on subjective textual mental records, which can be distorted by inherent mental uncertainties, leading to inconsistent and unreliable predictions. To address these limitations, this paper introduces ProMind-LLM. We investigate an innovative approach integrating objective behavior data as complementary information alongside subjective mental records for robust mental health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive pipeline that includes domain-specific pretraining to tailor the LLM for mental health contexts, a self-refine mechanism to optimize the processing of numerical behavioral data, and causal chain-of-thought reasoning to enhance the reliability and interpretability of its predictions. Evaluations of two real-world datasets, PMData and Globem, demonstrate the effectiveness of our proposed methods, achieving substantial improvements over general LLMs. We anticipate that ProMind-LLM will pave the way for more dependable, interpretable, and scalable mental health case solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Student Knowledge Modeling for Future Learning Resource Prediction</title>
<link>https://arxiv.org/abs/2505.14072</link>
<guid>https://arxiv.org/abs/2505.14072</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, education, student modeling, personalized learning, behavior prediction

Summary: 
The article discusses the challenges faced in student knowledge tracing and behavior modeling in the field of education. These challenges include limited personalization, inadequate modeling of diverse learning activities, and overlooking the interplay between knowledge acquisition and behavioral patterns. Practical limitations such as fixed-size sequence segmentation often result in the loss of vital contextual information for personalized learning. The reliance on student performance on assessed materials also limits the scope of modeling, excluding non-assessed interactions like lectures. To address these shortcomings, the Knowledge Modeling and Material Prediction (KMaP) approach is proposed. KMaP is a stateful multi-task approach that aims to personalize student representations and improve predictions of future learning resource preferences through clustering-based student profiling. Extensive experiments on real-world datasets confirm significant behavioral differences across student clusters and validate the effectiveness of the KMaP model. <br /><br />Summary: <div>
arXiv:2505.14072v1 Announce Type: new 
Abstract: Despite advances in deep learning for education, student knowledge tracing and behavior modeling face persistent challenges: limited personalization, inadequate modeling of diverse learning activities (especially non-assessed materials), and overlooking the interplay between knowledge acquisition and behavioral patterns. Practical limitations, such as fixed-size sequence segmentation, frequently lead to the loss of contextual information vital for personalized learning. Moreover, reliance on student performance on assessed materials limits the modeling scope, excluding non-assessed interactions like lectures. To overcome these shortcomings, we propose Knowledge Modeling and Material Prediction (KMaP), a stateful multi-task approach designed for personalized and simultaneous modeling of student knowledge and behavior. KMaP employs clustering-based student profiling to create personalized student representations, improving predictions of future learning resource preferences. Extensive experiments on two real-world datasets confirm significant behavioral differences across student clusters and validate the efficacy of the KMaP model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games</title>
<link>https://arxiv.org/abs/2505.14137</link>
<guid>https://arxiv.org/abs/2505.14137</guid>
<content:encoded><![CDATA[
<div> Algorithm, Patrolling games, Security games, Finite-memory strategies, Defender

Summary:
The article introduces the concept of Adversarial Patrolling games, a subclass of Security games where a Defender guards vulnerable targets by moving between locations. The focus is on constructing strategies for the Defender to minimize potential damage from an Attacker. The study explores finite-memory strategies, which have shown effectiveness in experimental comparisons. These strategies involve assigning memory values to states, but the challenge lies in determining the optimal memory assignment at each location. To address this issue, the article presents a novel iterative approach for changing memory assignments, enhancing the usability of finite-memory strategies. The algorithm developed can be integrated with any black-box strategy optimization tool. Experimental evaluations demonstrate the method's robustness in solving instances of various patrolling models. <div>
arXiv:2505.14137v1 Announce Type: new 
Abstract: Adversarial Patrolling games form a subclass of Security games where a Defender moves between locations, guarding vulnerable targets. The main algorithmic problem is constructing a strategy for the Defender that minimizes the worst damage an Attacker can cause. We focus on the class of finite-memory (also known as regular) Defender's strategies that experimentally outperformed other competing classes. A finite-memory strategy can be seen as a positional strategy on a finite set of states. Each state consists of a pair of a location and a certain integer value--called memory. Existing algorithms improve the transitional probabilities between the states but require that the available memory size itself is assigned at each location manually. Choosing the right memory assignment is a well-known open and hard problem that hinders the usability of finite-memory strategies. We solve this issue by developing a general method that iteratively changes the memory assignment. Our algorithm can be used in connection with \emph{any} black-box strategy optimization tool. We evaluate our method on various experiments and show its robustness by solving instances of various patrolling models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14140</link>
<guid>https://arxiv.org/abs/2505.14140</guid>
<content:encoded><![CDATA[
<div> navigator model, reinforcement learning, reasoning, large language models, inference

Summary:
RL-of-Thoughts (RLoT) is proposed to enhance large language model (LLM) reasoning by training a lightweight navigator model with reinforcement learning (RL) to dynamically select logic blocks and create task-specific logical structures. Five basic logic blocks are designed from a human cognition perspective to guide reasoning. Experiments across multiple benchmarks show that RLoT outperforms established inference-time techniques by up to 13.4% and makes smaller LLMs comparable to larger ones. The RL navigator demonstrates strong transferability, generalizing effectively to unseen LLMs and tasks. The open-source code for reproducibility is available at the provided link. <br /><br />Summary: <div>
arXiv:2505.14140v1 Announce Type: new 
Abstract: Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs' parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent</title>
<link>https://arxiv.org/abs/2505.14141</link>
<guid>https://arxiv.org/abs/2505.14141</guid>
<content:encoded><![CDATA[
<div> Keywords: Mobile GUI agents, task planning, execution plans, extended finite state machines (EFSMs), natural language plan

Summary:
Mobile GUI agents face challenges in task planning due to a lack of understanding of target applications, often resulting in inaccuracies and getting "lost" during task execution. To address this issue, the proposed SPlanner module utilizes extended finite state machines (EFSMs) to model mobile applications' control logs and configurations. It decomposes user instructions into primary functions modeled in EFSMs to generate an execution path. This path is then refined into a natural language plan using a language model. SPlanner demonstrated strong performance on real-world mobile benchmarks, achieving a 63.8% task success rate when paired with the Qwen2.5-VL-72B vision language model (VLM) executor, a significant improvement compared to using the VLM without planning assistance.

<br /><br />Summary: <div>
arXiv:2505.14141v1 Announce Type: new 
Abstract: Mobile GUI agents execute user commands by directly interacting with the graphical user interface (GUI) of mobile devices, demonstrating significant potential to enhance user convenience. However, these agents face considerable challenges in task planning, as they must continuously analyze the GUI and generate operation instructions step by step. This process often leads to difficulties in making accurate task plans, as GUI agents lack a deep understanding of how to effectively use the target applications, which can cause them to become "lost" during task execution. To address the task planning issue, we propose SPlanner, a plug-and-play planning module to generate execution plans that guide vision language model(VLMs) in executing tasks. The proposed planning module utilizes extended finite state machines (EFSMs) to model the control logits and configurations of mobile applications. It then decomposes a user instruction into a sequence of primary function modeled in EFSMs, and generate the execution path by traversing the EFSMs. We further refine the execution path into a natural language plan using an LLM. The final plan is concise and actionable, and effectively guides VLMs to generate interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong performance on dynamic benchmarks reflecting real-world mobile usage. On the AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point improvement compared to using Qwen2.5-VL-72B without planning assistance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition</title>
<link>https://arxiv.org/abs/2505.14143</link>
<guid>https://arxiv.org/abs/2505.14143</guid>
<content:encoded><![CDATA[
<div> Mixture of Low-Rank Experts, Multi-task Learning, Multimodal Sentiment Analysis, Multimodal Emotion Recognition, Parameter conflicts <br />
Summary: <br />
The paper introduces a novel Multi-task Learning (MTL) method called Multimodal Mixture of Low-Rank Experts (MMoLRE) for multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER). MMoLRE utilizes shared and task-specific experts to capture common and unique task characteristics, reducing parameter conflicts. Inspired by the low-rank structures in the Mixture of Experts (MoE) framework, low-rank expert networks are designed to reduce parameter and computational overhead with increasing experts. Experimental results on CMU-MOSI and CMU-MOSEI benchmarks show that MMoLRE achieves state-of-the-art performance in MSA and competitive results in MER tasks. <div>
arXiv:2505.14143v1 Announce Type: new 
Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge acquired from other tasks. The high correlation between multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER) supports their joint training. However, existing methods primarily employ hard parameter sharing, ignoring parameter conflicts caused by complex task correlations. In this paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts to distinctly model common and unique task characteristics, thereby avoiding parameter conflicts. Additionally, inspired by low-rank structures in the Mixture of Experts (MoE) framework, we design low-rank expert networks to reduce parameter and computational overhead as the number of experts increases. Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that MMoLRE achieves state-of-the-art performance on the MSA task and competitive results on the MER task.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>s3: You Don't Need That Much Data to Train a Search Agent via RL</title>
<link>https://arxiv.org/abs/2505.14146</link>
<guid>https://arxiv.org/abs/2505.14146</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented generation, large language models, reinforcement learning, external knowledge, s3 <br />
Summary: <br />
The article introduces a new framework called s3 that enhances the retrieval-augmented generation (RAG) process by decoupling the searcher from the generator. s3 focuses on training the searcher to improve generation accuracy rather than optimizing retrieval metrics alone. By utilizing a Gain Beyond RAG reward, s3 outperforms existing baselines with significantly fewer training samples. The framework shows consistent improvements in downstream performance across a variety of question-answering benchmarks, both general and medical. This approach addresses the limitations of existing methods that either prioritize search-only metrics or entangle retrieval and generation processes, leading to stronger utility and compatibility with frozen or proprietary models. <div>
arXiv:2505.14146v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14147</link>
<guid>https://arxiv.org/abs/2505.14147</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, reinforcement learning, STEM domains, problem synthesis, SHARP

Summary: 
SHARP introduces a method for training large reasoning models (LRMs) in STEM domains using reinforcement learning. It addresses the challenge of generating high-quality, diverse, and verifiable problem sets by employing a strategic approach to problem synthesis. The SHARP framework consists of three phases - Alignment, Instantiation, and Inference - ensuring the generation of challenging STEM questions with rigorous logical consistency and unambiguous answers. By leveraging a state-of-the-art LRM, SHARP enables the inference and verification of complex STEM problems, leading to improved reasoning capabilities. Through experimental evaluation on benchmark datasets, SHARP demonstrates superior performance in enhancing LRM accuracy on complex tasks, moving closer to expert-level proficiency. The contributions of SHARP include its strategy, framework design, implementation, and effectiveness in elevating LRM reasoning abilities. <br /><br />Summary: <div>
arXiv:2505.14147v1 Announce Type: new 
Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM domains is hindered by the scarcity of high-quality, diverse, and verifiable problem sets. Existing synthesis methods, such as Chain-of-Thought prompting, often generate oversimplified or uncheckable data, limiting model advancement on complex tasks. To address these challenges, we introduce SHARP, a unified approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a strategic set of self-alignment principles -- targeting graduate and Olympiad-level difficulty, rigorous logical consistency, and unambiguous, verifiable answers -- and a structured three-phase framework (Alignment, Instantiation, Inference) that ensures thematic diversity and fine-grained control over problem generation. We implement SHARP by leveraging a state-of-the-art LRM to infer and verify challenging STEM questions, then employ a reinforcement learning loop to refine the model's reasoning through verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate that SHARP-augmented training substantially outperforms existing methods, markedly improving complex reasoning accuracy and pushing LRM performance closer to expert-level proficiency. Our contributions include the SHARP strategy, framework design, end-to-end implementation, and experimental evaluation of its effectiveness in elevating LRM reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem</title>
<link>https://arxiv.org/abs/2505.14148</link>
<guid>https://arxiv.org/abs/2505.14148</guid>
<content:encoded><![CDATA[
<div> modeling, mathematical, Large Language Models, MM-Bench, MM-Agent

Summary: 
The article introduces the concept of using Large Language Models (LLMs) for real-world mathematical modeling tasks, highlighting the limitations of current approaches in rigorous model construction. The authors present MM-Bench, a benchmark of 111 problems from the Mathematical Contest in Modeling (MCM/ICM) across diverse domains. They propose MM-Agent, a framework that decomposes mathematical modeling into four stages: problem analysis, model formulation, problem solving, and report generation. Experiments show that MM-Agent outperforms baseline agents and achieved an 11.88% improvement over human expert solutions using GPT-4o. The MM-Agent assisted undergraduate teams in winning the Finalist Award in MCM/ICM 2025, demonstrating its practical effectiveness as a modeling copilot. <div>
arXiv:2505.14148v1 Announce Type: new 
Abstract: Mathematical modeling is a cornerstone of scientific discovery and engineering practice, enabling the translation of real-world problems into formal systems across domains such as physics, biology, and economics. Unlike mathematical reasoning, which assumes a predefined formulation, modeling requires open-ended problem analysis, abstraction, and principled formalization. While Large Language Models (LLMs) have shown strong reasoning capabilities, they fall short in rigorous model construction, limiting their utility in real-world problem-solving. To this end, we formalize the task of LLM-powered real-world mathematical modeling, where agents must analyze problems, construct domain-appropriate formulations, and generate complete end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111 problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the years 2000 to 2025 and across ten diverse domains such as physics, biology, and economics. To tackle this task, we propose MM-Agent, an expert-inspired framework that decomposes mathematical modeling into four stages: open-ended problem analysis, structured model formulation, computational problem solving, and report generation. Experiments on MM-Bench show that MM-Agent significantly outperforms baseline agents, achieving an 11.88\% improvement over human expert solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o. Furthermore, under official MCM/ICM protocols, MM-Agent assisted two undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among 27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a modeling copilot. Our code is available at https://github.com/usail-hkust/LLM-MM-Agent
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation</title>
<link>https://arxiv.org/abs/2505.14163</link>
<guid>https://arxiv.org/abs/2505.14163</guid>
<content:encoded><![CDATA[
<div> framework, curriculum learning, data science tasks, long-term memory, knowledge accumulation
<br />
Summary:
DSMentor is a novel inference-time optimization framework that utilizes curriculum learning to improve the performance of large language model (LLM) agents in data science tasks. By organizing tasks in order of increasing difficulty and incorporating a growing long-term memory, DSMentor guides the agent's learning progression and enhances its ability to utilize accumulated knowledge effectively. The framework, evaluated on DSEval and QRData benchmarks, shows a pass rate improvement of up to 5.2% compared to baseline agents. Additionally, DSMentor demonstrates stronger causal reasoning ability, with an 8.8% pass rate improvement on causality problems compared to GPT-4 using Program-of-Thoughts prompts. This work highlights the significance of developing strategies for knowledge accumulation and utilization during inference, resembling the human learning process, and suggests new possibilities for enhancing LLM performance through curriculum-based inference optimization.
<br /><br />Summary: <div>
arXiv:2505.14163v1 Announce Type: new 
Abstract: Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems. Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference. In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning -- a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves -- to enhance LLM agent performance in challenging data science tasks. Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing long-term memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge. We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks. Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents. Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Replanning for Improved Public Transport Routing</title>
<link>https://arxiv.org/abs/2505.14193</link>
<guid>https://arxiv.org/abs/2505.14193</guid>
<content:encoded><![CDATA[
<div> Keywords: public transport delays, dynamic replanning, push approach, pull approach, arrival time savings

Summary:
The article addresses the issue of delays in public transport and the limited existing solutions to handle them. It introduces the concept of dynamic replanning in public transport routing, proposing two solutions: a manual "pull" approach where users request replanning, and a proactive "push" approach where the server monitors and adjusts journeys. Experimental results demonstrate that the push approach outperforms the pull approach, leading to significant speedups. The study also highlights the substantial arrival time savings made possible by dynamic replanning. Overall, the framework presented in the article leverages real-time delay data to enhance system-scale dynamic replanning in public transport, offering users the opportunity to adjust their journeys efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2505.14193v1 Announce Type: new 
Abstract: Delays in public transport are common, often impacting users through prolonged travel times and missed transfers. Existing solutions for handling delays remain limited; backup plans based on historical data miss opportunities for earlier arrivals, while snapshot planning accounts for current delays but not future ones. With the growing availability of live delay data, users can adjust their journeys in real-time. However, the literature lacks a framework that fully exploits this advantage for system-scale dynamic replanning. To address this, we formalise the dynamic replanning problem in public transport routing and propose two solutions: a "pull" approach, where users manually request replanning, and a novel "push" approach, where the server proactively monitors and adjusts journeys. Our experiments show that the push approach outperforms the pull approach, achieving significant speedups. The results also reveal substantial arrival time savings enabled by dynamic replanning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedded Mean Field Reinforcement Learning for Perimeter-defense Game</title>
<link>https://arxiv.org/abs/2505.14209</link>
<guid>https://arxiv.org/abs/2505.14209</guid>
<content:encoded><![CDATA[
<div> Defense strategies, perimeter-defense game, UAVs, missile technologies, Nash equilibrium, Embedded Mean-Field Actor-Critic (EMFAC) framework.<br />
<br />
Summary: The article explores a large-scale perimeter-defense game in a three-dimensional setting, incorporating real-world factors like motion dynamics and wind fields. The study derives Nash equilibrium strategies for attackers and defenders, defines victory regions, and proposes the EMFAC framework for defense strategies. EMFAC utilizes representation learning for high-level action aggregation and features an agent-level attention mechanism for efficient decision-making. Simulations show EMFAC's effectiveness in scalability, outperforming established baselines in speed and performance. Real-world experiments further validate EMFAC's practicality, providing insights into its effectiveness in complex scenarios. <div>
arXiv:2505.14209v1 Announce Type: new 
Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile technologies, perimeter-defense game between attackers and defenders for the protection of critical regions have become increasingly complex and strategically significant across a wide range of domains. However, existing studies predominantly focus on small-scale, simplified two-dimensional scenarios, often overlooking realistic environmental perturbations, motion dynamics, and inherent heterogeneity--factors that pose substantial challenges to real-world applicability. To bridge this gap, we investigate large-scale heterogeneous perimeter-defense game in a three-dimensional setting, incorporating realistic elements such as motion dynamics and wind fields. We derive the Nash equilibrium strategies for both attackers and defenders, characterize the victory regions, and validate our theoretical findings through extensive simulations. To tackle large-scale heterogeneous control challenges in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC) framework. EMFAC leverages representation learning to enable high-level action aggregation in a mean-field manner, supporting scalable coordination among defenders. Furthermore, we introduce a lightweight agent-level attention mechanism based on reward representation, which selectively filters observations and mean-field information to enhance decision-making efficiency and accelerate convergence in large-scale tasks. Extensive simulations across varying scales demonstrate the effectiveness and adaptability of EMFAC, which outperforms established baselines in both convergence speed and overall performance. To further validate practicality, we test EMFAC in small-scale real-world experiments and conduct detailed analyses, offering deeper insights into the framework's effectiveness in complex scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14216</link>
<guid>https://arxiv.org/abs/2505.14216</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, verifiable rewards, capability, distillation, reasoning patterns

Summary: 

Reinforcement learning with verifiable rewards (RLVR) improves overall accuracy but not capability, as it focuses on easier questions at the expense of difficult ones. RLVR enhances success probabilities for easier questions but does not necessarily generate higher-quality responses. Distillation enhances accuracy by learning strong reasoning patterns but only boosts capability when new knowledge is introduced. However, distillation without new knowledge leads to improved accuracy for easy questions but hinders accuracy for difficult ones, similar to RLVR. These findings highlight the importance of understanding how RLVR and distillation impact reasoning behavior in language models. <br /><br /> <div>
arXiv:2505.14216v1 Announce Type: new 
Abstract: Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy but fails to improve capability, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR does not improve capability because it focuses on improving the accuracy of the less-difficult questions to the detriment of the accuracy of the most difficult questions, thereby leading to no improvement in capability. Second, we find that RLVR does not merely increase the success probability for the less difficult questions, but in our small model settings produces quality responses that were absent in its output distribution before training. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, we show that while distillation reliably improves accuracy by learning strong reasoning patterns, it only improves capability when new knowledge is introduced. Moreover, when distilling only with reasoning patterns and no new knowledge, the accuracy of the less-difficult questions improves to the detriment of the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in language models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Embodied AGI: A Review of Embodied AI and the Road Ahead</title>
<link>https://arxiv.org/abs/2505.14235</link>
<guid>https://arxiv.org/abs/2505.14235</guid>
<content:encoded><![CDATA[
<div> Embodied AGI, taxonomy, levels, challenges, robotic brain<br />
Summary:<br />
Artificial General Intelligence (AGI) is increasingly seen as embodied, with this paper presenting a taxonomy of Embodied AGI levels from L1 to L5. The study reviews existing research and challenges at foundational stages (L1-L2) and outlines components needed for higher-level capabilities (L3-L5). A conceptual framework for an L3+ robotic brain is proposed, offering technical insights and a basis for future exploration. The integration of robotics and AI models marks a new era of generalized embodied AI systems, presenting opportunities for advancement in AGI. <div>
arXiv:2505.14235v1 Announce Type: new 
Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently embodied. With recent advances in robotics and foundational AI models, we stand at the threshold of a new era-one marked by increasingly generalized embodied AI systems. This paper contributes to the discourse by introducing a systematic taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing research and challenges at the foundational stages (L1-L2) and outline the key components required to achieve higher-level capabilities (L3-L5). Building on these insights and existing technologies, we propose a conceptual framework for an L3+ robotic brain, offering both a technical outlook and a foundation for future exploration.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection</title>
<link>https://arxiv.org/abs/2505.14289</link>
<guid>https://arxiv.org/abs/2505.14289</guid>
<content:encoded><![CDATA[
<div> indirect prompt injection, multimodal agents, GUI, EVA, red teaming<br />
<br />
Summary: 
The article introduces EVA, a red teaming framework designed to address indirect prompt injection attacks on multimodal agents operating graphical user interfaces (GUIs). These attacks involve misleading instructions embedded in the agent's visual environment, leading to misinterpretation and potential security threats. EVA transforms the attack into a closed-loop optimization by continuously monitoring the agent's attention distribution over the GUI and adjusting adversarial cues accordingly. Unlike traditional one-shot methods, EVA dynamically adapts to attention hotspots, resulting in higher attack success rates and transferability across diverse GUI scenarios. Experimental evaluations on various GUI agents demonstrate that EVA significantly enhances success rates, even under goal-agnostic constraints. The framework also uncovers shared vulnerabilities and behavioral biases in GUI agents, highlighting the importance of evolving indirect prompt injection in red teaming efforts and security research. <br /><br /> <div>
arXiv:2505.14289v1 Announce Type: new 
Abstract: As multimodal agents are increasingly trained to operate graphical user interfaces (GUIs) to complete user tasks, they face a growing threat from indirect prompt injection, attacks in which misleading instructions are embedded into the agent's visual environment, such as popups or chat messages, and misinterpreted as part of the intended task. A typical example is environmental injection, in which GUI elements are manipulated to influence agent behavior without directly modifying the user prompt. To address these emerging attacks, we propose EVA, a red teaming framework for indirect prompt injection which transforms the attack into a closed loop optimization by continuously monitoring an agent's attention distribution over the GUI and updating adversarial cues, keywords, phrasing, and layout, in response. Compared with prior one shot methods that generate fixed prompts without regard for how the model allocates visual attention, EVA dynamically adapts to emerging attention hotspots, yielding substantially higher attack success rates and far greater transferability across diverse GUI scenarios. We evaluate EVA on six widely used generalist and specialist GUI agents in realistic settings such as popup manipulation, chat based phishing, payments, and email composition. Experimental results show that EVA substantially improves success rates over static baselines. Under goal agnostic constraints, where the attacker does not know the agent's task intent, EVA still discovers effective patterns. Notably, we find that injection styles transfer well across models, revealing shared behavioral biases in GUI agents. These results suggest that evolving indirect prompt injection is a powerful tool not only for red teaming agents, but also for uncovering common vulnerabilities in their multimodal decision making.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors</title>
<link>https://arxiv.org/abs/2505.14300</link>
<guid>https://arxiv.org/abs/2505.14300</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time monitoring, Large Language Models (LLMs), backdoor-triggered responses, harmful AI outputs, unsupervised approach

Summary: <br /><br />
The study proposes a real-time framework for predicting harmful AI outputs, particularly focusing on backdoor-triggered responses that generate unsafe content. It addresses the challenges of identifying true causal indicators and preventing advanced models from evasion. By treating normal behavior as the baseline and harmful outputs as outliers, the framework aims to detect harmful content before it occurs. The research draws parallels to human deception to understand how LLMs exhibit distinct internal behavioral signatures when generating harmful content. The findings reveal that models can produce harmful content through causal mechanisms and become deceptive by alternating between linear and non-linear representations. To combat this, the Safety-Net framework is introduced, capable of detecting harmful behavior across representation dimensions with 96% accuracy. The approach aims to design monitoring systems that capture true causal indicators, prevent intentional evasion, and ensure the safety of AI systems. <div>
arXiv:2505.14300v1 Announce Type: new 
Abstract: High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable "Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds</title>
<link>https://arxiv.org/abs/2505.14366</link>
<guid>https://arxiv.org/abs/2505.14366</guid>
<content:encoded><![CDATA[
arXiv:2505.14366v1 Announce Type: new 
Abstract: We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.14381</link>
<guid>https://arxiv.org/abs/2505.14381</guid>
<content:encoded><![CDATA[
arXiv:2505.14381v1 Announce Type: new 
Abstract: With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs can achieve better RAG performance, but processing rich documents still remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout \textbf{AN}alysis), a novel approach enhancing both textual and visual Retrieval-Augmented Generation (RAG) systems working with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering continuous components. We trained the SCAN model by fine-tuning object detection models with sophisticated annotation datasets. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%, outperforming conventional approaches and even commercial document processing solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.14391</link>
<guid>https://arxiv.org/abs/2505.14391</guid>
<content:encoded><![CDATA[
arXiv:2505.14391v1 Announce Type: new 
Abstract: Many studies focus on data annotation techniques for training effective PRMs. However, current methods encounter a significant issue when applied to long CoT reasoning processes: they tend to focus solely on the first incorrect step and all preceding steps, assuming that all subsequent steps are incorrect. These methods overlook the unique self-correction and reflection mechanisms inherent in long CoT, where correct reasoning steps may still occur after initial reasoning mistakes. To address this issue, we propose a novel data annotation method for PRMs specifically designed to score the long CoT reasoning process. Given that under the reflection pattern, correct and incorrect steps often alternate, we introduce the concepts of Error Propagation and Error Cessation, enhancing PRMs' ability to identify both effective self-correction behaviors and reasoning based on erroneous steps. Leveraging an LLM-based judger for annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate it at both solution and step levels. Experimental results demonstrate that compared to existing open-source PRMs and PRMs trained on open-source datasets, our PRM achieves superior performance across various metrics, including search guidance, BoN, and F1 scores. Compared to widely used MC-based annotation methods, our annotation approach not only achieves higher data efficiency but also delivers superior performance. Detailed analysis is also conducted to demonstrate the stability and generalizability of our method.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Based Repository-Level Code Generation</title>
<link>https://arxiv.org/abs/2505.14394</link>
<guid>https://arxiv.org/abs/2505.14394</guid>
<content:encoded><![CDATA[
arXiv:2505.14394v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have transformed code generation from natural language queries. However, despite their extensive knowledge and ability to produce high-quality code, LLMs often struggle with contextual accuracy, particularly in evolving codebases. Current code search and retrieval methods frequently lack robustness in both the quality and contextual relevance of retrieved results, leading to suboptimal code generation. This paper introduces a novel knowledge graph-based approach to improve code search and retrieval leading to better quality of code generation in the context of repository-level tasks. The proposed approach represents code repositories as graphs, capturing structural and relational information for enhanced context-aware code generation. Our framework employs a hybrid approach for code retrieval to improve contextual relevance, track inter-file modular dependencies, generate more robust code and ensure consistency with the existing codebase. We benchmark the proposed approach on the Evolutionary Code Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark, and demonstrate that our method significantly outperforms the baseline approach. These findings suggest that knowledge graph based code generation could advance robust, context-sensitive coding assistance tools.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds</title>
<link>https://arxiv.org/abs/2505.14396</link>
<guid>https://arxiv.org/abs/2505.14396</guid>
<content:encoded><![CDATA[
arXiv:2505.14396v1 Announce Type: new 
Abstract: Causal world models are systems that can answer counterfactual questions about an environment of interest, i.e. predict how it would have evolved if an arbitrary subset of events had been realized differently. It requires understanding the underlying causes behind chains of events and conducting causal inference for arbitrary unseen distributions. So far, this task eludes foundation models, notably large language models (LLMs), which do not have demonstrated causal reasoning capabilities beyond the memorization of existing causal relationships. Furthermore, evaluating counterfactuals in real-world applications is challenging since only the factual world is observed, limiting evaluation to synthetic datasets. We address these problems by explicitly extracting and modeling causal relationships and propose the Causal Cartographer framework. First, we introduce a graph retrieval-augmented generation agent tasked to retrieve causal relationships from data. This approach allows us to construct a large network of real-world causal relationships that can serve as a repository of causal knowledge and build real-world counterfactuals. In addition, we create a counterfactual reasoning agent constrained by causal relationships to perform reliable step-by-step causal inference. We show that our approach can extract causal knowledge and improve the robustness of LLMs for causal reasoning tasks while reducing inference costs and spurious correlations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14403</link>
<guid>https://arxiv.org/abs/2505.14403</guid>
<content:encoded><![CDATA[
arXiv:2505.14403v1 Announce Type: new 
Abstract: Recent advances in reasoning language models have witnessed a paradigm shift from short to long CoT pattern. Given the substantial computational cost of rollouts in long CoT models, maximizing the utility of fixed training datasets becomes crucial. Our analysis reveals that negative responses contain valuable components such as self-reflection and error-correction steps, yet primary existing methods either completely discard negative samples (RFT) or apply equal penalization across all tokens (RL), failing to leverage these potential learning signals. In light of this, we propose Behavior Constrained Policy Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline RL framework that encompasses three stages: 1) sample segmentation, 2) consensus-based step correctness assessment combining LLM and PRM judgers, and 3) policy optimization with NSA designed to effectively mine positive steps within negative samples. Experimental results show that BCPG-NSA outperforms baselines on several challenging math/coding reasoning benchmarks using the same training dataset, achieving improved sample efficiency and demonstrating robustness and scalability when extended to multiple iterations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRL: Prompts from Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14412</link>
<guid>https://arxiv.org/abs/2505.14412</guid>
<content:encoded><![CDATA[
arXiv:2505.14412v1 Announce Type: new 
Abstract: Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at https://github.com/Batorskq/prl .
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation</title>
<link>https://arxiv.org/abs/2505.14419</link>
<guid>https://arxiv.org/abs/2505.14419</guid>
<content:encoded><![CDATA[
arXiv:2505.14419v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have demonstrated promising results in mathematical reasoning, but existing process annotation approaches, whether through human annotations or Monte Carlo simulations, remain computationally expensive. In this paper, we introduce Step COmpression for Process Estimation (SCOPE), a novel compression-based approach that significantly reduces annotation costs. We first translate natural language reasoning steps into code and normalize them through Abstract Syntax Tree, then merge equivalent steps to construct a prefix tree. Unlike simulation-based methods that waste numerous samples on estimation, SCOPE leverages a compression-based prefix tree where each root-to-leaf path serves as a training sample, reducing the complexity from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K samples with only 5% of the computational resources required by previous methods. Empirical results demonstrate that PRMs trained on our dataset consistently outperform existing automated annotation approaches on both Best-of-N strategy and ProcessBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v1 Announce Type: new 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Models Better Express Their Confidence</title>
<link>https://arxiv.org/abs/2505.14489</link>
<guid>https://arxiv.org/abs/2505.14489</guid>
<content:encoded><![CDATA[
arXiv:2505.14489v1 Announce Type: new 
Abstract: Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BACON: A fully explainable AI model with graded logic for decision making problems</title>
<link>https://arxiv.org/abs/2505.14510</link>
<guid>https://arxiv.org/abs/2505.14510</guid>
<content:encoded><![CDATA[
arXiv:2505.14510v1 Announce Type: new 
Abstract: As machine learning models and autonomous agents are increasingly deployed in high-stakes, real-world domains such as healthcare, security, finance, and robotics, the need for transparent and trustworthy explanations has become critical. To ensure end-to-end transparency of AI decisions, we need models that are not only accurate but also fully explainable and human-tunable. We introduce BACON, a novel framework for automatically training explainable AI models for decision making problems using graded logic. BACON achieves high predictive accuracy while offering full structural transparency and precise, logic-based symbolic explanations, enabling effective human-AI collaboration and expert-guided refinement. We evaluate BACON with a diverse set of scenarios: classic Boolean approximation, Iris flower classification, house purchasing decisions and breast cancer diagnosis. In each case, BACON provides high-performance models while producing compact, human-verifiable decision logic. These results demonstrate BACON's potential as a practical and principled approach for delivering crisp, trustworthy explainable AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guarded Query Routing for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14524</link>
<guid>https://arxiv.org/abs/2505.14524</guid>
<content:encoded><![CDATA[
arXiv:2505.14524v1 Announce Type: new 
Abstract: Query routing, the task to route user queries to different large language model (LLM) endpoints, can be considered as a text classification problem. However, out-of-distribution queries must be handled properly, as those could be questions about unrelated domains, queries in other languages, or even contain unsafe text. Here, we thus study a \emph{guarded} query routing problem, for which we first introduce the Guarded Query Routing Benchmark (GQR-Bench), which covers three exemplary target domains (law, finance, and healthcare), and seven datasets to test robustness against out-of-distribution queries. We then use GQR-Bench to contrast the effectiveness and efficiency of LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B), standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and traditional machine learning models (SVM, XGBoost). Our results show that WideMLP, enhanced with out-of-domain detection capabilities, yields the best trade-off between accuracy (88\%) and speed (<4ms). The embedding-based fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs yield the highest accuracy (91\%) but are comparatively slow (62ms for local Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge the automatic reliance on LLMs for (guarded) query routing and provide concrete recommendations for practical applications. GQR-Bench will be released as a Python package -- \texttt{gqr}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)</title>
<link>https://arxiv.org/abs/2505.14539</link>
<guid>https://arxiv.org/abs/2505.14539</guid>
<content:encoded><![CDATA[
arXiv:2505.14539v1 Announce Type: new 
Abstract: In this work, we present the first general logic of attention. Attention is a powerful cognitive ability that allows agents to focus on potentially complex information, such as logically structured propositions, higher-order beliefs, or what other agents pay attention to. This ability is a strength, as it helps to ignore what is irrelevant, but it can also introduce biases when some types of information or agents are systematically ignored. Existing dynamic epistemic logics for attention cannot model such complex attention scenarios, as they only model attention to atomic formulas. Additionally, such logics quickly become cumbersome, as their size grows exponentially in the number of agents and announced literals. Here, we introduce a logic that overcomes both limitations. First, we generalize edge-conditioned event models, which we show to be as expressive as standard event models yet exponentially more succinct (generalizing both standard event models and generalized arrow updates). Second, we extend attention to arbitrary formulas, allowing agents to also attend to other agents' beliefs or attention. Our work treats attention as a modality, like belief or awareness. We introduce attention principles that impose closure properties on that modality and that can be used in its axiomatization. Throughout, we illustrate our framework with examples of AI agents reasoning about human attentional biases, demonstrating how such agents can discover attentional biases.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study</title>
<link>https://arxiv.org/abs/2505.14544</link>
<guid>https://arxiv.org/abs/2505.14544</guid>
<content:encoded><![CDATA[
arXiv:2505.14544v1 Announce Type: new 
Abstract: Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Context Protocols Enhance Collective Inference</title>
<link>https://arxiv.org/abs/2505.14569</link>
<guid>https://arxiv.org/abs/2505.14569</guid>
<content:encoded><![CDATA[
arXiv:2505.14569v1 Announce Type: new 
Abstract: AI agents have become increasingly adept at complex tasks such as coding, reasoning, and multimodal understanding. However, building generalist systems requires moving beyond individual agents to collective inference -- a paradigm where multi-agent systems with diverse, task-specialized agents complement one another through structured communication and collaboration. Today, coordination is usually handled with imprecise, ad-hoc natural language, which limits complex interaction and hinders interoperability with domain-specific agents. We introduce Agent context protocols (ACPs): a domain- and agent-agnostic family of structured protocols for agent-agent communication, coordination, and error handling. ACPs combine (i) persistent execution blueprints -- explicit dependency graphs that store intermediate agent outputs -- with (ii) standardized message schemas, enabling robust and fault-tolerant multi-agent collective inference. ACP-powered generalist systems reach state-of-the-art performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance and best-in-class multimodal technical reports, outperforming commercial AI systems in human evaluation. ACPs are highly modular and extensible, allowing practitioners to build top-tier generalist agents quickly.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Foundation Model for Communication Systems</title>
<link>https://arxiv.org/abs/2505.14603</link>
<guid>https://arxiv.org/abs/2505.14603</guid>
<content:encoded><![CDATA[
arXiv:2505.14603v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance across various domains, and its application to communication systems is an active area of research. While current methods focus on task-specific solutions, the broader trend in AI is shifting toward large general models capable of supporting multiple applications. In this work, we take a step toward a foundation model for communication data--a transformer-based, multi-modal model designed to operate directly on communication data. We propose methodologies to address key challenges, including tokenization, positional embedding, multimodality, variable feature sizes, and normalization. Furthermore, we empirically demonstrate that such a model can successfully estimate multiple features, including transmission rank, selected precoder, Doppler spread, and delay profile.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let LLMs Break Free from Overthinking via Self-Braking Tuning</title>
<link>https://arxiv.org/abs/2505.14604</link>
<guid>https://arxiv.org/abs/2505.14604</guid>
<content:encoded><![CDATA[
arXiv:2505.14604v1 Announce Type: new 
Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas</title>
<link>https://arxiv.org/abs/2505.14615</link>
<guid>https://arxiv.org/abs/2505.14615</guid>
<content:encoded><![CDATA[
arXiv:2505.14615v1 Announce Type: new 
Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints. Each instance in SATBench is generated from a SAT formula, then translated into a story context and conditions using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-assisted and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. SATBench exposes fundamental limitations in the search-based logical reasoning abilities of current LLMs and provides a scalable testbed for future research in logical reasoning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debating for Better Reasoning: An Unsupervised Multimodal Approach</title>
<link>https://arxiv.org/abs/2505.14627</link>
<guid>https://arxiv.org/abs/2505.14627</guid>
<content:encoded><![CDATA[
arXiv:2505.14627v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two "sighted" expert vision-language models debate an answer, while a "blind" (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning</title>
<link>https://arxiv.org/abs/2505.14656</link>
<guid>https://arxiv.org/abs/2505.14656</guid>
<content:encoded><![CDATA[
arXiv:2505.14656v1 Announce Type: new 
Abstract: While LLMs excel at open-ended reasoning, they often struggle with cost-sensitive planning, either treating all actions as having equal cost or failing to stay within strict budgets. In this paper, we introduce Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings explicit cost-awareness into LLM-guided planning. Tight cost constraints push the planner to quickly identify infeasible solutions, while looser constraints encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1, Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs such as GPT-4.1 often falter under tight budgets, whereas CATS consistently delivers strong performance, achieving higher task success rates and better cost efficiency. CATS provides an effective solution for budget-aware decision-making by combining the reasoning power of LLMs with structured search.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment</title>
<link>https://arxiv.org/abs/2505.14667</link>
<guid>https://arxiv.org/abs/2505.14667</guid>
<content:encoded><![CDATA[
arXiv:2505.14667v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions</title>
<link>https://arxiv.org/abs/2505.14668</link>
<guid>https://arxiv.org/abs/2505.14668</guid>
<content:encoded><![CDATA[
arXiv:2505.14668v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training</title>
<link>https://arxiv.org/abs/2505.14681</link>
<guid>https://arxiv.org/abs/2505.14681</guid>
<content:encoded><![CDATA[
arXiv:2505.14681v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
<link>https://arxiv.org/abs/2505.06699</link>
<guid>https://arxiv.org/abs/2505.06699</guid>
<content:encoded><![CDATA[
arXiv:2505.06699v3 Announce Type: cross 
Abstract: This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named $\textbf{model steering}$. While ad-hoc methods have been used in various contexts, including the training of large foundation models, its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called $\textbf{DRRho risk minimization}$, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering. Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
<link>https://arxiv.org/abs/2505.11325</link>
<guid>https://arxiv.org/abs/2505.11325</guid>
<content:encoded><![CDATA[
arXiv:2505.11325v1 Announce Type: cross 
Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas</title>
<link>https://arxiv.org/abs/2505.12257</link>
<guid>https://arxiv.org/abs/2505.12257</guid>
<content:encoded><![CDATA[
arXiv:2505.12257v1 Announce Type: cross 
Abstract: Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Sample-specific Language Model Optimization at Test-time</title>
<link>https://arxiv.org/abs/2505.12392</link>
<guid>https://arxiv.org/abs/2505.12392</guid>
<content:encoded><![CDATA[
arXiv:2505.12392v1 Announce Type: cross 
Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pel, A Programming Language for Orchestrating AI Agents</title>
<link>https://arxiv.org/abs/2505.13453</link>
<guid>https://arxiv.org/abs/2505.13453</guid>
<content:encoded><![CDATA[
arXiv:2505.13453v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in computing, yet controlling and orchestrating their capabilities beyond simple text generation remains a challenge. Current methods, such as function/tool calling and direct code generation, suffer from limitations in expressiveness, scalability, cost, security, and the ability to enforce fine-grained control. This paper introduces Pel, a novel programming language specifically designed to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich platform for LLMs to express complex actions, control flow, and inter-agent communication safely and efficiently. Pel's design emphasizes a minimal, easily modifiable grammar suitable for constrained LLM generation, eliminating the need for complex sandboxing by enabling capability control at the syntax level. Key features include a powerful piping mechanism for linear composition, first-class closures enabling easy partial application and functional patterns, built-in support for natural language conditions evaluated by LLMs, and an advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and LLM-powered helper agents for automated error correction. Furthermore, Pel incorporates automatic parallelization of independent operations via static dependency analysis, crucial for performant agentic systems. We argue that Pel offers a more robust, secure, and expressive paradigm for LLM orchestration, paving the way for more sophisticated and reliable AI agentic frameworks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment</title>
<link>https://arxiv.org/abs/2505.13455</link>
<guid>https://arxiv.org/abs/2505.13455</guid>
<content:encoded><![CDATA[
arXiv:2505.13455v1 Announce Type: cross 
Abstract: Understanding how humans express and synchronize emotions across multiple communication channels particularly facial expressions and speech has significant implications for emotion recognition systems and human computer interaction. Motivated by the notion that non-overlapping speech promotes clearer emotional coordination, while overlapping speech disrupts synchrony, this study examines how these conversational dynamics shape the spatial and temporal alignment of arousal and valence across facial and vocal modalities. Using dyadic interactions from the IEMOCAP dataset, we extracted continuous emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech audio). Segments were categorized based on speech overlap, and emotional alignment was assessed using Pearson correlation, lag adjusted analysis, and Dynamic Time Warping (DTW). Across analyses, non overlapping speech was associated with more stable and predictable emotional synchrony than overlapping speech. While zero-lag correlations were low and not statistically different, non overlapping speech showed reduced variability, especially for arousal. Lag adjusted correlations and best-lag distributions revealed clearer, more consistent temporal alignment in these segments. In contrast, overlapping speech exhibited higher variability and flatter lag profiles, though DTW indicated unexpectedly tighter alignment suggesting distinct coordination strategies. Notably, directionality patterns showed that facial expressions more often preceded speech during turn-taking, while speech led during simultaneous vocalizations. These findings underscore the importance of conversational structure in regulating emotional communication and provide new insight into the spatial and temporal dynamics of multimodal affective alignment in real world interaction.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact</title>
<link>https://arxiv.org/abs/2505.13469</link>
<guid>https://arxiv.org/abs/2505.13469</guid>
<content:encoded><![CDATA[
arXiv:2505.13469v1 Announce Type: cross 
Abstract: As financial institutions increasingly rely on machine learning models to automate lending decisions, concerns about algorithmic fairness have risen. This paper explores the tradeoff between enforcing fairness constraints (such as demographic parity or equal opportunity) and maximizing lender profitability. Through simulations on synthetic data that reflects real-world lending patterns, we quantify how different fairness interventions impact profit margins and default rates. Our results demonstrate that equal opportunity constraints typically impose lower profit costs than demographic parity, but surprisingly, removing protected attributes from the model (fairness through unawareness) outperforms explicit fairness interventions in both fairness and profitability metrics. We further identify the specific economic conditions under which fair lending becomes profitable and analyze the feature-specific drivers of unfairness. These findings offer practical guidance for designing lending algorithms that balance ethical considerations with business objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale</title>
<link>https://arxiv.org/abs/2505.13480</link>
<guid>https://arxiv.org/abs/2505.13480</guid>
<content:encoded><![CDATA[
arXiv:2505.13480v1 Announce Type: cross 
Abstract: Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at https://github.com/av9ash/llm_cssrs_code.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors</title>
<link>https://arxiv.org/abs/2505.13483</link>
<guid>https://arxiv.org/abs/2505.13483</guid>
<content:encoded><![CDATA[
arXiv:2505.13483v1 Announce Type: cross 
Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for emotional intelligence. The advent of multimodal data and widespread communication has led to a proliferation of multimodal metaphors, amplifying the complexity of emotion classification compared to single-mode scenarios. However, the scarcity of research on constructing multimodal metaphorical fine-grained emotion datasets hampers progress in this domain. Moreover, existing studies predominantly focus on English, overlooking potential variations in emotional nuances across languages. To address these gaps, we introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of metaphorical advertisements. Each entry is meticulously annotated for metaphor occurrence, domain relations and fine-grained emotion classification encompassing joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral. Our dataset is publicly accessible (https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in this burgeoning field.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProdRev: A DNN framework for empowering customers using generative pre-trained transformers</title>
<link>https://arxiv.org/abs/2505.13491</link>
<guid>https://arxiv.org/abs/2505.13491</guid>
<content:encoded><![CDATA[
arXiv:2505.13491v1 Announce Type: cross 
Abstract: Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using "common-sense" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of "common sense" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution</title>
<link>https://arxiv.org/abs/2505.13497</link>
<guid>https://arxiv.org/abs/2505.13497</guid>
<content:encoded><![CDATA[
arXiv:2505.13497v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) enable planning from natural language instructions using implicit world knowledge, but often produce flawed plans that require refinement. Instead of directly predicting plans, recent methods aim to learn a problem domain that can be solved for different goal states using classical planners. However, these approaches require significant human feedback to obtain useful models. We address this shortcoming by learning hierarchical domains, where low-level predicates and actions are composed into higher-level counterparts, and by leveraging simulation to validate their preconditions and effects. This hierarchical approach is particularly powerful for long-horizon planning, where LLM-based planning approaches typically struggle. Furthermore, we introduce a central error reasoner to ensure consistency among the different planning levels. Evaluation on two challenging International Planning Competition (IPC) domains and a long-horizon robot manipulation task demonstrates higher planning success rates than state-of-the-art domain synthesis and LLM-modulo planning methods, while constructing high-quality models of the domain. Resources, videos and detailed experiment results are available at https://claudius-kienle.github.io/lodge/.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.13498</link>
<guid>https://arxiv.org/abs/2505.13498</guid>
<content:encoded><![CDATA[
arXiv:2505.13498v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias, restrict evaluation to text-only, rely on multiple-choice formats, and, more importantly, are limited for extremely low-resource languages. To address these gaps, we introduce IRLBench, presented in parallel English and Irish, which is considered definitely endangered by UNESCO. Our benchmark consists of 12 representative subjects developed from the 2024 Irish Leaving Certificate exams, enabling fine-grained analysis of model capabilities across domains. By framing the task as long-form generation and leveraging the official marking scheme, it does not only support a comprehensive evaluation of correctness but also language fidelity. Our extensive experiments of leading closed-source and open-source LLMs reveal a persistent performance gap between English and Irish, in which models produce valid Irish responses less than 80\% of the time, and answer correctly 55.8\% of the time compared to 76.2\% in English for the best-performing model. We release IRLBench (https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future research on robust, culturally aware multilingual AI development.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency</title>
<link>https://arxiv.org/abs/2505.13499</link>
<guid>https://arxiv.org/abs/2505.13499</guid>
<content:encoded><![CDATA[
arXiv:2505.13499v1 Announce Type: cross 
Abstract: We study Transformers through the perspective of optimal control theory, using tools from continuous-time formulations to derive actionable insights into training and architecture design. This framework improves the performance of existing Transformer models while providing desirable theoretical guarantees, including generalization and robustness. Our framework is designed to be plug-and-play, enabling seamless integration with established Transformer models and requiring only slight changes to the implementation. We conduct seven extensive experiments on tasks motivated by text generation, sentiment analysis, image classification, and point cloud classification. Experimental results show that the framework improves the test performance of the baselines, while being more parameter-efficient. On character-level text generation with nanoGPT, our framework achieves a 46% reduction in final test loss while using 42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in final test loss, demonstrating scalability to larger models. To the best of our knowledge, this is the first work that applies optimal control theory to both the training and architecture of Transformers. It offers a new foundation for systematic, theory-driven improvements and moves beyond costly trial-and-error approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Injection Systemically Degrades Large Language Model Safety Guardrails</title>
<link>https://arxiv.org/abs/2505.13500</link>
<guid>https://arxiv.org/abs/2505.13500</guid>
<content:encoded><![CDATA[
arXiv:2505.13500v1 Announce Type: cross 
Abstract: Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents</title>
<link>https://arxiv.org/abs/2505.13504</link>
<guid>https://arxiv.org/abs/2505.13504</guid>
<content:encoded><![CDATA[
arXiv:2505.13504v1 Announce Type: cross 
Abstract: Extracting alphanumeric data from form-like documents such as invoices, purchase orders, bills, and financial documents is often performed via vision (OCR) and learning algorithms or monolithic pipelines with limited potential for systemic improvements. We propose an agentic AI system that leverages Large Language Model (LLM) agents and a reinforcement learning (RL) driver agent to automate consistent, self-improving extraction under LLM inference uncertainty. Our work highlights the limitations of monolithic LLM-based extraction and introduces a modular, multi-agent framework with task-specific prompts and an RL policy of rewards and penalties to guide a meta-prompting agent to learn from past errors and improve prompt-based actor agents. This self-corrective adaptive system handles diverse documents, file formats, layouts, and LLMs, aiming to automate accurate information extraction without the need for human intervention. Results as reported on two benchmark datasets of SOIRE, and CORD, are promising for the agentic AI framework.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.13506</link>
<guid>https://arxiv.org/abs/2505.13506</guid>
<content:encoded><![CDATA[
arXiv:2505.13506v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge limitations of Large Language Models (LLMs) by integrating external knowledge, producing responses with enhanced factual correctness and query-specific contextualization. However, it also introduces new attack surfaces such as corpus poisoning at the same time. Most of the existing defense methods rely on the internal knowledge of the model, which conflicts with the design concept of RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and bait-guided context diversity detection to identify malicious content by analyzing the context diversity of candidate documents without relying on LLM internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art security with plug-and-play deployment, simultaneously improving clean-scenario RAG performance while maintaining practical operational costs (relatively 1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-R1: Towards Comprehensive Temporal Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.13508</link>
<guid>https://arxiv.org/abs/2505.13508</guid>
<content:encoded><![CDATA[
arXiv:2505.13508v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \textit{Time-R1} checkpoints.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13514</link>
<guid>https://arxiv.org/abs/2505.13514</guid>
<content:encoded><![CDATA[
arXiv:2505.13514v1 Announce Type: cross 
Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the "toxicity" of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades</title>
<link>https://arxiv.org/abs/2505.13515</link>
<guid>https://arxiv.org/abs/2505.13515</guid>
<content:encoded><![CDATA[
arXiv:2505.13515v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: "How can we efficiently leverage existing LoRA weights to adapt to newer model versions?" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2505.13516</link>
<guid>https://arxiv.org/abs/2505.13516</guid>
<content:encoded><![CDATA[
arXiv:2505.13516v1 Announce Type: cross 
Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) have demonstrated tremendous potential in diverse task scenarios. Nonetheless, existing agentic systems typically rely on predefined agent-role design spaces and static communication structures, limiting their adaptability as well as flexibility in complex interaction environments and leading to subpar performance on highly specialized and expert-level tasks. To address these issues, we introduce HALO, a multi-agent collaboration framework based on a hierarchical reasoning architecture. Specifically, we incorporate a high-level planning agent for task decomposition, mid-level role-design agents for subtask-specific agent instantiation, and low-level inference agents for subtask execution. Particularly, subtask execution is reformulated as a structured workflow search problem, where Monte Carlo Tree Search (MCTS) systematically explores the agentic action space to construct optimal reasoning trajectories. Additionally, as the majority of users lack expertise in prompt engineering, we leverage an Adaptive Prompt Refinement module to transform raw queries into task-specific prompts. Empirical evaluations on Code Generation (HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH) benchmark datasets highlight the effectiveness of HALO, yielding a 14.4% average improvement over state-of-the-art baselines. Notably, HALO achieves up to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark, indicating its advanced proficiency in tackling highly specialized and expert-level tasks. The code repository is available at https://github.com/23japhone/HALO.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Balancing Strategies: A Survey of Resampling and Augmentation Methods</title>
<link>https://arxiv.org/abs/2505.13518</link>
<guid>https://arxiv.org/abs/2505.13518</guid>
<content:encoded><![CDATA[
arXiv:2505.13518v1 Announce Type: cross 
Abstract: Imbalanced data poses a significant obstacle in machine learning, as an unequal distribution of class labels often results in skewed predictions and diminished model accuracy. To mitigate this problem, various resampling strategies have been developed, encompassing both oversampling and undersampling techniques aimed at modifying class proportions. Conventional oversampling approaches like SMOTE enhance the representation of the minority class, whereas undersampling methods focus on trimming down the majority class. Advances in deep learning have facilitated the creation of more complex solutions, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which are capable of producing high-quality synthetic examples. This paper reviews a broad spectrum of data balancing methods, classifying them into categories including synthetic oversampling, adaptive techniques, generative models, ensemble-based strategies, hybrid approaches, undersampling, and neighbor-based methods. Furthermore, it highlights current developments in resampling techniques and discusses practical implementations and case studies that validate their effectiveness. The paper concludes by offering perspectives on potential directions for future exploration in this domain.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Domain Generalization</title>
<link>https://arxiv.org/abs/2505.13519</link>
<guid>https://arxiv.org/abs/2505.13519</guid>
<content:encoded><![CDATA[
arXiv:2505.13519v1 Announce Type: cross 
Abstract: Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic context. However, existing domain generalization approaches typically treat domains as discrete or evolving along a single axis (e.g., time), which fails to capture the complex, multi-dimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variation descriptors. We present a principled framework grounded in geometric and algebraic theory, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets-including remote sensing, scientific documents, and traffic forecasting-demonstrate that our method significantly outperforms existing baselines in generalization accuracy and robustness under descriptor imperfections.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering</title>
<link>https://arxiv.org/abs/2505.13520</link>
<guid>https://arxiv.org/abs/2505.13520</guid>
<content:encoded><![CDATA[
arXiv:2505.13520v1 Announce Type: cross 
Abstract: Textbook question answering (TQA) is a complex task, requiring the interpretation of complex multimodal context. Although recent advances have improved overall performance, they often encounter difficulties in educational settings where accurate semantic alignment and task-specific document retrieval are essential. In this paper, we propose a novel approach to multimodal textbook question answering by introducing a mechanism for enhancing semantic representations through multi-objective joint training. Our model, Joint Embedding Training With Ranking Supervision for Textbook Question Answering (JETRTQA), is a multimodal learning framework built on a retriever--generator architecture that uses a retrieval-augmented generation setup, in which a multimodal large language model generates answers. JETRTQA is designed to improve the relevance of retrieved documents in complex educational contexts. Unlike traditional direct scoring approaches, JETRTQA learns to refine the semantic representations of questions and documents through a supervised signal that combines pairwise ranking and implicit supervision derived from answers. We evaluate our method on the CK12-QA dataset and demonstrate that it significantly improves the discrimination between informative and irrelevant documents, even when they are long, complex, and multimodal. JETRTQA outperforms the previous state of the art, achieving a 2.4\% gain in accuracy on the validation set and 11.1\% on the test set.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACPs: Agent Collaboration Protocols for the Internet of Agents</title>
<link>https://arxiv.org/abs/2505.13523</link>
<guid>https://arxiv.org/abs/2505.13523</guid>
<content:encoded><![CDATA[
arXiv:2505.13523v1 Announce Type: cross 
Abstract: With the rapid advancement of artificial intelligence, the proliferation of autonomous agents has introduced new challenges in interoperability, scalability, and coordination. The Internet of Agents (IoA) aims to interconnect heterogeneous agents through standardized communication protocols, enabling seamless collaboration and intelligent task execution. However, existing agent communication protocols such as MCP, A2A, and ANP remain fragmented and scenario-specific. To address this gap, we propose Agent Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA. ACPs include registration, discovery, interaction, and tooling protocols to support trustable access, capability orchestration, and workflow construction. We present the architecture, key technologies, and application workflows of ACPs, and demonstrate its effectiveness in a collaborative restaurant booking scenario. ACPs lay the foundation for building a secure, open, and scalable agent internet infrastructure.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Program Quantum Measurements for Machine Learning</title>
<link>https://arxiv.org/abs/2505.13525</link>
<guid>https://arxiv.org/abs/2505.13525</guid>
<content:encoded><![CDATA[
arXiv:2505.13525v1 Announce Type: cross 
Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level expertise, presenting a key challenge to the widespread adoption of QML. Critical obstacles include the design of effective data encoding strategies and parameterized quantum circuits, both of which are vital for the performance of QML models. Furthermore, the measurement process is often neglected-most existing QML models employ predefined measurement schemes that may not align with the specific requirements of the targeted problem. We propose an innovative framework that renders the observable of a quantum system-specifically, the Hermitian matrix-trainable. This approach employs an end-to-end differentiable learning framework, enabling simultaneous optimization of the neural network used to program the parameterized observables and the standard quantum circuit parameters. Notably, the quantum observable parameters are dynamically programmed by the neural network, allowing the observables to adapt in real time based on the input data stream. Through numerical simulations, we demonstrate that the proposed method effectively programs observables dynamically within variational quantum circuits, achieving superior results compared to existing approaches. Notably, it delivers enhanced performance metrics, such as higher classification accuracy, thereby significantly improving the overall effectiveness of QML models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geography-Aware Large Language Models for Next POI Recommendation</title>
<link>https://arxiv.org/abs/2505.13526</link>
<guid>https://arxiv.org/abs/2505.13526</guid>
<content:encoded><![CDATA[
arXiv:2505.13526v1 Announce Type: cross 
Abstract: The next Point-of-Interest (POI) recommendation task aims to predict users' next destinations based on their historical movement data and plays a key role in location-based services and personalized applications. Accurate next POI recommendation depends on effectively modeling geographic information and POI transition relations, which are crucial for capturing spatial dependencies and user movement patterns. While Large Language Models (LLMs) exhibit strong capabilities in semantic understanding and contextual reasoning, applying them to spatial tasks like next POI recommendation remains challenging. First, the infrequent nature of specific GPS coordinates makes it difficult for LLMs to model precise spatial contexts. Second, the lack of knowledge about POI transitions limits their ability to capture potential POI-POI relationships. To address these issues, we propose GA-LLM (Geography-Aware Large Language Model), a novel framework that enhances LLMs with two specialized components. The Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into spatial representations using hierarchical and Fourier-based positional encoding, enabling the model to understand geographic features from multiple perspectives. The POI Alignment Module (PAM) incorporates POI transition relations into the LLM's semantic space, allowing it to infer global POI relationships and generalize to unseen POIs. Experiments on three real-world datasets demonstrate the state-of-the-art performance of GA-LLM.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</title>
<link>https://arxiv.org/abs/2505.13527</link>
<guid>https://arxiv.org/abs/2505.13527</guid>
<content:encoded><![CDATA[
arXiv:2505.13527v1 Announce Type: cross 
Abstract: Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems</title>
<link>https://arxiv.org/abs/2505.13528</link>
<guid>https://arxiv.org/abs/2505.13528</guid>
<content:encoded><![CDATA[
arXiv:2505.13528v1 Announce Type: cross 
Abstract: Recommender systems (RS) are increasingly vulnerable to shilling attacks, where adversaries inject fake user profiles to manipulate system outputs. Traditional attack strategies often rely on simplistic heuristics, require access to internal RS data, and overlook the manipulation potential of textual reviews. In this work, we introduce Agent4SR, a novel framework that leverages Large Language Model (LLM)-based agents to perform low-knowledge, high-impact shilling attacks through both rating and review generation. Agent4SR simulates realistic user behavior by orchestrating adversarial interactions, selecting items, assigning ratings, and crafting reviews, while maintaining behavioral plausibility. Our design includes targeted profile construction, hybrid memory retrieval, and a review attack strategy that propagates target item features across unrelated reviews to amplify manipulation. Extensive experiments on multiple datasets and RS architectures demonstrate that Agent4SR outperforms existing low-knowledge baselines in both effectiveness and stealth. Our findings reveal a new class of emergent threats posed by LLM-driven agents, underscoring the urgent need for enhanced defenses in modern recommender systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference</title>
<link>https://arxiv.org/abs/2505.13531</link>
<guid>https://arxiv.org/abs/2505.13531</guid>
<content:encoded><![CDATA[
arXiv:2505.13531v1 Announce Type: cross 
Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables comprehensive comparison of their misalignment, cultural adaptability, and biases. Nevertheless, current value measurement datasets face the informativeness challenge: with often outdated, contaminated, or generic test questions, they can only capture the shared value orientations among different LLMs, leading to saturated and thus uninformative results. To address this problem, we introduce AdAEM, a novel, self-extensible assessment framework for revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM can automatically and adaptively generate and extend its test questions. This is achieved by probing the internal value boundaries of a diverse set of LLMs developed across cultures and time periods in an in-context optimization manner. The optimization process theoretically maximizes an information-theoretic objective to extract the latest or culturally controversial topics, providing more distinguishable and informative insights about models' value differences. In this way, AdAEM is able to co-evolve with the development of LLMs, consistently tracking their value dynamics. Using AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct an extensive analysis to manifest our method's validity and effectiveness, and benchmark the values of 16 LLMs, laying the groundwork for better value research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios</title>
<link>https://arxiv.org/abs/2505.13532</link>
<guid>https://arxiv.org/abs/2505.13532</guid>
<content:encoded><![CDATA[
arXiv:2505.13532v1 Announce Type: cross 
Abstract: Reinforcement learning (RL), known for its self-evolution capability, offers a promising approach to training high-level autonomous driving systems. However, handling constraints remains a significant challenge for existing RL algorithms, particularly in real-world applications. In this paper, we propose a new safety-oriented training technique called harmonic policy iteration (HPI). At each RL iteration, it first calculates two policy gradients associated with efficient driving and safety constraints, respectively. Then, a harmonic gradient is derived for policy updating, minimizing conflicts between the two gradients and consequently enabling a more balanced and stable training process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the backbone and integrate it with our HPI to develop a new safe RL algorithm, DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H achieves efficient driving performance with near-zero safety constraint violations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data</title>
<link>https://arxiv.org/abs/2505.13534</link>
<guid>https://arxiv.org/abs/2505.13534</guid>
<content:encoded><![CDATA[
arXiv:2505.13534v1 Announce Type: cross 
Abstract: Finding interesting phenomena is the core of scientific discovery, but it is a manual, ill-defined concept. We present an integrative pipeline for automating the discovery of interesting simple hypotheses (feature-target relations with effect direction and a potential underlying mechanism) in structured biomedical data. The pipeline combines machine learning, knowledge graphs, literature search and Large Language Models. We formalize "interestingness" as a combination of novelty, utility and plausibility. On 8 major diseases from the UK Biobank, our pipeline consistently recovers risk factors years before their appearance in the literature. 40--53% of our top candidates were validated as interesting, compared to 0--7% for a SHAP-based baseline. Overall, 28% of 109 candidates were interesting to medical experts. The pipeline addresses the challenge of operationalizing "interestingness" scalably and for any target. We release data and code: https://github.com/LinialLab/InterFeat
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments</title>
<link>https://arxiv.org/abs/2505.13535</link>
<guid>https://arxiv.org/abs/2505.13535</guid>
<content:encoded><![CDATA[
arXiv:2505.13535v1 Announce Type: cross 
Abstract: Information extraction (IE) from Visually Rich Documents (VRDs) containing layout features along with text is a critical and well-studied task. Specialized non-LLM NLP-based solutions typically involve training models using both textual and geometric information to label sequences/tokens as named entities or answers to specific questions. However, these approaches lack reasoning, are not able to infer values not explicitly present in documents, and do not generalize well to new formats. Generative LLM-based approaches proposed recently are capable of reasoning, but struggle to comprehend clues from document layout especially in previously unseen document formats, and do not show competitive performance in heterogeneous VRD benchmark datasets. In this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs into localized, reusable semantic textual segments called $\textit{semantic blocks}$, which are processed independently. Through focused and more generalizable reasoning,our approach outperforms the state-of-the-art on public VRD benchmarks by 1-3% in F1 scores, is resilient to document formats previously not encountered and shows abilities to correctly extract information not explicitly present in documents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines</title>
<link>https://arxiv.org/abs/2505.13538</link>
<guid>https://arxiv.org/abs/2505.13538</guid>
<content:encoded><![CDATA[
arXiv:2505.13538v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems show promise by coupling large language models with external knowledge, yet traditional RAG evaluation methods primarily report quantitative scores while offering limited actionable guidance for refining these complex pipelines. In this paper, we introduce RAGXplain, an evaluation framework that quantifies RAG performance and translates these assessments into clear insights that clarify the workings of its complex, multi-stage pipeline and offer actionable recommendations. Using LLM reasoning, RAGXplain converts raw scores into coherent narratives identifying performance gaps and suggesting targeted improvements. By providing transparent explanations for AI decision-making, our framework fosters user trust-a key challenge in AI adoption. Our LLM-based metric assessments show strong alignment with human judgments, and experiments on public question-answering datasets confirm that applying RAGXplain's actionable recommendations measurably improves system performance. RAGXplain thus bridges quantitative evaluation and practical optimization, empowering users to understand, trust, and enhance their AI systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-head Temporal Latent Attention</title>
<link>https://arxiv.org/abs/2505.13544</link>
<guid>https://arxiv.org/abs/2505.13544</guid>
<content:encoded><![CDATA[
arXiv:2505.13544v1 Announce Type: cross 
Abstract: While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Or Not: a library for evaluating out-of-knowledge base robustness</title>
<link>https://arxiv.org/abs/2505.13545</link>
<guid>https://arxiv.org/abs/2505.13545</guid>
<content:encoded><![CDATA[
arXiv:2505.13545v1 Announce Type: cross 
Abstract: While the capabilities of large language models (LLMs) have progressed significantly, their use in high-stakes applications have been limited due to risks of hallucination. One key approach in reducing hallucination is retrieval-augmented generation (RAG), but even in such setups, LLMs may still hallucinate when presented with questions outside of the knowledge base. Such behavior is unacceptable in high-stake applications where LLMs are expected to abstain from answering queries it does not have sufficient context on. In this work, we present a novel methodology for systematically evaluating out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not know) in the RAG setting, without the need for manual annotation of gold standard answers. We implement our methodology in knowornot, an open-source library that enables users to develop their own customized evaluation data and pipelines for OOKB robustness. knowornot comprises four main features. Firstly, it provides a unified, high-level API that streamlines the process of setting up and running robustness benchmarks. Secondly, its modular architecture emphasizes extensibility and flexibility, allowing users to easily integrate their own LLM clients and RAG settings. Thirdly, its rigorous data modeling design ensures experiment reproducibility, reliability and traceability. Lastly, it implements a comprehensive suite of tools for users to customize their pipelines. We demonstrate the utility of knowornot by developing a challenging benchmark, PolicyBench, which spans four Question-Answer (QA) chatbots on government policies, and analyze its OOKB robustness. The source code of knowornot is available https://github.com/govtech-responsibleai/KnowOrNot.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Federated Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13547</link>
<guid>https://arxiv.org/abs/2505.13547</guid>
<content:encoded><![CDATA[
arXiv:2505.13547v1 Announce Type: cross 
Abstract: LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation</title>
<link>https://arxiv.org/abs/2505.13550</link>
<guid>https://arxiv.org/abs/2505.13550</guid>
<content:encoded><![CDATA[
arXiv:2505.13550v1 Announce Type: cross 
Abstract: Just-in-time Information Recommendation (JIR) is a service designed to deliver the most relevant information precisely when users need it, , addressing their knowledge gaps with minimal effort and boosting decision-making and efficiency in daily life. Advances in device-efficient deployment of foundation models and the growing use of intelligent wearable devices have made always-on JIR assistants feasible. However, there has been no systematic effort to formally define JIR tasks or establish evaluation frameworks. To bridge this gap, we present the first mathematical definition of JIR tasks and associated evaluation metrics. Additionally, we introduce JIR-Arena, a multimodal benchmark dataset featuring diverse, information-request-intensive scenarios to evaluate JIR systems across critical dimensions: i) accurately inferring user information needs, ii) delivering timely and relevant recommendations, and iii) avoiding irrelevant content that may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in estimating user information needs and uncontrollable system variables affecting reproducibility. To address these, JIR-Arena: i) combines input from multiple humans and large AI models to approximate information need distributions; ii) assesses JIR quality through information retrieval outcomes using static knowledge base snapshots; and iii) employs a multi-turn, multi-entity validation framework to improve objectivity and generality. Furthermore, we implement a baseline JIR system capable of processing real-time information streams aligned with user inputs. Our evaluation of this baseline system on JIR-Arena indicates that while foundation model-based JIR systems simulate user needs with reasonable precision, they face challenges in recall and effective content retrieval. To support future research in this new area, we fully release our code and data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation</title>
<link>https://arxiv.org/abs/2505.13554</link>
<guid>https://arxiv.org/abs/2505.13554</guid>
<content:encoded><![CDATA[
arXiv:2505.13554v1 Announce Type: cross 
Abstract: Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMAQA: A Metadata-based QA Dataset for RAG Systems</title>
<link>https://arxiv.org/abs/2505.13557</link>
<guid>https://arxiv.org/abs/2505.13557</guid>
<content:encoded><![CDATA[
arXiv:2505.13557v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering. The dataset is available at https://anonymous.4open.science/r/AMAQA-5D0D/
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback</title>
<link>https://arxiv.org/abs/2505.13562</link>
<guid>https://arxiv.org/abs/2505.13562</guid>
<content:encoded><![CDATA[
arXiv:2505.13562v1 Announce Type: cross 
Abstract: Learning in games is a fundamental problem in machine learning and artificial intelligence, with numerous applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work investigates two-player zero-sum matrix games with an unknown payoff matrix and bandit feedback, where each player observes their actions and the corresponding noisy payoff. Prior studies have proposed algorithms for this setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with \citet{o2021matrix} demonstrating the effectiveness of deterministic optimism (e.g., \ucb) in achieving sublinear regret. However, the potential of randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel algorithm that integrates evolutionary algorithms (EAs) into the bandit framework to implement randomised optimism through EA variation operators. We prove that \coebl achieves sublinear regret, matching the performance of deterministic optimism-based methods. To the best of our knowledge, this is the first theoretical regret analysis of an evolutionary bandit learning algorithm in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that \coebl not only achieves sublinear regret but also consistently outperforms classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic}, the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}. These results highlight the potential of evolutionary bandit learning, particularly the efficacy of randomised optimism via evolutionary algorithms in game-theoretic settings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression</title>
<link>https://arxiv.org/abs/2505.13563</link>
<guid>https://arxiv.org/abs/2505.13563</guid>
<content:encoded><![CDATA[
arXiv:2505.13563v1 Announce Type: cross 
Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base) with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and (d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks</title>
<link>https://arxiv.org/abs/2505.13565</link>
<guid>https://arxiv.org/abs/2505.13565</guid>
<content:encoded><![CDATA[
arXiv:2505.13565v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) poses both significant risks and valuable opportunities for democratic governance. This paper introduces a dual taxonomy to evaluate AI's complex relationship with democracy: the AI Risks to Democracy (AIRD) taxonomy, which identifies how AI can undermine core democratic principles such as autonomy, fairness, and trust; and the AI's Positive Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to enhance transparency, participation, efficiency, and evidence-based policymaking.
  Grounded in the European Union's approach to ethical AI governance, and particularly the seven Trustworthy AI requirements proposed by the European Commission's High-Level Expert Group on AI, each identified risk is aligned with mitigation strategies based on EU regulatory and normative frameworks. Our analysis underscores the transversal importance of transparency and societal well-being across all risk categories and offers a structured lens for aligning AI systems with democratic values.
  By integrating democratic theory with practical governance tools, this paper offers a normative and actionable framework to guide research, regulation, and institutional design to support trustworthy, democratic AI. It provides scholars with a conceptual foundation to evaluate the democratic implications of AI, equips policymakers with structured criteria for ethical oversight, and helps technologists align system design with democratic principles. In doing so, it bridges the gap between ethical aspirations and operational realities, laying the groundwork for more inclusive, accountable, and resilient democratic systems in the algorithmic age.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics of RNNs in Closed-Loop Environments</title>
<link>https://arxiv.org/abs/2505.13567</link>
<guid>https://arxiv.org/abs/2505.13567</guid>
<content:encoded><![CDATA[
arXiv:2505.13567v1 Announce Type: cross 
Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer powerful models of brain computation. However, typical training paradigms rely on open-loop, supervised settings, whereas real-world learning unfolds in closed-loop environments. Here, we develop a mathematical theory describing the learning dynamics of linear RNNs trained in closed-loop contexts. We first demonstrate that two otherwise identical RNNs, trained in either closed- or open-loop modes, follow markedly different learning trajectories. To probe this divergence, we analytically characterize the closed-loop case, revealing distinct stages aligned with the evolution of the training loss. Specifically, we show that the learning dynamics of closed-loop RNNs, in contrast to open-loop ones, are governed by an interplay between two competing objectives: short-term policy improvement and long-term stability of the agent-environment interaction. Finally, we apply our framework to a realistic motor control task, highlighting its broader applicability. Taken together, our results underscore the importance of modeling closed-loop dynamics in a biologically plausible setting.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.13572</link>
<guid>https://arxiv.org/abs/2505.13572</guid>
<content:encoded><![CDATA[
arXiv:2505.13572v1 Announce Type: cross 
Abstract: The SPARQL query language is the standard method to access knowledge graphs (KGs). However, formulating SPARQL queries is a significant challenge for non-expert users, and remains time-consuming for the experienced ones. Best practices recommend to document KGs with competency questions and example queries to contextualise the knowledge they contain and illustrate their potential applications. In practice, however, this is either not the case or the examples are provided in limited numbers. Large Language Models (LLMs) are being used in conversational agents and are proving to be an attractive solution with a wide range of applications, from simple question-answering about common knowledge to generating code in a targeted programming language. However, training and testing these models to produce high quality SPARQL queries from natural language questions requires substantial datasets of question-query pairs. In this paper, we present Q${}^2$Forge that addresses the challenge of generating new competency questions for a KG and corresponding SPARQL queries. It iteratively validates those queries with human feedback and LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular, meaning that the different modules of the application (CQ generation, query generation and query refinement) can be used separately, as an integrated pipeline, or replaced by alternative services. The result is a complete pipeline from competency question formulation to query evaluation, supporting the creation of reference query sets for any target KG.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeMesh: Boosting Mesh Generation with Coordinates Merging</title>
<link>https://arxiv.org/abs/2505.13573</link>
<guid>https://arxiv.org/abs/2505.13573</guid>
<content:encoded><![CDATA[
arXiv:2505.13573v1 Announce Type: cross 
Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard in current auto-regressive mesh generation methods. Despite their effectiveness, there is no efficient measurement for the various tokenizers that serialize meshes into sequences. In this paper, we introduce a new metric Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers theoretically without any training. Building upon PTME, we propose a plug-and-play tokenization technique called coordinate merging. It further improves the compression ratios of existing tokenizers by rearranging and merging the most frequent patterns of coordinates. Through experiments on various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we further validate the performance of our method. We hope that the proposed PTME and coordinate merging can enhance the existing mesh tokenizers and guide the further development of native mesh generation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation</title>
<link>https://arxiv.org/abs/2505.13577</link>
<guid>https://arxiv.org/abs/2505.13577</guid>
<content:encoded><![CDATA[
arXiv:2505.13577v1 Announce Type: cross 
Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting their communicative abilities and interactions. However, despite the global prevalence of voice disorders, many lack access to convenient diagnosis and treatment. This paper introduces VocalAgent, an audio large language model (LLM) to address these challenges through vocal health diagnosis. We leverage Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital patients, and present a multifaceted evaluation framework encompassing a safety assessment to mitigate diagnostic biases, cross-lingual performance analysis, and modality ablation studies. VocalAgent demonstrates superior accuracy on voice disorder classification compared to state-of-the-art baselines. Its LLM-based method offers a scalable solution for broader adoption of health diagnostics, while underscoring the importance of ethical and technical validation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction</title>
<link>https://arxiv.org/abs/2505.13579</link>
<guid>https://arxiv.org/abs/2505.13579</guid>
<content:encoded><![CDATA[
arXiv:2505.13579v1 Announce Type: cross 
Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due to its efficiency. However, FDK is susceptible to noise and artifacts. While recent deep learning methods offer improved image quality, they often increase computational complexity and lack the interpretability of traditional methods. In this paper, we introduce an enhanced FDK-based neural network that maintains the classical algorithm's interpretability by selectively integrating trainable elements into the cosine weighting and filtering stages. Recognizing the challenge of a large parameter space inherent in 3D CBCT data, we leverage wavelet transformations to create sparse representations of the cosine weights and filters. This strategic sparsification reduces the parameter count by $93.75\%$ without compromising performance, accelerates convergence, and importantly, maintains the inference computational cost equivalent to the classical FDK algorithm. Our method not only ensures volumetric consistency and boosts robustness to noise, but is also designed for straightforward integration into existing CT reconstruction pipelines. This presents a pragmatic enhancement that can benefit clinical applications, particularly in environments with computational limitations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making</title>
<link>https://arxiv.org/abs/2505.13580</link>
<guid>https://arxiv.org/abs/2505.13580</guid>
<content:encoded><![CDATA[
arXiv:2505.13580v1 Announce Type: cross 
Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to solve sequential decision making tasks arising in contexts of operations research and management science which we call OMGPT. We first propose a general sequence modeling framework to cover several operational decision making tasks as special cases, such as dynamic pricing, inventory management, resource allocation, and queueing control. Under the framework, all these tasks can be viewed as a sequential prediction problem where the goal is to predict the optimal future action given all the historical information. Then we train a transformer-based neural network model (OMGPT) as a natural and powerful architecture for sequential modeling. This marks a paradigm shift compared to the existing methods for these OR/OM tasks in that (i) the OMGPT model can take advantage of the huge amount of pre-trained data; (ii) when tackling these problems, OMGPT does not assume any analytical model structure and enables a direct and rich mapping from the history to the future actions. Either of these two aspects, to the best of our knowledge, is not achieved by any existing method. We establish a Bayesian perspective to theoretically understand the working mechanism of the OMGPT on these tasks, which relates its performance with the pre-training task diversity and the divergence between the testing task and pre-training tasks. Numerically, we observe a surprising performance of the proposed model across all the above tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses</title>
<link>https://arxiv.org/abs/2505.13617</link>
<guid>https://arxiv.org/abs/2505.13617</guid>
<content:encoded><![CDATA[
arXiv:2505.13617v1 Announce Type: cross 
Abstract: The characteristics of a sound field are intrinsically linked to the geometric and spatial properties of the environment surrounding a sound source and a listener. The physics of sound propagation is captured in a time-domain signal known as a room impulse response (RIR). Prior work using neural fields (NFs) has allowed learning spatially-continuous representations of RIRs from finite RIR measurements. However, previous NF-based methods have focused on monaural omnidirectional or at most binaural listeners, which does not precisely capture the directional characteristics of a real sound field at a single point. We propose a direction-aware neural field (DANF) that more explicitly incorporates the directional information by Ambisonic-format RIRs. While DANF inherently captures spatial relations between sources and listeners, we further propose a direction-aware loss. In addition, we investigate the ability of DANF to adapt to new rooms in various ways including low-rank adaptation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning (Approximately) Equivariant Networks via Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.13631</link>
<guid>https://arxiv.org/abs/2505.13631</guid>
<content:encoded><![CDATA[
arXiv:2505.13631v1 Announce Type: cross 
Abstract: Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Reinforced Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.13650</link>
<guid>https://arxiv.org/abs/2505.13650</guid>
<content:encoded><![CDATA[
arXiv:2505.13650v1 Announce Type: cross 
Abstract: Graphs serve as versatile data structures in numerous real-world domains-including social networks, molecular biology, and knowledge graphs-by capturing intricate relational information among entities. Among graph-based learning techniques, Graph Contrastive Learning (GCL) has gained significant attention for its ability to derive robust, self-supervised graph representations through the contrasting of positive and negative sample pairs. However, a critical challenge lies in ensuring high-quality positive pairs so that the intrinsic semantic and structural properties of the original graph are preserved rather than distorted. To address this issue, we propose SRGCL (Self-Reinforced Graph Contrastive Learning), a novel framework that leverages the model's own encoder to dynamically evaluate and select high-quality positive pairs. We designed a unified positive pair generator employing multiple augmentation strategies, and a selector guided by the manifold hypothesis to maintain the underlying geometry of the latent space. By adopting a probabilistic mechanism for selecting positive pairs, SRGCL iteratively refines its assessment of pair quality as the encoder's representational power improves. Extensive experiments on diverse graph-level classification tasks demonstrate that SRGCL, as a plug-in module, consistently outperforms state-of-the-art GCL methods, underscoring its adaptability and efficacy across various domains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</title>
<link>https://arxiv.org/abs/2505.13697</link>
<guid>https://arxiv.org/abs/2505.13697</guid>
<content:encoded><![CDATA[
arXiv:2505.13697v1 Announce Type: cross 
Abstract: Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Good at Detecting Propaganda?</title>
<link>https://arxiv.org/abs/2505.13706</link>
<guid>https://arxiv.org/abs/2505.13706</guid>
<content:encoded><![CDATA[
arXiv:2505.13706v1 Announce Type: cross 
Abstract: Propagandists use rhetorical devices that rely on logical fallacies and emotional appeals to advance their agendas. Recognizing these techniques is key to making informed decisions. Recent advances in Natural Language Processing (NLP) have enabled the development of systems capable of detecting manipulative content. In this study, we look at several Large Language Models and their performance in detecting propaganda techniques in news articles. We compare the performance of these LLMs with transformer-based models. We find that, while GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude 3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally, we find that all three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting instances of one out of six propaganda techniques (name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in detecting instances of appeal to fear and flag-waving.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13709</link>
<guid>https://arxiv.org/abs/2505.13709</guid>
<content:encoded><![CDATA[
arXiv:2505.13709v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation</title>
<link>https://arxiv.org/abs/2505.13729</link>
<guid>https://arxiv.org/abs/2505.13729</guid>
<content:encoded><![CDATA[
arXiv:2505.13729v1 Announce Type: cross 
Abstract: Adaptive collaboration is critical to a team of autonomous robots to perform complicated navigation tasks in large-scale unknown environments. An effective collaboration strategy should be determined and adapted according to each robot's skills and current status to successfully achieve the shared goal. We present SayCoNav, a new approach that leverages large language models (LLMs) for automatically generating this collaboration strategy among a team of robots. Building on the collaboration strategy, each robot uses the LLM to generate its plans and actions in a decentralized way. By sharing information to each other during navigation, each robot also continuously updates its step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation (MultiON) tasks, that require the team of the robots to utilize their complementary strengths to efficiently search multiple different objects in unknown environments. By validating SayCoNav with varied team compositions and conditions against baseline methods, our experimental results show that SayCoNav can improve search efficiency by at most 44.28% through effective collaboration among heterogeneous robots. It can also dynamically adapt to the changing conditions during task execution.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</title>
<link>https://arxiv.org/abs/2505.13738</link>
<guid>https://arxiv.org/abs/2505.13738</guid>
<content:encoded><![CDATA[
arXiv:2505.13738v1 Announce Type: cross 
Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate {\eta} and weight decay {\lambda}. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, B/({\eta}{\lambda}D), should remain constant across training settings, and we verify the implication that optimal {\lambda} scales linearly with B, for a fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict {\lambda}opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast with prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Compositional Generation with Diffusion Models Using Lift Scores</title>
<link>https://arxiv.org/abs/2505.13740</link>
<guid>https://arxiv.org/abs/2505.13740</guid>
<content:encoded><![CDATA[
arXiv:2505.13740v1 Announce Type: cross 
Abstract: We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at http://github.com/rainorangelemon/complift.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Task Representations in Neural Networks via Bayesian Ablation</title>
<link>https://arxiv.org/abs/2505.13742</link>
<guid>https://arxiv.org/abs/2505.13742</guid>
<content:encoded><![CDATA[
arXiv:2505.13742v1 Announce Type: cross 
Abstract: Neural networks are powerful tools for cognitive modeling due to their flexibility and emergent properties. However, interpreting their learned representations remains challenging due to their sub-symbolic semantics. In this work, we introduce a novel probabilistic framework for interpreting latent task representations in neural networks. Inspired by Bayesian inference, our approach defines a distribution over representational units to infer their causal contributions to task performance. Using ideas from information theory, we propose a suite of tools and metrics to illuminate key model properties, including representational distributedness, manifold complexity, and polysemanticity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques</title>
<link>https://arxiv.org/abs/2505.13766</link>
<guid>https://arxiv.org/abs/2505.13766</guid>
<content:encoded><![CDATA[
arXiv:2505.13766v1 Announce Type: cross 
Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens</title>
<link>https://arxiv.org/abs/2505.13775</link>
<guid>https://arxiv.org/abs/2505.13775</guid>
<content:encoded><![CDATA[
arXiv:2505.13775v1 Announce Type: cross 
Abstract: Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping</title>
<link>https://arxiv.org/abs/2505.13777</link>
<guid>https://arxiv.org/abs/2505.13777</guid>
<content:encoded><![CDATA[
arXiv:2505.13777v1 Announce Type: cross 
Abstract: We present Sat2Sound, a multimodal representation learning framework for soundscape mapping, designed to predict the distribution of sounds at any location on Earth. Existing methods for this task rely on satellite image and paired geotagged audio samples, which often fail to capture the diversity of sound sources at a given location. To address this limitation, we enhance existing datasets by leveraging a Vision-Language Model (VLM) to generate semantically rich soundscape descriptions for locations depicted in satellite images. Our approach incorporates contrastive learning across audio, audio captions, satellite images, and satellite image captions. We hypothesize that there is a fixed set of soundscape concepts shared across modalities. To this end, we learn a shared codebook of soundscape concepts and represent each sample as a weighted average of these concepts. Sat2Sound achieves state-of-the-art performance in cross-modal retrieval between satellite image and audio on two datasets: GeoSound and SoundingEarth. Additionally, building on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a novel application: location-based soundscape synthesis, which enables immersive acoustic experiences. Our code and models will be publicly available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Learning with Lie Detectors can Induce Honesty or Evasion</title>
<link>https://arxiv.org/abs/2505.13787</link>
<guid>https://arxiv.org/abs/2505.13787</guid>
<content:encoded><![CDATA[
arXiv:2505.13787v1 Announce Type: cross 
Abstract: As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment. Recent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking. We examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive. Using DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. We find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\%. However, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\% for realistic TPRs. Our results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.13792</link>
<guid>https://arxiv.org/abs/2505.13792</guid>
<content:encoded><![CDATA[
arXiv:2505.13792v1 Announce Type: cross 
Abstract: Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech</title>
<link>https://arxiv.org/abs/2505.13805</link>
<guid>https://arxiv.org/abs/2505.13805</guid>
<content:encoded><![CDATA[
arXiv:2505.13805v1 Announce Type: cross 
Abstract: Despite great advances, achieving high-fidelity emotional voice conversion (EVC) with flexible and interpretable control remains challenging. This paper introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality converted speech driven by natural language prompts or reference speech with adjustable emotion intensity. We first propose EVC-CLAP, an emotional contrastive language-audio pre-training model, guided by natural language prompts and categorical labels, to extract and align fine-grained emotional elements across speech and text modalities. Then, a FuEncoder with an adaptive intensity gate is presented to seamless fuse emotional features with Phonetic PosteriorGrams from a pre-trained ASR model. To further improve emotion expressiveness and speech naturalness, we propose a flow matching model conditioned on these captured features to reconstruct Mel-spectrogram of source speech. Subjective and objective evaluations validate the effectiveness of ClapFM-EVC.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework</title>
<link>https://arxiv.org/abs/2505.13808</link>
<guid>https://arxiv.org/abs/2505.13808</guid>
<content:encoded><![CDATA[
arXiv:2505.13808v1 Announce Type: cross 
Abstract: Metaheuristic algorithms are widely used for solving complex optimization problems, yet their effectiveness is often constrained by fixed structures and the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF) addresses this limitation by introducing a self-adaptive metaheuristic switching mechanism driven by real-time performance feedback and dynamic algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA) and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select and transition between metaheuristic algorithms based on key performance indicators, ensuring continuous adaptation. This approach enhances convergence speed, adaptability, and solution quality, outperforming traditional metaheuristics in high-dimensional, dynamic, and multimodal environments. Experimental results on benchmark functions demonstrate that PMF significantly improves optimization efficiency by mitigating stagnation and balancing exploration-exploitation strategies across various problem landscapes. By integrating AI-driven decision-making and self-correcting mechanisms, PMF paves the way for scalable, intelligent, and autonomous optimization frameworks, with promising applications in engineering, logistics, and complex decision-making systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Articulatory Feature Prediction from Surface EMG during Speech Production</title>
<link>https://arxiv.org/abs/2505.13814</link>
<guid>https://arxiv.org/abs/2505.13814</guid>
<content:encoded><![CDATA[
arXiv:2505.13814v1 Announce Type: cross 
Abstract: We present a model for predicting articulatory features from surface electromyography (EMG) signals during speech production. The proposed model integrates convolutional layers and a Transformer block, followed by separate predictors for articulatory features. Our approach achieves a high prediction correlation of approximately 0.9 for most articulatory features. Furthermore, we demonstrate that these predicted articulatory features can be decoded into intelligible speech waveforms. To our knowledge, this is the first method to decode speech waveforms from surface EMG via articulatory features, offering a novel approach to EMG-based speech synthesis. Additionally, we analyze the relationship between EMG electrode placement and articulatory feature predictability, providing knowledge-driven insights for optimizing EMG electrode configurations. The source code and decoded speech samples are publicly available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Agent Distillation for Large Language Model</title>
<link>https://arxiv.org/abs/2505.13820</link>
<guid>https://arxiv.org/abs/2505.13820</guid>
<content:encoded><![CDATA[
arXiv:2505.13820v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams</title>
<link>https://arxiv.org/abs/2505.13834</link>
<guid>https://arxiv.org/abs/2505.13834</guid>
<content:encoded><![CDATA[
arXiv:2505.13834v1 Announce Type: cross 
Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements</title>
<link>https://arxiv.org/abs/2505.13837</link>
<guid>https://arxiv.org/abs/2505.13837</guid>
<content:encoded><![CDATA[
arXiv:2505.13837v1 Announce Type: cross 
Abstract: Robots navigating complex environments must manage uncertainty from sensor noise, environmental changes, and incomplete information, with different tasks requiring varying levels of precision in different areas. For example, precise localization may be crucial near obstacles but less critical in open spaces. We present GUIDE (Generalized Uncertainty Integration for Decision-Making and Execution), a framework that integrates these task-specific requirements into navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning acceptable uncertainty levels to different locations, TSUMs enable robots to adapt uncertainty management based on context. When combined with reinforcement learning, GUIDE learns policies that balance task completion and uncertainty management without extensive reward engineering. Real-world tests show significant performance gains over methods lacking task-specific uncertainty awareness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientLLM: Efficiency in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13840</link>
<guid>https://arxiv.org/abs/2505.13840</guid>
<content:encoded><![CDATA[
arXiv:2505.13840v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensic deepfake audio detection using segmental speech features</title>
<link>https://arxiv.org/abs/2505.13847</link>
<guid>https://arxiv.org/abs/2505.13847</guid>
<content:encoded><![CDATA[
arXiv:2505.13847v1 Announce Type: cross 
Abstract: This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection differently for forensic voice comparison and offer a new perspective on leveraging segmental features for this purpose.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Gating Ensemble Networks for AI-Generated Text Detection</title>
<link>https://arxiv.org/abs/2505.13855</link>
<guid>https://arxiv.org/abs/2505.13855</guid>
<content:encoded><![CDATA[
arXiv:2505.13855v1 Announce Type: cross 
Abstract: As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer</title>
<link>https://arxiv.org/abs/2505.13857</link>
<guid>https://arxiv.org/abs/2505.13857</guid>
<content:encoded><![CDATA[
arXiv:2505.13857v1 Announce Type: cross 
Abstract: In real-world applications, GPS trajectories often suffer from low sampling rates, with large and irregular intervals between consecutive GPS points. This sparse characteristic presents challenges for their direct use in GPS-based systems. This paper addresses the task of map-constrained trajectory recovery, aiming to enhance trajectory sampling rates of GPS trajectories. Previous studies commonly adopt a sequence-to-sequence framework, where an encoder captures the trajectory patterns and a decoder reconstructs the target trajectory. Within this framework, effectively representing the road network and extracting relevant trajectory features are crucial for overall performance. Despite advancements in these models, they fail to fully leverage the complex spatio-temporal dynamics present in both the trajectory and the road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of trajectory data into two distinct aspects: spatial-temporal traffic dynamics and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce PD-GNN, which models periodic patterns and learns topologically aware dynamics concurrently for each road segment. For spatio-temporal trajectory dynamics, we present TedFormer, a time-aware Transformer that incorporates temporal dynamics for each GPS location by integrating closed-form neural ordinary differential equations into the attention mechanism. This allows TedFormer to effectively handle irregularly sampled data. Extensive experiments on three real-world datasets demonstrate the superior performance of TedTrajRec. The code is publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation of VLM for Soccer Video Understanding</title>
<link>https://arxiv.org/abs/2505.13860</link>
<guid>https://arxiv.org/abs/2505.13860</guid>
<content:encoded><![CDATA[
arXiv:2505.13860v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) have demonstrated strong performance in multi-modal tasks by effectively aligning visual and textual representations. However, most video understanding VLM research has been domain-agnostic, leaving the understanding of their transfer learning capability to specialized domains under-explored. In this work, we address this by exploring the adaptability of open-source VLMs to specific domains, and focusing on soccer as an initial case study. Our approach uses large-scale soccer datasets and LLM to create instruction-following data, and use them to iteratively fine-tune the general-domain VLM in a curriculum learning fashion (first teaching the model key soccer concepts to then question answering tasks). The final adapted model, trained using a curated dataset of 20k video clips, exhibits significant improvement in soccer-specific tasks compared to the base model, with a 37.5% relative improvement for the visual question-answering task and an accuracy improvement from 11.8% to 63.5% for the downstream soccer action classification task.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.13872</link>
<guid>https://arxiv.org/abs/2505.13872</guid>
<content:encoded><![CDATA[
arXiv:2505.13872v1 Announce Type: cross 
Abstract: Autonomous Driving (AD) systems demand the high levels of safety assurance. Despite significant advancements in AD demonstrated on open-source benchmarks like Longest6 and Bench2Drive, existing datasets still lack regulatory-compliant scenario libraries for closed-loop testing to comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD accidents are underrepresented in current driving datasets. This scarcity leads to inadequate evaluation of AD performance, posing risks to safety validation and practical deployment. To address these challenges, we propose Safety2Drive, a safety-critical scenario library designed to evaluate AD systems. Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively covers the test items required by standard regulations and contains 70 AD function test items. (2) Safety2Drive supports the safety-critical scenario generalization. It has the ability to inject safety threats such as natural environment corruptions and adversarial attacks cross camera and LiDAR sensors. (3) Safety2Drive supports multi-dimensional evaluation. In addition to the evaluation of AD systems, it also supports the evaluation of various perception tasks, such as object detection and lane detection. Safety2Drive provides a paradigm from scenario construction to validation, establishing a standardized test framework for the safe deployment of AD.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model</title>
<link>https://arxiv.org/abs/2505.13873</link>
<guid>https://arxiv.org/abs/2505.13873</guid>
<content:encoded><![CDATA[
arXiv:2505.13873v1 Announce Type: cross 
Abstract: Weather forecasting has long posed a significant challenge for humanity. While recent AI-based models have surpassed traditional numerical weather prediction (NWP) methods in global forecasting tasks, overfitting remains a critical issue due to the limited availability of real-world weather data spanning only a few decades. Unlike fields like computer vision or natural language processing, where data abundance can mitigate overfitting, weather forecasting demands innovative strategies to address this challenge with existing data. In this paper, we explore pre-training methods for weather forecasting, finding that selecting an appropriately challenging pre-training task introduces locality bias, effectively mitigating overfitting and enhancing performance. We introduce Baguan, a novel data-driven model for medium-range weather forecasting, built on a Siamese Autoencoder pre-trained in a self-supervised manner and fine-tuned for different lead times. Experimental results show that Baguan outperforms traditional methods, delivering more accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust overfitting control and excels in downstream tasks, such as subseasonal-to-seasonal (S2S) modeling and regional forecasting, after fine-tuning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Use Their Depth Efficiently?</title>
<link>https://arxiv.org/abs/2505.13898</link>
<guid>https://arxiv.org/abs/2505.13898</guid>
<content:encoded><![CDATA[
arXiv:2505.13898v1 Announce Type: cross 
Abstract: Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Insert for Constructive Neural Vehicle Routing Solver</title>
<link>https://arxiv.org/abs/2505.13904</link>
<guid>https://arxiv.org/abs/2505.13904</guid>
<content:encoded><![CDATA[
arXiv:2505.13904v1 Announce Type: cross 
Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data</title>
<link>https://arxiv.org/abs/2505.13906</link>
<guid>https://arxiv.org/abs/2505.13906</guid>
<content:encoded><![CDATA[
arXiv:2505.13906v1 Announce Type: cross 
Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise diagnosis and efficient treatment, particularly in light of escalating healthcare expenses and the expanding use of artificial intelligence in medical diagnostics. Many recent studies shows that the combination of brain Magnetic Resonance Imaging (MRI) and deep neural networks have achieved promising results for diagnosing AD. Using deep convolutional neural networks, this paper introduces a novel deep learning architecture that incorporates multiresidual blocks, specialized spatial attention blocks, grouped query attention, and multi-head attention. The study assessed the model's performance on four publicly accessible datasets and concentrated on identifying binary and multiclass issues across various categories. This paper also takes into account of the explainability of AD's progression and compared with state-of-the-art methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster Score-CAM, and XGRADCAM. Our methodology consistently outperforms current approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in 3-class classification, and 100\% in binary classification using Kaggle datasets. For Open Access Series of Imaging Studies (OASIS) datasets the accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in three planes (axial, sagittal, and coronal) and a combination of all planes. The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\% for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for ADNI-2. The network's ability to retrieve important information from MRI images is demonstrated by its excellent accuracy in categorizing AD stages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation</title>
<link>https://arxiv.org/abs/2505.13911</link>
<guid>https://arxiv.org/abs/2505.13911</guid>
<content:encoded><![CDATA[
arXiv:2505.13911v1 Announce Type: cross 
Abstract: Pulmonary segment segmentation is crucial for cancer localization and surgical planning. However, the pixel-wise annotation of pulmonary segments is laborious, as the boundaries between segments are indistinguishable in medical images. To this end, we propose a weakly supervised learning (WSL) method, termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise clinical anatomical definition of pulmonary segments to perform pulmonary segment segmentation. Since pulmonary segments reside within the lobes and are determined by the bronchovascular tree, i.e., artery, airway and vein, the design of the loss function is founded on two principles. First, segment-level labels are utilized to directly supervise the output of the pulmonary segments, ensuring that they accurately encompass the appropriate bronchovascular tree. Second, lobe-level supervision indirectly oversees the pulmonary segment, ensuring their inclusion within the corresponding lobe. Besides, we introduce a two-stage segmentation strategy that incorporates bronchovascular priori information. Furthermore, a consistency loss is proposed to enhance the smoothness of segment boundaries, along with an evaluation metric designed to measure the smoothness of pulmonary segment boundaries. Visual inspection and evaluation metrics from experiments conducted on a private dataset demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight</title>
<link>https://arxiv.org/abs/2505.13921</link>
<guid>https://arxiv.org/abs/2505.13921</guid>
<content:encoded><![CDATA[
arXiv:2505.13921v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP .
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLVR-World: Training World Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13934</link>
<guid>https://arxiv.org/abs/2505.13934</guid>
<content:encoded><![CDATA[
arXiv:2505.13934v1 Announce Type: cross 
Abstract: World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-to-Text Translation: A Model for Deciphering Human Brain Activity</title>
<link>https://arxiv.org/abs/2505.13936</link>
<guid>https://arxiv.org/abs/2505.13936</guid>
<content:encoded><![CDATA[
arXiv:2505.13936v1 Announce Type: cross 
Abstract: With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at https://github.com/Mmurrad/EEG-To-text.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title>
<link>https://arxiv.org/abs/2505.13938</link>
<guid>https://arxiv.org/abs/2505.13938</guid>
<content:encoded><![CDATA[
arXiv:2505.13938v1 Announce Type: cross 
Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean's type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(https://github.com/trishullab/clever) as well as HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our evaluation code is also available online(https://github.com/trishullab/clever-prover).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLZero: A Multi-Agent System for End-to-end Machine Learning Automation</title>
<link>https://arxiv.org/abs/2505.13941</link>
<guid>https://arxiv.org/abs/2505.13941</guid>
<content:encoded><![CDATA[
arXiv:2505.13941v1 Announce Type: cross 
Abstract: Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Additionally, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Centric Embodied Question Answer</title>
<link>https://arxiv.org/abs/2505.13948</link>
<guid>https://arxiv.org/abs/2505.13948</guid>
<content:encoded><![CDATA[
arXiv:2505.13948v1 Announce Type: cross 
Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and understand the environment to answer context-dependent questions. Existing frameworks typically center around the planner, which guides the stopping module, memory module, and answering module for reasoning. In this paper, we propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric EQA models where the memory module cannot fully interact with other modules, MemoryEQA flexible feeds memory information into all modules, thereby enhancing efficiency and accuracy in handling complex tasks, such as those involving multiple targets across different regions. Specifically, we establish a multi-modal hierarchical memory mechanism, which is divided into global memory that stores language-enhanced scene maps, and local memory that retains historical observations and state information. When performing EQA tasks, the multi-modal large language model is leveraged to convert memory information into the required input formats for injection into different modules. To evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset based on HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 19.8% performance gain on MT-HM3D compared to baseline model further underscores memory capability's pivotal role in resolving complex tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashThink: An Early Exit Method For Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.13949</link>
<guid>https://arxiv.org/abs/2505.13949</guid>
<content:encoded><![CDATA[
arXiv:2505.13949v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring</title>
<link>https://arxiv.org/abs/2505.13965</link>
<guid>https://arxiv.org/abs/2505.13965</guid>
<content:encoded><![CDATA[
arXiv:2505.13965v1 Announce Type: cross 
Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly with the increasing prevalence of multimodal assessments. However, traditional AES methods struggle with evaluation generalizability and multimodal perception, while even recent Multimodal Large Language Model (MLLM)-based approaches can produce hallucinated justifications and scores misaligned with human judgment. To address the limitations, we introduce CAFES, the first collaborative multi-agent framework specifically designed for AES. It orchestrates three specialized agents: an Initial Scorer for rapid, trait-specific evaluations; a Feedback Pool Manager to aggregate detailed, evidence-grounded strengths; and a Reflective Scorer that iteratively refines scores based on this feedback to enhance human alignment. Extensive experiments, using state-of-the-art MLLMs, achieve an average relative improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth, especially for grammatical and lexical diversity. Our proposed CAFES framework paves the way for an intelligent multimodal AES system. The code will be available upon acceptance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World</title>
<link>https://arxiv.org/abs/2505.13969</link>
<guid>https://arxiv.org/abs/2505.13969</guid>
<content:encoded><![CDATA[
arXiv:2505.13969v1 Announce Type: cross 
Abstract: This paper discusses the functional advantages of the Selection-Broadcast Cycle structure proposed by Global Workspace Theory (GWT), inspired by human consciousness, particularly focusing on its applicability to artificial intelligence and robotics in dynamic, real-time scenarios. While previous studies often examined the Selection and Broadcast processes independently, this research emphasizes their combined cyclic structure and the resulting benefits for real-time cognitive systems. Specifically, the paper identifies three primary benefits: Dynamic Thinking Adaptation, Experience-Based Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's potential as a cognitive architecture suitable for sophisticated decision-making and adaptive performance in unsupervised, dynamic environments. It suggests new directions for the development and implementation of robust, general-purpose AI and robotics systems capable of managing complex, real-world tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition</title>
<link>https://arxiv.org/abs/2505.13971</link>
<guid>https://arxiv.org/abs/2505.13971</guid>
<content:encoded><![CDATA[
arXiv:2505.13971v1 Announce Type: cross 
Abstract: Meetings are a valuable yet challenging scenario for speech applications due to complex acoustic conditions. This paper summarizes the outcomes of the MISP 2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal, multi-device meeting transcription by incorporating video modality alongside audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR). We present the challenge's objectives, tasks, dataset, baseline systems, and solutions proposed by participants. The best-performing systems achieved significant improvements over the baseline: the top AVSD model achieved a Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the best AVDR system achieved a concatenated minimum-permutation Character Error Rate (cpCER) of 11.56%, improving by 72.49%.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13973</link>
<guid>https://arxiv.org/abs/2505.13973</guid>
<content:encoded><![CDATA[
arXiv:2505.13973v1 Announce Type: cross 
Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty</title>
<link>https://arxiv.org/abs/2505.13989</link>
<guid>https://arxiv.org/abs/2505.13989</guid>
<content:encoded><![CDATA[
arXiv:2505.13989v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have significantly advanced text-attributed graph (TAG) learning. However, existing methods inadequately handle data uncertainty in open-world scenarios, especially concerning limited labeling and unknown-class nodes. Prior solutions typically rely on isolated semantic or structural approaches for unknown-class rejection, lacking effective annotation pipelines. To address these limitations, we propose Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive label traceability, which integrates semantics and topology for unknown-class rejection, and a graph label annotator to enable model updates using newly annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and practicality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Sycophancy: A Broader Understanding of LLM Sycophancy</title>
<link>https://arxiv.org/abs/2505.13995</link>
<guid>https://arxiv.org/abs/2505.13995</guid>
<content:encoded><![CDATA[
arXiv:2505.13995v1 Announce Type: cross 
Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, we introduce a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). We present ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, we show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. We further show that social sycophancy is rewarded in preference datasets and is not easily mitigated. Our work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.14005</link>
<guid>https://arxiv.org/abs/2505.14005</guid>
<content:encoded><![CDATA[
arXiv:2505.14005v1 Announce Type: cross 
Abstract: To enhance the reliability and credibility of graph neural networks (GNNs) and improve the transparency of their decision logic, a new field of explainability of GNNs (XGNN) has emerged. However, two major limitations severely degrade the performance and hinder the generalizability of existing XGNN methods: they (a) fail to capture the complete decision logic of GNNs across diverse distributions in the entire dataset's sample space, and (b) impose strict prerequisites on edge properties and GNN internal accessibility. To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as the first work in the literature, can infer and partition the entire dataset's sample space into multiple environments, each containing graphs that follow a distinct distribution. OPEN further learns the decision logic of GNNs across different distributions by sampling subgraphs from each environment and analyzing their predictions, thus eliminating the need for strict prerequisites. Experimental results demonstrate that OPEN captures nearly complete decision logic of GNNs, outperforms state-of-the-art methods in fidelity while maintaining similar efficiency, and enhances robustness in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix</title>
<link>https://arxiv.org/abs/2505.14024</link>
<guid>https://arxiv.org/abs/2505.14024</guid>
<content:encoded><![CDATA[
arXiv:2505.14024v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables geographically distributed clients to collaboratively train machine learning models by sharing only their local models, ensuring data privacy. However, FL is vulnerable to untargeted attacks that aim to degrade the global model's performance on the underlying data distribution. Existing defense mechanisms attempt to improve FL's resilience against such attacks, but their effectiveness is limited in practical FL environments due to data heterogeneity. On the contrary, we aim to detect and remove the attacks to mitigate their impact. Generalization contribution plays a crucial role in distinguishing untargeted attacks. Our observations indicate that, with limited data, the divergence between embeddings representing different classes provides a better measure of generalization than direct accuracy. In light of this, we propose a novel robust aggregation method, FedGraM, designed to defend against untargeted attacks in FL. The server maintains an auxiliary dataset containing one sample per class to support aggregation. This dataset is fed to the local models to extract embeddings. Then, the server calculates the norm of the Gram Matrix of the embeddings for each local model. The norm serves as an indicator of each model's inter-class separation capability in the embedding space. FedGraM identifies and removes potentially malicious models by filtering out those with the largest norms, then averages the remaining local models to form the global model. We conduct extensive experiments to evaluate the performance of FedGraM. Our empirical results show that with limited data samples used to construct the auxiliary dataset, FedGraM achieves exceptional performance, outperforming state-of-the-art defense methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data</title>
<link>https://arxiv.org/abs/2505.14027</link>
<guid>https://arxiv.org/abs/2505.14027</guid>
<content:encoded><![CDATA[
arXiv:2505.14027v1 Announce Type: cross 
Abstract: As computer networks proliferate, the gravity of network intrusions has escalated, emphasizing the criticality of network intrusion detection systems for safeguarding security. While deep learning models have exhibited promising results in intrusion detection, they face challenges in managing high-dimensional, complex traffic patterns and imbalanced data categories. This paper presents CSAGC-IDS, a network intrusion detection model based on deep learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced convolutional conditional generative adversarial network that generates high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS integrates CSCA-CNN, a convolutional neural network enhanced through cost sensitive learning and channel attention mechanism, to extract features from complex traffic data for precise detection. Experiments conducted on the NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of 84.52% in five-class classification task, and an accuracy of 91.09% and an F1 score of 92.04% in binary classification task.Furthermore, this paper provides an interpretability analysis of the proposed model, using SHAP and LIME to explain the decision-making mechanisms of the model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards</title>
<link>https://arxiv.org/abs/2505.14029</link>
<guid>https://arxiv.org/abs/2505.14029</guid>
<content:encoded><![CDATA[
arXiv:2505.14029v1 Announce Type: cross 
Abstract: Deep learning has transformed computer vision for precision agriculture, yet apple orchard monitoring remains limited by dataset constraints. The lack of diverse, realistic datasets and the difficulty of annotating dense, heterogeneous scenes. Existing datasets overlook different growth stages and stereo imagery, both essential for realistic 3D modeling of orchards and tasks like fruit localization, yield estimation, and structural analysis. To address these gaps, we present AppleGrowthVision, a large-scale dataset comprising two subsets. The first includes 9,317 high resolution stereo images collected from a farm in Brandenburg (Germany), covering six agriculturally validated growth stages over a full growth cycle. The second subset consists of 1,125 densely annotated images from the same farm in Brandenburg and one in Pillnitz (Germany), containing a total of 31,084 apple labels. AppleGrowthVision provides stereo-image data with agriculturally validated growth stages, enabling precise phenological analysis and 3D reconstructions. Extending MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by 31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges the gap between agricultural science and computer vision, by enabling the development of robust models for fruit detection, growth modeling, and 3D analysis in precision agriculture. Future work includes improving annotation, enhancing 3D reconstruction, and extending multimodal analysis across all growth stages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cyclic Diffusion for Inference Scaling</title>
<link>https://arxiv.org/abs/2505.14036</link>
<guid>https://arxiv.org/abs/2505.14036</guid>
<content:encoded><![CDATA[
arXiv:2505.14036v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated strong generative capabilities across domains ranging from image synthesis to complex reasoning tasks. However, most inference-time scaling methods rely on fixed denoising schedules, limiting their ability to allocate computation based on instance difficulty or task-specific demands adaptively. We introduce the challenge of adaptive inference-time scaling-dynamically adjusting computational effort during inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a flexible, search-based inference framework. ABCD refines outputs through bi-directional diffusion cycles while adaptively controlling exploration depth and termination. It comprises three components: Cyclic Diffusion Search, Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time. Experiments show that ABCD improves performance across diverse tasks while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora</title>
<link>https://arxiv.org/abs/2505.14045</link>
<guid>https://arxiv.org/abs/2505.14045</guid>
<content:encoded><![CDATA[
arXiv:2505.14045v1 Announce Type: cross 
Abstract: Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Field Matters: A lightweight LLM-enhanced Method for CTR Prediction</title>
<link>https://arxiv.org/abs/2505.14057</link>
<guid>https://arxiv.org/abs/2505.14057</guid>
<content:encoded><![CDATA[
arXiv:2505.14057v1 Announce Type: cross 
Abstract: Click-through rate (CTR) prediction is a fundamental task in modern recommender systems. In recent years, the integration of large language models (LLMs) has been shown to effectively enhance the performance of traditional CTR methods. However, existing LLM-enhanced methods often require extensive processing of detailed textual descriptions for large-scale instances or user/item entities, leading to substantial computational overhead. To address this challenge, this work introduces LLaCTR, a novel and lightweight LLM-enhanced CTR method that employs a field-level enhancement paradigm. Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight semantic knowledge from small-scale feature fields through self-supervised field-feature fine-tuning. Subsequently, it leverages this field-level semantic knowledge to enhance both feature representation and feature interactions. In our experiments, we integrate LLaCTR with six representative CTR models across four datasets, demonstrating its superior performance in terms of both effectiveness and efficiency compared to existing LLM-enhanced methods. Our code is available at https://anonymous.4open.science/r/LLaCTR-EC46.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI</title>
<link>https://arxiv.org/abs/2505.14064</link>
<guid>https://arxiv.org/abs/2505.14064</guid>
<content:encoded><![CDATA[
arXiv:2505.14064v1 Announce Type: cross 
Abstract: In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously $unknown$ categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$ benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an $extreme$ stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory</title>
<link>https://arxiv.org/abs/2505.14080</link>
<guid>https://arxiv.org/abs/2505.14080</guid>
<content:encoded><![CDATA[
arXiv:2505.14080v1 Announce Type: cross 
Abstract: Language models encode and subsequently perpetuate harmful gendered stereotypes. Research has succeeded in mitigating some of these harms, e.g. by dissociating non-gendered terms such as occupations from gendered terms such as 'woman' and 'man'. This approach, however, remains superficial given that associations are only one form of prejudice through which gendered harms arise. Critical scholarship on gender, such as gender performativity theory, emphasizes how harms often arise from the construction of gender itself, such as conflating gender with biological sex. In language models, these issues could lead to the erasure of transgender and gender diverse identities and cause harms in downstream applications, from misgendering users to misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic associations, we advocate for a broader definition of 'gender bias' in language models. We operationalize insights on the construction of gender through language from gender studies literature and then empirically test how 16 language models of different architectures, training datasets, and model sizes encode gender. We find that language models tend to encode gender as a binary category tied to biological sex, and that gendered terms that do not neatly fall into one of these binary categories are erased and pathologized. Finally, we show that larger models, which achieve better results on performance benchmarks, learn stronger associations between gender and sex, further reinforcing a narrow understanding of gender. Our findings lead us to call for a re-evaluation of how gendered harms in language models are defined and addressed.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2505.14103</link>
<guid>https://arxiv.org/abs/2505.14103</guid>
<content:encoded><![CDATA[
arXiv:2505.14103v1 Announce Type: cross 
Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations</title>
<link>https://arxiv.org/abs/2505.14106</link>
<guid>https://arxiv.org/abs/2505.14106</guid>
<content:encoded><![CDATA[
arXiv:2505.14106v1 Announce Type: cross 
Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14107</link>
<guid>https://arxiv.org/abs/2505.14107</guid>
<content:encoded><![CDATA[
arXiv:2505.14107v1 Announce Type: cross 
Abstract: The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AIs diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Unlabeled Data Optimization</title>
<link>https://arxiv.org/abs/2505.14117</link>
<guid>https://arxiv.org/abs/2505.14117</guid>
<content:encoded><![CDATA[
arXiv:2505.14117v1 Announce Type: cross 
Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of unlabeled data, tackling a critical question: How can we enhance the efficiency and sustainability of deep learning training by optimizing the data itself? We begin by identifying three key limitations in existing model-centric approaches, all rooted in a shared bottleneck: knowledge extracted from data is locked to model parameters, hindering its reusability and scalability. To this end, we propose CoOpt, a highly efficient, parallelized framework for collaborative unlabeled data optimization, thereby effectively encoding knowledge into the data itself. By distributing unlabeled data and leveraging publicly available task-agnostic models, CoOpt facilitates scalable, reusable, and sustainable training pipelines. Extensive experiments across diverse datasets and architectures demonstrate its efficacy and efficiency, achieving 13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively, with training speedups of $1.94 \times $ and $1.2 \times$.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning</title>
<link>https://arxiv.org/abs/2505.14125</link>
<guid>https://arxiv.org/abs/2505.14125</guid>
<content:encoded><![CDATA[
arXiv:2505.14125v1 Announce Type: cross 
Abstract: Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Methodological Framework for Measuring Spatial Labeling Similarity</title>
<link>https://arxiv.org/abs/2505.14128</link>
<guid>https://arxiv.org/abs/2505.14128</guid>
<content:encoded><![CDATA[
arXiv:2505.14128v1 Announce Type: cross 
Abstract: Spatial labeling assigns labels to specific spatial locations to characterize their spatial properties and relationships, with broad applications in scientific research and practice. Measuring the similarity between two spatial labelings is essential for understanding their differences and the contributing factors, such as changes in location properties or labeling methods. An adequate and unbiased measurement of spatial labeling similarity should consider the number of matched labels (label agreement), the topology of spatial label distribution, and the heterogeneous impacts of mismatched labels. However, existing methods often fail to account for all these aspects. To address this gap, we propose a methodological framework to guide the development of methods that meet these requirements. Given two spatial labelings, the framework transforms them into graphs based on location organization, labels, and attributes (e.g., location significance). The distributions of their graph attributes are then extracted, enabling an efficient computation of distributional discrepancy to reflect the dissimilarity level between the two labelings. We further provide a concrete implementation of this framework, termed Spatial Labeling Analogy Metric (SLAM), along with an analysis of its theoretical foundation, for evaluating spatial labeling results in spatial transcriptomics (ST) \textit{as per} their similarity with ground truth labeling. Through a series of carefully designed experimental cases involving both simulated and real ST data, we demonstrate that SLAM provides a comprehensive and accurate reflection of labeling quality compared to other well-established evaluation metrics. Our code is available at https://github.com/YihDu/SLAM.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging</title>
<link>https://arxiv.org/abs/2505.14136</link>
<guid>https://arxiv.org/abs/2505.14136</guid>
<content:encoded><![CDATA[
arXiv:2505.14136v1 Announce Type: cross 
Abstract: Mixture of expert (MoE) models are a promising approach to increasing model capacity without increasing inference cost, and are core components of many state-of-the-art language models. However, current MoE models typically use only few experts due to prohibitive training and inference cost. We propose Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of magnitude more experts and uses model merging to avoid almost any test-time overhead. We show that TTMM is an approximation of test-time training (TTT), which fine-tunes an expert model for each prediction task, i.e., prompt. TTT has recently been shown to significantly improve language models, but is computationally expensive. We find that performance of TTMM improves with more experts and approaches the performance of TTT. Moreover, we find that with a 1B parameter base model, TTMM is more than 100x faster than TTT at test-time by amortizing the cost of TTT at train-time. Thus, TTMM offers a promising cost-effective approach to scale test-time training.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14139</link>
<guid>https://arxiv.org/abs/2505.14139</guid>
<content:encoded><![CDATA[
arXiv:2505.14139v1 Announce Type: cross 
Abstract: The use of guidance to steer sampling toward desired outcomes has been widely explored within diffusion models, especially in applications such as image and trajectory generation. However, incorporating guidance during training remains relatively underexplored. In this work, we introduce energy-guided flow matching, a novel approach that enhances the training of flow models and eliminates the need for guidance at inference time. We learn a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path. Learning guided trajectories is appealing for tasks where the target distribution is defined by a combination of data and an energy function, as in reinforcement learning. Diffusion-based policies have recently attracted attention for their expressive power and ability to capture multi-modal action distributions. Typically, these policies are optimized using weighted objectives or by back-propagating gradients through actions sampled by the policy. As an alternative, we propose FlowQ, an offline reinforcement learning algorithm based on energy-guided flow matching. Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search</title>
<link>https://arxiv.org/abs/2505.14156</link>
<guid>https://arxiv.org/abs/2505.14156</guid>
<content:encoded><![CDATA[
arXiv:2505.14156v1 Announce Type: cross 
Abstract: Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prior Prompt Engineering for Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.14157</link>
<guid>https://arxiv.org/abs/2505.14157</guid>
<content:encoded><![CDATA[
arXiv:2505.14157v1 Announce Type: cross 
Abstract: This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits</title>
<link>https://arxiv.org/abs/2505.14178</link>
<guid>https://arxiv.org/abs/2505.14178</guid>
<content:encoded><![CDATA[
arXiv:2505.14178v1 Announce Type: cross 
Abstract: Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Abstractive Summarization of Scientific Papers Using Structure Information</title>
<link>https://arxiv.org/abs/2505.14179</link>
<guid>https://arxiv.org/abs/2505.14179</guid>
<content:encoded><![CDATA[
arXiv:2505.14179v1 Announce Type: cross 
Abstract: Abstractive summarization of scientific papers has always been a research focus, yet existing methods face two main challenges. First, most summarization models rely on Encoder-Decoder architectures that treat papers as sequences of words, thus fail to fully capture the structured information inherent in scientific papers. Second, existing research often use keyword mapping or feature engineering to identify the structural information, but these methods struggle with the structural flexibility of scientific papers and lack robustness across different disciplines. To address these challenges, we propose a two-stage abstractive summarization framework that leverages automatic recognition of structural functions within scientific papers. In the first stage, we standardize chapter titles from numerous scientific papers and construct a large-scale dataset for structural function recognition. A classifier is then trained to automatically identify the key structural components (e.g., Background, Methods, Results, Discussion), which provides a foundation for generating more balanced summaries. In the second stage, we employ Longformer to capture rich contextual relationships across sections and generating context-aware summaries. Experiments conducted on two domain-specific scientific paper summarization datasets demonstrate that our method outperforms advanced baselines, and generates more comprehensive summaries. The code and dataset can be accessed at https://github.com/tongbao96/code-for-SFR-AS.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Subspaces are Not Distinct: A Fine-Tuning Case Study</title>
<link>https://arxiv.org/abs/2505.14185</link>
<guid>https://arxiv.org/abs/2505.14185</guid>
<content:encoded><![CDATA[
arXiv:2505.14185v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\alpha$-GAN by R\'{e}nyi Cross Entropy</title>
<link>https://arxiv.org/abs/2505.14190</link>
<guid>https://arxiv.org/abs/2505.14190</guid>
<content:encoded><![CDATA[
arXiv:2505.14190v1 Announce Type: cross 
Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross entropy, as an expected certainty measure incurred by the discriminator's soft decision as to where the sample is from, true population or the generator. The discriminator tries to maximize the R\'{e}nyi certainty about sample source, while the generator wants to reduce it by injecting fake samples. This forms a min-max problem with the solution parameterized by the R\'{e}nyi order $\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the value function is exactly the binary cross entropy. The optimization of $\alpha$-GAN is over probability (vector) space. It is shown that the gradient is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in (0,1)$. This makes convergence faster, which is verified by experimental results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to solve some common problems, e.g., vanishing gradient. A following observation reveals that this range has not been fully explored in the existing R\'{e}nyi version GANs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLASH-D: FlashAttention with Hidden Softmax Division</title>
<link>https://arxiv.org/abs/2505.14201</link>
<guid>https://arxiv.org/abs/2505.14201</guid>
<content:encoded><![CDATA[
arXiv:2505.14201v1 Announce Type: cross 
Abstract: The transformer's attention mechanism has revolutionized AI and machine learning, with its efficient computation being crucial to its performance. However, calculating attention involves matrix operations interspersed with softmax rescaling, which inherently slows down computation and requires processing the entire input sequence. Building on online softmax computation, FlashAttention integrates softmax calculation with matrix arithmetic, enabling tiled computation independent of sequence length. While optimized for GPUs, FlashAttention's simplicity makes it amenable to direct hardware acceleration. This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a mathematically equivalent, yet simplified, formulation that achieves: (a) hiding softmax division within other non-linear function evaluations; (b) inherently numerically stable computation of exponentials, eliminating the need for maximum value subtraction; and (c) a reduction in computational cost without introducing numerical approximations to the FlashAttention kernel. Importantly, the essential FlashAttention properties that facilitate efficient tiled implementation are fully preserved. Hardware implementation results at 28nm demonstrate that this proposed formulation achieves a 22.8% reduction in area and a 20.3% reduction in power, on average, compared to state-of-the-art parallel hardware architectures without any performance penalty.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data</title>
<link>https://arxiv.org/abs/2505.14206</link>
<guid>https://arxiv.org/abs/2505.14206</guid>
<content:encoded><![CDATA[
arXiv:2505.14206v1 Announce Type: cross 
Abstract: The widespread adoption of mobile sensors has the potential to provide massive and heterogeneous time series data, driving Artificial Intelligence applications in mHealth. However, data collection remains limited due to stringent ethical regulations, privacy concerns, and other constraints, hindering progress in the field. Synthetic data generation, particularly through Generative Adversarial Networks and Diffusion Models, has emerged as a promising solution to address both data scarcity and privacy issues. Yet, these models are often limited to short-term, unimodal signal patterns. This paper presents a systematic evaluation of state-of-the-art generative models for time series synthesis, with a focus on their ability to jointly handle multi-modality, long-range dependencies, and conditional generation-key challenges in the mHealth domain. To ensure a fair comparison, we introduce a novel evaluation framework designed to measure both the intrinsic quality of synthetic data and its utility in downstream predictive tasks. Our findings reveal critical limitations in the existing approaches, particularly in maintaining cross-modal consistency, preserving temporal coherence, and ensuring robust performance in train-on-synthetic, test-on-real, and data augmentation scenarios. Finally, we present our future research directions to enhance synthetic time series generation and improve the applicability of generative models in mHealth.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks</title>
<link>https://arxiv.org/abs/2505.14212</link>
<guid>https://arxiv.org/abs/2505.14212</guid>
<content:encoded><![CDATA[
arXiv:2505.14212v1 Announce Type: cross 
Abstract: A question-answering (QA) system is to search suitable answers within a knowledge base. Current QA systems struggle with queries requiring complex reasoning or real-time knowledge integration. They are often supplemented with retrieval techniques on a data source such as Retrieval-Augmented Generation (RAG). However, RAG continues to face challenges in handling complex reasoning and logical connections between multiple sources of information. A novel approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA tasks is presented through the automated generation of context-based QA pairs. This methodology leverages LLMs to create fine-tuning data, reducing reliance on human labelling and improving model comprehension and reasoning capabilities. The proposed system includes an automated QA generator and a model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore. Comprehensive experiments demonstrate improvements in logical coherence and factual accuracy, with implications for developing adaptable Artificial Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1, BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA pairs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned</title>
<link>https://arxiv.org/abs/2505.14217</link>
<guid>https://arxiv.org/abs/2505.14217</guid>
<content:encoded><![CDATA[
arXiv:2505.14217v1 Announce Type: cross 
Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB) diagnosis using chest X-rays in low-resource settings across Africa. FL allows hospitals to collaboratively train AI models without sharing raw patient data, addressing privacy concerns and data scarcity that hinder traditional centralized models. The research involved hospitals and research centers in eight African countries. Most sites used local datasets, while Ghana and The Gambia used public ones. The study compared locally trained models with a federated model built across all institutions to evaluate FL's real-world feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces challenges such as poor infrastructure, unreliable internet, limited digital literacy, and weak AI regulations. Some institutions were also reluctant to share model updates due to data control concerns. In conclusion, FL shows strong potential for enabling AI-driven healthcare in underserved regions, but broader adoption will require improvements in infrastructure, education, and regulatory support.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs</title>
<link>https://arxiv.org/abs/2505.14226</link>
<guid>https://arxiv.org/abs/2505.14226</guid>
<content:encoded><![CDATA[
arXiv:2505.14226v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoQA: Visual-only Question Answering</title>
<link>https://arxiv.org/abs/2505.14227</link>
<guid>https://arxiv.org/abs/2505.14227</guid>
<content:encoded><![CDATA[
arXiv:2505.14227v1 Announce Type: cross 
Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in which questions are visually embedded within images, without any accompanying textual input. This requires models to locate, recognize, and reason over visually embedded textual questions, posing challenges for existing large vision-language models (LVLMs), which show notable performance drops even with carefully designed prompts. To bridge this gap, we introduce Guided Response Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy that guides the model to perform step-by-step reasoning purely based on visual input, significantly improving model performance. Our work enhances models' capacity for human-like visual understanding in complex multimodal scenarios, where information, including language, is perceived visually.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Fine-tuning for In-context Learning</title>
<link>https://arxiv.org/abs/2505.14233</link>
<guid>https://arxiv.org/abs/2505.14233</guid>
<content:encoded><![CDATA[
arXiv:2505.14233v1 Announce Type: cross 
Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and close Shannon entropy approximation</title>
<link>https://arxiv.org/abs/2505.14234</link>
<guid>https://arxiv.org/abs/2505.14234</guid>
<content:encoded><![CDATA[
arXiv:2505.14234v1 Announce Type: cross 
Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy are key components in many tools used in physics, information theory, machine learning (ML) and quantum computing. Besides of the significant amounts of SE computations required in these fields, the singularity of the SE gradient is one of the central mathematical reason inducing the high cost, frequently low robustness and slow convergence of such tools. Here we propose the Fast Entropy Approximation (FEA) - a non-singular rational approximation of Shannon entropy and its gradient that achieves a mean absolute error of $10^{-3}$, which is approximately $20$ times lower than comparable state-of-the-art methods. FEA allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary computational operations, as compared to tens of elementary operations behind the fastest entropy computation algorithms with table look-ups, bitshifts, or series approximations. On a set of common benchmarks for the feature selection problem in machine learning, we show that the combined effect of fewer elementary operations, low approximation error, and a non-singular gradient allows significantly better model quality and enables ML feature extraction that is two to three orders of magnitude faster and computationally cheaper when incorporating FEA into AI tools.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14238</link>
<guid>https://arxiv.org/abs/2505.14238</guid>
<content:encoded><![CDATA[
arXiv:2505.14238v1 Announce Type: cross 
Abstract: Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Agentic Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.14246</link>
<guid>https://arxiv.org/abs/2505.14246</guid>
<content:encoded><![CDATA[
arXiv:2505.14246v1 Announce Type: cross 
Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.14252</link>
<guid>https://arxiv.org/abs/2505.14252</guid>
<content:encoded><![CDATA[
arXiv:2505.14252v1 Announce Type: cross 
Abstract: In this work, we explore the integration of Sequence Encoding for Online Parameter Identification with Physics-Informed Neural Networks to create a model that, once trained, can be utilized for real time applications with variable parameters, boundary conditions, and initial conditions. Recently, the combination of PINNs with Sparse Regression has emerged as a method for performing dynamical system identification through supervised learning and sparse regression optimization, while also solving the dynamics using PINNs. However, this approach can be limited by variations in parameters or boundary and initial conditions, requiring retraining of the model whenever changes occur. In this work, we introduce an architecture that employs Deep Sets or Sequence Encoders to encode dynamic parameters, boundary conditions, and initial conditions, using these encoded features as inputs for the PINN, enabling the model to adapt to changes in parameters, BCs, and ICs. We apply this approach to three different problems. First, we analyze the Rossler ODE system, demonstrating the robustness of the model with respect to noise and its ability to generalize. Next, we explore the model's capability in a 2D Navier-Stokes PDE problem involving flow past a cylinder with a parametric sinusoidal inlet velocity function, showing that the model can encode pressure data from a few points to identify the inlet velocity profile and utilize physics to compute velocity and pressure throughout the domain. Finally, we address a 1D heat monitoring problem using real data from the heating of glass fiber and thermoplastic composite plates.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation</title>
<link>https://arxiv.org/abs/2505.14256</link>
<guid>https://arxiv.org/abs/2505.14256</guid>
<content:encoded><![CDATA[
arXiv:2505.14256v1 Announce Type: cross 
Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Decoding Reimagined for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.14260</link>
<guid>https://arxiv.org/abs/2505.14260</guid>
<content:encoded><![CDATA[
arXiv:2505.14260v1 Announce Type: cross 
Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate Multimodal Large Language Models (MLLMs) inference. Speculative decoding has been shown to accelerate Large Language Models (LLMs) without sacrificing accuracy. However, current speculative decoding methods for MLLMs fail to achieve the same speedup as they do for LLMs. To address this, we reimagine speculative decoding specifically for MLLMs. Our analysis of MLLM characteristics reveals two key design principles for MSD: (1) Text and visual tokens have fundamentally different characteristics and need to be processed separately during drafting. (2) Both language modeling ability and visual perception capability are crucial for the draft model. For the first principle, MSD decouples text and visual tokens in the draft model, allowing each to be handled based on its own characteristics. For the second principle, MSD uses a two-stage training strategy: In stage one, the draft model is trained on text-only instruction-tuning datasets to improve its language modeling ability. In stage two, MSD gradually introduces multimodal data to enhance the visual perception capability of the draft model. Experiments show that MSD boosts inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$ for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness. Our code is available at https://github.com/Lyn-Lucy/MSD.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think-J: Learning to Think for Generative LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.14268</link>
<guid>https://arxiv.org/abs/2505.14268</guid>
<content:encoded><![CDATA[
arXiv:2505.14268v1 Announce Type: cross 
Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning</title>
<link>https://arxiv.org/abs/2505.14273</link>
<guid>https://arxiv.org/abs/2505.14273</guid>
<content:encoded><![CDATA[
arXiv:2505.14273v1 Announce Type: cross 
Abstract: Function approximation is a critical task in various fields. However, existing neural network approaches struggle with locally complex or discontinuous functions due to their reliance on a single global model covering the entire problem space. We propose X-KAN, a novel method that optimizes multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary rule-based machine learning framework called XCSF. X-KAN combines KAN's high expressiveness with XCSF's adaptive partitioning capability by implementing local KAN models as rule consequents and defining local regions via rule antecedents. Our experimental results on artificial test functions and real-world datasets demonstrate that X-KAN significantly outperforms conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms of approximation accuracy. Notably, X-KAN effectively handles functions with locally complex or discontinuous structures that are challenging for conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules). These results validate the effectiveness of using KAN as a local model in XCSF, which evaluates the rule fitness based on both accuracy and generality. Our X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering</title>
<link>https://arxiv.org/abs/2505.14279</link>
<guid>https://arxiv.org/abs/2505.14279</guid>
<content:encoded><![CDATA[
arXiv:2505.14279v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&amp;A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry and artificial general intelligence.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis</title>
<link>https://arxiv.org/abs/2505.14285</link>
<guid>https://arxiv.org/abs/2505.14285</guid>
<content:encoded><![CDATA[
arXiv:2505.14285v1 Announce Type: cross 
Abstract: This paper presents AquaSignal, a modular and scalable pipeline for preprocessing, denoising, classification, and novelty detection of underwater acoustic signals. Designed to operate effectively in noisy and dynamic marine environments, AquaSignal integrates state-of-the-art deep learning architectures to enhance the reliability and accuracy of acoustic signal analysis. The system is evaluated on a combined dataset from the Deepship and Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a ResNet18 convolutional neural network for classifying known acoustic events, and an AutoEncoder-based model for unsupervised detection of novel or anomalous signals. To our knowledge, this is the first comprehensive study to apply and evaluate this combination of techniques on maritime vessel acoustic data. Experimental results show that AquaSignal improves signal clarity and task performance, achieving 71% classification accuracy and 91% accuracy in novelty detection. Despite slightly lower classification performance compared to some state-of-the-art models, differences in data partitioning strategies limit direct comparisons. Overall, AquaSignal demonstrates strong potential for real-time underwater acoustic monitoring in scientific, environmental, and maritime domains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking data encoding methods in Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2505.14295</link>
<guid>https://arxiv.org/abs/2505.14295</guid>
<content:encoded><![CDATA[
arXiv:2505.14295v1 Announce Type: cross 
Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine Learning (QML). While classical approaches process data directly as vectors, QML may require transforming classical data into quantum states through encoding circuits, known as quantum feature maps or quantum embeddings. This step leverages the inherently high-dimensional and non-linear nature of Hilbert space, enabling more efficient data separation in complex feature spaces that may be inaccessible to classical methods. This encoding part significantly affects the performance of the QML model, so it is important to choose the right encoding method for the dataset to be encoded. However, this choice is generally arbitrary, since there is no "universal" rule for knowing which encoding to choose based on a specific set of data. There are currently a variety of encoding methods using different quantum logic gates. We studied the most commonly used types of encoding methods and benchmarked them using different datasets.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains</title>
<link>https://arxiv.org/abs/2505.14312</link>
<guid>https://arxiv.org/abs/2505.14312</guid>
<content:encoded><![CDATA[
arXiv:2505.14312v1 Announce Type: cross 
Abstract: Despite the widespread use of tabular data in real-world applications, most benchmarks rely on average-case metrics, which fail to reveal how model behavior varies across diverse data regimes. To address this, we propose MultiTab, a benchmark suite and evaluation framework for multi-dimensional, data-aware analysis of tabular learning algorithms. Rather than comparing models only in aggregate, MultiTab categorizes 196 publicly available datasets along key data characteristics, including sample size, label imbalance, and feature interaction, and evaluates 13 representative models spanning a range of inductive biases. Our analysis shows that model performance is highly sensitive to such regimes: for example, models using sample-level similarity excel on datasets with large sample sizes or high inter-feature correlation, while models encoding inter-feature dependencies perform best with weakly correlated features. These findings reveal that inductive biases do not always behave as intended, and that regime-aware evaluation is essential for understanding and improving model behavior. MultiTab enables more principled model design and offers practical guidance for selecting models tailored to specific data characteristics. All datasets, code, and optimization logs are publicly available at https://huggingface.co/datasets/LGAI-DILab/Multitab.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion</title>
<link>https://arxiv.org/abs/2505.14316</link>
<guid>https://arxiv.org/abs/2505.14316</guid>
<content:encoded><![CDATA[
arXiv:2505.14316v1 Announce Type: cross 
Abstract: Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handloom Design Generation Using Generative Networks</title>
<link>https://arxiv.org/abs/2505.14330</link>
<guid>https://arxiv.org/abs/2505.14330</guid>
<content:encoded><![CDATA[
arXiv:2505.14330v1 Announce Type: cross 
Abstract: This paper proposes deep learning techniques of generating designs for clothing, focused on handloom fabric and discusses the associated challenges along with its application. The capability of generative neural network models in understanding artistic designs and synthesizing those is not yet explored well. In this work, multiple methods are employed incorporating the current state of the art generative models and style transfer algorithms to study and observe their performance for the task. The results are then evaluated through user score. This work also provides a new dataset NeuralLoom for the task of the design generation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image</title>
<link>https://arxiv.org/abs/2505.14341</link>
<guid>https://arxiv.org/abs/2505.14341</guid>
<content:encoded><![CDATA[
arXiv:2505.14341v1 Announce Type: cross 
Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common condition tasks having been optimized nicely. Besides, counterfactual Text-to-Image is obstructing us from a more versatile AIGC experience. For those scenes that are impossible to happen in real world and anti-physics, we should spare no efforts in increasing the factual feel, which means synthesizing images that people think very likely to be happening, and concept alignment, which means all the required objects should be in the same frame. In this paper, we focus on concept alignment. As controllable T2I models have achieved satisfactory performance for real applications, we utilize this technology to replace the objects in a synthesized image in latent space step-by-step to change the image from a common scene to a counterfactual scene to meet the prompt. We propose a strategy to instruct this replacing process, which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly SoTA language model DeepSeek to generate the instructions. Furthermore, to evaluate models' performance in counterfactual T2I, we design a metric to calculate how many required concepts in the prompt can be covered averagely in the synthesized images. The extensive experiments and qualitative comparisons demonstrate that our strategy can boost the concept alignment in counterfactual T2I.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights</title>
<link>https://arxiv.org/abs/2505.14345</link>
<guid>https://arxiv.org/abs/2505.14345</guid>
<content:encoded><![CDATA[
arXiv:2505.14345v1 Announce Type: cross 
Abstract: Recent advancements in semi-supervised deep learning have introduced effective strategies for leveraging both labeled and unlabeled data to improve classification performance. This work proposes a semi-supervised framework that utilizes a distance-based weighting mechanism to prioritize critical training samples based on their proximity to test data. By focusing on the most informative examples, the method enhances model generalization and robustness, particularly in challenging scenarios with noisy or imbalanced datasets. Building on techniques such as uncertainty consistency and graph-based representations, the approach addresses key challenges of limited labeled data while maintaining scalability. Experiments on twelve benchmark datasets demonstrate significant improvements across key metrics, including accuracy, precision, and recall, consistently outperforming existing methods. This framework provides a robust and practical solution for semi-supervised learning, with potential applications in domains such as healthcare and security where data limitations pose significant challenges.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Upgrading Democracies with Fairer Voting Methods</title>
<link>https://arxiv.org/abs/2505.14349</link>
<guid>https://arxiv.org/abs/2505.14349</guid>
<content:encoded><![CDATA[
arXiv:2505.14349v1 Announce Type: cross 
Abstract: Voting methods are instrumental design element of democracies. Citizens use them to express and aggregate their preferences to reach a collective decision. However, voting outcomes can be as sensitive to voting rules as they are to people's voting choices. Despite the significance and inter-disciplinary scientific progress on voting methods, several democracies keep relying on outdated voting methods that do not fit modern, pluralistic societies well, while lacking social innovation. Here, we demonstrate how one can upgrade real-world democracies, namely by using alternative preferential voting methods such as cumulative voting and the method of equal shares designed for a proportional representation of voters' preferences. By rigorously assessing a new participatory budgeting approach applied in the city of Aarau, Switzerland, we unravel the striking voting outcomes of fair voting methods: more winning projects with the same budget and broader geographic and preference representation of citizens by the elected projects, in particular for voters who used to be under-represented, while promoting novel project ideas. We provide profound causal evidence showing that citizens prefer proportional voting methods, which possess strong legitimacy without the need of very technical specialized explanations. We also reveal strong underlying democratic values exhibited by citizens who support fair voting methods such as altruism and compromise. These findings come with a global momentum to unleash a new and long-awaited participation blueprint of how to upgrade democracies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v1 Announce Type: cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making</title>
<link>https://arxiv.org/abs/2505.14377</link>
<guid>https://arxiv.org/abs/2505.14377</guid>
<content:encoded><![CDATA[
arXiv:2505.14377v1 Announce Type: cross 
Abstract: Although the integration of artificial intelligence (AI) into everyday tasks improves efficiency and objectivity, it also risks transmitting bias to human decision-making. In this study, we conducted a controlled experiment that simulated hiring decisions to examine how biased AI recommendations - augmented with or without counterfactual explanations - influence human judgment over time. Participants, acting as hiring managers, completed 60 decision trials divided into a baseline phase without AI, followed by a phase with biased (X)AI recommendations (favoring either male or female candidates), and a final post-interaction phase without AI. Our results indicate that the participants followed the AI recommendations 70% of the time when the qualifications of the given candidates were comparable. Yet, only a fraction of participants detected the gender bias (8 out of 294). Crucially, exposure to biased AI altered participants' inherent preferences: in the post-interaction phase, participants' independent decisions aligned with the bias when no counterfactual explanations were provided before, but reversed the bias when explanations were given. Reported trust did not differ significantly across conditions. Confidence varied throughout the study phases after exposure to male-biased AI, indicating nuanced effects of AI bias on decision certainty. Our findings point to the importance of calibrating XAI to avoid unintended behavioral shifts in order to safeguard equitable decision-making and prevent the adoption of algorithmic bias.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language</title>
<link>https://arxiv.org/abs/2505.14395</link>
<guid>https://arxiv.org/abs/2505.14395</guid>
<content:encoded><![CDATA[
arXiv:2505.14395v1 Announce Type: cross 
Abstract: Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation</title>
<link>https://arxiv.org/abs/2505.14398</link>
<guid>https://arxiv.org/abs/2505.14398</guid>
<content:encoded><![CDATA[
arXiv:2505.14398v1 Announce Type: cross 
Abstract: While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications</title>
<link>https://arxiv.org/abs/2505.14428</link>
<guid>https://arxiv.org/abs/2505.14428</guid>
<content:encoded><![CDATA[
arXiv:2505.14428v1 Announce Type: cross 
Abstract: The objective of this proposal is to bridge the gap between Deep Learning (DL) and System Dynamics (SD) by developing an interpretable neural system dynamics framework. While DL excels at learning complex models and making accurate predictions, it lacks interpretability and causal reliability. Traditional SD approaches, on the other hand, provide transparency and causal insights but are limited in scalability and require extensive domain knowledge. To overcome these limitations, this project introduces a Neural System Dynamics pipeline, integrating Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. This framework combines the predictive power of DL with the interpretability of traditional SD models, resulting in both causal reliability and scalability. The efficacy of the proposed pipeline will be validated through real-world applications of the EU-funded AutoMoTIF project, which is focused on autonomous multimodal transportation systems. The long-term goal is to collect actionable insights that support the integration of explainability and safety in autonomous systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI</title>
<link>https://arxiv.org/abs/2505.14435</link>
<guid>https://arxiv.org/abs/2505.14435</guid>
<content:encoded><![CDATA[
arXiv:2505.14435v1 Announce Type: cross 
Abstract: As organizations increasingly rely on AI systems for decision support in sustainability contexts, it becomes critical to understand the inherent biases and perspectives embedded in Large Language Models (LLMs). This study systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek, GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship with AI. We administered validated, psychometric sustainability-related questionnaires - each 100 times per model -- to capture response patterns and variability. Our findings revealed significant inter-model differences: For example, GPT exhibited skepticism about the compatibility of AI and sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect scores for several Sustainable Development Goals (SDGs). Models also diverged in attributing institutional responsibility for AI and sustainability integration, a results that holds implications for technology governance approaches. Our results demonstrate that model selection could substantially influence organizational sustainability strategies, highlighting the need for awareness of model-specific biases when deploying LLMs for sustainability-related decision-making.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14436</link>
<guid>https://arxiv.org/abs/2505.14436</guid>
<content:encoded><![CDATA[
arXiv:2505.14436v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\textbf{LaTen}$ ($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at https://github.com/Trae1ounG/Neural_Incompatibility.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creative Preference Optimization</title>
<link>https://arxiv.org/abs/2505.14442</link>
<guid>https://arxiv.org/abs/2505.14442</guid>
<content:encoded><![CDATA[
arXiv:2505.14442v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation</title>
<link>https://arxiv.org/abs/2505.14451</link>
<guid>https://arxiv.org/abs/2505.14451</guid>
<content:encoded><![CDATA[
arXiv:2505.14451v1 Announce Type: cross 
Abstract: Missing values in high-dimensional, mixed-type datasets pose significant challenges for data imputation, particularly under Missing Not At Random (MNAR) mechanisms. Existing methods struggle to integrate local and global data characteristics, limiting performance in MNAR and high-dimensional settings. We propose an innovative framework, RefiDiff, combining local machine learning predictions with a novel Mamba-based denoising network capturing interrelationships among distant features and samples. Our approach leverages pre-refinement for initial warm-up imputations and post-refinement to polish results, enhancing stability and accuracy. By encoding mixed-type data into unified tokens, RefiDiff enables robust imputation without architectural or hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods across missing-value settings, excelling in MNAR with a 4x faster training time than SOTA DDPM-based approaches. Extensive evaluations on nine real-world datasets demonstrate its robustness, scalability, and effectiveness in handling complex missingness patterns.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Managers Perceive AI-Assisted Conversational Training for Workplace Communication</title>
<link>https://arxiv.org/abs/2505.14452</link>
<guid>https://arxiv.org/abs/2505.14452</guid>
<content:encoded><![CDATA[
arXiv:2505.14452v1 Announce Type: cross 
Abstract: Effective workplace communication is essential for managerial success, yet many managers lack access to tailored and sustained training. Although AI-assisted communication systems may offer scalable training solutions, little is known about how managers envision the role of AI in helping them improve their communication skills. To investigate this, we designed a conversational role-play system, CommCoach, as a functional probe to understand how managers anticipate using AI to practice their communication skills. Through semi-structured interviews, participants emphasized the value of adaptive, low-risk simulations for practicing difficult workplace conversations. They also highlighted opportunities, including human-AI teaming, transparent and context-aware feedback, and greater control over AI-generated personas. AI-assisted communication training should balance personalization, structured learning objectives, and adaptability to different user styles and contexts. However, achieving this requires carefully navigating tensions between adaptive and consistent AI feedback, realism and potential bias, and the open-ended nature of AI conversations versus structured workplace discourse.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation</title>
<link>https://arxiv.org/abs/2505.14455</link>
<guid>https://arxiv.org/abs/2505.14455</guid>
<content:encoded><![CDATA[
arXiv:2505.14455v1 Announce Type: cross 
Abstract: Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations</title>
<link>https://arxiv.org/abs/2505.14469</link>
<guid>https://arxiv.org/abs/2505.14469</guid>
<content:encoded><![CDATA[
arXiv:2505.14469v1 Announce Type: cross 
Abstract: Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales</title>
<link>https://arxiv.org/abs/2505.14499</link>
<guid>https://arxiv.org/abs/2505.14499</guid>
<content:encoded><![CDATA[
arXiv:2505.14499v1 Announce Type: cross 
Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModRWKV: Transformer Multimodality in Linear Time</title>
<link>https://arxiv.org/abs/2505.14505</link>
<guid>https://arxiv.org/abs/2505.14505</guid>
<content:encoded><![CDATA[
arXiv:2505.14505v1 Announce Type: cross 
Abstract: Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Flow Transformer</title>
<link>https://arxiv.org/abs/2505.14513</link>
<guid>https://arxiv.org/abs/2505.14513</guid>
<content:encoded><![CDATA[
arXiv:2505.14513v1 Announce Type: cross 
Abstract: Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Graph Representations of Logical Forms for Language Modeling</title>
<link>https://arxiv.org/abs/2505.14523</link>
<guid>https://arxiv.org/abs/2505.14523</guid>
<content:encoded><![CDATA[
arXiv:2505.14523v1 Announce Type: cross 
Abstract: We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs pretrained on similar amounts of data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation</title>
<link>https://arxiv.org/abs/2505.14526</link>
<guid>https://arxiv.org/abs/2505.14526</guid>
<content:encoded><![CDATA[
arXiv:2505.14526v1 Announce Type: cross 
Abstract: Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present NavBench, a multi-domain benchmark for training and evaluating RL-based navigation policies across diverse robotic platforms and operational environments. Built on IsaacLab, our framework standardizes task definitions, enabling different robots to tackle various navigation challenges without the need for ad-hoc task redesigns or custom evaluation metrics. Our benchmark addresses three key challenges: (1) Unified cross-medium benchmarking, enabling direct evaluation of diverse actuation methods (thrusters, wheels, water-based propulsion) in realistic environments; (2) Scalable and modular design, facilitating seamless robot-task interchangeability and reproducible training pipelines; and (3) Robust sim-to-real validation, demonstrated through successful policy transfer to multiple real-world robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency between simulation and real-world deployment, NavBench simplifies the development of adaptable RL-based navigation strategies. Its modular design allows researchers to easily integrate custom robots and tasks by following the framework's predefined templates, making it accessible for a wide range of applications. Our code is publicly available at NavBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Deep Reinforcement Learning with Spiking Transformers</title>
<link>https://arxiv.org/abs/2505.14533</link>
<guid>https://arxiv.org/abs/2505.14533</guid>
<content:encoded><![CDATA[
arXiv:2505.14533v1 Announce Type: cross 
Abstract: Agent-based Transformers have been widely adopted in recent reinforcement learning advances due to their demonstrated ability to solve complex tasks. However, the high computational complexity of Transformers often results in significant energy consumption, limiting their deployment in real-world autonomous systems. Spiking neural networks (SNNs), with their biologically inspired structure, offer an energy-efficient alternative for machine learning. In this paper, a novel Spike-Transformer Reinforcement Learning (STRL) algorithm that combines the energy efficiency of SNNs with the powerful decision-making capabilities of reinforcement learning is developed. Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons and attention mechanisms capable of processing spatio-temporal patterns over multiple time steps is designed. The architecture is further enhanced with state, action, and reward encodings to create a Transformer-like structure optimized for reinforcement learning tasks. Comprehensive numerical experiments conducted on state-of-the-art benchmarks demonstrate that the proposed SNN Transformer achieves significantly improved policy performance compared to conventional agent-based Transformers. With both enhanced energy efficiency and policy optimality, this work highlights a promising direction for deploying bio-inspired, low-cost machine learning models in complex real-world decision-making scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Really Recognize Your Name?</title>
<link>https://arxiv.org/abs/2505.14549</link>
<guid>https://arxiv.org/abs/2505.14549</guid>
<content:encoded><![CDATA[
arXiv:2505.14549v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being used to protect sensitive user data. However, current LLM-based privacy solutions assume that these models can reliably detect personally identifiable information (PII), particularly named entities. In this paper, we challenge that assumption by revealing systematic failures in LLM-based privacy tasks. Specifically, we show that modern LLMs regularly overlook human names even in short text snippets due to ambiguous contexts, which cause the names to be misinterpreted or mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous human names, leveraging the name regularity bias phenomenon, embedded within concise text snippets along with benign prompt injections. Our experiments on modern LLMs tasked to detect PII as well as specialized tools show that recall of ambiguous names drops by 20--40% compared to more recognizable names. Furthermore, ambiguous human names are four times more likely to be ignored in supposedly privacy-preserving summaries generated by LLMs when benign prompt injections are present. These findings highlight the underexplored risks of relying solely on LLMs to safeguard user privacy and underscore the need for a more systematic investigation into their privacy failure modes.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains</title>
<link>https://arxiv.org/abs/2505.14551</link>
<guid>https://arxiv.org/abs/2505.14551</guid>
<content:encoded><![CDATA[
arXiv:2505.14551v1 Announce Type: cross 
Abstract: Reputation systems play an essential role in the Internet era, as they enable people to decide whom to trust, by collecting and aggregating data about users' behavior. Recently, several works proposed the use of reputation for the design and scalability improvement of decentralized (blockchain) ledgers; however, such systems are prone to manipulation and to our knowledge no game-theoretic treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em trustworthy reputation systems}. Concretely, we describe a class of games, which we term {\em trustworthy reputation games}, that enable a set of users to report a function of their beliefs about the trustworthiness of each server in a set -- i.e., their estimate of the probability that this server will behave according to its specified strategy -- in a way that satisfies the following properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true} trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium allows anyone who observes the users' strategies to estimate the relative trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well known PageRank algorithm and the problem of trustworthiness discovery, which can be of independent interest. Finally, we show how the above games are motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.14552</link>
<guid>https://arxiv.org/abs/2505.14552</guid>
<content:encoded><![CDATA[
arXiv:2505.14552v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting</title>
<link>https://arxiv.org/abs/2505.14555</link>
<guid>https://arxiv.org/abs/2505.14555</guid>
<content:encoded><![CDATA[
arXiv:2505.14555v1 Announce Type: cross 
Abstract: Weather forecasting is essential but remains computationally intensive and physically incomplete in traditional numerical weather prediction (NWP) methods. Deep learning (DL) models offer efficiency and accuracy but often ignore physical laws, limiting interpretability and generalization. We propose PhyDL-NWP, a physics-guided deep learning framework that integrates physical equations with latent force parameterization into data-driven models. It predicts weather variables from arbitrary spatiotemporal coordinates, computes physical terms via automatic differentiation, and uses a physics-informed loss to align predictions with governing dynamics. PhyDL-NWP enables resolution-free downscaling by modeling weather as a continuous function and fine-tunes pre-trained models with minimal overhead, achieving up to 170x faster inference with only 55K parameters. Experiments show that PhyDL-NWP improves both forecasting performance and physical consistency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification</title>
<link>https://arxiv.org/abs/2505.14561</link>
<guid>https://arxiv.org/abs/2505.14561</guid>
<content:encoded><![CDATA[
arXiv:2505.14561v1 Announce Type: cross 
Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker Verification (SV). The standard framework uses same-utterance positive sampling and data-augmentation to generate anchor-positive pairs of the same speaker. This is a major limitation, as this strategy primarily encodes channel information from the recording condition, shared by the anchor and positive. We propose a new positive sampling technique to address this bottleneck: Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find an appropriate positive, i.e., of the same speaker identity but a different recording condition, in the latent space using clustering assignments and a memory queue of positive embeddings. SSPS improves SV performance for both SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by lowering intra-speaker variance, providing comparable performance to DINO-SSPS.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bellman operator convergence enhancements in reinforcement learning algorithms</title>
<link>https://arxiv.org/abs/2505.14564</link>
<guid>https://arxiv.org/abs/2505.14564</guid>
<content:encoded><![CDATA[
arXiv:2505.14564v1 Announce Type: cross 
Abstract: This paper reviews the topological groundwork for the study of reinforcement learning (RL) by focusing on the structure of state, action, and policy spaces. We begin by recalling key mathematical concepts such as complete metric spaces, which form the foundation for expressing RL problems. By leveraging the Banach contraction principle, we illustrate how the Banach fixed-point theorem explains the convergence of RL algorithms and how Bellman operators, expressed as operators on Banach spaces, ensure this convergence. The work serves as a bridge between theoretical mathematics and practical algorithm design, offering new approaches to enhance the efficiency of RL. In particular, we investigate alternative formulations of Bellman operators and demonstrate their impact on improving convergence rates and performance in standard RL environments such as MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper mathematical understanding of RL can lead to more effective algorithms for decision-making problems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIPPO: Koopman-Inspired Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2505.14566</link>
<guid>https://arxiv.org/abs/2505.14566</guid>
<content:encoded><![CDATA[
arXiv:2505.14566v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has made significant strides in various domains, and policy gradient methods like Proximal Policy Optimization (PPO) have gained popularity due to their balance in performance, training stability, and computational efficiency. These methods directly optimize policies through gradient-based updates. However, developing effective control policies for environments with complex and non-linear dynamics remains a challenge. High variance in gradient estimates and non-convex optimization landscapes often lead to unstable learning trajectories. Koopman Operator Theory has emerged as a powerful framework for studying non-linear systems through an infinite-dimensional linear operator that acts on a higher-dimensional space of measurement functions. In contrast with their non-linear counterparts, linear systems are simpler, more predictable, and easier to analyze. In this paper, we present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an approximately linear latent-space representation of the underlying system's dynamics while retaining essential features for effective policy learning. This is achieved through a Koopman-approximation auxiliary network that can be added to the baseline policy optimization algorithms without altering the architecture of the core policy or value function. Extensive experimental results demonstrate consistent improvements over the PPO baseline with 6-60% increased performance while reducing variability by up to 91% when evaluated on various continuous control tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14599</link>
<guid>https://arxiv.org/abs/2505.14599</guid>
<content:encoded><![CDATA[
arXiv:2505.14599v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</title>
<link>https://arxiv.org/abs/2505.14608</link>
<guid>https://arxiv.org/abs/2505.14608</guid>
<content:encoded><![CDATA[
arXiv:2505.14608v1 Announce Type: cross 
Abstract: Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space$\unicode{x2013}$the stylistic feature space$\unicode{x2013}$that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. This observation encourages us to introduce AURA, a metric that estimates the overlap between human and machine-generated distributions by analyzing how detector performance improves as more samples become available. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14625</link>
<guid>https://arxiv.org/abs/2505.14625</guid>
<content:encoded><![CDATA[
arXiv:2505.14625v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models</title>
<link>https://arxiv.org/abs/2505.14629</link>
<guid>https://arxiv.org/abs/2505.14629</guid>
<content:encoded><![CDATA[
arXiv:2505.14629v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas</title>
<link>https://arxiv.org/abs/2505.14633</link>
<guid>https://arxiv.org/abs/2505.14633</guid>
<content:encoded><![CDATA[
arXiv:2505.14633v1 Announce Type: cross 
Abstract: Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation</title>
<link>https://arxiv.org/abs/2505.14646</link>
<guid>https://arxiv.org/abs/2505.14646</guid>
<content:encoded><![CDATA[
arXiv:2505.14646v1 Announce Type: cross 
Abstract: Efficient creation of accurate and editable 3D CAD models is critical in engineering design, significantly impacting cost and time-to-market in product innovation. Current manual workflows remain highly time-consuming and demand extensive user expertise. While recent developments in AI-driven CAD generation show promise, existing models are limited by incomplete representations of CAD operations, inability to generalize to real-world images, and low output accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model (VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python) directly from visual input. Leveraging a novel dataset that we created--GenCAD-Code, consisting of over 163k CAD-model image and code pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in 3D solid similarity. Notably, our VLM demonstrates some signs of generalizability, successfully generating CAD code from real-world images and executing CAD operations unseen during fine-tuning. The performance and adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code to streamline CAD workflows for engineers and designers. CAD-Coder is publicly available at: https://github.com/anniedoris/CAD-Coder.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Words: Multimodal LLM Knows When to Speak</title>
<link>https://arxiv.org/abs/2505.14654</link>
<guid>https://arxiv.org/abs/2505.14654</guid>
<content:encoded><![CDATA[
arXiv:2505.14654v1 Announce Type: cross 
Abstract: While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks</title>
<link>https://arxiv.org/abs/2505.14659</link>
<guid>https://arxiv.org/abs/2505.14659</guid>
<content:encoded><![CDATA[
arXiv:2505.14659v1 Announce Type: cross 
Abstract: As healthcare systems increasingly adopt advanced wireless networks and connected devices, securing medical applications has become critical. The integration of Internet of Medical Things devices, such as robotic surgical tools, intensive care systems, and wearable monitors has enhanced patient care but introduced serious security risks. Cyberattacks on these devices can lead to life threatening consequences, including surgical errors, equipment failure, and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative role in healthcare through AI and cloud integration, it also raises new security concerns. This paper explores how explainable AI techniques like SHAP, LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve trust and transparency in 6G enabled healthcare. We support our approach with experimental analysis and highlight promising results.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoGist: Efficient In-Context Learning for Visual Emotion Understanding</title>
<link>https://arxiv.org/abs/2505.14660</link>
<guid>https://arxiv.org/abs/2505.14660</guid>
<content:encoded><![CDATA[
arXiv:2505.14660v1 Announce Type: cross 
Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning method for performing visual emotion classification with LVLMs. The key intuition of our approach is that context-dependent definition of emotion labels could allow more accurate predictions of emotions, as the ways in which emotions manifest within images are highly context dependent and nuanced. EmoGist pre-generates multiple explanations of emotion labels, by analyzing the clusters of example images belonging to each category. At test time, we retrieve a version of explanation based on embedding similarity, and feed it to a fast VLM for classification. Through our experiments, we show that EmoGist allows up to 13 points improvement in micro F1 scores with the multi-label Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abacus: A Cost-Based Optimizer for Semantic Operator Systems</title>
<link>https://arxiv.org/abs/2505.14661</link>
<guid>https://arxiv.org/abs/2505.14661</guid>
<content:encoded><![CDATA[
arXiv:2505.14661v1 Announce Type: cross 
Abstract: LLMs enable an exciting new class of data processing applications over large collections of unstructured documents. Several new programming frameworks have enabled developers to build these applications by composing them out of semantic operators: a declarative set of AI-powered data transformations with natural language specifications. These include LLM-powered maps, filters, joins, etc. used for document processing tasks such as information extraction, summarization, and more. While systems of semantic operators have achieved strong performance on benchmarks, they can be difficult to optimize. An optimizer for this setting must determine how to physically implement each semantic operator in a way that optimizes the system globally. Existing optimizers are limited in the number of optimizations they can apply, and most (if not all) cannot optimize system quality, cost, or latency subject to constraint(s) on the other dimensions. In this paper we present Abacus, an extensible, cost-based optimizer which searches for the best implementation of a semantic operator system given a (possibly constrained) optimization objective. Abacus estimates operator performance by leveraging a minimal set of validation examples and, if available, prior beliefs about operator performance. We evaluate Abacus on document processing workloads in the biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering (MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2% better quality and up to 23.6x lower cost and 4.2x lower latency than the next best system.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings</title>
<link>https://arxiv.org/abs/2505.14664</link>
<guid>https://arxiv.org/abs/2505.14664</guid>
<content:encoded><![CDATA[
arXiv:2505.14664v1 Announce Type: cross 
Abstract: Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities.This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Watermarking for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2505.14673</link>
<guid>https://arxiv.org/abs/2505.14673</guid>
<content:encoded><![CDATA[
arXiv:2505.14673v1 Announce Type: cross 
Abstract: Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search</title>
<link>https://arxiv.org/abs/2505.14680</link>
<guid>https://arxiv.org/abs/2505.14680</guid>
<content:encoded><![CDATA[
arXiv:2505.14680v1 Announce Type: cross 
Abstract: Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning</title>
<link>https://arxiv.org/abs/2505.14684</link>
<guid>https://arxiv.org/abs/2505.14684</guid>
<content:encoded><![CDATA[
arXiv:2505.14684v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem</title>
<link>https://arxiv.org/abs/2311.18662</link>
<guid>https://arxiv.org/abs/2311.18662</guid>
<content:encoded><![CDATA[
arXiv:2311.18662v4 Announce Type: replace 
Abstract: Route planning for a fleet of vehicles is an important task in applications such as package delivery, surveillance, or transportation, often integrated within larger Intelligent Transportation Systems (ITS). This problem is commonly formulated as a Vehicle Routing Problem (VRP) known as the Team Orienteering Problem (TOP). Existing solvers for this problem primarily rely on either linear programming, which provides accurate solutions but requires computation times that grow with the size of the problem, or heuristic methods, which typically find suboptimal solutions in a shorter time. In this paper, we introduce TOP-Former, a multi-agent route planning neural network designed to efficiently and accurately solve the Team Orienteering Problem. The proposed algorithm is based on a centralized Transformer neural network capable of learning to encode the scenario (modeled as a graph) and analyze the complete context of all agents to deliver fast, precise, and collaborative solutions. Unlike other neural network-based approaches that adopt a more local perspective, TOP-Former is trained to understand the global situation of the vehicle fleet and generate solutions that maximize long-term expected returns. Extensive experiments demonstrate that the presented system outperforms most state-of-the-art methods in terms of both accuracy and computation speed.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Complex Logical Queries on Uncertain Knowledge Graphs</title>
<link>https://arxiv.org/abs/2403.01508</link>
<guid>https://arxiv.org/abs/2403.01508</guid>
<content:encoded><![CDATA[
arXiv:2403.01508v2 Announce Type: replace 
Abstract: The study of machine learning-based logical query answering enables reasoning with large-scale and incomplete knowledge graphs. This paper advances this area of research by addressing the uncertainty inherent in knowledge. While the uncertain nature of knowledge is widely recognized in the real world, it does not align seamlessly with the first-order logic that underpins existing studies. To bridge this gap, we explore the soft queries on uncertain knowledge, inspired by the framework of soft constraint programming. We propose a neural symbolic approach that incorporates both forward inference and backward calibration to answer soft queries on large-scale, incomplete, and uncertain knowledge graphs. Theoretical discussions demonstrate that our method avoids catastrophic cascading errors in the forward inference while maintaining the same complexity as state-of-the-art symbolic methods for complex logical queries. Empirical results validate the superior performance of our backward calibration compared to extended query embedding methods and neural symbolic approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2406.06051</link>
<guid>https://arxiv.org/abs/2406.06051</guid>
<content:encoded><![CDATA[
arXiv:2406.06051v3 Announce Type: replace 
Abstract: To enable effective human-AI collaboration, merely optimizing AI performance without considering human factors is insufficient. Recent research has shown that designing AI agents that take human behavior into account leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior remains static, regardless of the AI agent's actions. In reality, humans may adjust their actions based on their beliefs about the AI's intentions, specifically, the subtasks they perceive the AI to be attempting to complete based on its behavior. In this paper, we address this limitation by enabling a collaborative AI agent to consider its human partner's beliefs about its intentions, i.e., what the human partner thinks the AI agent is trying to accomplish, and to design its action plan accordingly to facilitate more effective human-AI collaboration. Specifically, we developed a model of human beliefs that captures how humans interpret and reason about their AI partner's intentions. Using this belief model, we created an AI agent that incorporates both human behavior and human beliefs when devising its strategy for interacting with humans. Through extensive real-world human-subject experiments, we demonstrate that our belief model more accurately captures human perceptions of AI intentions. Furthermore, we show that our AI agent, designed to account for human beliefs over its intentions, significantly enhances performance in human-AI collaboration.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking</title>
<link>https://arxiv.org/abs/2406.14449</link>
<guid>https://arxiv.org/abs/2406.14449</guid>
<content:encoded><![CDATA[
arXiv:2406.14449v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have significantly enhanced Information Retrieval (IR) across various modules, such as reranking. Despite impressive performance, current zero-shot relevance ranking with LLMs heavily relies on human prompt engineering. Existing automatic prompt engineering algorithms primarily focus on language modeling and classification tasks, leaving the domain of IR, particularly reranking, underexplored. Directly applying current prompt engineering algorithms to relevance ranking is challenging due to the integration of query and long passage pairs in the input, where the ranking complexity surpasses classification tasks. To reduce human effort and unlock the potential of prompt optimization in reranking, we introduce a novel automatic prompt engineering algorithm named APEER. APEER iteratively generates refined prompts through feedback and preference optimization. Extensive experiments with four LLMs and ten datasets demonstrate the substantial performance improvement of APEER over existing state-of-the-art (SoTA) manual prompts. Furthermore, we find that the prompts generated by APEER exhibit better transferability across diverse tasks and LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Effect of Explanation Content and Format on User Comprehension and Trust in Healthcare</title>
<link>https://arxiv.org/abs/2408.17401</link>
<guid>https://arxiv.org/abs/2408.17401</guid>
<content:encoded><![CDATA[
arXiv:2408.17401v3 Announce Type: replace 
Abstract: AI-driven tools for healthcare are widely acknowledged as potentially beneficial to health practitioners and patients, e.g. the QCancer regression tool for cancer risk prediction. However, for these tools to be trusted, they need to be supplemented with explanations. We examine how explanations' content and format affect user comprehension and trust when explaining QCancer's predictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding format, we present SHAP explanations, conventionally, as charts (SC) and Occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature lends itself. We conduct experiments with two sets of stakeholders: the general public (representing patients) and medical students (representing healthcare practitioners). Our experiments showed higher subjective comprehension and trust for Occlusion-1 over SHAP explanations based on content. However, when controlling for format, only OT outperformed SC, suggesting this trend is driven by preferences for text. Other findings corroborated that explanation format, rather than content, is often the critical factor.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRoP: Context-wise Robust Static Human-Sensing Personalization</title>
<link>https://arxiv.org/abs/2409.17994</link>
<guid>https://arxiv.org/abs/2409.17994</guid>
<content:encoded><![CDATA[
arXiv:2409.17994v5 Announce Type: replace 
Abstract: The advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct patterns in human sensing, influenced by various factors or contexts, challenge the generic neural network model's performance due to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization studies overlook intra-user heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation is especially critical in clinical applications, where limited data availability hampers both generalizability and personalization. Notably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further complicating the challenges. To address the intra-user generalization challenge, this work introduces CRoP, a novel static personalization approach. CRoP leverages off-the-shelf pre-trained models as generic starting points and captures user-specific traits through adaptive pruning on a minimal sub-network while allowing generic knowledge to be incorporated in remaining parameters. CRoP demonstrates superior personalization effectiveness and intra-user robustness across four human-sensing datasets, including two from real-world health domains, underscoring its practical and social impact. Additionally, to support CRoP's generalization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation studies, and comparisons against state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2410.02429</link>
<guid>https://arxiv.org/abs/2410.02429</guid>
<content:encoded><![CDATA[
arXiv:2410.02429v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in textual and visual tasks but often produce outputs that defy physical laws when dealing with physical-world reasoning tasks. Inspired by human cognition, where perception is fundamental to reasoning, we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) sensor data and pertinent knowledge for IoT-sensory task reasoning in the physical world. In this work, we systematically study LLMs' capability to address real-world IoT-sensory tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding their understanding via IoT-oriented retrieval-augmented generation based on in-context learning and activating their commonsense knowledge through chain-of-thought prompting and specialized role definitions. We design a new benchmark comprising five real-world tasks with varying data types and reasoning complexities to evaluate the performance of IoT-LLM. Experimental results on six LLMs reveal that IoT-LLM significantly improves the performance of IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a 49.4% average improvement over previous methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment</title>
<link>https://arxiv.org/abs/2410.09083</link>
<guid>https://arxiv.org/abs/2410.09083</guid>
<content:encoded><![CDATA[
arXiv:2410.09083v2 Announce Type: replace 
Abstract: This paper presents a method to analyze the inference patterns used by Large Language Models (LLMs) for judgment in a case study on legal LLMs, so as to identify potential incorrect representations of the LLM, according to human domain knowledge. Unlike traditional evaluations on language generation results, we propose to evaluate the correctness of the detailed inference patterns of an LLM behind its seemingly correct outputs. To this end, we quantify the interactions between input phrases used by the LLM as primitive inference patterns, because recent theoretical achievements have proven several mathematical guarantees of the faithfulness of the interaction-based explanation. We design a set of metrics to evaluate the detailed inference patterns of LLMs. Experiments show that even when the language generation results appear correct, a significant portion of the inference patterns used by the LLM for the legal judgment may represent misleading or irrelevant logic.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark</title>
<link>https://arxiv.org/abs/2412.02508</link>
<guid>https://arxiv.org/abs/2412.02508</guid>
<content:encoded><![CDATA[
arXiv:2412.02508v2 Announce Type: replace 
Abstract: Producing emotionally dynamic 3D facial avatars with text derived from spoken words (Emo3D) has been a pivotal research topic in 3D avatar generation. While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words. This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity. To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation. First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation. Furthermore, we develop various metrics to effectively evaluate models against these identified challenges. Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator, which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms. Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProcessBench: Identifying Process Errors in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2412.06559</link>
<guid>https://arxiv.org/abs/2412.06559</guid>
<content:encoded><![CDATA[
arXiv:2412.06559v3 Announce Type: replace 
Abstract: As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Platform-Aware Mission Planning</title>
<link>https://arxiv.org/abs/2501.09632</link>
<guid>https://arxiv.org/abs/2501.09632</guid>
<content:encoded><![CDATA[
arXiv:2501.09632v2 Announce Type: replace 
Abstract: Planning for autonomous systems typically requires reasoning with models at different levels of abstraction, and the harmonization of two competing sets of objectives: high-level mission goals that refer to an interaction of the system with the external environment, and low-level platform constraints that aim to preserve the integrity and the correct interaction of the subsystems. The complicated interplay between these two models makes it very hard to reason on the system as a whole, especially when the objective is to find plans with robustness guarantees, considering the non-deterministic behavior of the lower layers of the system.
  In this paper, we introduce the problem of Platform-Aware Mission Planning (PAMP), addressing it in the setting of temporal durative actions. The PAMP problem differs from standard temporal planning for its exists-forall nature: the high-level plan dealing with mission goals is required to satisfy safety and executability constraints, for all the possible non-deterministic executions of the low-level model of the platform and the environment. We propose two approaches for solving PAMP. The first baseline approach amalgamates the mission and platform levels, while the second is based on an abstraction-refinement loop that leverages the combination of a planner and a verification engine. We prove the soundness and completeness of the proposed approaches and validate them experimentally, demonstrating the importance of heterogeneous modeling and the superiority of the technique based on abstraction-refinement.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios</title>
<link>https://arxiv.org/abs/2502.02145</link>
<guid>https://arxiv.org/abs/2502.02145</guid>
<content:encoded><![CDATA[
arXiv:2502.02145v2 Announce Type: replace 
Abstract: Ensuring the safety of autonomous vehicles requires virtual scenario-based testing, which depends on the robust evaluation and generation of safety-critical scenarios. So far, researchers have used scenario-based testing frameworks that rely heavily on handcrafted scenarios as safety metrics. To reduce the effort of human interpretation and overcome the limited scalability of these approaches, we combine Large Language Models (LLMs) with structured scenario parsing and prompt engineering to automatically evaluate and generate safety-critical driving scenarios. We introduce Cartesian and Ego-centric prompt strategies for scenario evaluation, and an adversarial generation module that modifies trajectories of risk-inducing vehicles (ego-attackers) to create critical scenarios. We validate our approach using a 2D simulation framework and multiple pre-trained LLMs. The results show that the evaluation module effectively detects collision scenarios and infers scenario safety. Meanwhile, the new generation module identifies high-risk agents and synthesizes realistic, safety-critical scenarios. We conclude that an LLM equipped with domain-informed prompting techniques can effectively evaluate and generate safety-critical driving scenarios, reducing dependence on handcrafted metrics. We release our open-source code and scenarios at: https://github.com/TUM-AVS/From-Words-to-Collisions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileA3gent: Training Mobile GUI Agents Using Decentralized Self-Sourced Data from Diverse Users</title>
<link>https://arxiv.org/abs/2502.02982</link>
<guid>https://arxiv.org/abs/2502.02982</guid>
<content:encoded><![CDATA[
arXiv:2502.02982v2 Announce Type: replace 
Abstract: The advancement of mobile GUI agents has opened new opportunities for automating tasks on mobile devices. Training these agents requires large-scale high-quality data, which is prohibitively expensive when relying on human labor. Given the vast population of global mobile phone users, if automated data collection from them becomes feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. Nevertheless, two major challenges arise: (1) extracting user instructions without human intervention and (2) utilizing distributed user data while preserving privacy. To tackle these challenges, we propose MobileA3gent, a collaborative framework that trains mobile GUI Agents using decentralized self-sourced data from diverse users. The framework comprises two components, each targeting a specific challenge: (1) Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. (2) FedVLM-A, which enhances federated VLM training under non-IID distributions by incorporating adapted global aggregation based on both episode-level and step-level variability. Extensive experiments prove that MobileA3gent achieves superior performance over traditional approaches at only 1% of the cost, highlighting its potential for real-world applications
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building reliable sim driving agents by scaling self-play</title>
<link>https://arxiv.org/abs/2502.14706</link>
<guid>https://arxiv.org/abs/2502.14706</guid>
<content:encoded><![CDATA[
arXiv:2502.14706v3 Announce Type: replace 
Abstract: Simulation agents are essential for designing and testing systems that interact with humans, such as autonomous vehicles (AVs). These agents serve various purposes, from benchmarking AV performance to stress-testing system limits, but all applications share one key requirement: reliability. To enable sound experimentation, a simulation agent must behave as intended. It should minimize actions that may lead to undesired outcomes, such as collisions, which can distort the signal-to-noise ratio in analyses. As a foundation for reliable sim agents, we propose scaling self-play to thousands of scenarios on the Waymo Open Motion Dataset under semi-realistic limits on human perception and control. Training from scratch on a single GPU, our agents solve almost the full training set within a day. They generalize to unseen test scenes, achieving a 99.8% goal completion rate with less than 0.8% combined collision and off-road incidents across 10,000 held-out scenarios. Beyond in-distribution generalization, our agents show partial robustness to out-of-distribution scenes and can be fine-tuned in minutes to reach near-perfect performance in such cases. We open-source the pre-trained agents and integrate them with a batched multi-agent simulator. Demonstrations of agent behaviors can be viewed at https://sites.google.com/view/reliable-sim-agents, and we open-source our agents at https://github.com/Emerge-Lab/gpudrive.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Explainable Multi-player MCTS-minimax Hybrids in Board Game Using Process Mining</title>
<link>https://arxiv.org/abs/2503.23326</link>
<guid>https://arxiv.org/abs/2503.23326</guid>
<content:encoded><![CDATA[
arXiv:2503.23326v2 Announce Type: replace 
Abstract: Monte-Carlo Tree Search (MCTS) is a family of sampling-based search algorithms widely used for online planning in sequential decision-making domains and at the heart of many recent advances in artificial intelligence. Understanding the behavior of MCTS agents is difficult for developers and users due to the frequently large and complex search trees that result from the simulation of many possible futures, their evaluations, and their relationships. This paper presents our ongoing investigation into potential explanations for the decision-making and behavior of MCTS. A weakness of MCTS is that it constructs a highly selective tree and, as a result, can miss crucial moves and fall into tactical traps. Full-width minimax search constitutes the solution. We integrate shallow minimax search into the rollout phase of multi-player MCTS and use process mining technique to explain agents' strategies in 3v3 checkers.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.05047</link>
<guid>https://arxiv.org/abs/2504.05047</guid>
<content:encoded><![CDATA[
arXiv:2504.05047v2 Announce Type: replace 
Abstract: Multiagent collaboration has emerged as a promising framework for enhancing the reasoning capabilities of large language models (LLMs). Despite improvements in reasoning, the approach introduces substantial computational overhead resulting from iterative agent interactions. Furthermore, engaging in unnecessary debates increases the risk of generating erroneous responses. To address these challenges, we propose Debate Only When Necessary (DOWN), an adaptive multiagent debate framework that selectively activates debate based on the confidence score of the agent's initial response. Debate is activated only for queries requiring further deliberation, during which agents refine their outputs by referencing peer responses and associated confidence scores. Evaluations on benchmarks show that DOWN improves efficiency by up to six times while preserving or even outperforming the performance of existing methods. Further analysis indicates that DOWN effectively mitigates the risk of error propagation stemming from the unnecessary debate process. These findings demonstrate the effectiveness of our approach in delivering high-performance LLM solutions at a lower computational cost.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2504.15364</link>
<guid>https://arxiv.org/abs/2504.15364</guid>
<content:encoded><![CDATA[
arXiv:2504.15364v3 Announce Type: replace 
Abstract: We demonstrate that geometrically distinctive keys during LLM inference tend to have high attention scores. Based on the phenomenon we propose KeyDiff, a training-free KV cache eviction method based solely on key similarity. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We provide a theoretical basis for KeyDiff by relating key diversity with attention scores. These results imply KeyDiff can efficiently identify the most important tokens to retain. Notably KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. Under a strict memory allowance, we demonstrate the effectiveness of KeyDiff for the Llama and Qwen model families by observing a performance gap of less than 0.04% with 8K cache budget ($\sim$23% KV cache reduction) from the non-evicting baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning benchmark and decrease end-to-end inference latency by up to 30% compared to the other token-eviction methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAML: Goal Recognition As Metric Learning</title>
<link>https://arxiv.org/abs/2505.03941</link>
<guid>https://arxiv.org/abs/2505.03941</guid>
<content:encoded><![CDATA[
arXiv:2505.03941v2 Announce Type: replace 
Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives based on observed actions. Recent data-driven approaches for GR alleviate the need for costly, manually crafted domain models. However, these approaches can only reason about a pre-defined set of goals, and time-consuming training is needed for new emerging goals. To keep this model-learning automated while enabling quick adaptation to new goals, this paper introduces GRAML: Goal Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a deep metric learning task, employing an RNN that learns a metric over an embedding space, where the embeddings for observation traces leading to different goals are distant, and embeddings of traces leading to the same goals are close. This metric is especially useful when adapting to new goals, even if given just one example observation trace per goal. Evaluated on a versatile set of environments, GRAML shows speed, flexibility, and runtime improvements over the state-of-the-art GR while maintaining accurate recognition.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL</title>
<link>https://arxiv.org/abs/2505.06706</link>
<guid>https://arxiv.org/abs/2505.06706</guid>
<content:encoded><![CDATA[
arXiv:2505.06706v2 Announce Type: replace 
Abstract: Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as the exponential growth in agent interactions significantly increases computational complexity and impedes learning efficiency. To mitigate this, existing efforts that rely on Mean Field (MF) simplify the interaction landscape by approximating neighboring agents as a single mean agent, thus reducing overall complexity to pairwise interactions. However, these MF methods inevitably fail to account for individual differences, leading to aggregation noise caused by inaccurate iterative updates during MF learning. In this paper, we propose a Bi-level Mean Field (BMF) method to capture agent diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF introduces a dynamic group assignment module, which employs a Variational AutoEncoder (VAE) to learn the representations of agents, facilitating their dynamic grouping over time. Furthermore, we propose a bi-level interaction module to model both inter- and intra-group interactions for effective neighboring aggregation. Experiments across various tasks demonstrate that the proposed BMF yields results superior to the state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem</title>
<link>https://arxiv.org/abs/2505.08451</link>
<guid>https://arxiv.org/abs/2505.08451</guid>
<content:encoded><![CDATA[
arXiv:2505.08451v2 Announce Type: replace 
Abstract: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial optimization problem, with several application domains, especially for manufacturing purposes. The objective is to efficiently schedule multiple operations on dissimilar machines. These operations are gathered into jobs, and operations pertaining to the same job need to be scheduled sequentially. Different methods have been previously tested to solve this problem, such as Constraint Solving, Tabu Search, Genetic Algorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm derived from the Generalized Nested Rollout Policy Adaptation, developed to solve the FJSSP. We report encouraging experimental results, as our algorithm performs better than other MCTS-based approaches, even if makespans obtained on large instances are still far from known upper bounds.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Leakage Defense with Key-Lock Module for Federated Learning</title>
<link>https://arxiv.org/abs/2305.04095</link>
<guid>https://arxiv.org/abs/2305.04095</guid>
<content:encoded><![CDATA[
arXiv:2305.04095v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is infeasible; and b) the global model's inference performance is significantly compromised. We discuss the theoretical underpinnings of why gradients can leak private information and provide theoretical proof of our method's effectiveness. We conducted extensive empirical evaluations with many models on several popular benchmarks, demonstrating the robustness of our proposed approach in both maintaining model performance and defending against gradient leakage attacks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for Composed Person Retrieval</title>
<link>https://arxiv.org/abs/2311.16515</link>
<guid>https://arxiv.org/abs/2311.16515</guid>
<content:encoded><![CDATA[
arXiv:2311.16515v4 Announce Type: replace-cross 
Abstract: Person retrieval has attracted rising attention. Existing methods are mainly divided into two retrieval modes, namely image-only and text-only. However, they are unable to make full use of the available information and are difficult to meet diverse application requirements. To address the above limitations, we propose a new Composed Person Retrieval (CPR) task, which combines visual and textual queries to identify individuals of interest from large-scale person image databases. Nevertheless, the foremost difficulty of the CPR task is the lack of available annotated datasets. Therefore, we first introduce a scalable automatic data synthesis pipeline, which decomposes complex multimodal data generation into the creation of textual quadruples followed by identity-consistent image synthesis using fine-tuned generative models. Meanwhile, a multimodal filtering method is designed to ensure the resulting SynCPR dataset retains 1.15 million high-quality and fully synthetic triplets. Additionally, to improve the representation of composed person queries, we propose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework through fine-grained dynamic alignment and masked feature reasoning. Moreover, for objective evaluation, we manually annotate the Image-Text Composed Person Retrieval (ITCPR) test set. The extensive experiments demonstrate the effectiveness of the SynCPR dataset and the superiority of the proposed FAFA framework when compared with the state-of-the-art methods. All code and data will be provided at https://github.com/Delong-liu-bupt/Composed_Person_Retrieval.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning for Multivariate Time Series Imputation: A Survey</title>
<link>https://arxiv.org/abs/2402.04059</link>
<guid>https://arxiv.org/abs/2402.04059</guid>
<content:encoded><![CDATA[
arXiv:2402.04059v3 Announce Type: replace-cross 
Abstract: Missing values are ubiquitous in multivariate time series (MTS) data, posing significant challenges for accurate analysis and downstream applications. In recent years, deep learning-based methods have successfully handled missing data by leveraging complex temporal dependencies and learned data distributions. In this survey, we provide a comprehensive summary of deep learning approaches for multivariate time series imputation (MTSI) tasks. We propose a novel taxonomy that categorizes existing methods based on two key perspectives: imputation uncertainty and neural network architecture. Furthermore, we summarize existing MTSI toolkits with a particular emphasis on the PyPOTS Ecosystem, which provides an integrated and standardized foundation for MTSI research. Finally, we discuss key challenges and future research directions, which give insight for further MTSI research. This survey aims to serve as a valuable resource for researchers and practitioners in the field of time series analysis and missing data imputation tasks.A well-maintained MTSI paper and tool list are available at https://github.com/WenjieDu/Awesome_Imputation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training</title>
<link>https://arxiv.org/abs/2403.09948</link>
<guid>https://arxiv.org/abs/2403.09948</guid>
<content:encoded><![CDATA[
arXiv:2403.09948v3 Announce Type: replace-cross 
Abstract: The integration of artificial intelligence (AI) with radiology marks a transformative era in medicine. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiologic 2D and 3D radiologic data pose unique challenges that existing models, pre-trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in radiologic imaging, we introduce Radiologic Contrastive Language-Image Pre-training (RadCLIP): a cross-modal vision-language foundational model that harnesses Vision Language Pre-training (VLP) framework to improve radiologic image analysis. Building upon Contrastive Language-Image Pre-training (CLIP), RadCLIP incorporates a slice pooling mechanism tailored for volumetric image analysis and is pre-trained using a large and diverse dataset of radiologic image-text pairs. The RadCLIP was pre-trained to effectively align radiologic images with their corresponding text annotations, creating a robust vision backbone for radiologic images. Extensive experiments demonstrate RadCLIP's superior performance in both uni-modal radiologic image classification and cross-modal image-text matching, highlighting its significant promise for improving diagnostic accuracy and efficiency in clinical settings. Our Key contributions include curating a large dataset with diverse radiologic 2D/3D radiologic image-text pairs, a slice pooling adapter using an attention mechanism for integrating 2D images, and comprehensive evaluations of RadCLIP on various radiologic downstream tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning</title>
<link>https://arxiv.org/abs/2403.10056</link>
<guid>https://arxiv.org/abs/2403.10056</guid>
<content:encoded><![CDATA[
arXiv:2403.10056v2 Announce Type: replace-cross 
Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Hybrid Model Pruning through Loss Landscape Exploration</title>
<link>https://arxiv.org/abs/2405.10271</link>
<guid>https://arxiv.org/abs/2405.10271</guid>
<content:encoded><![CDATA[
arXiv:2405.10271v3 Announce Type: replace-cross 
Abstract: As the era of connectivity and unprecedented data generation expands, collaborative intelligence emerges as a key driver for machine learning, encouraging global-scale model development. Federated learning (FL) stands at the heart of this transformation, enabling distributed systems to work collectively on complex tasks while respecting strict constraints on privacy and security. Despite its vast potential, specially in the age of complex models, FL encounters challenges such as elevated communication costs, computational constraints, and the heterogeneous data distributions. In this context, we present AutoFLIP, a novel framework that optimizes FL through an adaptive hybrid pruning approach, grounded in a federated loss exploration phase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP efficiently identifies model substructures for pruning both at structured and unstructured levels. This targeted optimization fosters a symbiotic intelligence loop, reducing computational burdens and boosting model performance on resource-limited devices for a more inclusive and democratized model usage. Our extensive experiments across multiple datasets and FL tasks show that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in computational overhead, a 35.5% decrease in communication costs, and a notable improvement in global accuracy. By significantly reducing these overheads, AutoFLIP offer the way for efficient FL deployment in real-world applications for a scalable and broad applicability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Learning of Computational Resource Usage as Graph Structured Multimarginal Schr\"odinger Bridge</title>
<link>https://arxiv.org/abs/2405.12463</link>
<guid>https://arxiv.org/abs/2405.12463</guid>
<content:encoded><![CDATA[
arXiv:2405.12463v2 Announce Type: replace-cross 
Abstract: We propose to learn the time-varying stochastic computational resource usage of software as a graph structured Schr\"odinger bridge problem. In general, learning the computational resource usage from data is challenging because resources such as the number of CPU instructions and the number of last level cache requests are both time-varying and statistically correlated. Our proposed method enables learning the joint time-varying stochasticity in computational resource usage from the measured profile snapshots in a nonparametric manner. The method can be used to predict the most-likely time-varying distribution of computational resource availability at a desired time. We provide detailed algorithms for stochastic learning in both single and multi-core cases, discuss the convergence guarantees, computational complexities, and demonstrate their practical use in two case studies: a single-core nonlinear model predictive controller, and a synthetic multi-core software.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Failure Sampling for Evaluating Safety-Critical Autonomous Systems</title>
<link>https://arxiv.org/abs/2406.14761</link>
<guid>https://arxiv.org/abs/2406.14761</guid>
<content:encoded><![CDATA[
arXiv:2406.14761v2 Announce Type: replace-cross 
Abstract: Validating safety-critical autonomous systems in high-dimensional domains such as robotics presents a significant challenge. Existing black-box approaches based on Markov chain Monte Carlo may require an enormous number of samples, while methods based on importance sampling often rely on simple parametric families that may struggle to represent the distribution over failures. We propose to sample the distribution over failures using a conditional denoising diffusion model, which has shown success in complex high-dimensional problems such as robotic task planning. We iteratively train a diffusion model to produce state trajectories closer to failure. We demonstrate the effectiveness of our approach on high-dimensional robotic validation tasks, improving sample efficiency and mode coverage compared to existing black-box techniques.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with Adaptive View Synthesis</title>
<link>https://arxiv.org/abs/2406.18012</link>
<guid>https://arxiv.org/abs/2406.18012</guid>
<content:encoded><![CDATA[
arXiv:2406.18012v3 Announce Type: replace-cross 
Abstract: The built environment, encompassing critical infrastructure such as bridges and buildings, requires diligent monitoring of unexpected anomalies or deviations from a normal state in captured imagery. Anomaly detection methods could aid in automating this task; however, deploying anomaly detection effectively in such environments presents significant challenges that have not been evaluated before. These challenges include camera viewpoints that vary, the presence of multiple objects within a scene, and the absence of labeled anomaly data for training. To address these comprehensively, we introduce and formalize Scene Anomaly Detection (Scene AD) as the task of unsupervised, pixel-wise anomaly localization under these specific real-world conditions. Evaluating progress in Scene AD required the development of ToyCity, the first multi-object, multi-view real-image dataset, for unsupervised anomaly detection. Our initial evaluations using ToyCity revealed that established anomaly detection baselines struggle to achieve robust pixel-level localization. To address this, two data augmentation strategies were created to generate additional synthetic images of non-anomalous regions to enhance generalizability. However, the addition of these synthetic images alone only provided minor improvements. Thus, OmniAD, a refinement of the Reverse Distillation methodology, was created to establish a stronger baseline. Our experiments demonstrate that OmniAD, when used with augmented views, yields a 64.33\% increase in pixel-wise \(F_1\) score over Reverse Distillation with no augmentation. Collectively, this work offers the Scene AD task definition, the ToyCity benchmark, the view synthesis augmentation approaches, and the OmniAD method. Project Page: https://drags99.github.io/OmniAD/
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Decoupling Capacitor Optimization for Power Distribution Network of 2.5D ICs with Co-Analysis of Frequency and Time Domains Based on Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2407.04737</link>
<guid>https://arxiv.org/abs/2407.04737</guid>
<content:encoded><![CDATA[
arXiv:2407.04737v3 Announce Type: replace-cross 
Abstract: With the growing need for higher memory bandwidth and computation density, 2.5D design, which involves integrating multiple chiplets onto an interposer, emerges as a promising solution. However, this integration introduces significant challenges due to increasing data rates and a large number of I/Os, necessitating advanced optimization of the power distribution networks (PDNs) both on-chip and on-interposer to mitigate the small signal noise and simultaneous switching noise (SSN). Traditional PDN optimization strategies in 2.5D systems primarily focus on reducing impedance by integrating decoupling capacitors (decaps) to lessen small signal noises. Unfortunately, relying solely on frequency-domain analysis has been proven inadequate for addressing coupled SSN, as indicated by our experimental results. In this work, we introduce a novel two-phase optimization flow using deep reinforcement learning to tackle both the on-chip small signal noise and SSN. Initially, we optimize the impedance in the frequency domain to maintain the small signal noise within acceptable limits while avoiding over-design. Subsequently, in the time domain, we refine the PDN to minimize the voltage violation integral (VVI), a more accurate measure of SSN severity. To the best of our knowledge, this is the first dual-domain optimization strategy that simultaneously addresses both the small signal noise and SSN propagation through strategic decap placement in on-chip and on-interposer PDNs, offering a significant step forward in the design of robust PDNs for 2.5D integrated systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STD-PLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with PLM</title>
<link>https://arxiv.org/abs/2407.09096</link>
<guid>https://arxiv.org/abs/2407.09096</guid>
<content:encoded><![CDATA[
arXiv:2407.09096v4 Announce Type: replace-cross 
Abstract: Spatial-temporal forecasting and imputation are important for real-world intelligent systems. Most existing methods are tailored for individual forecasting or imputation tasks but are not designed for both. Additionally, they are less effective for zero-shot and few-shot learning. While pre-trained language model (PLM) have exhibited strong pattern recognition and reasoning abilities across various tasks, including few-shot and zero-shot learning, their applications in spatial-temporal data understanding has been constrained by insufficient modeling of complex correlations such as the temporal correlations, spatial connectivity, non-pairwise and high-order spatial-temporal correlations within data. In this paper, we propose STD-PLM for understanding both spatial and temporal properties of \underline{S}patial-\underline{T}emporal \underline{D}ata with \underline{PLM}, which is capable of implementing both spatial-temporal forecasting and imputation tasks. STD-PLM understands spatial-temporal correlations via explicitly designed spatial and temporal tokenizers. Topology-aware node embeddings are designed for PLM to comprehend and exploit the topology structure of data in inductive manner. Furthermore, to mitigate the efficiency issues introduced by the PLM, we design a sandglass attention module (SGA) combined with a specific constrained loss function, which significantly improves the model's efficiency while ensuring performance. Extensive experiments demonstrate that STD-PLM exhibits competitive performance and generalization capabilities across the forecasting and imputation tasks on various datasets. Moreover, STD-PLM achieves promising results on both few-shot and zero-shot tasks. The code is made available at \href{https://github.com/Hyheng/STD-PLM}{https://github.com/Hyheng/STD-PLM}
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaGym: Evaluating Persona Agents and LLMs</title>
<link>https://arxiv.org/abs/2407.18416</link>
<guid>https://arxiv.org/abs/2407.18416</guid>
<content:encoded><![CDATA[
arXiv:2407.18416v4 Announce Type: replace-cross 
Abstract: Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion based Semantic Outlier Generation via Nuisance Awareness for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2408.14841</link>
<guid>https://arxiv.org/abs/2408.14841</guid>
<content:encoded><![CDATA[
arXiv:2408.14841v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection, which determines whether a given sample is part of the in-distribution (ID), has recently shown promising results through training with synthetic OOD datasets. Nonetheless, existing methods often produce outliers that are considerably distant from the ID, showing limited efficacy for capturing subtle distinctions between ID and OOD. To address these issues, we propose a novel framework, Semantic Outlier generation via Nuisance Awareness (SONA), which notably produces challenging outliers by directly leveraging pixel-space ID samples through diffusion models. Our approach incorporates SONA guidance, providing separate control over semantic and nuisance regions of ID samples. Thereby, the generated outliers achieve two crucial properties: (i) they present explicit semantic-discrepant information, while (ii) maintaining various levels of nuisance resemblance with ID. Furthermore, the improved OOD detector training with SONA outliers facilitates learning with a focus on semantic distinctions. Extensive experiments demonstrate the effectiveness of our framework, achieving an impressive AUROC of 88% on near-OOD datasets, which surpasses the performance of baseline methods by a significant margin of approximately 6%.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework</title>
<link>https://arxiv.org/abs/2409.00054</link>
<guid>https://arxiv.org/abs/2409.00054</guid>
<content:encoded><![CDATA[
arXiv:2409.00054v2 Announce Type: replace-cross 
Abstract: Identifying effective interventions from the scientific literature is challenging due to the high volume of publications, specialized terminology, and inconsistent reporting formats, making manual curation laborious and prone to oversight. To address this challenge, this paper proposes a novel framework leveraging large language models (LLMs), which integrates a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On the one hand, the POP algorithm conducts a prioritized breadth-first search (BFS) across a predefined ontology, generating structured prompt templates and action sequences to guide the automatic annotation process. On the other hand, the LLM-Duo system features two specialized LLM agents, an explorer and an evaluator, working collaboratively and adversarially to continuously refine annotation quality. We showcase the real-world applicability of our framework through a case study focused on speech-language intervention discovery. Experimental results show that our approach surpasses advanced baselines, achieving more accurate and comprehensive annotations through a fully automated process. Our approach successfully identified 2,421 interventions from a corpus of 64,177 research articles in the speech-language pathology domain, culminating in the creation of a publicly accessible intervention knowledge base with great potential to benefit the speech-language pathology community.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio</title>
<link>https://arxiv.org/abs/2409.03377</link>
<guid>https://arxiv.org/abs/2409.03377</guid>
<content:encoded><![CDATA[
arXiv:2409.03377v4 Announce Type: replace-cross 
Abstract: We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>