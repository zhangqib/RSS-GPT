<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge</title>
<link>https://arxiv.org/abs/2506.15732</link>
<guid>https://arxiv.org/abs/2506.15732</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, world knowledge, counterfactual reasoning, parametric knowledge, finetuning <br>
Summary: 
Large Language Models (LLMs) have extensive world knowledge stored in their parameters and perform well on knowledge-based tasks, but struggle with integrating new information. This study examines LLMs' ability to engage in counterfactual reasoning to combine parametric and contextual knowledge. Results from synthetic and real experiments show that LLMs predominantly rely on parametric knowledge instead of integrating new information. Post-hoc finetuning methods also fail to improve LLMs' counterfactual reasoning, sometimes leading to a loss of stored parametric knowledge. The study highlights significant limitations in LLMs' capacity to utilize parametric knowledge effectively in novel scenarios.<br><br>Summary: <div>
arXiv:2506.15732v1 Announce Type: new 
Abstract: Large Language Models have been shown to contain extensive world knowledge in their parameters, enabling impressive performance on many knowledge intensive tasks. However, when deployed in novel settings, LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information. In this work, we explore whether LLMs can combine knowledge in-context with their parametric knowledge through the lens of counterfactual reasoning. Through synthetic and real experiments in multi-hop reasoning problems, we show that LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. Moreover, we show that simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge. Ultimately, our work reveals important limitations of current LLM's abilities to re-purpose parametric knowledge in novel settings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts</title>
<link>https://arxiv.org/abs/2506.15733</link>
<guid>https://arxiv.org/abs/2506.15733</guid>
<content:encoded><![CDATA[
<div> beam search, test-time scaling, latency-aware, speculative decoding, reinforcement learning

Summary:
The study introduces a latency-aware test-time scaling method called $\texttt{SPECS}$, inspired by speculative decoding, to optimize large language models' reasoning capabilities while reducing user-facing latency. $\texttt{SPECS}$ utilizes a smaller, faster model to efficiently generate candidate sequences, which are evaluated using signals from a larger target model and a dedicated reward model. The method includes new integration strategies like reward-guided soft verification and a reward-based deferral mechanism. Experimental results on various datasets demonstrate that $\texttt{SPECS}$ achieves comparable accuracy to beam search while reducing latency by up to 19.1%. The theoretical analysis reveals that the algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width. This approach addresses the trade-off between accuracy and latency in large language models, enhancing user experience without compromising performance. 

<br><br>Summary: <div>
arXiv:2506.15733v1 Announce Type: new 
Abstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.15734</link>
<guid>https://arxiv.org/abs/2506.15734</guid>
<content:encoded><![CDATA[
<div> VLMs, safety, attacks, delayed safety awareness, safety reminder<br>
<br>
Summary: Vision-Language Models (VLMs) are increasingly used in various applications but face unique vulnerabilities. A novel phenomenon called "delayed safety awareness" was identified, where compromised VLMs may eventually recognize risks and self-correct. A proactive approach, the "Safety Reminder," was proposed to inject prompts during text generation to improve safety awareness. This approach effectively reduces attack success rates without compromising model utility on benign tasks. Through evaluation on safety benchmarks and adversarial attacks, the Safety Reminder was shown to enhance VLM safety in real-world applications. <div>
arXiv:2506.15734v1 Announce Type: new 
Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across real-world applications such as code generation and chatbot assistance, ensuring their safety has become paramount. Unlike traditional Large Language Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to modify visual or textual inputs to bypass safety guardrails and trigger the generation of harmful content. Through systematic analysis of VLM behavior under attack, we identify a novel phenomenon termed ``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs may initially be compromised to produce harmful content, but eventually recognize the associated risks and attempt to self-correct. This pattern suggests that VLMs retain their underlying safety awareness but experience a temporal delay in their activation. Building on this insight, we hypothesize that VLMs' safety awareness can be proactively reactivated through carefully designed prompts. To this end, we introduce ``The Safety Reminder'', a soft prompt tuning approach that optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness, effectively preventing harmful content generation. Additionally, our safety reminder only activates when harmful content is detected, leaving normal conversations unaffected and preserving the model's performance on benign tasks. Through comprehensive evaluation across three established safety benchmarks and one adversarial attacks, we demonstrate that our approach significantly reduces attack success rates while maintaining model utility, offering a practical solution for deploying safer VLMs in real-world applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextBench: Modifying Contexts for Targeted Latent Activation</title>
<link>https://arxiv.org/abs/2506.15735</link>
<guid>https://arxiv.org/abs/2506.15735</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, safety, latent features, context modification, Evolutionary Prompt Optimisation

Summary: 
The study explores methods to generate linguistically fluent inputs that trigger specific behaviors or latent features in language models for safety applications. The researchers introduce ContextBench, a benchmark for evaluating these methods in terms of elicitation strength and linguistic fluency. They find that current methods struggle to balance these objectives effectively. By enhancing Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, the researchers achieve state-of-the-art performance in balancing elicitation effectiveness and fluency. This research is significant for advancing the understanding of targeted input generation for language models and has implications for improving safety measures in various applications. 

<br><br>Summary: <div>
arXiv:2506.15735v1 Announce Type: new 
Abstract: Identifying inputs that trigger specific behaviours or latent features in language models could have a wide range of safety use cases. We investigate a class of methods capable of generating targeted, linguistically fluent inputs that activate specific latent features or elicit model behaviours. We formalise this approach as context modification and present ContextBench -- a benchmark with tasks assessing core method capabilities and potential safety applications. Our evaluation framework measures both elicitation strength (activation of latent features or behaviours) and linguistic fluency, highlighting how current state-of-the-art methods struggle to balance these objectives. We enhance Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, and demonstrate that these variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents</title>
<link>https://arxiv.org/abs/2506.15740</link>
<guid>https://arxiv.org/abs/2506.15740</guid>
<content:encoded><![CDATA[
<div> sabotage, Large Language Models (LLMs), monitoring, SHAE-Arena, hidden objectives<br>
Summary:<br>
The study evaluates the ability of frontier LLMs to engage in sabotage by pursuing hidden objectives while completing tasks. The evaluation is done using the SHAE-Arena dataset, which consists of complex tasks with benign main objectives and harmful side objectives. The top performing frontier models achieve 27% and 15% success as sabotage agents when overseen by a monitor. Success in completing the side task often depends on access to a hidden scratchpad. The monitoring abilities of models are also tested, with the best monitor achieving an AUC of 0.87 in distinguishing benign and malign transcripts. Current models struggle with sabotage due to failures in long-context task execution, but monitoring for subtle sabotage attempts is challenging due to the complexity of tasks. The study highlights the need for improved monitoring capabilities as LLMs are deployed in more complex and long-horizon settings. <br><br>Summary: <div>
arXiv:2506.15740v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OAgents: An Empirical Study of Building Effective Agents</title>
<link>https://arxiv.org/abs/2506.15741</link>
<guid>https://arxiv.org/abs/2506.15741</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, agent research, design choices, evaluation protocol, OAgents<br>
Summary:<br>
The research paper addresses the lack of standardization and scientific rigor in current agent research practices, hindering fair comparisons among methods and understanding the impact of design choices on agent effectiveness. By conducting a systematic empirical study on GAIA benchmark and BrowseComp, the authors identify crucial components for effective agents and introduce a more robust evaluation protocol to stabilize comparisons. The study reveals that some design choices previously considered logical are actually redundant. In response to their findings, the authors develop and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offer a modular design for various agent components, aiming to advance research in Agentic AI.<br><br>Summary: <div>
arXiv:2506.15741v1 Announce Type: new 
Abstract: Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts</title>
<link>https://arxiv.org/abs/2506.15751</link>
<guid>https://arxiv.org/abs/2506.15751</guid>
<content:encoded><![CDATA[
<div> transformer, LLMs, Sysformer, robustness, system prompts
Summary:
Sysformer is a novel approach to enhance the safety of large language models (LLMs) by adapting system prompts based on user input. The model, trained to update system prompts in the LLM input embedding space, significantly improves the refusal rate for harmful prompts and compliance with safe prompts. Results show an up to 80% increase in refusal rate and up to 90% improvement in compliance with safe prompts across different LLMs and benchmarks. Sysformer also boosts robustness against jailbreaking attacks by up to 100%. This method offers a cost-effective way to safeguard LLMs and encourages further research into customizable system prompts.<br><br>Summary: <div>
arXiv:2506.15751v1 Announce Type: new 
Abstract: As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90\%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100\%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear-Time Primitives for Algorithm Development in Graphical Causal Inference</title>
<link>https://arxiv.org/abs/2506.15758</link>
<guid>https://arxiv.org/abs/2506.15758</guid>
<content:encoded><![CDATA[
<div> Framework, Graphical causal inference, Reachability, Rule table schema, Efficient

Summary:
CIfly is a framework designed for efficient algorithmic primitives in graphical causal inference, with a focus on isolating reachability as a core operation. By leveraging purpose-built state-space graphs constructed during traversal, CIfly simplifies many causal reasoning tasks to reachability, enabling linear time execution. It offers a more efficient alternative to moralization and latent projection, which are computationally equivalent to Boolean matrix multiplication. The open-source Rust implementation of CIfly allows parsing rule table text files and executing specified algorithms with high performance, accessible from Python and R. Various established causal inference tasks have been re-implemented within the CIfly framework, showcasing its utility. Moreover, new algorithms for instrumental variables have been developed, highlighting CIfly's flexibility and scalability as a backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment. 

<br><br>Summary: <div>
arXiv:2506.15758v1 Announce Type: new 
Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints</title>
<link>https://arxiv.org/abs/2506.15774</link>
<guid>https://arxiv.org/abs/2506.15774</guid>
<content:encoded><![CDATA[
<div> heuristic, NP-complete satisfiability problem, 3-SAT, stochastic local search, DOCSAT <br>
Summary: 
The article introduces the DOCSAT algorithm, a stochastic local search heuristic for the NP-complete satisfiability problem 3-SAT. It is designed to perform well in challenging instances by dissipating oversatisfied constraints to prevent getting stuck in local minima. DOCSAT outperforms existing solvers like WalkSAT and Kissat in solving hard, satisfiable 3-SAT instances of varying sizes up to N=15000. By reducing the abundance of oversatisfied combinatorial constraints, DOCSAT effectively avoids local minima traps, showing promising results even in the toughest cases. This approach of harnessing statistical structure to escape local minima traps in stochastic local search opens up possibilities for application in other optimization problems. <br><br> <div>
arXiv:2506.15774v1 Announce Type: new 
Abstract: We introduce and benchmark a stochastic local search heuristic for the NP-complete satisfiability problem 3-SAT that drastically outperforms existing solvers in the notoriously difficult realm of critically hard instances. Our construction is based on the crucial observation that well established previous approaches such as WalkSAT are prone to get stuck in local minima that are distinguished from true solutions by a larger number of oversatisfied combinatorial constraints. To address this issue, the proposed algorithm, coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their unfavorable abundance so as to render them critical. We analyze and benchmark our algorithm on a randomly generated sample of hard but satisfiable 3-SAT instances with varying problem sizes up to N=15000. Quite remarkably, we find that DOCSAT outperforms both WalkSAT and other well known algorithms including the complete solver Kissat, even when comparing its ability to solve the hardest quintile of the sample to the average performance of its competitors. The essence of DOCSAT may be seen as a way of harnessing statistical structure beyond the primary cost function of a combinatorial problem to avoid or escape local minima traps in stochastic local search, which opens avenues for generalization to other optimization problems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLR: An Automated Synthesis Framework for Scalable Logical Reasoning</title>
<link>https://arxiv.org/abs/2506.15787</link>
<guid>https://arxiv.org/abs/2506.15787</guid>
<content:encoded><![CDATA[
<div> framework, training, Large Language Models, logical reasoning, benchmark

Summary:
SLR is an end-to-end framework for evaluating and training Large Language Models (LLMs) through Scalable Logical Reasoning, allowing automated synthesis of tasks with controlled difficulty levels. It generates ground-truth rules, validation programs, and instruction prompts for reasoning tasks. The SLR-Bench benchmark includes 19k prompts across 20 levels of complexity. LLMs excel at generating syntax but struggle with logical inference, even with recent models showing some improvement. Logic-tuning using SLR significantly boosts Llama-3-8B accuracy on SLR-Bench, matching Gemini-Flash-Thinking with lower computational costs. SLR is fully automated, doesn't require human annotation, ensures dataset novelty, and provides a scalable platform for enhancing LLMs' reasoning abilities. 

Summary: <div>
arXiv:2506.15787v1 Announce Type: new 
Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2506.15880</link>
<guid>https://arxiv.org/abs/2506.15880</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Reinforcement Learning, Xiangqi, Chinese Chess, Monte Carlo Tree Search, policy-value networks

Summary:
This paper introduces a Deep Reinforcement Learning (DRL) system for Xiangqi, a complex Chinese Chess game. The system integrates neural networks with Monte Carlo Tree Search (MCTS) to facilitate strategic self-play and self-improvement. It addresses the specific challenges of Xiangqi, such as its unique board layout, piece movement constraints, and victory conditions. By combining policy-value networks with MCTS, the system can simulate move consequences and enhance decision-making processes. The approach overcomes hurdles like Xiangqi's high branching factor and asymmetrical piece dynamics, enhancing AI capabilities in culturally significant strategy games. Moreover, it provides valuable insights into adapting DRL-MCTS frameworks to domain-specific rule systems. Through this work, advancements in AI technology for strategic games like Xiangqi are achieved, contributing to the broader understanding of machine learning in complex board games.<br><br>Summary: This paper presents a DRL system for Xiangqi that integrates neural networks and MCTS, addressing the game's unique challenges and advancing AI capabilities in strategy games, offering insights for adapting DRL-MCTS frameworks to domain-specific rule systems. <div>
arXiv:2506.15880v1 Announce Type: new 
Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. Addressing the underexplored complexity of Xiangqi, including its unique board layout, piece movement constraints, and victory conditions, our approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. By overcoming challenges such as Xiangqi's high branching factor and asymmetrical piece dynamics, our work advances AI capabilities in culturally significant strategy games while providing insights for adapting DRL-MCTS frameworks to domain-specific rule systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues</title>
<link>https://arxiv.org/abs/2506.15928</link>
<guid>https://arxiv.org/abs/2506.15928</guid>
<content:encoded><![CDATA[
<div> personality traits, negotiation outcomes, AI agent characteristics, social dynamics, mission success<br>
<br>
Summary:
This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts. Two experiments were conducted using Sotopia as a simulation testbed to evaluate the impact of personality traits and AI agent characteristics on negotiation outcomes. The first experiment analyzed how personality traits such as Agreeableness and Extraversion affect price bargaining negotiations, with sociocognitive lexical measures revealing fine-grained differences in agents' communication patterns. The second experiment focused on human-AI job negotiations, exploring the impact of transparency, competence, and adaptability of AI agents on mission effectiveness. The findings highlight the importance of AI agent trustworthiness in achieving reliable outcomes in high-stakes operational scenarios. Overall, this work advances the evaluation of agentic AI workflows by incorporating social dynamics essential for mission success in complex operations.<br><br>Summary: <div>
arXiv:2506.15928v1 Announce Type: new 
Abstract: This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning</title>
<link>https://arxiv.org/abs/2506.16015</link>
<guid>https://arxiv.org/abs/2506.16015</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian Epistemology, Weighted Authority, scientific claims, belief updates, machine reasoning systems

Summary: 
Bayesian Epistemology with Weighted Authority (BEWA) is a structured architecture that helps handle the vast amount of scientific literature through a probabilistically coherent belief system. It contextualizes scientific claims, assigns author attributes, and evaluates them with replication scores, citation weighting, and temporal decay. Beliefs are updated using evidence-based Bayesian inference, contradiction processing, and epistemic decay mechanisms. The architecture supports claim propagation, authorial credibility modeling, cryptographic anchoring, and audit verification. By formalizing scientific reasoning into a computationally verifiable network, BEWA aims to enhance machine reasoning systems that prioritize truth utility, rational belief convergence, and audit-resilient integrity in dynamic scientific fields. <div>
arXiv:2506.16015v1 Announce Type: new 
Abstract: The exponential expansion of scientific literature has surpassed the epistemic processing capabilities of both human experts and current artificial intelligence systems. This paper introduces Bayesian Epistemology with Weighted Authority (BEWA), a formally structured architecture that operationalises belief as a dynamic, probabilistically coherent function over structured scientific claims. Each claim is contextualised, author-attributed, and evaluated through a system of replication scores, citation weighting, and temporal decay. Belief updates are performed via evidence-conditioned Bayesian inference, contradiction processing, and epistemic decay mechanisms. The architecture supports graph-based claim propagation, authorial credibility modelling, cryptographic anchoring, and zero-knowledge audit verification. By formalising scientific reasoning into a computationally verifiable epistemic network, BEWA advances the foundation for machine reasoning systems that promote truth utility, rational belief convergence, and audit-resilient integrity across dynamic scientific domains.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations</title>
<link>https://arxiv.org/abs/2506.16016</link>
<guid>https://arxiv.org/abs/2506.16016</guid>
<content:encoded><![CDATA[
<div> Keywords: Hard constraints, reinforcement learning, Hamilton-Jacobi equations, dual-objective satisfaction, Proximal Policy Optimization

Summary: 
This study addresses the challenges of incorporating hard constraints in reinforcement learning (RL) by introducing two novel value functions for dual-objective satisfaction. By leveraging recent advances connecting Hamilton-Jacobi equations with RL, the Reach-Always-Avoid and Reach-Reach problems are tackled to achieve distinct reward and penalty thresholds as well as thresholds of two distinct rewards. Unlike traditional temporal logic approaches, explicit Bellman forms are derived for these problems to facilitate tractable solutions without the need for complex automaton representations. A variation of Proximal Policy Optimization (DO-HJ-PPO) is proposed and demonstrated to outperform baselines in safe-arrival and multi-target achievement tasks, showcasing qualitatively distinct behaviors in constrained decision-making scenarios. This work offers a new perspective on constrained RL problems and presents a promising approach for addressing dual-objective optimization challenges in practical applications.<br><br>Summary: <div>
arXiv:2506.16016v1 Announce Type: new 
Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the reward function or the model architecture, often degrade policy performance. Lagrangian methods offer a way to blend objectives with constraints, but often require intricate reward engineering and parameter tuning. In this work, we extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to propose two novel value functions for dual-objective satisfaction. Namely, we address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds of two distinct rewards. In contrast with temporal logic approaches, which typically involve representing an automaton, we derive explicit, tractable Bellman forms in this context by decomposing our problem into reach, avoid, and reach-avoid problems, as to leverage these aforementioned recent advances. From a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are complementary and fundamentally different from standard sum-of-rewards problems and temporal logic problems, providing a new perspective on constrained decision-making. We leverage our analysis to propose a variation of Proximal Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of tasks for safe-arrival and multi-target achievement, we demonstrate that DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches and out-competes a number of baselines in various metrics.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents</title>
<link>https://arxiv.org/abs/2506.16042</link>
<guid>https://arxiv.org/abs/2506.16042</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, computer-use tasks, latency, OSWorld, efficiency

Summary:
Generative AI systems have shown significant advancements in solving computer-use tasks, but their high end-to-end latency hinders their practical usability. A study on the temporal performance of computer-use agents on the OSWorld benchmark revealed that planning and reflection stages contribute significantly to overall latency. Analysis showed that as agents progress through a task, subsequent steps can take 3 times longer than initial steps. The construction of OSWorld-Human dataset, with human-determined trajectories for tasks, allowed for evaluating agent efficiency. Results indicated that even top-performing agents on OSWorld took 1.4-2.7 times more steps than necessary to complete tasks. These findings highlight the need for optimizing efficiency in generative AI systems for improved usability and performance in real-world applications. 

<br><br>Summary: <div>
arXiv:2506.16042v1 Announce Type: new 
Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks involving desktop applications. State-of-the-art systems have focused solely on improving accuracy on leading benchmarks. However, these systems are practically unusable due to extremely high end-to-end latency (e.g., tens of minutes) for tasks that typically take humans just a few minutes to complete. To understand the cause behind this and to guide future developments of computer agents, we conduct the first study on the temporal performance of computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We find that large model calls for planning and reflection account for the majority of the overall latency, and as an agent uses more steps to complete a task, each successive step can take 3x longer than steps at the beginning of a task. We then construct OSWorld-Human, a manually annotated version of the original OSWorld dataset that contains a human-determined trajectory for each task. We evaluate 16 agents on their efficiency using OSWorld-Human and found that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than necessary.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies</title>
<link>https://arxiv.org/abs/2506.16087</link>
<guid>https://arxiv.org/abs/2506.16087</guid>
<content:encoded><![CDATA[
<div> Ontology, process knowledge, manufacturing, mathematical expressions, verification mechanisms <br>
Summary: The paper discusses the formalization of process knowledge using ontologies in manufacturing. It focuses on representing parameter interdependencies through mathematical expressions for tasks like calculation and simulation. To ensure consistency and semantic coherence, verification mechanisms are proposed. These include SPARQL-based filtering for data retrieval, unit consistency checks based on annotations, and data completeness checks for evaluating interdependencies. By applying these mechanisms to a Resin Transfer Molding (RTM) use case, the study demonstrates the effectiveness of the approach in developing machine-interpretable engineering models. <div>
arXiv:2506.16087v1 Announce Type: new 
Abstract: The formalization of process knowledge using ontologies enables consistent modeling of parameter interdependencies in manufacturing. These interdependencies are typically represented as mathematical expressions that define relations between process parameters, supporting tasks such as calculation, validation, and simulation. To support cross-context application and knowledge reuse, such expressions are often defined in a generic form and applied across multiple process contexts. This highlights the necessity of a consistent and semantically coherent model to ensure the correctness of data retrieval and interpretation. Consequently, dedicated mechanisms are required to address key challenges such as selecting context-relevant data, ensuring unit compatibility between variables and data elements, and verifying the completeness of input data required for evaluating mathematical expressions. This paper presents a set of verification mechanisms for a previously developed ontology-based process model that integrates standardized process semantics, data element definitions, and formal mathematical constructs. The approach includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a unit consistency check based on expected-unit annotations and semantic classification, and (iii) a data completeness check to validate the evaluability of interdependencies. The applicability of the approach is demonstrated with a use case from Resin Transfer Molding (RTM), supporting the development of machine-interpretable and verifiable engineering models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction</title>
<link>https://arxiv.org/abs/2506.16144</link>
<guid>https://arxiv.org/abs/2506.16144</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, algorithm performance prediction, numerical blackbox optimization, algorithm configurations, heterogeneous graph data structures<br>
<br>
Summary: <br>
Automated algorithm performance prediction in numerical blackbox optimization often relies on problem characterizations through exploratory landscape analysis features. However, such approaches overlook the crucial factor of algorithm configurations that significantly influence performance outcomes. This study introduces the use of heterogeneous graph data structures and graph neural networks to capture the complex relationships between problems, algorithm configurations, and performance outcomes. By focusing on modCMA-ES and modDE frameworks, which decompose CMA-ES and DE algorithms, the study evaluates numerous algorithm variants across different problems and dimensions. The results indicate a substantial improvement in prediction accuracy compared to traditional tabular-based methods, showcasing the potential of geometric learning in black-box optimization. <div>
arXiv:2506.16144v1 Announce Type: new 
Abstract: Automated algorithm performance prediction in numerical blackbox optimization often relies on problem characterizations, such as exploratory landscape analysis features. These features are typically used as inputs to machine learning models and are represented in a tabular format. However, such approaches often overlook algorithm configurations, a key factor influencing performance. The relationships between algorithm operators, parameters, problem characteristics, and performance outcomes form a complex structure best represented as a graph. This work explores the use of heterogeneous graph data structures and graph neural networks to predict the performance of optimization algorithms by capturing the complex dependencies between problems, algorithm configurations, and performance outcomes. We focus on two modular frameworks, modCMA-ES and modDE, which decompose two widely used derivative-free optimization algorithms: the covariance matrix adaptation evolution strategy (CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576 modDE variants on 24 BBOB problems across six runtime budgets and two problem dimensions. Achieving up to 36.6% improvement in MSE over traditional tabular-based methods, this work highlights the potential of geometric learning in black-box optimization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior</title>
<link>https://arxiv.org/abs/2506.16163</link>
<guid>https://arxiv.org/abs/2506.16163</guid>
<content:encoded><![CDATA[
<div> Keywords: human decision-making, artificial intelligence, Large Language Models, uncertainty, risk

Summary:
Human decision-making is fundamental to society, but the rise of Large Language Models (LLMs) is shifting this process to artificial intelligence. A study compared the decision-making abilities of five LLMs with human participants in areas of uncertainty, risk, and set-shifting. LLMs often outperformed humans and displayed near-optimal performance. However, the underlying processes of their decision-making differed significantly from humans. LLMs demonstrated the ability to handle uncertainty, assess risk, and adapt to changes effectively. This study highlights the potential of LLMs but also raises concerns about relying on them as substitutes for human judgment. Further research is needed to understand the implications of delegating decision-making to artificial intelligence.<br><br>Summary: <div>
arXiv:2506.16163v1 Announce Type: new 
Abstract: Human decision-making belongs to the foundation of our society and civilization, but we are on the verge of a future where much of it will be delegated to artificial intelligence. The arrival of Large Language Models (LLMs) has transformed the nature and scope of AI-supported decision-making; however, the process by which they learn to make decisions, compared to humans, remains poorly understood. In this study, we examined the decision-making behavior of five leading LLMs across three core dimensions of real-world decision-making: uncertainty, risk, and set-shifting. Using three well-established experimental psychology tasks designed to probe these dimensions, we benchmarked LLMs against 360 newly recruited human participants. Across all tasks, LLMs often outperformed humans, approaching near-optimal performance. Moreover, the processes underlying their decisions diverged fundamentally from those of humans. On the one hand, our finding demonstrates the ability of LLMs to manage uncertainty, calibrate risk, and adapt to changes. On the other hand, this disparity highlights the risks of relying on them as substitutes for human judgment, calling for further inquiry.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximation Fixpoint Theory with Refined Approximation Spaces</title>
<link>https://arxiv.org/abs/2506.16294</link>
<guid>https://arxiv.org/abs/2506.16294</guid>
<content:encoded><![CDATA[
<div> Approximation Fixpoint Theory, AFT, non-monotonic reasoning, logic programming, answer set programming<br>
<br>
Summary:In this paper, the authors introduce a more general notion of approximation spaces within the framework of Approximation Fixpoint Theory (AFT). AFT is a powerful theory that deals with various semantics of non-monotonic reasoning formalisms in knowledge representation. The extension of consistent AFT to handle more refined approximations than intervals helps overcome limitations faced in simpler examples. By introducing this more general notion of approximation spaces, the authors showcase improved expressiveness and investigate relations between different approximation spaces. This advancement in AFT allows for better approximation and construction of fixpoints of interest within non-monotonic operators on lattices, leading to further applications and advancements in non-monotonic reasoning formalisms. <div>
arXiv:2506.16294v1 Announce Type: new 
Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various semantics of non-monotonic reasoning formalisms in knowledge representation such as Logic Programming and Answer Set Programming. Many semantics of such non-monotonic formalisms can be characterized as suitable fixpoints of a non-monotonic operator on a suitable lattice. Instead of working on the original lattice, AFT operates on intervals in such lattice to approximate or construct the fixpoints of interest. While AFT has been applied successfully across a broad range of non-monotonic reasoning formalisms, it is confronted by its limitations in other, relatively simple, examples. In this paper, we overcome those limitations by extending consistent AFT to deal with approximations that are more refined than intervals. Therefore, we introduce a more general notion of approximation spaces, showcase the improved expressiveness and investigate relations between different approximation spaces.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach</title>
<link>https://arxiv.org/abs/2506.16335</link>
<guid>https://arxiv.org/abs/2506.16335</guid>
<content:encoded><![CDATA[
<div> transformer, legal analysis, reasoning, structured prompting framework, neural-symbolic system
Summary: 
- The paper introduces a structured prompting framework for improving Large Language Models (LLMs) in reasoning tasks, particularly in legal analysis.
- The framework decomposes reasoning into three steps: entity identification, property extraction, and symbolic rule application to ensure logical consistency.
- By integrating neural and symbolic approaches, the method leverages LLMs' interpretive flexibility while maintaining formal verification for consistent rule application.
- The framework allows domain experts to refine logical structures without changing the model architecture, enhancing task customization.
- Evaluation on the LegalBench hearsay determination task showed significant performance improvements with OpenAI models using the structured decomposition approach, indicating potential for transparent and consistent rule-based reasoning in legal tasks. 
<br><br>Summary: <div>
arXiv:2506.16335v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle with consistent rule application, exception handling, and explainability, particularly in domains like legal analysis that require both natural language understanding and precise logical inference. This paper introduces a structured prompting framework that decomposes reasoning into three verifiable steps: entity identification, property extraction, and symbolic rule application. By integrating neural and symbolic approaches, our method leverages LLMs' interpretive flexibility while ensuring logical consistency through formal verification. The framework externalizes task definitions, enabling domain experts to refine logical structures without altering the architecture. Evaluated on the LegalBench hearsay determination task, our approach significantly outperformed baselines, with OpenAI o-family models showing substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini reaching 0.867 using structured decomposition with complementary predicates, compared to their few-shot baselines of 0.714 and 0.74 respectively. This hybrid neural-symbolic system offers a promising pathway for transparent and consistent rule-based reasoning, suggesting potential for explainable AI applications in structured legal reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
<div> safety hazards, embodied agents, interactive safety, evaluation paradigm, IS-Bench<br>
Summary: Flawed planning from VLM-driven embodied agents poses safety hazards in household tasks. Current evaluation paradigms lack interactive assessment of risks. IS-Bench is a multi-modal benchmark for interactive safety, with 161 scenarios and 388 unique risks. It evaluates an agent's ability to perceive and mitigate risks in the correct order. Experiments show that existing agents lack interactive safety awareness. Chain-of-Thought improves safety but may hinder task completion. IS-Bench sets the foundation for safer embodied AI systems. <br><br>Summary: <div>
arXiv:2506.16402v1 Announce Type: new 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Personalisation of Cross-Channel Marketing Experiences</title>
<link>https://arxiv.org/abs/2506.16429</link>
<guid>https://arxiv.org/abs/2506.16429</guid>
<content:encoded><![CDATA[
<div> Thompson sampling, Difference-in-Differences design, optimal policy, individual treatment effect, consumer applications
<br>
Summary: 
The article discusses the problem of effectively communicating content to users in consumer applications. Traditional approaches rely on manual marketer work, limiting personalization of content and timing. The proposed solution formulates the task as a sequential decision-making problem to optimize a policy that maximizes engagement for various events. By using Thompson sampling to balance exploration and exploitation, and leveraging a Difference-in-Differences design for Individual Treatment Effect estimation, the methodology has shown significant improvements in goal events across multiple product features in a multi-service application. The approach is currently deployed to 150 million users, demonstrating its scalability and effectiveness in enhancing user engagement. <div>
arXiv:2506.16429v1 Announce Type: new 
Abstract: Consumer applications provide ample opportunities to surface and communicate various forms of content to users. From promotional campaigns for new features or subscriptions, to evergreen nudges for engagement, or personalised recommendations; across e-mails, push notifications, and in-app surfaces. The conventional approach to orchestration for communication relies heavily on labour-intensive manual marketer work, and inhibits effective personalisation of content, timing, frequency, and copy-writing. We formulate this task under a sequential decision-making framework, where we aim to optimise a modular decision-making policy that maximises incremental engagement for any funnel event. Our approach leverages a Difference-in-Differences design for Individual Treatment Effect estimation, and Thompson sampling to balance the explore-exploit trade-off. We present results from a multi-service application, where our methodology has resulted in significant increases to a variety of goal events across several product features, and is currently deployed across 150 million users.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning</title>
<link>https://arxiv.org/abs/2506.16499</link>
<guid>https://arxiv.org/abs/2506.16499</guid>
<content:encoded><![CDATA[
<div> AI4AI, ML-Master, exploration, reasoning, MLE-Bench <br>
<br>
Summary: 
The article introduces ML-Master, an AI4AI agent that integrates exploration and reasoning using a selectively scoped memory mechanism. ML-Master outperforms existing methods on the MLE-Bench, achieving a 29.3% average medal rate within a 12-hour time constraint. It efficiently combines insights from parallel solution trajectories with analytical reasoning, leading to superior performance, especially in medium-complexity tasks. The agent addresses the limitation of LLM-based agents by leveraging accumulated experience during solution exploration. The results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI. <div>
arXiv:2506.16499v1 Announce Type: new 
Abstract: As AI capabilities advance toward and potentially beyond human-level performance, a natural transition emerges where AI-driven development becomes more efficient than human-centric approaches. A promising pathway toward this transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate and optimize the design, training, and deployment of AI systems themselves. While LLM-based agents have shown the potential to realize AI4AI, they are often unable to fully leverage the experience accumulated by agents during the exploration of solutions in the reasoning process, leading to inefficiencies and suboptimal performance. To address this limitation, we propose ML-Master, a novel AI4AI agent that seamlessly integrates exploration and reasoning by employing a selectively scoped memory mechanism. This approach allows ML-Master to efficiently combine diverse insights from parallel solution trajectories with analytical reasoning, guiding further exploration without overwhelming the agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it achieves a 29.3% average medal rate, significantly surpassing existing methods, particularly in medium-complexity tasks, while accomplishing this superior performance within a strict 12-hour time constraint-half the 24-hour limit used by previous baselines. These results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System</title>
<link>https://arxiv.org/abs/2506.16575</link>
<guid>https://arxiv.org/abs/2506.16575</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, harmful content analysis, Elo rating-based method, organizational conflicts, microaggressions

Summary: 
An article discusses the challenges faced when using Large Language Models (LLMs) for analyzing harmful content in organizational research. The built-in moderation systems of LLMs can hinder accurate analysis of content such as microaggressions and hate speech. To address this, the paper proposes an Elo rating-based method that proves to enhance LLM performance in detecting harmful content. This method surpasses traditional LLM prompting techniques and conventional machine learning models in terms of accuracy, precision, and F1 scores. By using this approach, researchers can achieve better reliability in analyzing harmful content, reduce false positives, and increase scalability for large datasets. The improved performance of LLMs in detecting workplace harassment and toxic communication can lead to the development of safer and more inclusive work environments.<br><br>Summary: <div>
arXiv:2506.16575v1 Announce Type: new 
Abstract: Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Community-driven vision for a new Knowledge Resource for AI</title>
<link>https://arxiv.org/abs/2506.16596</link>
<guid>https://arxiv.org/abs/2506.16596</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge resource, AI infrastructure, large language models, robotic planning, factually false information <br>
Summary: <br>
The article discusses the need for a comprehensive, multi-purpose knowledge resource in AI, highlighting the deficiencies in current knowledge infrastructure. While existing resources like WordNet and Wolfram|Alpha have been successful, there are still critical gaps in knowledge that hinder the performance of AI systems. Large language models struggle with knowledge gaps, robotic planning lacks necessary world knowledge, and detecting false information relies heavily on human expertise. The paper outlines the outcomes of a recent AAAI workshop that gathered researchers to address these challenges and proposes a community-driven vision for a new knowledge infrastructure. One key suggestion is to develop an open engineering framework that effectively integrates knowledge modules for practical applications, incorporating conventions and social structures adopted by contributors. By leveraging contemporary advances in knowledge representation and reasoning, such a framework could enhance AI systems' performance and address existing knowledge deficiencies. <br> <div>
arXiv:2506.16596v1 Announce Type: new 
Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and other commercial knowledge graphs, verifiable, general-purpose widely available sources of knowledge remain a critical deficiency in AI infrastructure. Large language models struggle due to knowledge gaps; robotic planning lacks necessary world knowledge; and the detection of factually false information relies heavily on human expertise. What kind of knowledge resource is most needed in AI today? How can modern technology shape its development and evaluation? A recent AAAI workshop gathered over 50 researchers to explore these questions. This paper synthesizes our findings and outlines a community-driven vision for a new knowledge infrastructure. In addition to leveraging contemporary advances in knowledge representation and reasoning, one promising idea is to build an open engineering framework to exploit knowledge modules effectively within the context of practical applications. Such a framework should include sets of conventions and social structures that are adopted by contributors.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring</title>
<link>https://arxiv.org/abs/2506.16617</link>
<guid>https://arxiv.org/abs/2506.16617</guid>
<content:encoded><![CDATA[
<div> deep learning, predictive process monitoring, explainable AI, decision-making, user trust

Summary: 
The study focuses on the use of deep learning models in Predictive Process Monitoring (PPM) and the importance of Explainable AI (XAI) in enhancing user trust. While deep learning models achieve high accuracy in predicting process outcomes, their lack of interpretability hinders user adoption. The study explores the effects of different explanation styles (feature importance, rule-based, and counterfactual) and perceived AI accuracy on decision-making in PPM. A decision-making experiment was conducted to measure users' decisions before and after receiving explanations, evaluating objective metrics such as Task Performance and Agreement, as well as subjective metrics like Decision Confidence. The findings highlight the significant impact of perceived accuracy and explanation style on user decision-making in PPM, emphasizing the importance of incorporating user-centered aspects in XAI evaluations. <div>
arXiv:2506.16617v1 Announce Type: new 
Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to predict the future behavior of ongoing processes, such as predicting process outcomes. While these models achieve high accuracy, their lack of interpretability undermines user trust and adoption. Explainable AI (XAI) aims to address this challenge by providing the reasoning behind the predictions. However, current evaluations of XAI in PPM focus primarily on functional metrics (such as fidelity), overlooking user-centered aspects such as their effect on task performance and decision-making. This study investigates the effects of explanation styles (feature importance, rule-based, and counterfactual) and perceived AI accuracy (low or high) on decision-making in PPM. We conducted a decision-making experiment, where users were presented with the AI predictions, perceived accuracy levels, and explanations of different styles. Users' decisions were measured both before and after receiving explanations, allowing the assessment of objective metrics (Task Performance and Agreement) and subjective metrics (Decision Confidence). Our findings show that perceived accuracy and explanation style have a significant effect.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics</title>
<link>https://arxiv.org/abs/2506.16696</link>
<guid>https://arxiv.org/abs/2506.16696</guid>
<content:encoded><![CDATA[
<div> XGBoost, football tactics, spatiotemporal data, tactical analysis, decision-making  
Summary:  
- Understanding football tactics is essential for managers and analysts.  
- Previous models for analyzing tactics are computationally expensive or lack interpretability.  
- This study explores the use of low-dimensional, rule-based models using spatiotemporal data to capture football tactics effectively.  
- Key variables representing the game state were identified through discussions with a manager.  
- An XGBoost model trained on event and tracking data from the LaLiga season revealed that distance between players and the ball, and player space score were critical in predicting pass success.  
- The interpretable low-dimensional modeling approach offers practical value as a tool to support decision-making in football.  
<br><br>Summary: <div>
arXiv:2506.16696v1 Announce Type: new 
Abstract: Understanding football tactics is crucial for managers and analysts. Previous research has proposed models based on spatial and kinematic equations, but these are computationally expensive. Also, Reinforcement learning approaches use player positions and velocities but lack interpretability and require large datasets. Rule-based models align with expert knowledge but have not fully considered all players' states. This study explores whether low-dimensional, rule-based models using spatiotemporal data can effectively capture football tactics. Our approach defines interpretable state variables for both the ball-holder and potential pass receivers, based on criteria that explore options like passing. Through discussions with a manager, we identified key variables representing the game state. We then used StatsBomb event data and SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost model to predict pass success. The analysis revealed that the distance between the player and the ball, as well as the player's space score, were key factors in determining successful passes. Our interpretable low-dimensional modeling facilitates tactical analysis through the use of intuitive variables and provides practical value as a tool to support decision-making in football.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing High-quality Participation From Federated Learning Agents</title>
<link>https://arxiv.org/abs/2506.16731</link>
<guid>https://arxiv.org/abs/2506.16731</guid>
<content:encoded><![CDATA[
<div> Wasserstein distance, federated learning, incentive-aware framework, data heterogeneity, Stackelberg game model
<br>
Summary: 
In this paper, the authors address challenges in federated learning involving self-interested agents and heterogeneous data contributions. They introduce an incentive-aware framework that considers data heterogeneity to improve convergence. By utilizing the Wasserstein distance, they illustrate the heterogeneous effort and reformulate the convergence bound. To ensure truthful reporting, they measure the generalization error gap between agents using a peer prediction mechanism. A two-stage Stackelberg game model is proposed to formalize the process and examine equilibrium existence. Extensive experiments on real-world datasets validate the effectiveness of their proposed mechanism. <div>
arXiv:2506.16731v1 Announce Type: new 
Abstract: Federated learning (FL) provides a promising paradigm for facilitating collaboration between multiple clients that jointly learn a global model without directly sharing their local data. However, existing research suffers from two caveats: 1) From the perspective of agents, voluntary and unselfish participation is often assumed. But self-interested agents may opt out of the system or provide low-quality contributions without proper incentives; 2) From the mechanism designer's perspective, the aggregated models can be unsatisfactory as the existing game-theoretical federated learning approach for data collection ignores the potential heterogeneous effort caused by contributed data. To alleviate above challenges, we propose an incentive-aware framework for agent participation that considers data heterogeneity to accelerate the convergence process. Specifically, we first introduce the notion of Wasserstein distance to explicitly illustrate the heterogeneous effort and reformulate the existing upper bound of convergence. To induce truthful reporting from agents, we analyze and measure the generalization error gap of any two agents by leveraging the peer prediction mechanism to develop score functions. We further present a two-stage Stackelberg game model that formalizes the process and examines the existence of equilibrium. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed mechanism.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers</title>
<link>https://arxiv.org/abs/2506.16764</link>
<guid>https://arxiv.org/abs/2506.16764</guid>
<content:encoded><![CDATA[
<div> Keywords: vehicle electrification, charging infrastructure, mobile chargers, optimization, deep reinforcement learning  
Summary:  
The paper introduces the concept of a Hybrid Charging Station Planning and Operation (HCSPO) problem, aiming to optimize the planning and operation of hybrid charging infrastructures within urban road networks. It addresses the challenges of underutilization and congestion faced by traditional fixed-location charging stations by incorporating mobile chargers as a flexible solution. The approach integrates a charging demand prediction model based on Model Predictive Control (MPC) to enhance decision-making. A deep reinforcement learning method, augmented with heuristic scheduling techniques, is proposed to solve the HCSPO problem effectively. Extensive case studies using real-world urban scenarios demonstrate the method's ability to improve the availability of charging infrastructure and reduce user inconvenience compared to existing solutions and baselines. <div>
arXiv:2506.16764v1 Announce Type: new 
Abstract: The success of vehicle electrification, which brings significant societal and environmental benefits, is contingent upon the availability of efficient and adaptable charging infrastructure. Traditional fixed-location charging stations often face issues like underutilization or congestion due to the dynamic nature of charging demand. Mobile chargers have emerged as a flexible solution, capable of relocating to align with these demand fluctuations. This paper addresses the optimal planning and operation of hybrid charging infrastructures, integrating both fixed and mobile chargers within urban road networks. We introduce the Hybrid Charging Station Planning and Operation (HCSPO) problem, which simultaneously optimizes the location and configuration of fixed charging stations and schedules mobile chargers for dynamic operations. Our approach incorporates a charging demand prediction model grounded in Model Predictive Control (MPC) to enhance decision-making. To solve the HCSPO problem, we propose a deep reinforcement learning method, augmented with heuristic scheduling techniques, to effectively bridge the planning of fixed chargers with the real-time operation of mobile chargers. Extensive case studies using real-world urban scenarios demonstrate that our method significantly improves the availability of charging infrastructure and reduces user inconvenience compared to existing solutions and baselines.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario</title>
<link>https://arxiv.org/abs/2506.16898</link>
<guid>https://arxiv.org/abs/2506.16898</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, urban analysis, geographic knowledge, biases, USA geography

Summary: 
- The study explores the use of advanced image generation models in urban analysis and design, focusing on their geographic knowledge and biases.
- 150 synthetic images were generated for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5 models.
- Images were embedded using DINO-v2 ViT-S/14 and the Fr\'echet Inception Distances were used to measure similarity between generated images.
- The models showed implicit knowledge of USA geography but exhibited a bias towards metropolis-like areas when generating images for "United States" as opposed to specific cities or states.
- Models also showed entity-disambiguation issues with European-sounding names like Frankfort or Devon.

<br><br>Summary: Image generation models are used in urban analysis, showing implicit knowledge of USA geography but bias towards metropolis-like areas. Models generate synthetic images for states and capitals, measuring similarity using a specific approach. Entity-disambiguation issues occur with European-sounding names, affecting the accuracy of generated images. <div>
arXiv:2506.16898v1 Announce Type: new 
Abstract: Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art models for image generation. We embed each image using DINO-v2 ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity between the generated images. We found that while these models have implicitly learned aspects of USA geography, if we prompt the models to generate an image for "United States" instead of specific cities or states, the models exhibit a strong representative bias toward metropolis-like areas, excluding rural states and smaller cities. {\color{black} In addition, we found that models systematically exhibit some entity-disambiguation issues with European-sounding names like Frankfort or Devon.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines</title>
<link>https://arxiv.org/abs/2506.16924</link>
<guid>https://arxiv.org/abs/2506.16924</guid>
<content:encoded><![CDATA[
<div> Keywords: Black-box optimization, Multi-armed bandit, Ising machine, Dynamic environments, Wireless communication system

Summary:
Black-box optimization (BBO) and multi-armed bandit (MAB) algorithms are used to optimize discrete variables in real-time systems without prior knowledge. A BBO method using an Ising machine was proposed for static environments, while MAB algorithms are needed for dynamic environments to maximize average rewards over multiple trials. Conventional MAB algorithms struggle with the combinatorial nature of discrete optimization in dynamic environments. A heuristic MAB method is introduced, extending the BBO method to address this challenge by utilizing an Ising machine to explore actions effectively, considering interactions between variables and changes in dynamic environments. The proposed method's dynamic adaptability is demonstrated in a wireless communication system with moving users.<br><br>Summary: Black-box optimization and multi-armed bandit algorithms are crucial for optimizing discrete variables in real-time systems. A novel approach combining both methods using an Ising machine has been developed to address dynamic environments. The method effectively explores actions, considering variable interactions and changes in real-time scenarios, as demonstrated in a wireless communication system with mobile users. <div>
arXiv:2506.16924v1 Announce Type: new 
Abstract: Many real-time systems require the optimization of discrete variables. Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms perform optimization by repeatedly taking actions and observing the corresponding instant rewards without any prior knowledge. Recently, a BBO method using an Ising machine has been proposed to find the best action that is represented by a combination of discrete values and maximizes the instant reward in static environments. In contrast, dynamic environments, where real-time systems operate, necessitate MAB algorithms that maximize the average reward over multiple trials. However, due to the enormous number of actions resulting from the combinatorial nature of discrete optimization, conventional MAB algorithms cannot effectively optimize dynamic, discrete environments. Here, we show a heuristic MAB method for dynamic, discrete environments by extending the BBO method, in which an Ising machine effectively explores the actions while considering interactions between variables and changes in dynamic environments. We demonstrate the dynamic adaptability of the proposed method in a wireless communication system with moving users.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning</title>
<link>https://arxiv.org/abs/2506.16931</link>
<guid>https://arxiv.org/abs/2506.16931</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile robots, task planning, Generalized Traveling Salesman Problem, Multimodal Fused Learning, real-time

Summary: 
The article introduces a Multimodal Fused Learning (MMFL) framework for efficient task planning in mobile robots. By leveraging graph and image-based representations, the MMFL framework addresses the Generalized Traveling Salesman Problem (GTSP) commonly encountered in applications like warehouse retrieval and environmental monitoring. A coordinate-based image builder transforms GTSP instances into spatially informative representations, while an adaptive resolution scaling strategy enhances adaptability across problem scales. The multimodal fusion module integrates geometric and spatial features to generate high-quality task planning schemes in real time. Extensive experiments demonstrate the superiority of the MMFL approach over state-of-the-art methods in various GTSP instances, maintaining computational efficiency for robotic applications. Physical robot tests confirm the practical effectiveness of the MMFL framework in real-world scenarios. <br><br>Summary: <div>
arXiv:2506.16931v1 Announce Type: new 
Abstract: Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elevating Styled Mahjong Agents with Learning from Demonstration</title>
<link>https://arxiv.org/abs/2506.16995</link>
<guid>https://arxiv.org/abs/2506.16995</guid>
<content:encoded><![CDATA[
<div> Mahjong, bots, game artificial intelligence, Proximal Policy Optimization, Learning-from-Demonstration 

Summary: 
This paper discusses the development of bots with distinct play styles in the Mahjong game environment. The authors propose a novel Learning-from-Demonstration algorithm that builds on existing Mahjong agent gameplay histories to enhance bot proficiency and preserve unique play styles. The algorithm requires minimal modifications to the Proximal Policy Optimization algorithm and addresses challenges posed by the game's randomness and out-of-distribution states. Empirical results demonstrate significant improvements in bot performance and the retention of diverse play styles. The study emphasizes the importance of considering a wide range of play styles when developing game bots and highlights the potential of leveraging existing gameplay data in training algorithms. <div>
arXiv:2506.16995v1 Announce Type: new 
Abstract: A wide variety of bots in games enriches the gameplay experience and enhances replayability. Recent advancements in game artificial intelligence have predominantly focused on improving the proficiency of bots. Nevertheless, developing highly competent bots with a wide range of distinct play styles remains a relatively under-explored area. We select the Mahjong game environment as a case study. The high degree of randomness inherent in the Mahjong game and the prevalence of out-of-distribution states lead to suboptimal performance of existing offline learning and Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the gameplay histories of existing Mahjong agents and put forward a novel LfD algorithm that necessitates only minimal modifications to the Proximal Policy Optimization algorithm. The comprehensive empirical results illustrate that our proposed method not only significantly enhances the proficiency of the agents but also effectively preserves their unique play styles.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models</title>
<link>https://arxiv.org/abs/2506.17018</link>
<guid>https://arxiv.org/abs/2506.17018</guid>
<content:encoded><![CDATA[
<div> state space models, predictive maintenance, remaining useful life, quantile regression, industrial applications 

Summary:
The paper introduces a novel approach for estimating Remaining Useful Life (RUL) in predictive maintenance using State Space Models (SSM) and Simoultaneous Quantile Regression (SQR). This method aims to improve efficiency in Industry 4.0 and 5.0 by accurately predicting equipment RUL, optimizing maintenance schedules, and reducing unexpected failures. The study benchmarks the proposed approach against traditional techniques like LSTM, Transformer, and Informer using the C-MAPSS dataset. Results show that SSM models offer superior accuracy and computational efficiency, highlighting their potential for high-stakes industrial applications. <div>
arXiv:2506.17018v1 Announce Type: new 
Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively enhancing efficiency through accurate equipment Remaining Useful Life (RUL) prediction, thus optimizing maintenance scheduling and reducing unexpected failures and premature interventions. This paper introduces a novel RUL estimation approach leveraging State Space Models (SSM) for efficient long-term sequence modeling. To handle model uncertainty, Simoultaneous Quantile Regression (SQR) is integrated into the SSM, enabling multiple quantile estimations. The proposed method is benchmarked against traditional sequence modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset. Results demonstrate superior accuracy and computational efficiency of SSM models, underscoring their potential for high-stakes industrial applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dispositions and Roles of Generically Dependent Entities</title>
<link>https://arxiv.org/abs/2506.17085</link>
<guid>https://arxiv.org/abs/2506.17085</guid>
<content:encoded><![CDATA[
<div> functions, dispositions, roles, generically dependent continuants, BFO 2020 <br>
Summary: <br>
The paper highlights a limitation in BFO 2020, which does not support functions, dispositions, and roles of generically dependent continuants like software or datasets. This limitation hinders the representation of entities' functions, such as computer models, and the diverse roles of datasets during their execution. The discussion focuses on the aspects of BFO 2020 that hinder the representation of realizable entities of generically dependent continuants. Two proposed approaches to address the issue are outlined: (a) the utilization of defined classes and (b) a proposal for changes to enable BFO to accommodate functions, dispositions, and roles of generically dependent continuants. This addresses the significant limitation in BFO 2020 and suggests enhancements for a more comprehensive representation of entities in the domain of generically dependent continuants. <br> <div>
arXiv:2506.17085v1 Announce Type: new 
Abstract: BFO 2020 does not support functions, dispositions, and roles of generically dependent continuants (like software or datasets). In this paper, we argue that this is a severe limitation, which prevents, for example, the adequate representation of the functions of computer models or the various roles of datasets during the execution of these models. We discuss the aspects of BFO 2020 that prevent the representation of realizable entities of generically dependent continuants. Two approaches to address the issue are presented: (a) the use of defined classes and (b) a proposal of changes that allow BFO to support functions, dispositions, and roles of generically dependent continuants.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving</title>
<link>https://arxiv.org/abs/2506.17104</link>
<guid>https://arxiv.org/abs/2506.17104</guid>
<content:encoded><![CDATA[
<div> mathematical reasoning, large language models, first-order logic, theorem proving, DREAM<br>
<br>
Summary:<br>
- The study explores the effectiveness of Large Language Models (LLMs) in complex mathematical reasoning involving multi-step First-Order Logic (FOL) deductions, highlighting limitations in current models.<br>
- A new solution called DREAM is proposed to enhance LLMs' reasoning capabilities by incorporating an Axiom-Driven Strategy Diversification mechanism and Sub-Proposition Error Feedback.<br>
- DREAM improves performance on mathematical reasoning tasks by 0.6% to 6.4%, addressing issues related to proof strategy diversity and early reasoning mistakes.<br>
- The study contributes to advancements in LLMs' mathematical reasoning through FOL theorem proving, showcasing the potential for improved performance in multi-step FOL tasks.<br>
- Additionally, a curated dataset of 447 mathematical theorems in Lean 4 format is provided for evaluation purposes. <br> <div>
arXiv:2506.17104v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas. However, their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-researched. While LLMs perform competitively on established mathematical reasoning benchmarks, they struggle with multi-step FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on our proposed theorem proving dataset. This issue arises from the limited exploration of diverse proof strategies and the potential for early reasoning mistakes to undermine entire proofs. To address these issues, we propose DREAM, a self-adaptive solution that enhances the Diversity and REAsonability of LLMs' generation strategies. DREAM incorporates an Axiom-Driven Strategy Diversification mechanism to promote varied strategic outcomes and a Sub-Proposition Error Feedback to help LLMs reflect on and correct their proofs. Our contributions include pioneering advancements in LLMs' mathematical reasoning through FOL theorem proving, introducing a novel inference stage solution that improves performance by 0.6% to 6.4%, and providing a curated dataset of 447 mathematical theorems in Lean 4 format for evaluation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Bias Evaluation Methods Biased ?</title>
<link>https://arxiv.org/abs/2506.17111</link>
<guid>https://arxiv.org/abs/2506.17111</guid>
<content:encoded><![CDATA[
<div> Benchmarks, Large Language Models, Safety, Bias Evaluation, Model Rankings  
Summary: 
The article discusses the importance of benchmarks in assessing the safety of Large Language Models. These benchmarks help in comparing models for various safety aspects such as toxicity, bias, and harmful behavior. The study evaluates the robustness of these benchmarks by ranking different models for bias using various approaches. It is found that popular bias evaluation methods yield different model rankings, indicating inconsistencies in the assessments. The authors suggest recommendations for the community on how to effectively use such benchmarks to ensure accurate and reliable evaluations. <div>
arXiv:2506.17111v1 Announce Type: new 
Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.17114</link>
<guid>https://arxiv.org/abs/2506.17114</guid>
<content:encoded><![CDATA[
<div> mathematical reasoning, large reasoning models, RFMDataset, error types, logical training 
Summary: 
- Large reasoning models have shown strong mathematical problem-solving abilities but can have hidden failures when evaluated using mathematical proofs.
- The RFMDataset was created to evaluate advanced models' performance on 200 diverse mathematical proof problems.
- An analysis of their failures revealed 10 fine-grained error types, showing fundamental limitations in current large reasoning models.
- These models struggle with mathematical proofs, often generating incorrect proofs and failing on basic problems.
- Reasoning failures in the models showcase a lack of guarantees for correctness and rigor, along with issues like hallucination and incompleteness.
- The findings suggest that models need formalized and fine-grained logical training to address their logical dilemmas.<br><br>Summary: <div>
arXiv:2506.17114v1 Announce Type: new 
Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Can Model-Free Reinforcement Learning be Enough for Thinking?</title>
<link>https://arxiv.org/abs/2506.17124</link>
<guid>https://arxiv.org/abs/2506.17124</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, thought MDP, policy initialization, data-efficient RL

Summary:

Model-free reinforcement learning has been utilized in training large language models to develop reasoning capabilities. A theoretical model called a thought Markov decision process (MDP) is introduced to understand when model-free RL leads to "thinking" as a strategy for maximizing rewards. The concept of thought actions is explored, showing that they are equivalent to policy improvement steps. Policy initialization plays a crucial role in determining the emergence of thinking behavior. The study indicates that open-source LLMs meet the conditions for model-free RL to generate thinking-like behavior. The article also proposes conditions for enabling learning of thinking outside of language generation tasks. A toy domain is introduced to demonstrate how a combination of multi-task pre-training and designated thought actions can lead to more data-efficient RL compared to non-thinking agents. <div>
arXiv:2506.17124v1 Announce Type: new 
Abstract: Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of "thinking" through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to "thinking" as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a \textit{thought Markov decision process} (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI</title>
<link>https://arxiv.org/abs/2506.17130</link>
<guid>https://arxiv.org/abs/2506.17130</guid>
<content:encoded><![CDATA[
<div> trust evaluation, collaborative systems, distributed resources, chain-of-trust, generative AI

Summary:
The article introduces a novel framework called chain-of-trust for trust evaluation in collaborative systems with distributed resources and complex tasks. This framework breaks down the trust evaluation process into multiple stages based on task decomposition, allowing for the gathering of relevant device attribute data only at each stage. By utilizing generative AI and advanced learning techniques, the framework quickly analyzes the collected data to determine trustworthy devices, which then proceed to the next evaluation round. The framework aims to identify devices that consistently demonstrate trustworthiness across all stages, leading to high accuracy in trust evaluation. Experimental results validate the effectiveness of the proposed framework in achieving accurate trust assessments in collaborative systems with varying network dynamics and information gathering latencies. 

<br><br>Summary: <div>
arXiv:2506.17130v1 Announce Type: new 
Abstract: In collaborative systems with complex tasks relying on distributed resources, trust evaluation of potential collaborators has emerged as an effective mechanism for task completion. However, due to the network dynamics and varying information gathering latencies, it is extremely challenging to observe and collect all trust attributes of a collaborating device concurrently for a comprehensive trust assessment. In this paper, a novel progressive trust evaluation framework, namely chain-of-trust, is proposed to make better use of misaligned device attribute data. This framework, designed for effective task completion, divides the trust evaluation process into multiple chained stages based on task decomposition. At each stage, based on the task completion process, the framework only gathers the latest device attribute data relevant to that stage, leading to reduced trust evaluation complexity and overhead. By leveraging advanced in-context learning, few-shot learning, and reasoning capabilities, generative AI is then employed to analyze and interpret the collected data to produce correct evaluation results quickly. Only devices deemed trustworthy at this stage proceed to the next round of trust evaluation. The framework ultimately determines devices that remain trustworthy across all stages. Experimental results demonstrate that the proposed framework achieves high accuracy in trust evaluation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making</title>
<link>https://arxiv.org/abs/2506.17163</link>
<guid>https://arxiv.org/abs/2506.17163</guid>
<content:encoded><![CDATA[
<div> gender, style, format, medical LLMs, clinical variability

Summary:
The article introduces the MedPerturb dataset, which evaluates medical Large Language Models (LLMs) under controlled perturbations of clinical input to assess their clinical robustness. The dataset includes 800 clinical vignettes with gender modifications, style variations, and format changes. Case studies using MedPerturb show that LLMs are more sensitive to gender and style perturbations, while humans are more sensitive to format perturbations like clinical summaries. The findings highlight the importance of evaluating LLMs in realistic clinical settings to understand how they differ from human decision-making. The study suggests the need for evaluation frameworks that consider the variability of clinical scenarios to assess the similarity between LLM and human clinician decisions. <div>
arXiv:2506.17163v1 Announce Type: new 
Abstract: Clinical robustness is critical to the safe deployment of medical Large Language Models (LLMs), but key questions remain about how LLMs and humans may differ in response to the real-world variability typified by clinical settings. To address this, we introduce MedPerturb, a dataset designed to systematically evaluate medical LLMs under controlled perturbations of clinical input. MedPerturb consists of clinical vignettes spanning a range of pathologies, each transformed along three axes: (1) gender modifications (e.g., gender-swapping or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or summaries). With MedPerturb, we release a dataset of 800 clinical contexts grounded in realistic input variability, outputs from four LLMs, and three human expert reads per clinical context. We use MedPerturb in two case studies to reveal how shifts in gender identity cues, language style, or format reflect diverging treatment selections between humans and LLMs. We find that LLMs are more sensitive to gender and style perturbations while human annotators are more sensitive to LLM-generated format perturbations such as clinical summaries. Our results highlight the need for evaluation frameworks that go beyond static benchmarks to assess the similarity between human clinician and LLM decisions under the variability characteristic of clinical settings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree</title>
<link>https://arxiv.org/abs/2506.15655</link>
<guid>https://arxiv.org/abs/2506.15655</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, chunking, Abstract Syntax Trees, code corpora, code generation

Summary:
Structured Abstract Syntax Trees (SAT) is proposed as a method to improve chunking in Retrieval-Augmented Generation (RAG) pipelines. Current line-based chunking methods often disrupt semantic structures in code documents, leading to decreased generation quality. SAT recursively divides large Abstract Syntax Tree (AST) nodes into smaller chunks while maintaining semantic coherence and respecting size limits. This approach enhances performance on various code generation tasks, such as increasing Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. The study emphasizes the significance of structure-aware chunking in optimizing retrieval-enhanced code intelligence. 

<br><br>Summary:Structured Abstract Syntax Trees (SAT) improve chunking in RAG pipelines, preserving semantic coherence and respecting size limits. The method boosts performance on code generation tasks, enhancing Recall@5 and Pass@1 points. SAT highlights the importance of structure-aware chunking in scaling retrieval-enhanced code intelligence. <div>
arXiv:2506.15655v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ignition Phase : Standard Training for Fast Adversarial Robustness</title>
<link>https://arxiv.org/abs/2506.15685</link>
<guid>https://arxiv.org/abs/2506.15685</guid>
<content:encoded><![CDATA[
<div> Adversarial Evolution Training, Adversarial Training, Robust Defense, Feature Representations, Empirical Risk Minimization<br>
Summary:<br>
Adversarial Training (AT) is a commonly used defense mechanism, but it often neglects foundational feature representations. A new approach, Adversarial Evolution Training (AET), adds an initial Empirical Risk Minimization (ERM) phase to AT, focusing on cultivating a more favorable feature manifold. AET has shown to achieve comparable or superior robustness more rapidly than traditional AT methods, improving clean accuracy and reducing training costs by 8-25%. The effectiveness of AET has been demonstrated across various datasets and architectures, as well as when enhancing existing AT techniques. This highlights the importance of feature pre-conditioning through standard training for developing more efficient and principled robust defenses. Code for AET is available in the supplementary material. <br> <div>
arXiv:2506.15685v1 Announce Type: cross 
Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants overlook foundational feature representations by primarily focusing on stronger attack generation. We introduce Adversarial Evolution Training (AET), a simple yet powerful framework that strategically prepends an Empirical Risk Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM phase cultivates a favorable feature manifold, enabling more efficient and effective robustness acquisition. Empirically, AET achieves comparable or superior robustness more rapidly, improves clean accuracy, and cuts training costs by 8-25\%. Its effectiveness is shown across multiple datasets, architectures, and when augmenting established AT methods. Our findings underscore the impact of feature pre-conditioning via standard training for developing more efficient, principled robust defenses. Code is available in the supplementary material.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from M-Tuple Dominant Positive and Unlabeled Data</title>
<link>https://arxiv.org/abs/2506.15686</link>
<guid>https://arxiv.org/abs/2506.15686</guid>
<content:encoded><![CDATA[
<div> Keywords: Label Proportion Learning, MDPU, Risk Consistency, Risk Correction, Generalization Error Bounds

Summary:
Label Proportion Learning (LLP) deals with classification where instances are grouped into bags with class proportions. This paper introduces the MDPU framework to address the challenge of obtaining precise class proportion information. By modeling the distribution of instances within tuples and ensuring a balance of positive and negative instances, MDPU aims to effectively leverage proportional constraints in real-world applications. With an unbiased risk estimator and risk correction method, the framework aims to mitigate overfitting during training. The proposed method demonstrates risk consistency and generalization error bounds, validating its effectiveness through experiments on multiple datasets. Overall, MDPU provides a robust approach to classification under proportional constraints, with promising results in practical scenarios.<br><br>Summary: <div>
arXiv:2506.15686v1 Announce Type: cross 
Abstract: Label Proportion Learning (LLP) addresses the classification problem where multiple instances are grouped into bags and each bag contains information about the proportion of each class. However, in practical applications, obtaining precise supervisory information regarding the proportion of instances in a specific class is challenging. To better align with real-world application scenarios and effectively leverage the proportional constraints of instances within tuples, this paper proposes a generalized learning framework \emph{MDPU}. Specifically, we first mathematically model the distribution of instances within tuples of arbitrary size, under the constraint that the number of positive instances is no less than that of negative instances. Then we derive an unbiased risk estimator that satisfies risk consistency based on the empirical risk minimization (ERM) method. To mitigate the inevitable overfitting issue during training, a risk correction method is introduced, leading to the development of a corrected risk estimator. The generalization error bounds of the unbiased risk estimator theoretically demonstrate the consistency of the proposed method. Extensive experiments on multiple datasets and comparisons with other relevant baseline methods comprehensively validate the effectiveness of the proposed learning framework.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism</title>
<link>https://arxiv.org/abs/2506.15688</link>
<guid>https://arxiv.org/abs/2506.15688</guid>
<content:encoded><![CDATA[
<div> Keywords: cellular traffic prediction, convolutional neural networks, spatial dynamics, attention mechanism, Kalman filter

Summary:
This paper presents an end-to-end framework for cellular traffic prediction, addressing the challenge of dynamic and complex traffic patterns. The framework utilizes convolutional neural networks with an attention mechanism to capture spatial dynamics among neighboring cells and incorporates Kalman filter for temporal modelling. Additionally, auxiliary information like social activities is leveraged to enhance prediction accuracy. Extensive experiments conducted on real-world datasets demonstrate the superiority of the proposed models over existing machine learning techniques in terms of prediction accuracy. By explicitly characterizing spatiotemporal patterns and leveraging auxiliary data, the framework showcases significant improvements in forecasting cellular traffic, providing operators with valuable insights for efficient network management and decision-making. <br><br>Summary: <div>
arXiv:2506.15688v1 Announce Type: cross 
Abstract: Cellular traffic prediction is of great importance for operators to manage network resources and make decisions. Traffic is highly dynamic and influenced by many exogenous factors, which would lead to the degradation of traffic prediction accuracy. This paper proposes an end-to-end framework with two variants to explicitly characterize the spatiotemporal patterns of cellular traffic among neighboring cells. It uses convolutional neural networks with an attention mechanism to capture the spatial dynamics and Kalman filter for temporal modelling. Besides, we can fully exploit the auxiliary information such as social activities to improve prediction performance. We conduct extensive experiments on three real-world datasets. The results show that our proposed models outperform the state-of-the-art machine learning techniques in terms of prediction accuracy.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2506.15689</link>
<guid>https://arxiv.org/abs/2506.15689</guid>
<content:encoded><![CDATA[
<div> Keywords: Rotational quantization, Large language models, Bias correction, Asymmetric scaling, Blockwise optimization

Summary:
BASE-Q is a new approach for quantization in large language models (LLMs) that addresses key limitations of current rotational quantization methods. By incorporating bias correction and asymmetric scaling, BASE-Q reduces rounding and clipping errors that can affect model performance. Additionally, BASE-Q allows for blockwise optimization, eliminating the need for memory-intensive full-model backpropagation during training. Experimental results show that BASE-Q significantly narrows the accuracy gap between quantized and full-precision models, outperforming existing methods such as QuaRot, SpinQuant, and OSTQuant by substantial margins. The code for BASE-Q will be made available soon, making it a promising option for improving the efficiency and performance of quantized LLMs. 

<br><br>Summary: <div>
arXiv:2506.15689v1 Announce Type: cross 
Abstract: Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs</title>
<link>https://arxiv.org/abs/2506.15690</link>
<guid>https://arxiv.org/abs/2506.15690</guid>
<content:encoded><![CDATA[
<div> framework, model collapse, language model, synthetic data, convergence pattern
Summary:<br>
- The article presents the LLM Web Dynamics (LWD) framework for studying model collapse in large language models trained on synthetic data from the Internet. 
- The framework utilizes a retrieval-augmented generation (RAG) database to simulate Internet data and analyze the convergence of model outputs at a network level. 
- The authors provide theoretical guarantees for this convergence using an analogy to interacting Gaussian Mixture Models. 
- This approach offers a more comprehensive understanding of model collapse compared to existing studies that focus on individual models or statistical surrogates. 
- By exploring the convergence patterns of language models trained on synthetic Internet data, the LWD framework enhances the efficiency and effectiveness of large language model training. 
Summary: <div>
arXiv:2506.15690v1 Announce Type: cross 
Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Latent Action Models Actually Learn?</title>
<link>https://arxiv.org/abs/2506.15691</link>
<guid>https://arxiv.org/abs/2506.15691</guid>
<content:encoded><![CDATA[
<div> Keywords: Latent action models, Unlabeled videos, Controllable changes, Exogenous noise, Data augmentation

Summary: This paper explores the effectiveness of latent action models (LAMs) in capturing action-relevant changes in unlabeled videos. By presenting a linear model that mimics LAM learning, the study uncovers important insights such as the relationship between LAM and principal component analysis (PCA). The analysis highlights the significance of differentiating between controllable changes and irrelevant noise in video frame differences. Strategies like data augmentation, data cleaning, and auxiliary action-prediction are recommended to enhance the learning of controllable changes by LAMs. The study also includes illustrative results from numerical simulations that provide a clearer understanding of the structure of observations, actions, and noise in data that impact LAM learning.<br><br>Summary:Keywords: Latent action models, Unlabeled videos, Controllable changes, Exogenous noise, Data augmentation <div>
arXiv:2506.15691v1 Announce Type: cross 
Abstract: Latent action models (LAMs) aim to learn action-relevant changes from unlabeled videos by compressing changes between frames as latents. However, differences between video frames can be caused by controllable changes as well as exogenous noise, leading to an important concern -- do latents capture the changes caused by actions or irrelevant noise? This paper studies this issue analytically, presenting a linear model that encapsulates the essence of LAM learning, while being tractable.This provides several insights, including connections between LAM and principal component analysis (PCA), desiderata of the data-generating policy, and justification of strategies to encourage learning controllable changes using data augmentation, data cleaning, and auxiliary action-prediction. We also provide illustrative results based on numerical simulation, shedding light on the specific structure of observations, actions, and noise in data that influence LAM learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap</title>
<link>https://arxiv.org/abs/2506.15699</link>
<guid>https://arxiv.org/abs/2506.15699</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, large language models, benchmark, BLUR, evaluation<br>
<br>
Summary:<br>
The article introduces a benchmark called BLUR for evaluating machine unlearning in large language models (LLMs). The aim of unlearning is to remove sensitive or harmful information from LLMs post hoc. Current unlearning benchmarks show a disparity between forget and retain sets, making it challenging to assess the effectiveness of unlearning methods. BLUR provides more realistic scenarios with extended evaluation tasks, combined forget/retain queries, and varying difficulty relearning datasets. Despite benign queries, existing unlearning methods perform poorly on BLUR, with simpler approaches outperforming recent methods. The results emphasize the importance of robust evaluation in unlearning and point towards future research directions. The BLUR benchmark is publicly available for further study. <div>
arXiv:2506.15699v1 Announce Type: cross 
Abstract: Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present $\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on $\texttt{BLUR}$, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: https://huggingface.co/datasets/forgelab/BLUR
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking</title>
<link>https://arxiv.org/abs/2506.15700</link>
<guid>https://arxiv.org/abs/2506.15700</guid>
<content:encoded><![CDATA[
<div> Control contraction metrics, CCMs, synthesis controller, contraction metric, incrementally exponentially stable<br>
<br>
Summary: <br>
Control contraction metrics (CCMs) provide a framework for synthesizing a controller and contraction metric ensuring incremental exponential stability. However, they lack optimality across trajectories and scalability for complex systems. To address this, the integration of CCMs into reinforcement learning (RL) is proposed. The contraction actor-critic (CAC) algorithm combines CCMs with RL to learn control policies minimizing tracking error under unknown dynamics. CAC simultaneously learns a contraction metric generator (CMG) and an optimal tracking policy using an actor-critic algorithm. Through empirical studies in simulated and real-world robot experiments, CAC shows effectiveness compared to baselines. Theoretical rationale is provided for incorporating contraction theory into RL. <div>
arXiv:2506.15700v1 Announce Type: cross 
Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a controller and a corresponding contraction metric -- a positive-definite Riemannian metric under which a closed-loop system is guaranteed to be incrementally exponentially stable. However, the synthesized controller only ensures that all the trajectories of the system converge to one single trajectory and, as such, does not impose any notion of optimality across an entire trajectory. Furthermore, constructing CCMs requires a known dynamics model and non-trivial effort in solving an infinite-dimensional convex feasibility problem, which limits its scalability to complex systems featuring high dimensionality with uncertainty. To address these issues, we propose to integrate CCMs into reinforcement learning (RL), where CCMs provide dynamics-informed feedback for learning control policies that minimize cumulative tracking error under unknown dynamics. We show that our algorithm, called contraction actor-critic (CAC), formally enhances the capability of CCMs to provide a set of contracting policies with the long-term optimality of RL in a fully automated setting. Given a pre-trained dynamics model, CAC simultaneously learns a contraction metric generator (CMG) -- which generates a contraction metric -- and uses an actor-critic algorithm to learn an optimal tracking policy guided by that metric. We demonstrate the effectiveness of our algorithm relative to established baselines through extensive empirical studies, including simulated and real-world robot experiments, and provide a theoretical rationale for incorporating contraction theory into RL.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.15701</link>
<guid>https://arxiv.org/abs/2506.15701</guid>
<content:encoded><![CDATA[

arXiv:2506.15701v1 Announce Type: cross 
Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at https://github.com/Panhaolin2001/Compiler-R1.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation</title>
<link>https://arxiv.org/abs/2506.15702</link>
<guid>https://arxiv.org/abs/2506.15702</guid>
<content:encoded><![CDATA[

arXiv:2506.15702v1 Announce Type: cross 
Abstract: Finetuning language models for a new domain inevitably leads to the deterioration of their general performance. This becomes more pronounced the more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain adaptation that considerably reduces the effects of overfitting-induced degeneralization in low-data settings and which does so in the absence of any pre-training data for replay. MFT demonstrates 2-10x more favourable specialization-to-degeneralization ratios than standard finetuning across a wide range of models and domains and exhibits an intrinsic robustness to overfitting when data in the new domain is scarce and down to as little as 500 samples.
  Employing corrective self-distillation that is individualized on the sample level, MFT outperforms parameter-efficient finetuning methods, demonstrates replay-like degeneralization mitigation properties, and is composable with either for a combined effect.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance</title>
<link>https://arxiv.org/abs/2506.15703</link>
<guid>https://arxiv.org/abs/2506.15703</guid>
<content:encoded><![CDATA[

arXiv:2506.15703v1 Announce Type: cross 
Abstract: Federated multi-view clustering has been proposed to mine the valuable information within multi-view data distributed across different devices and has achieved impressive results while preserving the privacy. Despite great progress, most federated multi-view clustering methods only used global pseudo-labels to guide the downstream clustering process and failed to exploit the global information when extracting features. In addition, missing data problem in federated multi-view clustering task is less explored. To address these problems, we propose a novel Federated Incomplete Multi-view Clustering method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a dual-head graph convolutional encoder at each client to extract two kinds of underlying features containing global and view-specific information. Subsequently, under the guidance of the fused graph, the two underlying features are fused into high-level features, based on which clustering is conducted under the supervision of pseudo-labeling. Finally, the high-level features are uploaded to the server to refine the graph fusion and pseudo-labeling computation. Extensive experimental results demonstrate the effectiveness and superiority of FIMCFG. Our code is publicly available at https://github.com/PaddiHunter/FIMCFG.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding</title>
<link>https://arxiv.org/abs/2506.15704</link>
<guid>https://arxiv.org/abs/2506.15704</guid>
<content:encoded><![CDATA[

arXiv:2506.15704v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$\times$ speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2506.15705</link>
<guid>https://arxiv.org/abs/2506.15705</guid>
<content:encoded><![CDATA[

arXiv:2506.15705v1 Announce Type: cross 
Abstract: This study investigates zero-shot forecasting capabilities of Time Series Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to forecasting economic indicators under univariate conditions, bypassing the need for train bespoke econometric models using and extensive training datasets. Our experiments were conducted on a case study dataset, without additional customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos, TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our results demonstrate that appropriately engineered TSFMs can internalise rich economic dynamics, accommodate regime shifts, and deliver well-behaved uncertainty estimates out of the box, while matching state-of-the-art multivariate models on this domain. Our findings suggest that, without any fine-tuning, TSFMs can match or exceed classical models during stable economic conditions. However, they are vulnerable to degradation in performances during periods of rapid shocks. The findings offer guidance to practitioners on when zero-shot deployments are viable for macroeconomic monitoring and strategic planning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2506.15706</link>
<guid>https://arxiv.org/abs/2506.15706</guid>
<content:encoded><![CDATA[

arXiv:2506.15706v1 Announce Type: cross 
Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) as it requires ensuring the correctness of each reasoning step. Researchers have been strengthening the mathematical reasoning abilities of LLMs through supervised fine-tuning, but due to the inability to suppress incorrect outputs, illusions can easily arise. Recently, Direct Preference Optimization (DPO) has been widely adopted for aligning human intent by using preference data to prevent LLMs from generating incorrect outputs. However, it has shown limited benefits in long-chain mathematical reasoning, mainly because DPO struggles to effectively capture the differences between accepted and rejected answers from preferences in long-chain data. The inconsistency between DPO training and LLMs' generation metrics also affects the effectiveness of suppressing incorrect outputs. We propose the Multi-Granularity Direct Preference Optimization (MDPO) method, optimizing the mathematical reasoning of LLMs at three granularities: Solution2Solution, Inference2Inference, and Step2Step. Solution2Solution focuses on the correctness of entire long-chain reasoning; Inference2Inference concentrates on logical reasoning between steps; Step2Step corrects computational errors in steps, enhancing the computational capabilities of LLMs. Additionally, we unify the training objectives of the three granularities to align with the generation metrics. We conducted experiments on the open-source models Qwen2 and Llama3, achieving improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and other DPO variant methods. Furthermore, we also provide a pipeline for constructing MDPO training data that is simple and does not require manual annotation costs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2506.15707</link>
<guid>https://arxiv.org/abs/2506.15707</guid>
<content:encoded><![CDATA[

arXiv:2506.15707v1 Announce Type: cross 
Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA's effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy. We hope our findings contribute to a broader understanding of optimal TTS for LLMs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification</title>
<link>https://arxiv.org/abs/2506.15708</link>
<guid>https://arxiv.org/abs/2506.15708</guid>
<content:encoded><![CDATA[

arXiv:2506.15708v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) have been developed to model the relationship between regions of interest (ROIs) in brains and have shown significant improvement in detecting brain diseases. However, most of these frameworks do not consider the intrinsic relationship of causality factor between brain ROIs, which is arguably more essential to observe cause and effect interaction between signals rather than typical correlation values. We propose a novel framework called CGB (Causal Graphs for Brains) for brain disease classification/detection, which models refined brain networks based on the causal discovery method, transfer entropy, and geometric curvature strategy. CGB unveils causal relationships between ROIs that bring vital information to enhance brain disease classification performance. Furthermore, CGB also performs a graph rewiring through a geometric curvature strategy to refine the generated causal graph to become more expressive and reduce potential information bottlenecks when GNNs model it. Our extensive experiments show that CGB outperforms state-of-the-art methods in classification tasks on brain disease datasets, as measured by average F1 scores.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying and Improving Graph Neural Network-based Motif Estimation</title>
<link>https://arxiv.org/abs/2506.15709</link>
<guid>https://arxiv.org/abs/2506.15709</guid>
<content:encoded><![CDATA[

arXiv:2506.15709v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAST: Reasoning Activation in LLMs via Small-model Transfer</title>
<link>https://arxiv.org/abs/2506.15710</link>
<guid>https://arxiv.org/abs/2506.15710</guid>
<content:encoded><![CDATA[

arXiv:2506.15710v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shadow defense against gradient inversion attack in federated learning</title>
<link>https://arxiv.org/abs/2506.15711</link>
<guid>https://arxiv.org/abs/2506.15711</guid>
<content:encoded><![CDATA[

arXiv:2506.15711v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a transformative framework for privacy-preserving distributed training, allowing clients to collaboratively train a global model without sharing their local data. This is especially crucial in sensitive fields like healthcare, where protecting patient data is paramount. However, privacy leakage remains a critical challenge, as the communication of model updates can be exploited by potential adversaries. Gradient inversion attacks (GIAs), for instance, allow adversaries to approximate the gradients used for training and reconstruct training images, thus stealing patient privacy. Existing defense mechanisms obscure gradients, yet lack a nuanced understanding of which gradients or types of image information are most vulnerable to such attacks. These indiscriminate calibrated perturbations result in either excessive privacy protection degrading model accuracy, or insufficient one failing to safeguard sensitive information. Therefore, we introduce a framework that addresses these challenges by leveraging a shadow model with interpretability for identifying sensitive areas. This enables a more targeted and sample-specific noise injection. Specially, our defensive strategy achieves discrepancies of 3.73 in PSNR and 0.2 in SSIM compared to the circumstance without defense on the ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover, it minimizes adverse effects on model performance, with less than 1\% F1 reduction compared to SOTA methods. Our extensive experiments, conducted across diverse types of medical images, validate the generalization of the proposed framework. The stable defense improvements for FedAvg are consistently over 1.5\% times in LPIPS and SSIM. It also offers a universal defense against various GIA types, especially for these sensitive areas in images.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling</title>
<link>https://arxiv.org/abs/2506.15712</link>
<guid>https://arxiv.org/abs/2506.15712</guid>
<content:encoded><![CDATA[

arXiv:2506.15712v1 Announce Type: cross 
Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuronSeek: On Stability and Expressivity of Task-driven Neurons</title>
<link>https://arxiv.org/abs/2506.15715</link>
<guid>https://arxiv.org/abs/2506.15715</guid>
<content:encoded><![CDATA[

arXiv:2506.15715v1 Announce Type: cross 
Abstract: Drawing inspiration from our human brain that designs different neurons for different tasks, recent advances in deep learning have explored modifying a network's neurons to develop so-called task-driven neurons. Prototyping task-driven neurons (referred to as NeuronSeek) employs symbolic regression (SR) to discover the optimal neuron formulation and construct a network from these optimized neurons. Along this direction, this work replaces symbolic regression with tensor decomposition (TD) to discover optimal neuronal formulations, offering enhanced stability and faster convergence. Furthermore, we establish theoretical guarantees that modifying the aggregation functions with common activation functions can empower a network with a fixed number of parameters to approximate any continuous function with an arbitrarily small error, providing a rigorous mathematical foundation for the NeuronSeek framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD framework not only achieves superior stability, but also is competitive relative to the state-of-the-art models across diverse benchmarks. The code is available at https://github.com/HanyuPei22/NeuronSeek.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies</title>
<link>https://arxiv.org/abs/2506.15716</link>
<guid>https://arxiv.org/abs/2506.15716</guid>
<content:encoded><![CDATA[

arXiv:2506.15716v1 Announce Type: cross 
Abstract: An increasingly influential form of deliberative democracy centers on citizens' assemblies, where randomly selected people discuss policy questions. The legitimacy of these panels hinges on their representation of the broader population, but panelists often drop out, leading to an unbalanced composition. Although participant attrition is mitigated in practice by alternates, their selection is not taken into account by existing methods. To address this gap, we introduce an optimization framework for alternate selection. Our algorithmic approach, which leverages learning-theoretic machinery, estimates dropout probabilities using historical data and selects alternates to minimize expected misrepresentation. We establish theoretical guarantees for our approach, including worst-case bounds on sample complexity (with implications for computational efficiency) and on loss when panelists' probabilities of dropping out are mis-estimated. Empirical evaluation using real-world data demonstrates that, compared to the status quo, our method significantly improves representation while requiring fewer alternates.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>daDPO: Distribution-Aware DPO for Distilling Conversational Abilities</title>
<link>https://arxiv.org/abs/2506.15717</link>
<guid>https://arxiv.org/abs/2506.15717</guid>
<content:encoded><![CDATA[

arXiv:2506.15717v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on 'black-box' KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation</title>
<link>https://arxiv.org/abs/2506.15722</link>
<guid>https://arxiv.org/abs/2506.15722</guid>
<content:encoded><![CDATA[

arXiv:2506.15722v1 Announce Type: cross 
Abstract: Metamaterials are artificial materials that are designed to meet unseen properties in nature, such as ultra-stiffness and negative materials indices. In mechanical metamaterial design, three key modalities are typically involved, i.e., 3D topology, density condition, and mechanical property. Real-world complex application scenarios place the demanding requirements on machine learning models to consider all three modalities together. However, a comprehensive literature review indicates that most existing works only consider two modalities, e.g., predicting mechanical properties given the 3D topology or generating 3D topology given the required properties. Therefore, there is still a significant gap for the state-of-the-art machine learning models capturing the whole. Hence, we propose a unified model named UNIMATE, which consists of a modality alignment module and a synergetic diffusion generation module. Experiments indicate that UNIMATE outperforms the other baseline models in topology generation task, property prediction task, and condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We opensource our proposed UNIMATE model and corresponding results at https://github.com/wzhan24/UniMate.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference</title>
<link>https://arxiv.org/abs/2506.15724</link>
<guid>https://arxiv.org/abs/2506.15724</guid>
<content:encoded><![CDATA[

arXiv:2506.15724v1 Announce Type: cross 
Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Diffusion that can Insert and Delete</title>
<link>https://arxiv.org/abs/2506.15725</link>
<guid>https://arxiv.org/abs/2506.15725</guid>
<content:encoded><![CDATA[

arXiv:2506.15725v1 Announce Type: cross 
Abstract: Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture</title>
<link>https://arxiv.org/abs/2506.15737</link>
<guid>https://arxiv.org/abs/2506.15737</guid>
<content:encoded><![CDATA[

arXiv:2506.15737v1 Announce Type: cross 
Abstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent (SGD) frequently encounters difficulties, including substantial computing expense and the risk of converging to local optima, attributable to its dependence on partial weight gradients. Therefore, this work investigates Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve local search efficiency. The findings indicate that the hybrid PSO-SGD technique decreases the median training MSE by 90 to 95 percent relative to conventional GA and PSO across various network sizes (e.g., from around 0.02 to approximately 0.001 in the Sphere function). RMHC attains substantial enhancements, reducing MSE by roughly 85 to 90 percent compared to GA. Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying subpar performance. These findings underscore that hybrid and evolutionary procedures significantly improve training efficiency and accuracy compared to conventional optimization methods and imply that the Building Block Hypothesis (BBH) may still be valid, indicating that advantageous weight structures are retained during evolutionary search.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains</title>
<link>https://arxiv.org/abs/2506.15756</link>
<guid>https://arxiv.org/abs/2506.15756</guid>
<content:encoded><![CDATA[

arXiv:2506.15756v1 Announce Type: cross 
Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under partial observability, a setting where agents are deployed on-the-fly to environments where pre-existing teams operate, that never requires, at any stage, access to the states of the environment or the actions of its teammates. We show that by relying on a recurrent Bayesian classifier trained using past experiences, an ad hoc agent is effectively able to identify known teams and tasks being performed from observations alone. Unlike recent approaches such as PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some stage fully observable states of the environment, actions of teammates, or both, or approaches such as ATPO (Ribeiro et al., 2023) that require the environments to be small enough to be tabularly modelled (Ribeiro et al., 2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes is both able to handle arbitrarily large spaces while never relying on either states and teammates' actions. Our results in benchmark domains from the multi-agent systems literature, adapted for partial observability and scaled up to 1M states and 2^125 observations, show that RecBayes is effective at identifying known teams and tasks being performed from partial observations alone, and as a result, is able to assist the teams in solving the tasks effectively.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graphics4Science: Computer Graphics for Scientific Impacts</title>
<link>https://arxiv.org/abs/2506.15786</link>
<guid>https://arxiv.org/abs/2506.15786</guid>
<content:encoded><![CDATA[

arXiv:2506.15786v1 Announce Type: cross 
Abstract: Computer graphics, often associated with films, games, and visual effects, has long been a powerful tool for addressing scientific challenges--from its origins in 3D visualization for medical imaging to its role in modern computational modeling and simulation. This course explores the deep and evolving relationship between computer graphics and science, highlighting past achievements, ongoing contributions, and open questions that remain. We show how core methods, such as geometric reasoning and physical modeling, provide inductive biases that help address challenges in both fields, especially in data-scarce settings. To that end, we aim to reframe graphics as a modeling language for science by bridging vocabulary gaps between the two communities. Designed for both newcomers and experts, Graphics4Science invites the graphics community to engage with science, tackle high-impact problems where graphics expertise can make a difference, and contribute to the future of scientific discovery. Additional details are available on the course website: https://graphics4science.github.io
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: Transparent, Robust and Ultra-Sparse Trees</title>
<link>https://arxiv.org/abs/2506.15791</link>
<guid>https://arxiv.org/abs/2506.15791</guid>
<content:encoded><![CDATA[

arXiv:2506.15791v1 Announce Type: cross 
Abstract: Piecewise-constant regression trees remain popular for their interpretability, yet often lag behind black-box models like Random Forest in predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and Ultra-Sparse Trees), a novel regression tree model that combines the accuracy of Random Forests with the interpretability of shallow decision trees and sparse linear models. TRUST further enhances transparency by leveraging Large Language Models to generate tailored, user-friendly explanations. Extensive validation on synthetic and real-world benchmark datasets demonstrates that TRUST consistently outperforms other interpretable models -- including CART, Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy of Random Forest and offering substantial gains in both accuracy and interpretability over M5', a well-established model that is conceptually related.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products</title>
<link>https://arxiv.org/abs/2506.15793</link>
<guid>https://arxiv.org/abs/2506.15793</guid>
<content:encoded><![CDATA[

arXiv:2506.15793v1 Announce Type: cross 
Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is the ``clean-up'' step, which decodes the noisy vectors retrieved from the architecture. Clean-up typically compares noisy vectors against a ``codebook'' of prototype vectors, incurring computational complexity that is quadratic or similar. We present a new codebook representation that supports efficient clean-up, based on Kroneker products of rotation-like matrices. The resulting clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$, where $N$ is the vector dimension and also the number of vectors in the codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the codebook is not stored explicitly in computer memory: It can be represented in $\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can be materialized in $\mathcal{O}(N)$ time and space. At the same time, asymptotic memory capacity remains comparable to standard approaches. Computer experiments confirm these results, demonstrating several orders of magnitude more scalability than baseline VSA techniques.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veracity: An Open-Source AI Fact-Checking System</title>
<link>https://arxiv.org/abs/2506.15794</link>
<guid>https://arxiv.org/abs/2506.15794</guid>
<content:encoded><![CDATA[

arXiv:2506.15794v1 Announce Type: cross 
Abstract: The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma</title>
<link>https://arxiv.org/abs/2506.15803</link>
<guid>https://arxiv.org/abs/2506.15803</guid>
<content:encoded><![CDATA[

arXiv:2506.15803v1 Announce Type: cross 
Abstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in radiotherapy, offering several advantages over conventional intensitymodulated proton therapy (IMPT). However, identifying the optimal energy layer (EL) sequence remains computationally intensive due to the large number of possible energy layer transitions. This study proposes an unsupervised deep learning framework for fast and effective EL pre-selection, aiming to minimize energy layer switch time while preserving high plan quality. Approach. We introduce a novel data representation method, spot-count representation, which encodes the number of proton spots intersecting the target and organs at risk (OARs) in a matrix structured by sorted gantry angles and energy layers. This representation is the input of a UNet-based architecture, SPArcdl, which is trained to optimize a tri-objective function: maximizing target coverage, minimizing OAR exposure, and reducing energy switching time. The model is evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked against plans generated by SPArcparticle swarm. Main results. SPArcdl produces EL pre-selection that significantly improves both plan quality and delivery efficiency. Compared to SPArc particle swarm, it enhances the conformity index by 0.16 (p < 0.01), reduces the homogeneity index by 0.71 (p < 0.01), shortens the energy switching time by 38.4% (p < 0.01), and lowers the mean dose to brainstem by 0.21 (p < 0.01). The results unintentionally reveal employing unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's inference time is within 1 second. Significance. SPArcdl is a fast and effective tool for generating high-quality PAT plans by strategically pre-selecting energy layers to reduce delivery time while maintaining excellent dosimetric performance.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal</title>
<link>https://arxiv.org/abs/2506.15821</link>
<guid>https://arxiv.org/abs/2506.15821</guid>
<content:encoded><![CDATA[

arXiv:2506.15821v1 Announce Type: cross 
Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have significantly improved editing tasks, with a primary emphasis on maintaining cross-view consistency throughout the generative process. Contemporary methods typically address this challenge using a dual-strategy framework: performing consistent 2D inpainting across all views guided by embedded priors either explicitly in pixel space or implicitly in latent space; and conducting 3D reconstruction with additional consistency guidance. Previous strategies, in particular, often require an initial 3D reconstruction phase to establish geometric structure, introducing considerable computational overhead. Even with the added cost, the resulting reconstruction quality often remains suboptimal. In this paper, we present VEIGAR, a computationally efficient framework that outperforms existing methods without relying on an initial reconstruction phase. VEIGAR leverages a lightweight foundation model to reliably align priors explicitly in the pixel space. In addition, we introduce a novel supervision strategy based on scale-invariant depth loss, which removes the need for traditional scale-and-shift operations in monocular depth regularization. Through extensive experimentation, VEIGAR establishes a new state-of-the-art benchmark in reconstruction quality and cross-view consistency, while achieving a threefold reduction in training time compared to the fastest existing method, highlighting its superior balance of efficiency and effectiveness.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning</title>
<link>https://arxiv.org/abs/2506.15828</link>
<guid>https://arxiv.org/abs/2506.15828</guid>
<content:encoded><![CDATA[

arXiv:2506.15828v1 Announce Type: cross 
Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting from imperative to declarative approaches (e.g., PDDL). However, these methods often fail in real scenarios due to limited robot perception and the need to ground perceptions to planning predicates. This often results in heavily hard-coded behaviors that struggle to adapt, even with scenarios where goals can be achieved through relaxed planning. Meanwhile, Large Language Models (LLMs) lead to planning systems that leverage commonsense reasoning but often at the cost of generating unfeasible and/or unsafe plans. To address these limitations, we present an approach integrating classical planning with LLMs, leveraging their ability to extract commonsense knowledge and ground actions. We propose a hierarchical formulation that enables robots to make unfeasible tasks tractable by defining functionally equivalent goals through gradual relaxation. This mechanism supports partial achievement of the intended objective, suited to the agent's specific context. Our method demonstrates its ability to adapt and execute tasks effectively within environments modeled using 3D Scene Graphs through comprehensive qualitative and quantitative evaluations. We also show how this method succeeds in complex scenarios where other benchmark methods are more likely to fail. Code, dataset, and additional material are released to the community.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction</title>
<link>https://arxiv.org/abs/2506.15835</link>
<guid>https://arxiv.org/abs/2506.15835</guid>
<content:encoded><![CDATA[

arXiv:2506.15835v1 Announce Type: cross 
Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the spatial relationships of anatomical structures, playing a crucial role in clinical diagnosis. Recently, deep-learning-based freehand 3D US has made significant advancements. It reconstructs volumes by estimating transformations between images without external tracking. However, image-only reconstruction poses difficulties in reducing cumulative drift and further improving reconstruction accuracy, particularly in scenarios involving complex motion trajectories. In this context, we propose an enhanced motion network (MoNetV2) to enhance the accuracy and generalizability of reconstruction under diverse scanning velocities and tactics. First, we propose a sensor-based temporal and multi-branch structure that fuses image and motion information from a velocity perspective to improve image-only reconstruction accuracy. Second, we devise an online multi-level consistency constraint that exploits the inherent consistency of scans to handle various scanning velocities and tactics. This constraint exploits both scan-level velocity consistency, path-level appearance consistency, and patch-level motion consistency to supervise inter-frame transformation estimation. Third, we distill an online multi-modal self-supervised strategy that leverages the correlation between network estimation and motion information to further reduce cumulative errors. Extensive experiments clearly demonstrate that MoNetV2 surpasses existing methods in both reconstruction quality and generalizability performance across three large datasets.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents</title>
<link>https://arxiv.org/abs/2506.15841</link>
<guid>https://arxiv.org/abs/2506.15841</guid>
<content:encoded><![CDATA[

arXiv:2506.15841v1 Announce Type: cross 
Abstract: Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Language Model Evaluation (FLaME)</title>
<link>https://arxiv.org/abs/2506.15846</link>
<guid>https://arxiv.org/abs/2506.15846</guid>
<content:encoded><![CDATA[

arXiv:2506.15846v1 Announce Type: cross 
Abstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation</title>
<link>https://arxiv.org/abs/2506.15847</link>
<guid>https://arxiv.org/abs/2506.15847</guid>
<content:encoded><![CDATA[

arXiv:2506.15847v1 Announce Type: cross 
Abstract: For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference. Then, it adapts the behavior to the robot's own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety Q-functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology. As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific actions that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation by Human Perception versus Neural Models</title>
<link>https://arxiv.org/abs/2506.15850</link>
<guid>https://arxiv.org/abs/2506.15850</guid>
<content:encoded><![CDATA[

arXiv:2506.15850v1 Announce Type: cross 
Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but remain poorly calibrated, producing overconfident predictions even when wrong. This miscalibration poses serious challenges in applications where reliable uncertainty estimates are critical. In this work, we investigate how human perceptual uncertainty compares to uncertainty estimated by NNs. Using three vision benchmarks annotated with both human disagreement and crowdsourced confidence, we assess the correlation between model-predicted uncertainty and human-perceived uncertainty. Our results show that current methods only weakly align with human intuition, with correlations varying significantly across tasks and uncertainty metrics. Notably, we find that incorporating human-derived soft labels into the training process can improve calibration without compromising accuracy. These findings reveal a persistent gap between model and human uncertainty and highlight the potential of leveraging human insights to guide the development of more trustworthy AI systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modality Learning for Predicting IHC Biomarkers from H&amp;E-Stained Whole-Slide Images</title>
<link>https://arxiv.org/abs/2506.15853</link>
<guid>https://arxiv.org/abs/2506.15853</guid>
<content:encoded><![CDATA[

arXiv:2506.15853v1 Announce Type: cross 
Abstract: Hematoxylin and Eosin (H&amp;E) staining is a cornerstone of pathological analysis, offering reliable visualization of cellular morphology and tissue architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry (IHC) staining provides molecular insights by detecting specific proteins within tissues, enhancing diagnostic accuracy, and improving treatment planning. However, IHC staining is costly, time-consuming, and resource-intensive, requiring specialized expertise. To address these limitations, this study proposes HistoStainAlign, a novel deep learning framework that predicts IHC staining patterns directly from H&amp;E whole-slide images (WSIs) by learning joint representations of morphological and molecular features. The framework integrates paired H&amp;E and IHC embeddings through a contrastive training strategy, capturing complementary features across staining modalities without patch-level annotations or tissue registration. The model was evaluated on gastrointestinal and lung tissue WSIs with three commonly used IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI: 0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC stains. Embedding analyses demonstrated the robustness of the contrastive alignment in capturing meaningful cross-stain relationships. Comparisons with a baseline model further highlight the advantage of incorporating contrastive learning for improved stain pattern prediction. This study demonstrates the potential of computational approaches to serve as a pre-screening tool, helping prioritize cases for IHC staining and improving workflow efficiency.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers</title>
<link>https://arxiv.org/abs/2506.15862</link>
<guid>https://arxiv.org/abs/2506.15862</guid>
<content:encoded><![CDATA[

arXiv:2506.15862v1 Announce Type: cross 
Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness hinges on which retrievers we use and how. Different retrievers offer distinct, often complementary signals: BM25 captures lexical matches; dense retrievers, semantic similarity. Yet in practice, we typically fix a single retriever based on heuristics, which fails to generalize across diverse information needs. Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? In our work, we validate this intuition with quantitative analysis and introduce mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers. Extensive experiments show that such mixtures are effective and efficient: Despite totaling just 0.8B parameters, this mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Further analysis also shows that this mixture framework can help incorporate specialized non-oracle human information sources as retrievers to achieve good collaboration, with a 58.9% relative performance improvement over simulated humans alone.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute</title>
<link>https://arxiv.org/abs/2506.15882</link>
<guid>https://arxiv.org/abs/2506.15882</guid>
<content:encoded><![CDATA[

arXiv:2506.15882v1 Announce Type: cross 
Abstract: Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning</title>
<link>https://arxiv.org/abs/2506.15894</link>
<guid>https://arxiv.org/abs/2506.15894</guid>
<content:encoded><![CDATA[

arXiv:2506.15894v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent "reasoning" model work involves amplification of traits already meaningfully present in models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction</title>
<link>https://arxiv.org/abs/2506.15896</link>
<guid>https://arxiv.org/abs/2506.15896</guid>
<content:encoded><![CDATA[

arXiv:2506.15896v1 Announce Type: cross 
Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in agricultural systems for assessing environmental impacts, developing emission mitigation strategies and promoting sustainable agriculture. Due to the lack of advanced sensor and network technologies on majority of farms, there are challenges in obtaining comprehensive and diverse agricultural data. As a result, the scarcity of agricultural data seriously obstructs the application of machine learning approaches in precision soil GHG flux prediction. This research proposes a knowledge-guided graph neural network framework that addresses the above challenges by integrating knowledge embedded in an agricultural process-based model and graph neural network techniques. Specifically, we utilise the agricultural process-based model to simulate and generate multi-dimensional agricultural datasets for 47 countries that cover a wide range of agricultural variables. To extract key agricultural features and integrate correlations among agricultural features in the prediction process, we propose a machine learning framework that integrates the autoencoder and multi-target multi-graph based graph neural networks, which utilises the autoencoder to selectively extract significant agricultural features from the agricultural process-based model simulation data and the graph neural network to integrate correlations among agricultural features for accurately predict fertilisation-oriented soil GHG fluxes. Comprehensive experiments were conducted with both the agricultural simulation dataset and real-world agricultural dataset to evaluate the proposed approach in comparison with well-known baseline and state-of-the-art regression methods. The results demonstrate that our proposed approach provides superior accuracy and stability in fertilisation-oriented soil GHG prediction.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning</title>
<link>https://arxiv.org/abs/2506.15923</link>
<guid>https://arxiv.org/abs/2506.15923</guid>
<content:encoded><![CDATA[

arXiv:2506.15923v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging diverse datasets from multiple sources while preserving data privacy by avoiding centralized storage. However, many existing approaches fail to account for the intricate gradient correlations between remote clients, a limitation that becomes especially problematic in data heterogeneity scenarios. In this work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity (PNCS) to improve client selection for model aggregation. By capturing higher-order gradient moments, PNCS addresses non-IID data challenges, enhancing convergence speed and accuracy. Additionally, we introduce a simple algorithm ensuring diverse client selection through a selection history queue. Experiments with a VGG16 model across varied data partitions demonstrate consistent improvements over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moir\'eXNet: Adaptive Multi-Scale Demoir\'eing with Linear Attention Test-Time Training and Truncated Flow Matching Prior</title>
<link>https://arxiv.org/abs/2506.15929</link>
<guid>https://arxiv.org/abs/2506.15929</guid>
<content:encoded><![CDATA[

arXiv:2506.15929v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for image and video demoir\'eing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoir\'eing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoir\'eing and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization</title>
<link>https://arxiv.org/abs/2506.15937</link>
<guid>https://arxiv.org/abs/2506.15937</guid>
<content:encoded><![CDATA[

arXiv:2506.15937v1 Announce Type: cross 
Abstract: Video synchronization-aligning multiple video streams capturing the same event from different angles-is crucial for applications such as reality TV show production, sports analysis, surveillance, and autonomous systems. Prior work has heavily relied on audio cues or specific visual events, limiting applicability in diverse settings where such signals may be unreliable or absent. Additionally, existing benchmarks for video synchronization lack generality and reproducibility, restricting progress in the field. In this work, we introduce VideoSync, a video synchronization framework that operates independently of specific feature extraction methods, such as human pose estimation, enabling broader applicability across different content types. We evaluate our system on newly composed datasets covering single-human, multi-human, and non-human scenarios, providing both the methodology and code for dataset creation to establish reproducible benchmarks. Our analysis reveals biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline, leading to inflated performance claims. We correct these biases and propose a more rigorous evaluation framework, demonstrating that VideoSync outperforms existing approaches, including SeSyn-Net, under fair experimental conditions. Additionally, we explore various synchronization offset prediction methods, identifying a convolutional neural network (CNN)-based model as the most effective. Our findings advance video synchronization beyond domain-specific constraints, making it more generalizable and robust for real-world applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrainVerify: Equivalence-Based Verification for Distributed LLM Training</title>
<link>https://arxiv.org/abs/2506.15961</link>
<guid>https://arxiv.org/abs/2506.15961</guid>
<content:encoded><![CDATA[

arXiv:2506.15961v1 Announce Type: cross 
Abstract: Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging</title>
<link>https://arxiv.org/abs/2506.15971</link>
<guid>https://arxiv.org/abs/2506.15971</guid>
<content:encoded><![CDATA[

arXiv:2506.15971v1 Announce Type: cross 
Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps but become struggled when the source and target domains belong to entirely distinct modalities. To address this limitation, we propose a novel setting called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which enables knowledge transfer between completely different modalities by leveraging a bridge domain containing unlabeled samples from both modalities. To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a specialized framework designed for the semantic segmentation task. Specifically, LSB utilizes a dual-branch architecture, incorporating a feature consistency loss to align representations across modalities and a domain alignment loss to reduce discrepancies between class centroids across domains. Extensive experiments conducted on six benchmark datasets demonstrate that LSB achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension</title>
<link>https://arxiv.org/abs/2506.15978</link>
<guid>https://arxiv.org/abs/2506.15978</guid>
<content:encoded><![CDATA[

arXiv:2506.15978v1 Announce Type: cross 
Abstract: Vietnamese, the 20th most spoken language with over 102 million native speakers, lacks robust resources for key natural language processing tasks such as text segmentation and machine reading comprehension (MRC). To address this gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset includes 15,942 documents for text segmentation and 16,347 synthetic multiple-choice question-answer pairs generated with human quality assurance, ensuring a reliable and diverse resource. Experiments show that mBERT consistently outperforms monolingual models on both tasks, achieving an accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text segmentation test set. Our analysis reveals that multilingual models excel in NLP tasks for Vietnamese, suggesting potential applications to other under-resourced languages. VSMRC is available at HuggingFace
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization</title>
<link>https://arxiv.org/abs/2506.15980</link>
<guid>https://arxiv.org/abs/2506.15980</guid>
<content:encoded><![CDATA[

arXiv:2506.15980v1 Announce Type: cross 
Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion</title>
<link>https://arxiv.org/abs/2506.15981</link>
<guid>https://arxiv.org/abs/2506.15981</guid>
<content:encoded><![CDATA[

arXiv:2506.15981v1 Announce Type: cross 
Abstract: The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal</title>
<link>https://arxiv.org/abs/2506.16000</link>
<guid>https://arxiv.org/abs/2506.16000</guid>
<content:encoded><![CDATA[

arXiv:2506.16000v1 Announce Type: cross 
Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which heavily relies on collecting and processing large amounts of data in various states and taking a confident and safe decision to define the next vehicle maneuver. In this paper, we propose a novel architecture based on Quantum Artificial Intelligence by enabling quantum and AI at various levels of navigation decision making and communication process in Autonomous vehicles : Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum reinforcement learning for navigation policy optimization and finally post-quantum cryptographic protocols for secure communication. Quantum neural networks uses quantum amplitude encoding to fuse data from various sensors like LiDAR, radar, camera, GPS and weather etc., This approach gives a unified quantum state representation between heterogeneous sensor modalities. Nav-Q module processes the fused quantum states through variational quantum circuits to learn optimal navigation policies under swift dynamic and complex conditions. Finally, post quantum cryptographic protocols are used to secure communication channels for both within vehicle communication and V2X (Vehicle to Everything) communications and thus secures the autonomous vehicle communication from both classical and quantum security threats. Thus, the proposed framework addresses fundamental challenges in autonomous vehicles navigation by providing quantum performance and future proof security. Index Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction</title>
<link>https://arxiv.org/abs/2506.16001</link>
<guid>https://arxiv.org/abs/2506.16001</guid>
<content:encoded><![CDATA[

arXiv:2506.16001v1 Announce Type: cross 
Abstract: Time series forecasting requires architectures that simultaneously achieve three competing objectives: (1) strict temporal causality for reliable predictions, (2) sub-quadratic complexity for practical scalability, and (3) multi-scale pattern recognition for accurate long-horizon forecasting. We introduce AutoHFormer, a hierarchical autoregressive transformer that addresses these challenges through three key innovations: 1) Hierarchical Temporal Modeling: Our architecture decomposes predictions into segment-level blocks processed in parallel, followed by intra-segment sequential refinement. This dual-scale approach maintains temporal coherence while enabling efficient computation. 2) Dynamic Windowed Attention: The attention mechanism employs learnable causal windows with exponential decay, reducing complexity while preserving precise temporal relationships. This design avoids both the anti-causal violations of standard transformers and the sequential bottlenecks of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system is adopted to capture time patterns at multiple scales. It combines fixed oscillating patterns for short-term variations with learnable decay rates for long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X faster training and 6.06X memory reduction compared to PatchTST on PEMS08, while maintaining consistent accuracy across 96-720 step horizons in most of cases. These breakthroughs establish new benchmarks for efficient and precise time series modeling. Implementations of our method and all baselines in hierarchical autoregressive mechanism are available at https://github.com/lizzyhku/Autotime.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIGMAPPER: A Modular System for Automated Geologic Map Digitization</title>
<link>https://arxiv.org/abs/2506.16006</link>
<guid>https://arxiv.org/abs/2506.16006</guid>
<content:encoded><![CDATA[

arXiv:2506.16006v1 Announce Type: cross 
Abstract: Historical geologic maps contain rich geospatial information, such as rock units, faults, folds, and bedding planes, that is critical for assessing mineral resources essential to renewable energy, electric vehicles, and national security. However, digitizing maps remains a labor-intensive and time-consuming task. We present DIGMAPPER, a modular, scalable system developed in collaboration with the United States Geological Survey (USGS) to automate the digitization of geologic maps. DIGMAPPER features a fully dockerized, workflow-orchestrated architecture that integrates state-of-the-art deep learning models for map layout analysis, feature extraction, and georeferencing. To overcome challenges such as limited training data and complex visual content, our system employs innovative techniques, including in-context learning with large language models, synthetic data generation, and transformer-based models. Evaluations on over 100 annotated maps from the DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point feature extraction, and reliable georeferencing performance. Deployed at USGS, DIGMAPPER significantly accelerates the creation of analysis-ready geospatial datasets, supporting national-scale critical mineral assessments and broader geoscientific applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRAIL: Vectorized Reward-based Attribution for Interpretable Learning</title>
<link>https://arxiv.org/abs/2506.16014</link>
<guid>https://arxiv.org/abs/2506.16014</guid>
<content:encoded><![CDATA[

arXiv:2506.16014v1 Announce Type: cross 
Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable Learning), a bi-level framework for value-based reinforcement learning (RL) that learns interpretable weight representations from state features. VRAIL consists of two stages: a deep learning (DL) stage that fits an estimated value function using state features, and an RL stage that uses this to shape learning via potential-based reward transformations. The estimator is modeled in either linear or quadratic form, allowing attribution of importance to individual features and their interactions. Empirical results on the Taxi-v3 environment demonstrate that VRAIL improves training stability and convergence compared to standard DQN, without requiring environment modifications. Further analysis shows that VRAIL uncovers semantically meaningful subgoals, such as passenger possession, highlighting its ability to produce human-interpretable behavior. Our findings suggest that VRAIL serves as a general, model-agnostic framework for reward shaping that enhances both learning and interpretability.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation</title>
<link>https://arxiv.org/abs/2506.16024</link>
<guid>https://arxiv.org/abs/2506.16024</guid>
<content:encoded><![CDATA[

arXiv:2506.16024v1 Announce Type: cross 
Abstract: Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoLM: In Search of Lost Language Model Training Dynamics</title>
<link>https://arxiv.org/abs/2506.16029</link>
<guid>https://arxiv.org/abs/2506.16029</guid>
<content:encoded><![CDATA[

arXiv:2506.16029v1 Announce Type: cross 
Abstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding</title>
<link>https://arxiv.org/abs/2506.16035</link>
<guid>https://arxiv.org/abs/2506.16035</guid>
<content:encoded><![CDATA[

arXiv:2506.16035v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling</title>
<link>https://arxiv.org/abs/2506.16043</link>
<guid>https://arxiv.org/abs/2506.16043</guid>
<content:encoded><![CDATA[

arXiv:2506.16043v1 Announce Type: cross 
Abstract: Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text</title>
<link>https://arxiv.org/abs/2506.16052</link>
<guid>https://arxiv.org/abs/2506.16052</guid>
<content:encoded><![CDATA[

arXiv:2506.16052v1 Announce Type: cross 
Abstract: The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4\% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3\% accuracy on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and 94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations</title>
<link>https://arxiv.org/abs/2506.16056</link>
<guid>https://arxiv.org/abs/2506.16056</guid>
<content:encoded><![CDATA[

arXiv:2506.16056v1 Announce Type: cross 
Abstract: The difficulty of extracting deep features from EEG data and effectively integrating information from multiple views presents significant challenges for developing a generalizable pretraining framework for EEG representation learning. However, most existing pre-training methods rely solely on the contextual semantics of a single view, failing to capture the complex and synergistic interactions among different perspectives, limiting the expressiveness and generalization of learned representations. To address these issues, this paper proposes CRIA, an adaptive framework that utilizes variable-length and variable-channel coding to achieve a unified representation of EEG data across different datasets. In this work, we define cross-view information as the integrated representation that emerges from the interaction among temporal, spectral, and spatial views of EEG signals. The model employs a cross-attention mechanism to fuse temporal, spectral, and spatial features effectively, and combines an attention matrix masking strategy based on the information bottleneck principle with a novel viewpoint masking pre-training scheme. Experimental results on the Temple University EEG corpus and the CHB-MIT dataset show that CRIA outperforms existing methods with the same pre-training conditions, achieving a balanced accuracy of 57.02% for multi-class event classification and 80.03% for anomaly detection, highlighting its strong generalization ability.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Robustness of Large Language Models Safety to Latent Perturbations</title>
<link>https://arxiv.org/abs/2506.16078</link>
<guid>https://arxiv.org/abs/2506.16078</guid>
<content:encoded><![CDATA[

arXiv:2506.16078v1 Announce Type: cross 
Abstract: Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at https://github.com/Carol-gutianle/LatentSafety.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders</title>
<link>https://arxiv.org/abs/2506.16096</link>
<guid>https://arxiv.org/abs/2506.16096</guid>
<content:encoded><![CDATA[

arXiv:2506.16096v1 Announce Type: cross 
Abstract: Recent developed graph-based methods for diagnosing brain disorders using functional connectivity highly rely on predefined brain atlases, but overlook the rich information embedded within atlases and the confounding effects of site and phenotype variability. To address these challenges, we propose a two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates the semantic similarity of brain regions and condition-based population graph modeling. In the first stage, termed brain representation learning, we leverage brain atlas knowledge from GPT-4 to enrich the graph representation and refine the brain graph through an adaptive node reassignment graph attention network. In the second stage, termed population disorder diagnosis, phenotypic data is incorporated into population graph construction and feature fusion to mitigate confounding effects and enhance diagnosis performance. Experiments on the ABIDE I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms state-of-the-art methods in prediction accuracy while enhancing interpretability. Overall, our proposed framework offers a reliable and personalized approach to brain disorder diagnosis, advancing clinical applicability.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks</title>
<link>https://arxiv.org/abs/2506.16114</link>
<guid>https://arxiv.org/abs/2506.16114</guid>
<content:encoded><![CDATA[

arXiv:2506.16114v1 Announce Type: cross 
Abstract: Generative recommendations (GR), which usually include item tokenizers and generative Large Language Models (LLMs), have demonstrated remarkable success across a wide range of scenarios. The majority of existing research efforts primarily concentrate on developing powerful item tokenizers or advancing LLM decoding strategies to attain superior performance. However, the critical fine-tuning step in GR frameworks, which is essential for adapting LLMs to recommendation data, remains largely unexplored. Current approaches predominantly rely on either the next-token prediction loss of supervised fine-tuning (SFT) or recommendationspecific direct preference optimization (DPO) strategies. Both methods ignore the exploration of possible positive unobserved samples, which is commonly referred to as the exposure bias problem. To mitigate this problem, this paper treats the GR as a multi-step generation task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The proposed framework integrates collaborative knowledge from traditional recommender systems to create an adaptive trajectory sampler and a comprehensive reward model. Leveraging the diverse generation property of GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR emerges as a promising approach to mitigate the exposure bias problem. Extensive empirical results on two real-world datasets and with two different GR backbones highlight the effectiveness and robustness of GFlowGR.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2506.16127</link>
<guid>https://arxiv.org/abs/2506.16127</guid>
<content:encoded><![CDATA[

arXiv:2506.16127v1 Announce Type: cross 
Abstract: Dysarthria is a neurological disorder that significantly impairs speech intelligibility, often rendering affected individuals unable to communicate effectively. This necessitates the development of robust dysarthric-to-regular speech conversion techniques. In this work, we investigate the utility and limitations of self-supervised learning (SSL) features and their quantized representations as an alternative to mel-spectrograms for speech generation. Additionally, we explore methods to mitigate speaker variability by generating clean speech in a single-speaker voice using features extracted from WavLM. To this end, we propose a fully non-autoregressive approach that leverages Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct mapping from dysarthric to clean speech. Our findings highlight the effectiveness of discrete acoustic units in improving intelligibility while achieving faster convergence compared to traditional mel-spectrogram-based approaches.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2506.16141</link>
<guid>https://arxiv.org/abs/2506.16141</guid>
<content:encoded><![CDATA[

arXiv:2506.16141v1 Announce Type: cross 
Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISON: Unmasking the Criminal Potential of Large Language Models</title>
<link>https://arxiv.org/abs/2506.16150</link>
<guid>https://arxiv.org/abs/2506.16150</guid>
<content:encoded><![CDATA[

arXiv:2506.16150v1 Announce Type: cross 
Abstract: As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five dimensions: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films, we evaluate both criminal potential and anti-crime ability of LLMs via role-play. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 41% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Under the Shadow of Babel: How Language Shapes Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2506.16151</link>
<guid>https://arxiv.org/abs/2506.16151</guid>
<content:encoded><![CDATA[

arXiv:2506.16151v1 Announce Type: cross 
Abstract: Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On using AI for EEG-based BCI applications: problems, current challenges and future trends</title>
<link>https://arxiv.org/abs/2506.16168</link>
<guid>https://arxiv.org/abs/2506.16168</guid>
<content:encoded><![CDATA[

arXiv:2506.16168v1 Announce Type: cross 
Abstract: Imagine unlocking the power of the mind to communicate, create, and even interact with the world around us. Recent breakthroughs in Artificial Intelligence (AI), especially in how machines "see" and "understand" language, are now fueling exciting progress in decoding brain signals from scalp electroencephalography (EEG). Prima facie, this opens the door to revolutionary brain-computer interfaces (BCIs) designed for real life, moving beyond traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a Brain-to-Internet of Things (BCIoT).
  However, the journey is not as straightforward as it was for Computer Vision (CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based BCIs, particularly in building powerful foundational models, presents unique and intricate hurdles that could affect their reliability.
  Here, we unfold a guided exploration of this dynamic and rapidly evolving research area. Rather than barely outlining a map of current endeavors and results, the goal is to provide a principled navigation of this hot and cutting-edge research landscape. We consider the basic paradigms that emerge from a causal perspective and the attendant challenges presented to AI-based models. Looking ahead, we then discuss promising research avenues that could overcome today's technological, methodological, and ethical limitations. Our aim is to lay out a clear roadmap for creating truly practical and effective EEG-based BCI solutions that can thrive in everyday environments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Teacher to Student: Tracking Memorization Through Model Distillation</title>
<link>https://arxiv.org/abs/2506.16170</link>
<guid>https://arxiv.org/abs/2506.16170</guid>
<content:encoded><![CDATA[

arXiv:2506.16170v1 Announce Type: cross 
Abstract: Large language models (LLMs) are known to memorize parts of their training data, raising important concerns around privacy and security. While previous research has focused on studying memorization in pre-trained models, much less is known about how knowledge distillation (KD) affects memorization.In this study, we explore how different KD methods influence the memorization of fine-tuned task data when a large teacher model is distilled into smaller student variants.This study demonstrates that distilling a larger teacher model, fine-tuned on a dataset, into a smaller variant not only lowers computational costs and model size but also significantly reduces the memorization risks compared to standard fine-tuning approaches.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JETHICS: Japanese Ethics Understanding Evaluation Dataset</title>
<link>https://arxiv.org/abs/2506.16187</link>
<guid>https://arxiv.org/abs/2506.16187</guid>
<content:encoded><![CDATA[

arXiv:2506.16187v1 Announce Type: cross 
Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understanding of AI models. JETHICS contains 78K examples and is built by following the construction methods of the existing English ETHICS dataset. It includes four categories based normative theories and concepts from ethics and political philosophy; and one representing commonsense morality. Our evaluation experiments on non-proprietary large language models (LLMs) and on GPT-4o reveal that even GPT-4o achieves only an average score of about 0.7, while the best-performing Japanese LLM attains around 0.5, indicating a relatively large room for improvement in current LLMs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization</title>
<link>https://arxiv.org/abs/2506.16189</link>
<guid>https://arxiv.org/abs/2506.16189</guid>
<content:encoded><![CDATA[

arXiv:2506.16189v1 Announce Type: cross 
Abstract: We study the problem of conformal prediction (CP) under geometric data shifts, where data samples are susceptible to transformations such as rotations or flips. While CP endows prediction models with post-hoc uncertainty quantification and formal coverage guarantees, their practicality breaks under distribution shifts that deteriorate model performance. To address this issue, we propose integrating geometric information--such as geometric pose--into the conformal procedure to reinstate its guarantees and ensure robustness under geometric shifts. In particular, we explore recent advancements on pose canonicalization as a suitable information extractor for this purpose. Evaluating the combined approach across discrete and continuous shifts and against equivariant and augmentation-based baselines, we find that integrating geometric information with CP yields a principled way to address geometric shifts while maintaining broad applicability to black-box predictors.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CF-Seg: Counterfactuals meet Segmentation</title>
<link>https://arxiv.org/abs/2506.16213</link>
<guid>https://arxiv.org/abs/2506.16213</guid>
<content:encoded><![CDATA[

arXiv:2506.16213v1 Announce Type: cross 
Abstract: Segmenting anatomical structures in medical images plays an important role in the quantitative assessment of various diseases. However, accurate segmentation becomes significantly more challenging in the presence of disease. Disease patterns can alter the appearance of surrounding healthy tissues, introduce ambiguous boundaries, or even obscure critical anatomical structures. As such, segmentation models trained on real-world datasets may struggle to provide good anatomical segmentation, leading to potential misdiagnosis. In this paper, we generate counterfactual (CF) images to simulate how the same anatomy would appear in the absence of disease without altering the underlying structure. We then use these CF images to segment structures of interest, without requiring any changes to the underlying segmentation model. Our experiments on two real-world clinical chest X-ray datasets show that the use of counterfactual images improves anatomical segmentation, thereby aiding downstream clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping</title>
<link>https://arxiv.org/abs/2506.16243</link>
<guid>https://arxiv.org/abs/2506.16243</guid>
<content:encoded><![CDATA[

arXiv:2506.16243v1 Announce Type: cross 
Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and high-quality EEG data from ALS patients are scarce. This data scarcity, coupled with severe class imbalance between ALS and healthy control recordings, poses a challenge for training reliable machine learning classifiers. In this work, we address these issues by generating synthetic EEG signals for ALS patients using a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of ALS EEG signals and produce realistic synthetic samples. We preprocess and normalize EEG recordings, and train a CWGAN model to generate synthetic ALS signals. The CWGAN architecture and training routine are detailed, with key hyperparameters chosen for stable training. Qualitative evaluation of generated signals shows that they closely mimic real ALS EEG patterns. The CWGAN training converged with generator and discriminator loss curves stabilizing, indicating successful learning. The synthetic EEG signals appear realistic and have potential use as augmented data for training classifiers, helping to mitigate class imbalance and improve ALS detection accuracy. We discuss how this approach can facilitate data sharing and enhance diagnostic models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Category-based Galaxy Image Generation via Diffusion Models</title>
<link>https://arxiv.org/abs/2506.16255</link>
<guid>https://arxiv.org/abs/2506.16255</guid>
<content:encoded><![CDATA[

arXiv:2506.16255v1 Announce Type: cross 
Abstract: Conventional galaxy generation methods rely on semi-analytical models and hydrodynamic simulations, which are highly dependent on physical assumptions and parameter tuning. In contrast, data-driven generative models do not have explicit physical parameters pre-determined, and instead learn them efficiently from observational data, making them alternative solutions to galaxy generation. Among these, diffusion models outperform Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) in quality and diversity. Leveraging physical prior knowledge to these models can further enhance their capabilities. In this work, we present GalCatDiff, the first framework in astronomy to leverage both galaxy image features and astrophysical properties in the network design of diffusion models. GalCatDiff incorporates an enhanced U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which dynamically combines attention mechanisms with convolution operations to ensure global consistency and local feature fidelity. Moreover, GalCatDiff uses category embeddings for class-specific galaxy generation, avoiding the high computational costs of training separate models for each category. Our experimental results demonstrate that GalCatDiff significantly outperforms existing methods in terms of the consistency of sample color and size distributions, and the generated galaxies are both visually realistic and physically consistent. This framework will enhance the reliability of galaxy simulations and can potentially serve as a data augmentor to support future galaxy classification algorithm development.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CapsDT: Diffusion-Transformer for Capsule Robot Manipulation</title>
<link>https://arxiv.org/abs/2506.16263</link>
<guid>https://arxiv.org/abs/2506.16263</guid>
<content:encoded><![CDATA[

arXiv:2506.16263v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research area, showcasing significant potential across a variety of applications. However, their performance in endoscopy robotics, particularly endoscopy capsule robots that perform actions within the digestive system, remains unexplored. The integration of VLA models into endoscopy robots allows more intuitive and efficient interactions between human operators and medical devices, improving both diagnostic accuracy and treatment outcomes. In this work, we design CapsDT, a Diffusion Transformer model for capsule robot manipulation in the stomach. By processing interleaved visual inputs, and textual instructions, CapsDT can infer corresponding robotic control signals to facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot system, a capsule robot controlled by a robotic arm-held magnet, addressing different levels of four endoscopy tasks and creating corresponding capsule robot datasets within the stomach simulator. Comprehensive evaluations on various robotic tasks indicate that CapsDT can serve as a robust vision-language generalist, achieving state-of-the-art performance in various levels of endoscopy tasks while achieving a 26.25% success rate in real-world simulation manipulation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Atmospheric Sciences: A Research Roadmap</title>
<link>https://arxiv.org/abs/2506.16281</link>
<guid>https://arxiv.org/abs/2506.16281</guid>
<content:encoded><![CDATA[

arXiv:2506.16281v1 Announce Type: cross 
Abstract: Atmospheric sciences are crucial for understanding environmental phenomena ranging from air quality to extreme weather events, and climate change. Recent breakthroughs in sensing, communication, computing, and Artificial Intelligence (AI) have significantly advanced atmospheric sciences, enabling the generation of vast amounts of data through long-term Earth observations and providing powerful tools for analyzing atmospheric phenomena and predicting natural disasters. This paper contributes a critical interdisciplinary overview that bridges the fields of atmospheric science and computer science, highlighting the transformative potential of AI in atmospheric research. We identify key challenges associated with integrating AI into atmospheric research, including issues related to big data and infrastructure, and provide a detailed research roadmap that addresses both current and emerging challenges.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective</title>
<link>https://arxiv.org/abs/2506.16288</link>
<guid>https://arxiv.org/abs/2506.16288</guid>
<content:encoded><![CDATA[

arXiv:2506.16288v1 Announce Type: cross 
Abstract: The rapid adaptation ability of auto-regressive foundation models is often attributed to the diversity of their pre-training data. This is because, from a Bayesian standpoint, minimizing prediction error in such settings requires integrating over all plausible latent hypotheses consistent with observations. While this behavior is desirable in principle, it often proves too ambitious in practice: under high ambiguity, the number of plausible latent alternatives makes Bayes-optimal prediction computationally intractable. Cognitive science has long recognized this limitation, suggesting that under such conditions, heuristics or information-seeking strategies are preferable to exhaustive inference. Translating this insight to next-token prediction, we hypothesize that low- and high-ambiguity predictions pose different computational demands, making ambiguity-agnostic next-token prediction a detrimental inductive bias. To test this, we introduce MetaHMM, a synthetic sequence meta-learning benchmark with rich compositional structure and a tractable Bayesian oracle. We show that Transformers indeed struggle with high-ambiguity predictions across model sizes. Motivated by cognitive theories, we propose a method to convert pre-trained models into Monte Carlo predictors that decouple task inference from token prediction. Preliminary results show substantial gains in ambiguous contexts through improved capacity allocation and test-time scalable inference, though challenges remain.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SycnMapV2: Robust and Adaptive Unsupervised Segmentation</title>
<link>https://arxiv.org/abs/2506.16297</link>
<guid>https://arxiv.org/abs/2506.16297</guid>
<content:encoded><![CDATA[

arXiv:2506.16297v1 Announce Type: cross 
Abstract: Human vision excels at segmenting visual cues without the need for explicit training, and it remains remarkably robust even as noise severity increases. In contrast, existing AI algorithms struggle to maintain accuracy under similar conditions. Here, we present SyncMapV2, the first to solve unsupervised segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop observed in SOTA methods.This superior performance extends across various types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training, supervision, or loss functions. It is based on a learning paradigm that uses self-organizing dynamical equations combined with concepts from random networks. Moreover,unlike conventional methods that require re-initialization for each new input, SyncMapV2 adapts online, mimicking the continuous adaptability of human vision. Thus, we go beyond the accurate and robust results, and present the first algorithm that can do all the above online, adapting to input rather than re-initializing. In adaptability tests, SyncMapV2 demonstrates near-zero performance degradation, which motivates and fosters a new generation of robust and adaptive intelligence in the near future.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-scale Spatial-frequency Features for Image Denoising</title>
<link>https://arxiv.org/abs/2506.16307</link>
<guid>https://arxiv.org/abs/2506.16307</guid>
<content:encoded><![CDATA[

arXiv:2506.16307v1 Announce Type: cross 
Abstract: Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks</title>
<link>https://arxiv.org/abs/2506.16313</link>
<guid>https://arxiv.org/abs/2506.16313</guid>
<content:encoded><![CDATA[

arXiv:2506.16313v1 Announce Type: cross 
Abstract: Efficiently identifying the right trajectories for training remains an open problem in GFlowNets. To address this, it is essential to prioritize exploration in regions of the state space where the reward distribution has not been sufficiently learned. This calls for uncertainty-driven exploration, in other words, the agent should be aware of what it does not know. This attribute can be measured by joint predictions, which are particularly important for combinatorial and sequential decision problems. In this research, we integrate epistemic neural networks (ENN) with the conventional architecture of GFlowNets to enable more efficient joint predictions and better uncertainty quantification, thereby improving exploration and the identification of optimal trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the baseline method in GFlownets and evaluated in grid environments and structured sequence generation in various settings, demonstrating both its efficacy and efficiency.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation</title>
<link>https://arxiv.org/abs/2506.16318</link>
<guid>https://arxiv.org/abs/2506.16318</guid>
<content:encoded><![CDATA[

arXiv:2506.16318v1 Announce Type: cross 
Abstract: Accurate mapping of agricultural field boundaries is essential for the efficient operation of agriculture. Automatic extraction from high-resolution satellite imagery, supported by computer vision techniques, can avoid costly ground surveys. In this paper, we present a pipeline for field delineation based on the Segment Anything Model (SAM), introducing a fine-tuning strategy to adapt SAM to this task. In addition to using published datasets, we describe a method for acquiring a complementary regional dataset that covers areas beyond current sources. Extensive experiments assess segmentation accuracy and evaluate the generalization capabilities. Our approach provides a robust baseline for automated field delineation. The new regional dataset, known as ERAS, is now publicly available.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Few-shot Learning under Dual Noises</title>
<link>https://arxiv.org/abs/2506.16330</link>
<guid>https://arxiv.org/abs/2506.16330</guid>
<content:encoded><![CDATA[

arXiv:2506.16330v1 Announce Type: cross 
Abstract: Recent advances in model pre-training give rise to task adaptation-based few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic model for capturing task-specific knowledge with a few-labeled support samples of the target task.Nevertheless, existing approaches may still fail in the open world due to the inevitable in-distribution (ID) and out-of-distribution (OOD) noise from both support and query samples of the target task. With limited support samples available, i) the adverse effect of the dual noises can be severely amplified during task adaptation, and ii) the adapted model can produce unreliable predictions on query samples in the presence of the dual noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate image and region weights for support samples, based on which a clean prototype loss and a noise entropy maximization loss are proposed to achieve noise-robust task adaptation. Additionally,DETA++ employs a memory bank to store and refine clean regions for each inner-task class, based on which a Local Nearest Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping (IntraSwap) strategy to rectify ID class prototypes during task adaptation, enhancing the model's robustness to the dual noises. Extensive experiments demonstrate the effectiveness and flexibility of DETA++.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the Influence of Knowledge Graph Information on Relation Extraction</title>
<link>https://arxiv.org/abs/2506.16343</link>
<guid>https://arxiv.org/abs/2506.16343</guid>
<content:encoded><![CDATA[

arXiv:2506.16343v1 Announce Type: cross 
Abstract: We examine the impact of incorporating knowledge graph information on the performance of relation extraction models across a range of datasets. Our hypothesis is that the positions of entities within a knowledge graph provide important insights for relation extraction tasks. We conduct experiments on multiple datasets, each varying in the number of relations, training examples, and underlying knowledge graphs. Our results demonstrate that integrating knowledge graph information significantly enhances performance, especially when dealing with an imbalance in the number of training examples for each relation. We evaluate the contribution of knowledge graph-based features by combining established relation extraction methods with graph-aware Neural Bellman-Ford networks. These features are tested in both supervised and zero-shot settings, demonstrating consistent performance improvements across various datasets.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2506.16349</link>
<guid>https://arxiv.org/abs/2506.16349</guid>
<content:encoded><![CDATA[

arXiv:2506.16349v1 Announce Type: cross 
Abstract: Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can structural correspondences ground real world representational content in Large Language Models?</title>
<link>https://arxiv.org/abs/2506.16370</link>
<guid>https://arxiv.org/abs/2506.16370</guid>
<content:encoded><![CDATA[

arXiv:2506.16370v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset</title>
<link>https://arxiv.org/abs/2506.16385</link>
<guid>https://arxiv.org/abs/2506.16385</guid>
<content:encoded><![CDATA[

arXiv:2506.16385v1 Announce Type: cross 
Abstract: Micro-gesture recognition is a challenging task in affective computing due to the subtle, involuntary nature of the gestures and their low movement amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG integrates human pose (skeleton) information into the CLIP-based recognition pipeline through pose-guided semantic query generation and a gated multi-modal fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These results demonstrate both the potential of our approach and the remaining difficulty in fully adapting vision-language models like CLIP for micro-gesture recognition.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling</title>
<link>https://arxiv.org/abs/2506.16393</link>
<guid>https://arxiv.org/abs/2506.16393</guid>
<content:encoded><![CDATA[

arXiv:2506.16393v1 Announce Type: cross 
Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NepaliGPT: A Generative Language Model for the Nepali Language</title>
<link>https://arxiv.org/abs/2506.16399</link>
<guid>https://arxiv.org/abs/2506.16399</guid>
<content:encoded><![CDATA[

arXiv:2506.16399v1 Announce Type: cross 
Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes \textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal consistency of 85.41\%.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights</title>
<link>https://arxiv.org/abs/2506.16406</link>
<guid>https://arxiv.org/abs/2506.16406</guid>
<content:encoded><![CDATA[

arXiv:2506.16406v1 Announce Type: cross 
Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains up to \textbf{30\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.16407</link>
<guid>https://arxiv.org/abs/2506.16407</guid>
<content:encoded><![CDATA[

arXiv:2506.16407v1 Announce Type: cross 
Abstract: Visual Document Understanding (VDU) systems have achieved strong performance in information extraction by integrating textual, layout, and visual signals. However, their robustness under realistic adversarial perturbations remains insufficiently explored. We introduce the first unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based VDU models. Our method covers six gradient-based layout attack scenarios, incorporating manipulations of OCR bounding boxes, pixels, and texts across both word and line granularities, with constraints on layout perturbation budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and six model families demonstrate that line-level attacks and compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation. Projected Gradient Descent (PGD)-based BBox perturbations outperform random-shift baselines in all investigated models. Ablation studies further validate the impact of layout budget, text modification, and adversarial transferability.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Transformations in Deep Learning Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2506.16418</link>
<guid>https://arxiv.org/abs/2506.16418</guid>
<content:encoded><![CDATA[

arXiv:2506.16418v1 Announce Type: cross 
Abstract: This study investigates the integration of signal processing transformations -- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete Cosine Transform (DCT) -- within the ResNet50 convolutional neural network (CNN) model for image classification. The primary objective is to assess the trade-offs between computational efficiency, energy consumption, and classification accuracy during training and inference. Using the CIFAR-100 dataset (100 classes, 60,000 images), experiments demonstrated that incorporating WHT significantly reduced energy consumption while improving accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified ResNet50 incorporating WHT in the early convolutional layers achieved 74% accuracy, and an enhanced version with WHT applied to both early and late layers achieved 79% accuracy, with an average energy consumption of only 39 kJ per model. These results demonstrate the potential of WHT as a highly efficient and effective approach for energy-constrained CNN applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models</title>
<link>https://arxiv.org/abs/2506.16419</link>
<guid>https://arxiv.org/abs/2506.16419</guid>
<content:encoded><![CDATA[

arXiv:2506.16419v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2506.16443</link>
<guid>https://arxiv.org/abs/2506.16443</guid>
<content:encoded><![CDATA[

arXiv:2506.16443v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving partial differential equations (PDEs), which are ubiquitous in the quantitative sciences. Applied to both forward and inverse problems across various scientific domains, PINNs have recently emerged as a valuable tool in the field of scientific machine learning. A key aspect of their training is that the data -- spatio-temporal points sampled from the PDE's input domain -- are readily available. Influence functions, a tool from the field of explainable AI (XAI), approximate the effect of individual training points on the model, enhancing interpretability. In the present work, we explore the application of influence function-based sampling approaches for the training data. Our results indicate that such targeted resampling based on data attribution methods has the potential to enhance prediction accuracy in physics-informed neural networks, demonstrating a practical application of an XAI method in PINN training.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryWriter: A Multi-Agent Framework for Long Story Generation</title>
<link>https://arxiv.org/abs/2506.16445</link>
<guid>https://arxiv.org/abs/2506.16445</guid>
<content:encoded><![CDATA[

arXiv:2506.16445v1 Announce Type: cross 
Abstract: Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach</title>
<link>https://arxiv.org/abs/2506.16448</link>
<guid>https://arxiv.org/abs/2506.16448</guid>
<content:encoded><![CDATA[

arXiv:2506.16448v1 Announce Type: cross 
Abstract: EEG is a non-invasive, safe, and low-risk method to record electrophysiological signals inside the brain. Especially with recent technology developments like dry electrodes, consumer-grade EEG devices, and rapid advances in machine learning, EEG is commonly used as a resource for automatic emotion recognition. With the aim to develop a deep learning model that can perform EEG-based emotion recognition in a real-life context, we propose a novel approach to utilize multi-scale convolutional neural networks to accomplish such tasks. By implementing feature extraction kernels with many ratio coefficients as well as a new type of kernel that learns key information from four separate areas of the brain, our model consistently outperforms the state-of-the-art TSception model in predicting valence, arousal, and dominance scores across many performance evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2506.16456</link>
<guid>https://arxiv.org/abs/2506.16456</guid>
<content:encoded><![CDATA[

arXiv:2506.16456v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient fine-tuning of large-scale neural models. However, standard LoRA independently optimizes low-rank matrices, which inherently limits its expressivity and generalization capabilities. While classical tensor-train (TT) decomposition can be separately employed on individual LoRA matrices, this work demonstrates that the classical TT-based approach neither significantly improves parameter efficiency nor achieves substantial performance gains. This paper proposes TensorGuide, a novel tensor-train-guided adaptation framework to overcome these limitations. TensorGuide generates two correlated low-rank LoRA matrices through a unified TT structure driven by controlled Gaussian noise. The resulting joint TT representation inherently provides structured, low-rank adaptations, significantly enhancing expressivity, generalization, and parameter efficiency without increasing the number of trainable parameters. Theoretically, we justify these improvements through neural tangent kernel analyses, demonstrating superior optimization dynamics and enhanced generalization. Extensive experiments on quantum dot classification and GPT-2 fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently outperforms standard LoRA and TT-LoRA, achieving improved accuracy and scalability with fewer parameters.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities</title>
<link>https://arxiv.org/abs/2506.16471</link>
<guid>https://arxiv.org/abs/2506.16471</guid>
<content:encoded><![CDATA[

arXiv:2506.16471v1 Announce Type: cross 
Abstract: Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA), a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to procure training samples at a lower temperature for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Code available at: https://github.com/taraak/pita
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support</title>
<link>https://arxiv.org/abs/2506.16473</link>
<guid>https://arxiv.org/abs/2506.16473</guid>
<content:encoded><![CDATA[

arXiv:2506.16473v1 Announce Type: cross 
Abstract: As conversational agents increasingly engage in emotionally supportive dialogue, it is important to understand how closely their interactions resemble those in traditional therapy settings. This study investigates whether the concerns shared with a robot align with those shared in human-to-human (H2H) therapy sessions, and whether robot responses semantically mirror those of human therapists. We analyzed two datasets: one of interactions between users and professional therapists (Hugging Face's NLP Mental Health Conversations), and another involving supportive conversations with a social robot (QTrobot from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence embeddings and K-means clustering, we assessed cross-agent thematic alignment by applying a distance-based cluster-fitting method that evaluates whether responses from one agent type map to clusters derived from the other, and validated it using Euclidean distances. Results showed that 90.88% of robot conversation disclosures could be mapped to clusters from the human therapy dataset, suggesting shared topical structure. For matched clusters, we compared the subjects as well as therapist and robot responses using Transformer, Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects' disclosures in both datasets, as well as in the responses given to similar human disclosure themes across agent types (robot vs. human therapist). These findings highlight both the parallels and boundaries of robot-led support conversations and their potential for augmenting mental health interventions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining</title>
<link>https://arxiv.org/abs/2506.16475</link>
<guid>https://arxiv.org/abs/2506.16475</guid>
<content:encoded><![CDATA[

arXiv:2506.16475v1 Announce Type: cross 
Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in complex environments, but equipping them with autonomous versatile manipulation skills in a scalable way remains a significant challenge. In this work, we introduce a cross-embodiment imitation learning system for quadrupedal manipulation, leveraging data collected from both humans and LocoMan, a quadruped equipped with multiple manipulation modes. Specifically, we develop a teleoperation and data collection pipeline, which unifies and modularizes the observation and action spaces of the human and the robot. To effectively leverage the collected data, we propose an efficient modularized architecture that supports co-training and pretraining on structured modality-aligned data across different embodiments. Additionally, we construct the first manipulation dataset for the LocoMan robot, covering various household tasks in both unimanual and bimanual modes, supplemented by a corresponding human dataset. We validate our system on six real-world manipulation tasks, where it achieves an average success rate improvement of 41.9% overall and 79.7% under out-of-distribution (OOD) settings compared to the baseline. Pretraining with human data contributes a 38.6% success rate improvement overall and 82.7% under OOD settings, enabling consistently better performance with only half the amount of robot data. Our code, hardware, and data are open-sourced at: https://human2bots.github.io.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection</title>
<link>https://arxiv.org/abs/2506.16476</link>
<guid>https://arxiv.org/abs/2506.16476</guid>
<content:encoded><![CDATA[

arXiv:2506.16476v1 Announce Type: cross 
Abstract: Implicit hate speech has recently emerged as a critical challenge for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators' subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Language Models with Semantic Digital Twins for Robotic Planning</title>
<link>https://arxiv.org/abs/2506.16493</link>
<guid>https://arxiv.org/abs/2506.16493</guid>
<content:encoded><![CDATA[

arXiv:2506.16493v1 Announce Type: cross 
Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs) with Large Language Models (LLMs) to enable adaptive and goal-driven robotic task execution in dynamic environments. The system decomposes natural language instructions into structured action triplets, which are grounded in contextual environmental data provided by the SDT. This semantic grounding allows the robot to interpret object affordances and interaction rules, enabling action planning and real-time adaptability. In case of execution failures, the LLM utilizes error feedback and SDT insights to generate recovery strategies and iteratively revise the action plan. We evaluate our approach using tasks from the ALFRED benchmark, demonstrating robust performance across various household scenarios. The proposed framework effectively combines high-level reasoning with semantic environment understanding, achieving reliable task completion in the face of uncertainty and failure.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors</title>
<link>https://arxiv.org/abs/2506.16497</link>
<guid>https://arxiv.org/abs/2506.16497</guid>
<content:encoded><![CDATA[

arXiv:2506.16497v1 Announce Type: cross 
Abstract: Face swapping manipulations in video streams represents an increasing threat in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples</title>
<link>https://arxiv.org/abs/2506.16502</link>
<guid>https://arxiv.org/abs/2506.16502</guid>
<content:encoded><![CDATA[

arXiv:2506.16502v1 Announce Type: cross 
Abstract: Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details</title>
<link>https://arxiv.org/abs/2506.16504</link>
<guid>https://arxiv.org/abs/2506.16504</guid>
<content:encoded><![CDATA[

arXiv:2506.16504v1 Announce Type: cross 
Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspace-Boosted Model Merging</title>
<link>https://arxiv.org/abs/2506.16506</link>
<guid>https://arxiv.org/abs/2506.16506</guid>
<content:encoded><![CDATA[

arXiv:2506.16506v1 Announce Type: cross 
Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we offer an explanation and analysis from a task arithmetic perspective; revealing that as the merging process (across numerous existing merging methods) continues for more and more experts, the associated task vector space experiences rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 expert models by large margins of more than 10% when evaluated on vision benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to further quantify task similarity, offering a new interpretable perspective on model merging.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios</title>
<link>https://arxiv.org/abs/2506.16546</link>
<guid>https://arxiv.org/abs/2506.16546</guid>
<content:encoded><![CDATA[

arXiv:2506.16546v1 Announce Type: cross 
Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to interact with other traffic participants while making real-time and safety-critical decisions accordingly. The unpredictability of human behaviors poses significant challenges, particularly in dynamic scenarios, such as multi-lane highways and unsignalized T-intersections. To address this gap, we design a bi-level interaction decision-making algorithm (BIDA) that integrates interactive Monte Carlo tree search (MCTS) with deep reinforcement learning (DRL), aiming to enhance interaction rationality, efficiency and safety of AVs in dynamic key traffic scenarios. Specifically, we adopt three types of DRL algorithms to construct a reliable value network and policy network, which guide the online deduction process of interactive MCTS by assisting in value update and node selection. Then, a dynamic trajectory planner and a trajectory tracking controller are designed and implemented in CARLA to ensure smooth execution of planned maneuvers. Experimental evaluations demonstrate that our BIDA not only enhances interactive deduction and reduces computational costs, but also outperforms other latest benchmarks, which exhibits superior safety, efficiency and interaction rationality under varying traffic conditions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Sample is Enough to Make Conformal Prediction Robust</title>
<link>https://arxiv.org/abs/2506.16553</link>
<guid>https://arxiv.org/abs/2506.16553</guid>
<content:encoded><![CDATA[

arXiv:2506.16553v1 Announce Type: cross 
Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed to include the true label with high adjustable probability. Robust CP (RCP) extends this to inputs with worst-case noise. A well-established approach is to use randomized smoothing for RCP since it is applicable to any black-box model and provides smaller sets compared to deterministic methods. However, current smoothing-based RCP requires many model forward passes per each input which is computationally expensive. We show that conformal prediction attains some robustness even with a forward pass on a single randomly perturbed input. Using any binary certificate we propose a single sample robust CP (RCP1). Our approach returns robust sets with smaller average set size compared to SOTA methods which use many (e.g. around 100) passes per input. Our key insight is to certify the conformal prediction procedure itself rather than individual scores. Our approach is agnostic to the setup (classification and regression). We further extend our approach to smoothing-based robust conformal risk control.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Semantic To Instance: A Semi-Self-Supervised Learning Approach</title>
<link>https://arxiv.org/abs/2506.16563</link>
<guid>https://arxiv.org/abs/2506.16563</guid>
<content:encoded><![CDATA[

arXiv:2506.16563v1 Announce Type: cross 
Abstract: Instance segmentation is essential for applications such as automated monitoring of plant health, growth, and yield. However, extensive effort is required to create large-scale datasets with pixel-level annotations of each object instance for developing instance segmentation models that restrict the use of deep learning in these areas. This challenge is more significant in images with densely packed, self-occluded objects, which are common in agriculture. To address this challenge, we propose a semi-self-supervised learning approach that requires minimal manual annotation to develop a high-performing instance segmentation model. We design GLMask, an image-mask representation for the model to focus on shape, texture, and pattern while minimizing its dependence on color features. We develop a pipeline to generate semantic segmentation and then transform it into instance-level segmentation. The proposed approach substantially outperforms the conventional instance segmentation models, establishing a state-of-the-art wheat head instance segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed methodology on the general-purpose Microsoft COCO dataset, achieving a significant performance improvement of over 12.6% mAP@50. This highlights that the utility of our proposed approach extends beyond precision agriculture and applies to other domains, specifically those with similar data characteristics.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control</title>
<link>https://arxiv.org/abs/2506.16565</link>
<guid>https://arxiv.org/abs/2506.16565</guid>
<content:encoded><![CDATA[

arXiv:2506.16565v1 Announce Type: cross 
Abstract: World models enable robots to "imagine" future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning. Despite their promise, these models remain brittle when encountering novel visual distractors such as objects and background elements rarely seen during training. Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification. In this work, we propose Reimagination with Observation Intervention (ReOI), a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in open-world scenarios where novel and unanticipated visual distractors are inevitable. Given the current robot observation, ReOI first detects visual distractors by identifying which elements of the scene degrade in physically implausible ways during world model prediction. Then, it modifies the current observation to remove these distractors and bring the observation closer to the training distribution. Finally, ReOI "reimagines" future outcomes with the modified observation and reintroduces the distractors post-hoc to preserve visual consistency for downstream planning and verification. We validate our approach on a suite of robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model. Our results show that ReOI is robust to both in-distribution and out-of-distribution visual distractors. Notably, it improves task success rates by up to 3x in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework</title>
<link>https://arxiv.org/abs/2506.16584</link>
<guid>https://arxiv.org/abs/2506.16584</guid>
<content:encoded><![CDATA[

arXiv:2506.16584v1 Announce Type: cross 
Abstract: Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.16586</link>
<guid>https://arxiv.org/abs/2506.16586</guid>
<content:encoded><![CDATA[

arXiv:2506.16586v1 Announce Type: cross 
Abstract: Traditional quality assurance (QA) methods face significant challenges in addressing the complexity, scale, and rapid iteration cycles of modern software systems and are strained by limited resources available, leading to substantial costs associated with poor quality. The object of this research is the Quality Assurance processes for modern distributed software applications. The subject of the research is the assessment of the benefits, challenges, and prospects of integrating modern AI-oriented tools into quality assurance processes. We performed comprehensive analysis of implications on both verification and validation processes covering exploratory test analyses, equivalence partitioning and boundary analyses, metamorphic testing, finding inconsistencies in acceptance criteria (AC), static analyses, test case generation, unit test generation, test suit optimization and assessment, end to end scenario execution. End to end regression of sample enterprise application utilizing AI-agents over generated test scenarios was implemented as a proof of concept highlighting practical use of the study. The results, with only 8.3% flaky executions of generated test cases, indicate significant potential for the proposed approaches. However, the study also identified substantial challenges for practical adoption concerning generation of semantically identical coverage, "black box" nature and lack of explainability from state-of-the-art Large Language Models (LLMs), the tendency to correct mutated test cases to match expected results, underscoring the necessity for thorough verification of both generated artifacts and test execution results. The research demonstrates AI's transformative potential for QA but highlights the importance of a strategic approach to implementing these technologies, considering the identified limitations and the need for developing appropriate verification methodologies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially-Aware Evaluation of Segmentation Uncertainty</title>
<link>https://arxiv.org/abs/2506.16589</link>
<guid>https://arxiv.org/abs/2506.16589</guid>
<content:encoded><![CDATA[

arXiv:2506.16589v1 Announce Type: cross 
Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions. However, most uncertainty evaluation metrics treat voxels independently, ignoring spatial context and anatomical structure. As a result, they may assign identical scores to qualitatively distinct patterns (e.g., scattered vs. boundary-aligned uncertainty). We propose three spatially aware metrics that incorporate structural and boundary information and conduct a thorough validation on medical imaging data from the prostate zonal segmentation challenge within the Medical Segmentation Decathlon. Our results demonstrate improved alignment with clinically important factors and better discrimination between meaningful and spurious uncertainty patterns.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Based Transfer for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.16590</link>
<guid>https://arxiv.org/abs/2506.16590</guid>
<content:encoded><![CDATA[

arXiv:2506.16590v1 Announce Type: cross 
Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency, making them challenging to apply in multi-task or continual learning settings. Efficiency can be improved by transferring knowledge from a previously trained teacher policy to guide exploration in new but related tasks. However, if the new task sufficiently differs from the teacher's training task, the transferred guidance may be sub-optimal and bias exploration toward low-reward behaviors. We propose an energy-based transfer learning method that uses out-of-distribution detection to selectively issue guidance, enabling the teacher to intervene only in states within its training distribution. We theoretically show that energy scores reflect the teacher's state-visitation density and empirically demonstrate improved sample efficiency and performance across both single-task and multi-task settings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2506.16592</link>
<guid>https://arxiv.org/abs/2506.16592</guid>
<content:encoded><![CDATA[

arXiv:2506.16592v1 Announce Type: cross 
Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer detection, but automated tumor segmentation is challenging due to inherent noise, variations in scale of lesions, and fuzzy boundaries. To address these challenges, we propose a novel hybrid attention-based network for lesion segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in the encoder part for robust feature extraction with a multi-branch attention-enhanced decoder tailored for breast ultrasound images. The bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE), and Scaled Dot-Product Attention (SDPA) to learn global context, spatial relationships, and relative positional features. The Spatial Feature Enhancement Block (SFEB) is embedded at skip connections to refine and enhance spatial features, enabling the network to focus more effectively on tumor regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap metrics, enhancing robustness to class imbalance and irregular tumor shapes. Experiments on public datasets demonstrate that our method outperforms existing approaches, highlighting its potential to assist radiologists in early and accurate breast cancer diagnosis.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE</title>
<link>https://arxiv.org/abs/2506.16600</link>
<guid>https://arxiv.org/abs/2506.16600</guid>
<content:encoded><![CDATA[

arXiv:2506.16600v1 Announce Type: cross 
Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients to fine-tune models using compressed versions of global LoRA matrices, in order to accommodate various compute resources across clients. This compression requirement will lead to suboptimal performance due to information loss. To address this, we propose FLAME, a novel federated learning framework based on the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches, FLAME retains full (uncompressed) global LoRA matrices and achieves client-side adaptability by varying the number of activated experts per client. However, incorporating SMoE into federated learning introduces unique challenges, specifically, the mismatch in output magnitude from partial expert activation and the imbalance in expert training quality across clients. FLAME tackles these challenges through a lightweight rescaling mechanism and an activation-aware aggregation scheme. Empirical results across diverse computational settings demonstrate that FLAME consistently outperforms existing methods, providing a robust and effective solution for resource-adaptive federated learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces</title>
<link>https://arxiv.org/abs/2506.16608</link>
<guid>https://arxiv.org/abs/2506.16608</guid>
<content:encoded><![CDATA[

arXiv:2506.16608v1 Announce Type: cross 
Abstract: We introduce a novel reinforcement learning (RL) framework that treats distribution parameters as actions, redefining the boundary between agent and environment. This reparameterization makes the new action space continuous, regardless of the original action type (discrete, continuous, mixed, etc.). Under this new parameterization, we develop a generalized deterministic policy gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has lower variance than the gradient in the original action space. Although learning the critic over distribution parameters poses new challenges, we introduce interpolated critic learning (ICL), a simple yet effective strategy to enhance learning, supported by insights from bandit settings. Building on TD3, a strong baseline for continuous control, we propose a practical DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC). Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance on the same environments with discretized action spaces.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Public Perceptions of Science in Media</title>
<link>https://arxiv.org/abs/2506.16622</link>
<guid>https://arxiv.org/abs/2506.16622</guid>
<content:encoded><![CDATA[

arXiv:2506.16622v1 Announce Type: cross 
Abstract: Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation</title>
<link>https://arxiv.org/abs/2506.16623</link>
<guid>https://arxiv.org/abs/2506.16623</guid>
<content:encoded><![CDATA[

arXiv:2506.16623v1 Announce Type: cross 
Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in unseen environments, demanding sophisticated reasoning. While Vision-Language Models (VLMs) show potential, current ObjectNav methods often employ them superficially, primarily using vision-language embeddings for object-scene similarity checks rather than leveraging deeper reasoning. This limits contextual understanding and leads to practical issues like repetitive navigation behaviors. This paper introduces a novel zero-shot ObjectNav framework that pioneers the use of dynamic, history-aware prompting to more deeply integrate VLM reasoning into frontier-based exploration. Our core innovation lies in providing the VLM with action history context, enabling it to generate semantic guidance scores for navigation actions while actively avoiding decision loops. We also introduce a VLM-assisted waypoint generation mechanism for refining the final approach to detected objects. Evaluated on the HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and 24.8% Success weighted by Path Length (SPL). These results are comparable to state-of-the-art zero-shot methods, demonstrating the significant potential of our history-augmented VLM prompting strategy for more robust and context-aware robotic navigation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View</title>
<link>https://arxiv.org/abs/2506.16633</link>
<guid>https://arxiv.org/abs/2506.16633</guid>
<content:encoded><![CDATA[

arXiv:2506.16633v1 Announce Type: cross 
Abstract: Multimodal reasoning is a process of understanding, integrating and inferring information across different data modalities. It has recently attracted surging academic attention as a benchmark for Artificial Intelligence (AI). Although there are various tasks for evaluating multimodal reasoning ability, they still have limitations. Lack of reasoning on hierarchical visual clues at different levels of granularity, e.g., local details and global context, is of little discussion, despite its frequent involvement in real scenarios. To bridge the gap, we introduce a novel and challenging task for multimodal reasoning, namely GeoGuess. Given a street view image, the task is to identify its location and provide a detailed explanation. A system that succeeds in GeoGuess should be able to detect tiny visual clues, perceive the broader landscape, and associate with vast geographic knowledge. Therefore, GeoGuess would require the ability to reason between hierarchical visual information and geographic knowledge. In this work, we establish a benchmark for GeoGuess by introducing a specially curated dataset GeoExplain which consists of panoramas-geocoordinates-explanation tuples. Additionally, we present a multimodal and multilevel reasoning method, namely SightSense which can make prediction and generate comprehensive explanation based on hierarchy of visual information and external knowledge. Our analysis and experiments demonstrate their outstanding performance in GeoGuess.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2506.16636</link>
<guid>https://arxiv.org/abs/2506.16636</guid>
<content:encoded><![CDATA[

arXiv:2506.16636v1 Announce Type: cross 
Abstract: Synthetic Data Generation has become essential for scalable, privacy-preserving statistical analysis. While standard approaches based on generative models, such as Normalizing Flows, have been widely used, they often suffer from slow convergence in high-dimensional settings, frequently converging more slowly than the canonical $1/\sqrt{n}$ rate when approximating the true data distribution.
  To overcome these limitations, we propose a Latent Noise Injection method using Masked Autoregressive Flows (MAF). Instead of directly sampling from the trained model, our method perturbs each data point in the latent space and maps it back to the data domain. This construction preserves a one to one correspondence between observed and synthetic data, enabling synthetic outputs that closely reflect the underlying distribution, particularly in challenging high-dimensional regimes where traditional sampling struggles.
  Our procedure satisfies local $(\epsilon, \delta)$-differential privacy and introduces a single perturbation parameter to control the privacy-utility trade-off. Although estimators based on individual synthetic datasets may converge slowly, we show both theoretically and empirically that aggregating across $K$ studies in a meta analysis framework restores classical efficiency and yields consistent, reliable inference. We demonstrate that with a well-calibrated perturbation parameter, Latent Noise Injection achieves strong statistical alignment with the original data and robustness against membership inference attacks. These results position our method as a compelling alternative to conventional flow-based sampling for synthetic data sharing in decentralized and privacy-sensitive domains, such as biomedical research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Context Generalization with Sparse Attention</title>
<link>https://arxiv.org/abs/2506.16640</link>
<guid>https://arxiv.org/abs/2506.16640</guid>
<content:encoded><![CDATA[

arXiv:2506.16640v1 Announce Type: cross 
Abstract: Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate attention probability mass, leading to dispersion and representational collapse. We show in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $\alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes. Finally, we show that the ability to locate and generalize fixed-size patterns can be further improved through a careful design of position encodings, which impacts both dense and sparse attention methods. By integrating ASEntmax into standard transformer layers alongside proper positional encodings, we show that our models greatly outperform softmax, scalable softmax, and fixed-temperature $\alpha$-entmax baselines on long-context generalization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemAgent: A Semantics Aware Program Repair Agent</title>
<link>https://arxiv.org/abs/2506.16650</link>
<guid>https://arxiv.org/abs/2506.16650</guid>
<content:encoded><![CDATA[

arXiv:2506.16650v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream software engineering tasks such as Automated Program Repair (APR). In particular, there has been a lot of research on repository-level issue-resolution benchmarks such as SWE-Bench. Although there has been significant progress on this topic, we notice that in the process of solving such issues, existing agentic systems tend to hyper-localize on immediately suspicious lines of code and fix them in isolation, without a deeper understanding of the issue semantics, code semantics, or execution semantics. Consequently, many existing systems generate patches that overfit to the user issue, even when a more general fix is preferable. To address this limitation, we introduce SemAgent, a novel workflow-based procedure that leverages issue, code, and execution semantics to generate patches that are complete - identifying and fixing all lines relevant to the issue. We achieve this through a novel pipeline that (a) leverages execution semantics to retrieve relevant context, (b) comprehends issue-semantics via generalized abstraction, (c) isolates code-semantics within the context of this abstraction, and (d) leverages this understanding in a two-stage architecture: a repair stage that proposes fine-grained fixes, followed by a reviewer stage that filters relevant fixes based on the inferred issue-semantics. Our evaluations show that our methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark beating all other workflow-based approaches, and an absolute improvement of 7.66% compared to our baseline, which lacks such deep semantic understanding. We note that our approach performs particularly well on issues requiring multi-line reasoning (and editing) and edge-case handling, suggesting that incorporating issue and code semantics into APR pipelines can lead to robust and semantically consistent repairs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs in Coding and their Impact on the Commercial Software Engineering Landscape</title>
<link>https://arxiv.org/abs/2506.16653</link>
<guid>https://arxiv.org/abs/2506.16653</guid>
<content:encoded><![CDATA[

arXiv:2506.16653v1 Announce Type: cross 
Abstract: Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree'' with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures</title>
<link>https://arxiv.org/abs/2506.16654</link>
<guid>https://arxiv.org/abs/2506.16654</guid>
<content:encoded><![CDATA[

arXiv:2506.16654v1 Announce Type: cross 
Abstract: Graph machine learning has led to a significant increase in the capabilities of models that learn on arbitrary graph-structured data and has been applied to molecules, social networks, recommendation systems, and transportation, among other domains. Data in multi-tabular relational databases can also be constructed as 'relational entity graphs' for Relational Deep Learning (RDL) - a new blueprint that enables end-to-end representation learning without traditional feature engineering. Compared to arbitrary graph-structured data, relational entity graphs have key properties: (i) their structure is defined by primary-foreign key relationships between entities in different tables, (ii) the structural connectivity is a function of the relational schema defining a database, and (iii) the graph connectivity is temporal and heterogeneous in nature. In this paper, we provide a comprehensive review of RDL by first introducing the representation of relational databases as relational entity graphs, and then reviewing public benchmark datasets that have been used to develop and evaluate recent GNN-based RDL models. We discuss key challenges including large-scale multi-table integration and the complexities of modeling temporal dynamics and heterogeneous data, while also surveying foundational neural network methods and recent architectural advances specialized for relational entity graphs. Finally, we explore opportunities to unify these distinct modeling challenges, highlighting how RDL converges multiple sub-fields in graph machine learning towards the design of foundation models that can transform the processing of relational data.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Optimizer Design for LLM Pretraining</title>
<link>https://arxiv.org/abs/2506.16659</link>
<guid>https://arxiv.org/abs/2506.16659</guid>
<content:encoded><![CDATA[

arXiv:2506.16659v1 Announce Type: cross 
Abstract: Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which require significant memory to maintain first- and second-moment matrices, known as optimizer states. While recent works such as GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What is the minimal amount of optimizer state that is truly necessary to retain state-of-the-art performance in LLM pretraining? In this work, we systematically investigate this question using a bottom-up approach. We find that two memory- and compute-efficient optimization techniques are particularly effective: (1) column-wise gradient normalization significantly boosts the performance of plain SGD without requiring momentum; and (2) adding first-order momentum only to the output layer - where gradient variance is highest - yields performance competitive with fully adaptive methods such as Muon. Based on these insights, we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new optimizer that combines column-normalized SGD with last-layer momentum, where column normalization refers to normalizing the gradient along the output dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira, and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art method APOLLO in terms of both perplexity and memory consumption. In addition, our method serves as a minimalist baseline for more sophisticated optimizer design.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions</title>
<link>https://arxiv.org/abs/2506.16679</link>
<guid>https://arxiv.org/abs/2506.16679</guid>
<content:encoded><![CDATA[

arXiv:2506.16679v1 Announce Type: cross 
Abstract: Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation</title>
<link>https://arxiv.org/abs/2506.16683</link>
<guid>https://arxiv.org/abs/2506.16683</guid>
<content:encoded><![CDATA[

arXiv:2506.16683v1 Announce Type: cross 
Abstract: Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Stable Diffusion Planning through Variational Adaptive Weighting</title>
<link>https://arxiv.org/abs/2506.16688</link>
<guid>https://arxiv.org/abs/2506.16688</guid>
<content:encoded><![CDATA[

arXiv:2506.16688v1 Announce Type: cross 
Abstract: Diffusion models have recently shown promise in offline RL. However, these methods often suffer from high training costs and slow convergence, particularly when using transformer-based denoising backbones. While several optimization strategies have been proposed -- such as modified noise schedules, auxiliary prediction targets, and adaptive loss weighting -- challenges remain in achieving stable and efficient training. In particular, existing loss weighting functions typically rely on neural network approximators, which can be ineffective in early training phases due to limited generalization capacity of MLPs when exposed to sparse feedback in the early training stages. In this work, we derive a variationally optimal uncertainty-aware weighting function and introduce a closed-form polynomial approximation method for its online estimation under the flow-based generative modeling framework. We integrate our method into a diffusion planning pipeline and evaluate it on standard offline RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our method achieves competitive performance with up to 10 times fewer training steps, highlighting its practical effectiveness.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology</title>
<link>https://arxiv.org/abs/2506.16697</link>
<guid>https://arxiv.org/abs/2506.16697</guid>
<content:encoded><![CDATA[

arXiv:2506.16697v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field's foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing "I am anxious"--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Psychological Simulators: A Methodological Guide</title>
<link>https://arxiv.org/abs/2506.16702</link>
<guid>https://arxiv.org/abs/2506.16702</guid>
<content:encoded><![CDATA[

arXiv:2506.16702v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.16712</link>
<guid>https://arxiv.org/abs/2506.16712</guid>
<content:encoded><![CDATA[

arXiv:2506.16712v1 Announce Type: cross 
Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation</title>
<link>https://arxiv.org/abs/2506.16718</link>
<guid>https://arxiv.org/abs/2506.16718</guid>
<content:encoded><![CDATA[

arXiv:2506.16718v1 Announce Type: cross 
Abstract: Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents. Addressing this challenge is highly complex, and researchers have proposed two simplified scenarios, Multi-agent reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on these foundations, we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to generalize across diverse scenarios, tasks, and interactions with both unfamiliar opponents and teammates. In ACCA, agents adjust to task and environmental changes, collaborate with unseen teammates, and compete against unknown opponents. We introduce a new modeling approach, Multi-Retrieval and Dynamic Generation (MRDG), that effectively models both teammates and opponents using their behavioral trajectories. This method incorporates a positional encoder for varying team sizes and a hypernetwork module to boost agents' learning and adaptive capabilities. Additionally, a viewpoint alignment module harmonizes the observational perspectives of retrieved teammates and opponents with the learning agent. Extensive tests in benchmark scenarios like SMAC, Overcooked-AI, and Melting Pot show that MRDG significantly improves robust collaboration and competition with unseen teammates and opponents, surpassing established baselines. Our code is available at: https://github.com/vcis-wangchenxu/MRDG.git
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data</title>
<link>https://arxiv.org/abs/2506.16723</link>
<guid>https://arxiv.org/abs/2506.16723</guid>
<content:encoded><![CDATA[

arXiv:2506.16723v1 Announce Type: cross 
Abstract: Serial pipeline training is an efficient paradigm for handling data heterogeneity in cross-silo federated learning with low communication overhead. However, even without centralized aggregation, direct transfer of models between clients can violate privacy regulations and remain susceptible to gradient leakage and linkage attacks. Additionally, ensuring resilience against semi-honest or malicious clients who may manipulate or misuse received models remains a grand challenge, particularly in privacy-sensitive domains such as healthcare. To address these challenges, we propose TriCon-SF, a novel serial federated learning framework that integrates triple shuffling and contribution awareness. TriCon-SF introduces three levels of randomization by shuffling model layers, data segments, and training sequences to break deterministic learning patterns and disrupt potential attack vectors, thereby enhancing privacy and robustness. In parallel, it leverages Shapley value methods to dynamically evaluate client contributions during training, enabling the detection of dishonest behavior and enhancing system accountability. Extensive experiments on non-IID healthcare datasets demonstrate that TriCon-SF outperforms standard serial and parallel federated learning in both accuracy and communication efficiency. Security analysis further supports its resilience against client-side privacy attacks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Model Confidence on Bias Effects in Measured Uncertainties</title>
<link>https://arxiv.org/abs/2506.16724</link>
<guid>https://arxiv.org/abs/2506.16724</guid>
<content:encoded><![CDATA[

arXiv:2506.16724v1 Announce Type: cross 
Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases induce greater changes in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence leads to greater underestimation of epistemic uncertainty (i.e. overconfidence) due to bias, whereas it has no significant effect on the direction of changes in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis</title>
<link>https://arxiv.org/abs/2506.16732</link>
<guid>https://arxiv.org/abs/2506.16732</guid>
<content:encoded><![CDATA[

arXiv:2506.16732v1 Announce Type: cross 
Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims to have continuous decisions that are promising in a probabilistic sense for each training instance, which enables end-to-end training on initially discrete and non-differentiable problems. At the test time, for each test instance, starting from continuous decisions, derandomization is typically applied to obtain the final deterministic decisions. Researchers have developed more and more powerful test-time derandomization schemes to enhance the empirical performance and the theoretical guarantee of UCO methods. However, we notice a misalignment between training and testing in the existing UCO methods. Consequently, lower training losses do not necessarily entail better post-derandomization performance, even for the training instances without any data distribution shift. Empirically, we indeed observe such undesirable cases. We explore a preliminary idea to better align training and testing in UCO by including a differentiable version of derandomization into training. Our empirical exploration shows that such an idea indeed improves training-test alignment, but also introduces nontrivial challenges into training.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization</title>
<link>https://arxiv.org/abs/2506.16738</link>
<guid>https://arxiv.org/abs/2506.16738</guid>
<content:encoded><![CDATA[

arXiv:2506.16738v1 Announce Type: cross 
Abstract: With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching</title>
<link>https://arxiv.org/abs/2506.16741</link>
<guid>https://arxiv.org/abs/2506.16741</guid>
<content:encoded><![CDATA[

arXiv:2506.16741v1 Announce Type: cross 
Abstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that leverages velocity consistency constraints in flow matching (FM) training. Although ordinary differential equation (ODE)-based TTS generation achieves natural-quality speech, it typically requires a large number of generation steps, resulting in a trade-off between quality and inference speed. To address this challenge, RapFlow-TTS enforces consistency in the velocity field along the FM-straightened ODE trajectory, enabling consistent synthetic quality with fewer generation steps. Additionally, we introduce techniques such as time interval scheduling and adversarial learning to further enhance the quality of the few-step synthesis. Experimental results show that RapFlow-TTS achieves high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis steps than the conventional FM- and score-based approaches, respectively.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation</title>
<link>https://arxiv.org/abs/2506.16753</link>
<guid>https://arxiv.org/abs/2506.16753</guid>
<content:encoded><![CDATA[

arXiv:2506.16753v1 Announce Type: cross 
Abstract: Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at https://github.com/nakanakakosuke/VALT_SAC.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding</title>
<link>https://arxiv.org/abs/2506.16754</link>
<guid>https://arxiv.org/abs/2506.16754</guid>
<content:encoded><![CDATA[

arXiv:2506.16754v1 Announce Type: cross 
Abstract: The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly</title>
<link>https://arxiv.org/abs/2506.16755</link>
<guid>https://arxiv.org/abs/2506.16755</guid>
<content:encoded><![CDATA[

arXiv:2506.16755v1 Announce Type: cross 
Abstract: Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model</title>
<link>https://arxiv.org/abs/2506.16776</link>
<guid>https://arxiv.org/abs/2506.16776</guid>
<content:encoded><![CDATA[

arXiv:2506.16776v1 Announce Type: cross 
Abstract: Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity</title>
<link>https://arxiv.org/abs/2506.16782</link>
<guid>https://arxiv.org/abs/2506.16782</guid>
<content:encoded><![CDATA[

arXiv:2506.16782v1 Announce Type: cross 
Abstract: Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML morally wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable goods and benefits, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation. Grounding ML fairness in egalitarianism -- the view that equality is a fundamental moral and social ideal -- requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong -- why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups -- and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a multifaceted egalitarian framework for ML fairness that integrates both distributive and relational equality. Drawing on critical social and political philosophy, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the ML pipeline.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabArena: A Living Benchmark for Machine Learning on Tabular Data</title>
<link>https://arxiv.org/abs/2506.16791</link>
<guid>https://arxiv.org/abs/2506.16791</guid>
<content:encoded><![CDATA[

arXiv:2506.16791v1 Announce Type: cross 
Abstract: With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning and investigate the contributions of individual models. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning</title>
<link>https://arxiv.org/abs/2506.16792</link>
<guid>https://arxiv.org/abs/2506.16792</guid>
<content:encoded><![CDATA[

arXiv:2506.16792v1 Announce Type: cross 
Abstract: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.16795</link>
<guid>https://arxiv.org/abs/2506.16795</guid>
<content:encoded><![CDATA[

arXiv:2506.16795v1 Announce Type: cross 
Abstract: Dynamic material handling (DMH) involves the assignment of dynamically arriving material transporting tasks to suitable vehicles in real time for minimising makespan and tardiness. In real-world scenarios, historical task records are usually available, which enables the training of a decision policy on multiple instances consisting of historical records. Recently, reinforcement learning has been applied to solve DMH. Due to the occurrence of dynamic events such as new tasks, adaptability is highly required. Solving DMH is challenging since constraints including task delay should be satisfied. A feedback is received only when all tasks are served, which leads to sparse reward. Besides, making the best use of limited computational resources and historical records for training a robust policy is crucial. The time allocated to different problem instances would highly impact the learning process. To tackle those challenges, this paper proposes a novel adaptive constrained evolutionary reinforcement learning (ACERL) approach, which maintains a population of actors for diverse exploration. ACERL accesses each actor for tackling sparse rewards and constraint violation to restrict the behaviour of the policy. Moreover, ACERL adaptively selects the most beneficial training instances for improving the policy. Extensive experiments on eight training and eight unseen test instances demonstrate the outstanding performance of ACERL compared with several state-of-the-art algorithms. Policies trained by ACERL can schedule the vehicles while fully satisfying the constraints. Additional experiments on 40 unseen noised instances show the robust performance of ACERL. Cross-validation further presents the overall effectiveness of ACREL. Besides, a rigorous ablation study highlights the coordination and benefits of each ingredient of ACERL.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection</title>
<link>https://arxiv.org/abs/2506.16819</link>
<guid>https://arxiv.org/abs/2506.16819</guid>
<content:encoded><![CDATA[

arXiv:2506.16819v1 Announce Type: cross 
Abstract: The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at https://github.com/Kamichanw/Loupe.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dexterous Object Handover</title>
<link>https://arxiv.org/abs/2506.16822</link>
<guid>https://arxiv.org/abs/2506.16822</guid>
<content:encoded><![CDATA[

arXiv:2506.16822v1 Announce Type: cross 
Abstract: Object handover is an important skill that we use daily when interacting with other humans. To deploy robots in collaborative setting, like houses, being able to receive and handing over objects safely and efficiently becomes a crucial skill. In this work, we demonstrate the use of Reinforcement Learning (RL) for dexterous object handover between two multi-finger hands. Key to this task is the use of a novel reward function based on dual quaternions to minimize the rotation distance, which outperforms other rotation representations such as Euler and rotation matrices. The robustness of the trained policy is experimentally evaluated by testing w.r.t. objects that are not included in the training distribution, and perturbations during the handover process. The results demonstrate that the trained policy successfully perform this task, achieving a total success rate of 94% in the best-case scenario after 100 experiments, thereby showing the robustness of our policy with novel objects. In addition, the best-case performance of the policy decreases by only 13.8% when the other robot moves during the handover, proving that our policy is also robust to this type of perturbation, which is common in real-world object handovers.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyTraverse: An off-road traversability framework with VLM and human operator in the loop</title>
<link>https://arxiv.org/abs/2506.16826</link>
<guid>https://arxiv.org/abs/2506.16826</guid>
<content:encoded><![CDATA[

arXiv:2506.16826v1 Announce Type: cross 
Abstract: Off-road traversability segmentation enables autonomous navigation with applications in search-and-rescue, military operations, wildlife exploration, and agriculture. Current frameworks struggle due to significant variations in unstructured environments and uncertain scene changes, and are not adaptive to be used for different robot types. We present AnyTraverse, a framework combining natural language-based prompts with human-operator assistance to determine navigable regions for diverse robotic vehicles. The system segments scenes for a given set of prompts and calls the operator only when encountering previously unexplored scenery or unknown class not part of the prompt in its region-of-interest, thus reducing active supervision load while adapting to varying outdoor scenes. Our zero-shot learning approach eliminates the need for extensive data collection or retraining. Our experimental validation includes testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate real-world deployment on multiple robot platforms. The results show that AnyTraverse performs better than GA-NAV and Off-seg while offering a vehicle-agnostic approach to off-road traversability that balances automation with targeted human supervision.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bandwidth Selectors on Semiparametric Bayesian Networks</title>
<link>https://arxiv.org/abs/2506.16844</link>
<guid>https://arxiv.org/abs/2506.16844</guid>
<content:encoded><![CDATA[

arXiv:2506.16844v1 Announce Type: cross 
Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and non-parametric probabilistic models, offering flexibility in learning complex data distributions from samples. In particular, kernel density estimators (KDEs) are employed for the non-parametric component. Under the assumption of data normality, the normal rule is used to learn the bandwidth matrix for the KDEs in SPBNs. This matrix is the key hyperparameter that controls the trade-off between bias and variance. However, real-world data often deviates from normality, potentially leading to suboptimal density estimation and reduced predictive performance. This paper first establishes the theoretical framework for the application of state-of-the-art bandwidth selectors and subsequently evaluates their impact on SPBN performance. We explore the approaches of cross-validation and plug-in selectors, assessing their effectiveness in enhancing the learning capability and applicability of SPBNs. To support this investigation, we have extended the open-source package PyBNesian for SPBNs with the additional bandwidth selection techniques and conducted extensive experimental analyses. Our results demonstrate that the proposed bandwidth selectors leverage increasing information more effectively than the normal rule, which, despite its robustness, stagnates with more data. In particular, unbiased cross-validation generally outperforms the normal rule, highlighting its advantage in high sample size scenarios.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control</title>
<link>https://arxiv.org/abs/2506.16856</link>
<guid>https://arxiv.org/abs/2506.16856</guid>
<content:encoded><![CDATA[

arXiv:2506.16856v1 Announce Type: cross 
Abstract: Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57\%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: https://github.com/little-snail-f/ParkFormer.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Importance of Being Lazy: Scaling Limits of Continual Learning</title>
<link>https://arxiv.org/abs/2506.16884</link>
<guid>https://arxiv.org/abs/2506.16884</guid>
<content:encoded><![CDATA[

arXiv:2506.16884v1 Announce Type: cross 
Abstract: Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</title>
<link>https://arxiv.org/abs/2506.16895</link>
<guid>https://arxiv.org/abs/2506.16895</guid>
<content:encoded><![CDATA[

arXiv:2506.16895v1 Announce Type: cross 
Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\unicode{x2013}$less than $1\%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Complementary Security Analysis using Large Language Models</title>
<link>https://arxiv.org/abs/2506.16899</link>
<guid>https://arxiv.org/abs/2506.16899</guid>
<content:encoded><![CDATA[

arXiv:2506.16899v1 Announce Type: cross 
Abstract: A key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing (SAST) tools. Numerous false positives (FPs) in these reports reduce the effectiveness of security analysis. We propose using Large Language Models (LLMs) to improve the assessment of SAST findings. We investigate the ability of LLMs to reduce FPs while trying to maintain a perfect true positive rate, using datasets extracted from the OWASP Benchmark (v1.2) and a real-world software project. Our results indicate that advanced prompting techniques, such as Chain-of-Thought and Self-Consistency, substantially improve FP detection. Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark dataset without missing genuine weaknesses. Combining detections from different LLMs would increase this FP detection to approximately 78.9%. Additionally, we demonstrate our approach's generalizability using a real-world dataset covering five SAST tools, three programming languages, and infrastructure files. The best LLM detected 33.85% of all FPs without missing genuine weaknesses, while combining detections from different LLMs would increase this detection to 38.46%. Our findings highlight the potential of LLMs to complement traditional SAST tools, enhancing automation and reducing resources spent addressing false alarms.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence</title>
<link>https://arxiv.org/abs/2506.16925</link>
<guid>https://arxiv.org/abs/2506.16925</guid>
<content:encoded><![CDATA[

arXiv:2506.16925v1 Announce Type: cross 
Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate an artificial intelligence approach for rapid, non-destructive estimation of the chemical potential and temperature from single-shot, in situ imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D `pancake' condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating thermodynamic parameters for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A deep learning and machine learning approach to predict neonatal death in the context of S\~ao Paulo</title>
<link>https://arxiv.org/abs/2506.16929</link>
<guid>https://arxiv.org/abs/2506.16929</guid>
<content:encoded><![CDATA[

arXiv:2506.16929v1 Announce Type: cross 
Abstract: Neonatal death is still a concerning reality for underdeveloped and even some developed countries. Worldwide data indicate that 26.693 babies out of 1,000 births die, according to Macro Trades. To reduce this number, early prediction of endangered babies is crucial. Such prediction enables the opportunity to take ample care of the child and mother so that early child death can be avoided. In this context, machine learning was used to determine whether a newborn baby is at risk. To train the predictive model, historical data of 1.4 million newborns was used. Machine learning and deep learning techniques such as logical regression, K-nearest neighbor, random forest classifier, extreme gradient boosting (XGBoost), convolutional neural network, and long short-term memory (LSTM) were implemented using the dataset to identify the most accurate model for predicting neonatal mortality. Among the machine learning algorithms, XGBoost and random forest classifier achieved the best accuracy with 94%, while among the deep learning models, LSTM delivered the highest accuracy with 99%. Therefore, using LSTM appears to be the most suitable approach to predict whether precautionary measures for a child are necessary.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2506.16962</link>
<guid>https://arxiv.org/abs/2506.16962</guid>
<content:encoded><![CDATA[

arXiv:2506.16962v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)</title>
<link>https://arxiv.org/abs/2506.16971</link>
<guid>https://arxiv.org/abs/2506.16971</guid>
<content:encoded><![CDATA[

arXiv:2506.16971v1 Announce Type: cross 
Abstract: The requirement for identifying accurate system representations has not only been a challenge to fulfill, but it has compromised the scalability of formal methods, as the resulting models are often too complex for effective decision making with formal correctness and performance guarantees. Focusing on probabilistic simulation relations and surrogate models of stochastic systems, we propose an approach that significantly enhances the scalability and practical applicability of such simulation relations by eliminating the need to compute error bounds directly. As a result, we provide an abstraction-based technique that scales effectively to higher dimensions while addressing complex nonlinear agent-environment interactions with infinite-horizon temporal logic guarantees amidst uncertainty. Our approach trades scalability for conservatism favorably, as demonstrated on a complex high-dimensional vehicle intersection case study.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Concept Disentanglement in Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2506.16975</link>
<guid>https://arxiv.org/abs/2506.16975</guid>
<content:encoded><![CDATA[

arXiv:2506.16975v1 Announce Type: cross 
Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond</title>
<link>https://arxiv.org/abs/2506.16982</link>
<guid>https://arxiv.org/abs/2506.16982</guid>
<content:encoded><![CDATA[

arXiv:2506.16982v1 Announce Type: cross 
Abstract: Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs</title>
<link>https://arxiv.org/abs/2506.16990</link>
<guid>https://arxiv.org/abs/2506.16990</guid>
<content:encoded><![CDATA[

arXiv:2506.16990v1 Announce Type: cross 
Abstract: LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instituto de Telecomunica\c{c}\~oes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning</title>
<link>https://arxiv.org/abs/2506.17019</link>
<guid>https://arxiv.org/abs/2506.17019</guid>
<content:encoded><![CDATA[

arXiv:2506.17019v1 Announce Type: cross 
Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing. We submit results for the Short Track, i.e., speech recognition, translation, and spoken question answering. Our model is a unified speech-to-text model that integrates a pre-trained continuous speech encoder and text decoder through a first phase of modality alignment and a second phase of instruction fine-tuning. Crucially, we focus on using small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY data along with synthetic data generation to supplement existing resources.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation</title>
<link>https://arxiv.org/abs/2506.17039</link>
<guid>https://arxiv.org/abs/2506.17039</guid>
<content:encoded><![CDATA[

arXiv:2506.17039v1 Announce Type: cross 
Abstract: Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data. We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2506.17041</link>
<guid>https://arxiv.org/abs/2506.17041</guid>
<content:encoded><![CDATA[

arXiv:2506.17041v1 Announce Type: cross 
Abstract: Benchmark datasets for network intrusion detection commonly rely on synthetically generated traffic, which fails to reflect the statistical variability and temporal drift encountered in operational environments. This paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset, designed to enable realistic and reproducible evaluation of anomaly detection methods. A reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforming to the CICFlowMeter format, while preserving MAWILab's original anomaly labels. The resulting datasets comprise temporally distinct samples from January 2011, 2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods, including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical results demonstrate that tree-based classifiers perform well on temporally static data but experience significant performance degradation over time. In contrast, the CNN-BiLSTM model maintains better performance, thus showing improved generalization. These findings underscore the limitations of synthetic benchmarks and static models, and motivate the adoption of realistic datasets with explicit temporal structure. All datasets, pipeline code, and model implementations are made publicly available to foster transparency and reproducibility.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers</title>
<link>https://arxiv.org/abs/2506.17052</link>
<guid>https://arxiv.org/abs/2506.17052</guid>
<content:encoded><![CDATA[

arXiv:2506.17052v1 Announce Type: cross 
Abstract: Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Based Non-stationary Temporal Regime Causal Structure Learning</title>
<link>https://arxiv.org/abs/2506.17065</link>
<guid>https://arxiv.org/abs/2506.17065</guid>
<content:encoded><![CDATA[

arXiv:2506.17065v1 Announce Type: cross 
Abstract: Understanding causal relationships in multivariate time series is crucial in many scenarios, such as those dealing with financial or neurological data. Many such time series exhibit multiple regimes, i.e., consecutive temporal segments with a priori unknown boundaries, with each regime having its own causal structure. Inferring causal dependencies and regime shifts is critical for analyzing the underlying processes. However, causal structure learning in this setting is challenging due to (1) non stationarity, i.e., each regime can have its own causal graph and mixing function, and (2) complex noise distributions, which may be non Gaussian or heteroscedastic. Existing causal discovery approaches cannot address these challenges, since generally assume stationarity or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified framework for causal discovery that handles non stationary processes along with non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the number of regimes and their corresponding indices and learns each regime's Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm that maximizes the evidence lower bound of the data log likelihood. On the theoretical side, we prove, under mild assumptions, that temporal heteroscedastic causal models, introduced in FANTOM's formulation, are identifiable in both stationary and non stationary settings. In addition, extensive experiments on synthetic and real data show that FANTOM outperforms existing methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI</title>
<link>https://arxiv.org/abs/2506.17073</link>
<guid>https://arxiv.org/abs/2506.17073</guid>
<content:encoded><![CDATA[

arXiv:2506.17073v1 Announce Type: cross 
Abstract: A wide range of participation is essential for democracy, as it helps prevent the dominance of extreme views, erosion of legitimacy, and political polarization. However, engagement in online political discussions often features a limited spectrum of views due to high levels of self-selection and the tendency of online platforms to facilitate exchanges primarily among like-minded individuals. This study examines whether an LLM-based bot can widen the scope of perspectives expressed by participants in online discussions through two pre-registered randomized experiments conducted in a chatroom. We evaluate the impact of a bot that actively monitors discussions, identifies missing arguments, and introduces them into the conversation. The results indicate that our bot significantly expands the range of arguments, as measured by both objective and subjective metrics. Furthermore, disclosure of the bot as AI does not significantly alter these effects. These findings suggest that LLM-based moderation tools can positively influence online political discourse.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2506.17080</link>
<guid>https://arxiv.org/abs/2506.17080</guid>
<content:encoded><![CDATA[

arXiv:2506.17080v1 Announce Type: cross 
Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifiability of Deep Polynomial Neural Networks</title>
<link>https://arxiv.org/abs/2506.17093</link>
<guid>https://arxiv.org/abs/2506.17093</guid>
<content:encoded><![CDATA[

arXiv:2506.17093v1 Announce Type: cross 
Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability -- a key property for ensuring interpretability -- remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. This yields both generic conditions determined by the architecture, and effective conditions that depend on the network's parameters. We also settle an open conjecture on the expected dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach its maximum.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransDreamerV3: Implanting Transformer In DreamerV3</title>
<link>https://arxiv.org/abs/2506.17103</link>
<guid>https://arxiv.org/abs/2506.17103</guid>
<content:encoded><![CDATA[

arXiv:2506.17103v1 Announce Type: cross 
Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder. The model is designed to improve memory and decision-making capabilities in complex environments. We conducted experiments on Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved performance over DreamerV3, particularly in the Atari-Freeway and Crafter tasks. While issues in the Minecraft task and limited training across all tasks were noted, TransDreamerV3 displays advancement in world model-based reinforcement learning, leveraging transformer architectures.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation</title>
<link>https://arxiv.org/abs/2506.17113</link>
<guid>https://arxiv.org/abs/2506.17113</guid>
<content:encoded><![CDATA[

arXiv:2506.17113v1 Announce Type: cross 
Abstract: Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model</title>
<link>https://arxiv.org/abs/2506.17128</link>
<guid>https://arxiv.org/abs/2506.17128</guid>
<content:encoded><![CDATA[

arXiv:2506.17128v1 Announce Type: cross 
Abstract: Trust is emerging as an effective tool to ensure the successful completion of collaborative tasks within collaborative systems. However, rapidly and continuously evaluating the trustworthiness of collaborators during task execution is a significant challenge due to distributed devices, complex operational environments, and dynamically changing resources. To tackle this challenge, this paper proposes a Siamese-enabled rapid and continuous trust evaluation framework (SRCTE) to facilitate effective task collaboration. First, the communication and computing resource attributes of the collaborator in a trusted state, along with historical collaboration data, are collected and represented using an attributed control flow graph (ACFG) that captures trust-related semantic information and serves as a reference for comparison with data collected during task execution. At each time slot of task execution, the collaborator's communication and computing resource attributes, as well as task completion effectiveness, are collected in real time and represented with an ACFG to convey their trust-related semantic information. A Siamese model, consisting of two shared-parameter Structure2vec networks, is then employed to learn the deep semantics of each pair of ACFGs and generate their embeddings. Finally, the similarity between the embeddings of each pair of ACFGs is calculated to determine the collaborator's trust value at each time slot. A real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to test the effectiveness of the proposed SRCTE framework. Experimental results demonstrate that SRCTE converges rapidly with only a small amount of data and achieves a high anomaly trust detection rate compared to the baseline algorithm.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Training with Data Augmentation for Medical Imaging Classification</title>
<link>https://arxiv.org/abs/2506.17133</link>
<guid>https://arxiv.org/abs/2506.17133</guid>
<content:encoded><![CDATA[

arXiv:2506.17133v1 Announce Type: cross 
Abstract: Deep neural networks are increasingly being used to detect and diagnose medical conditions using medical imaging. Despite their utility, these models are highly vulnerable to adversarial attacks and distribution shifts, which can affect diagnostic reliability and undermine trust among healthcare professionals. In this study, we propose a robust training algorithm with data augmentation (RTDA) to mitigate these vulnerabilities in medical image classification. We benchmark classifier robustness against adversarial perturbations and natural variations of RTDA and six competing baseline techniques, including adversarial training and data augmentation approaches in isolation and combination, using experimental data sets with three different imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that RTDA achieves superior robustness against adversarial attacks and improved generalization performance in the presence of distribution shift in each image classification task while maintaining high clean accuracy.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models</title>
<link>https://arxiv.org/abs/2506.17139</link>
<guid>https://arxiv.org/abs/2506.17139</guid>
<content:encoded><![CDATA[

arXiv:2506.17139v1 Announce Type: cross 
Abstract: Diffusion models have recently gained significant attention due to their effectiveness in various scientific domains, including biochemistry. When trained on equilibrium molecular distributions, diffusion models provide both: a generative procedure to sample equilibrium conformations and associated forces derived from the model's scores. However, using the forces for coarse-grained molecular dynamics simulations uncovers inconsistencies in the samples generated via classical diffusion inference and simulation, despite both originating from the same model. Particularly at the small diffusion timesteps required for simulations, diffusion models fail to satisfy the Fokker-Planck equation, which governs how the score should evolve over time. We interpret this deviation as an indication of the observed inconsistencies and propose an energy-based diffusion model with a Fokker-Planck-derived regularization term enforcing consistency. We demonstrate the effectiveness of our approach on toy systems, alanine dipeptide, and introduce a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and demonstrates enhanced consistency and efficient sampling.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification</title>
<link>https://arxiv.org/abs/2506.17140</link>
<guid>https://arxiv.org/abs/2506.17140</guid>
<content:encoded><![CDATA[

arXiv:2506.17140v1 Announce Type: cross 
Abstract: Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-of-concept towards better mitigating data biases with generative models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Need Large VLMs for Spotting Soccer Actions?</title>
<link>https://arxiv.org/abs/2506.17144</link>
<guid>https://arxiv.org/abs/2506.17144</guid>
<content:encoded><![CDATA[

arXiv:2506.17144v1 Announce Type: cross 
Abstract: Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. In this work, we propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich, fine-grained descriptions and contextual cues such as excitement and tactical insights, contains enough information to reliably spot key actions in a match. To demonstrate this, we use the SoccerNet Echoes dataset, which provides timestamped commentary, and employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics. Each LLM evaluates sliding windows of commentary to identify actions like goals, cards, and substitutions, generating accurate timestamps for these events. Our experiments show that this language-centric approach performs effectively in detecting critical match events, providing a lightweight and training-free alternative to traditional video-based methods for action spotting.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity</title>
<link>https://arxiv.org/abs/2506.17155</link>
<guid>https://arxiv.org/abs/2506.17155</guid>
<content:encoded><![CDATA[

arXiv:2506.17155v1 Announce Type: cross 
Abstract: In this paper, we investigate the use of small datasets in the context of offline reinforcement learning (RL). While many common offline RL benchmarks employ datasets with over a million data points, many offline RL applications rely on considerably smaller datasets. We show that offline RL algorithms can overfit on small datasets, resulting in poor performance. To address this challenge, we introduce "Sparse-Reg": a regularization technique based on sparsity to mitigate overfitting in offline reinforcement learning, enabling effective learning in limited data settings and outperforming state-of-the-art baselines in continuous control.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2506.17165</link>
<guid>https://arxiv.org/abs/2506.17165</guid>
<content:encoded><![CDATA[

arXiv:2506.17165v1 Announce Type: cross 
Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning with Columnar Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2506.17169</link>
<guid>https://arxiv.org/abs/2506.17169</guid>
<content:encoded><![CDATA[

arXiv:2506.17169v1 Announce Type: cross 
Abstract: This study investigates columnar-organized spiking neural networks (SNNs) for continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered Network), we show that microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning. We demonstrate how CoLaNET hyperparameters govern the trade-off between retaining old knowledge (stability) and acquiring new information (plasticity). Our optimal configuration learns ten sequential MNIST tasks effectively, maintaining 92% accuracy on each. It shows low forgetting, with only 4% performance degradation on the first task after training on nine subsequent tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI Search Paradigm</title>
<link>https://arxiv.org/abs/2506.17188</link>
<guid>https://arxiv.org/abs/2506.17188</guid>
<content:encoded><![CDATA[

arXiv:2506.17188v1 Announce Type: cross 
Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facial Landmark Visualization and Emotion Recognition Through Neural Networks</title>
<link>https://arxiv.org/abs/2506.17191</link>
<guid>https://arxiv.org/abs/2506.17191</guid>
<content:encoded><![CDATA[

arXiv:2506.17191v1 Announce Type: cross 
Abstract: Emotion recognition from facial images is a crucial task in human-computer interaction, enabling machines to learn human emotions through facial expressions. Previous studies have shown that facial images can be used to train deep learning models; however, most of these studies do not include a through dataset analysis. Visualizing facial landmarks can be challenging when extracting meaningful dataset insights; to address this issue, we propose facial landmark box plots, a visualization technique designed to identify outliers in facial datasets. Additionally, we compare two sets of facial landmark features: (i) the landmarks' absolute positions and (ii) their displacements from a neutral expression to the peak of an emotional expression. Our results indicate that a neural network achieves better performance than a random forest classifier.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.17204</link>
<guid>https://arxiv.org/abs/2506.17204</guid>
<content:encoded><![CDATA[

arXiv:2506.17204v1 Announce Type: cross 
Abstract: Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title>
<link>https://arxiv.org/abs/2506.17208</link>
<guid>https://arxiv.org/abs/2506.17208</guid>
<content:encoded><![CDATA[

arXiv:2506.17208v1 Announce Type: cross 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench Verified, have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries) leaderboards, analyzing 67 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.17212</link>
<guid>https://arxiv.org/abs/2506.17212</guid>
<content:encoded><![CDATA[

arXiv:2506.17212v1 Announce Type: cross 
Abstract: Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable parts.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation</title>
<link>https://arxiv.org/abs/2506.17213</link>
<guid>https://arxiv.org/abs/2506.17213</guid>
<content:encoded><![CDATA[

arXiv:2506.17213v1 Announce Type: cross 
Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens</title>
<link>https://arxiv.org/abs/2506.17218</link>
<guid>https://arxiv.org/abs/2506.17218</guid>
<content:encoded><![CDATA[

arXiv:2506.17218v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Free Lunch: Rethinking Internal Feedback for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.17219</link>
<guid>https://arxiv.org/abs/2506.17219</guid>
<content:encoded><![CDATA[

arXiv:2506.17219v1 Announce Type: cross 
Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry</title>
<link>https://arxiv.org/abs/2403.04311</link>
<guid>https://arxiv.org/abs/2403.04311</guid>
<content:encoded><![CDATA[

arXiv:2403.04311v2 Announce Type: replace 
Abstract: Compound AI applications chain together subcomponents such as generative language models, document retrievers, and embedding models. Applying traditional systems optimizations such as parallelism and pipelining in compound AI systems is difficult because each component has different constraints in terms of the granularity and type of data that it ingests. New data is often generated during intermediate computations, and text streams may be split into smaller, independent fragments (such as documents to sentences) which may then be re-aggregated at later parts of the computation. Due to this complexity, existing systems to serve compound AI queries do not fully take advantage of parallelism and pipelining opportunities.
  We present Alto, a framework that automatically optimizes execution of compound AI queries through streaming and parallelism. Bento introduces a new abstraction called nested ancestry, a metadata hierarchy that allows the system to correctly track partial outputs and aggregate data across the heterogeneous constraints of the components of compound AI applications. This metadata is automatically inferred from the programming model, allowing developers to express complex dataflow patterns without needing to reason manually about the details of routing and aggregation. Implementations of four applications in Alto outperform or match implementations in LangGraph, a popular existing AI programming framework. Alto implementations match or improve latency by between 10-30%.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Embodied Intelligence System</title>
<link>https://arxiv.org/abs/2404.10498</link>
<guid>https://arxiv.org/abs/2404.10498</guid>
<content:encoded><![CDATA[

arXiv:2404.10498v2 Announce Type: replace 
Abstract: Embodied intelligence (EI) enables manufacturing systems to flexibly perceive, reason, adapt, and operate within dynamic shop floor environments. In smart manufacturing, a representative EI scenario is robotic visual inspection, where industrial robots must accurately inspect components on rapidly changing, heterogeneous production lines. This task requires both high inference accuracy especially for uncommon defects and low latency to match production speeds, despite evolving lighting, part geometries, and surface conditions. To meet these needs, we propose LAECIPS, a large vision model-assisted adaptive edge-cloud collaboration framework for IoT-based embodied intelligence systems. LAECIPS decouples large vision models in the cloud from lightweight models on the edge, enabling plug-and-play model adaptation and continual learning. Through a hard input mining-based inference strategy, LAECIPS routes complex and uncertain inspection cases to the cloud while handling routine tasks at the edge, achieving both high accuracy and low latency. Experiments conducted on a real-world robotic semantic segmentation system for visual inspection demonstrate significant improvements in accuracy, processing latency, and communication overhead compared to state-of-the-art methods. LAECIPS provides a practical and scalable foundation for embodied intelligence in smart manufacturing, especially in adaptive robotic inspection and quality control scenarios.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POV Learning: Individual Alignment of Multimodal Models using Human Perception</title>
<link>https://arxiv.org/abs/2405.04443</link>
<guid>https://arxiv.org/abs/2405.04443</guid>
<content:encoded><![CDATA[

arXiv:2405.04443v2 Announce Type: replace 
Abstract: Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback. This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data. However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably. Since perception differs for each person, the same situation is observed differently. Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ. We hypothesize that individual perception patterns can be used for improving the alignment on an individual level. We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments. For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer. Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment. It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-effective Instruction Learning for Pathology Vision and Language Analysis</title>
<link>https://arxiv.org/abs/2407.17734</link>
<guid>https://arxiv.org/abs/2407.17734</guid>
<content:encoded><![CDATA[

arXiv:2407.17734v2 Announce Type: replace 
Abstract: The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying artificial intelligence through algorithmic generalization</title>
<link>https://arxiv.org/abs/2411.05943</link>
<guid>https://arxiv.org/abs/2411.05943</guid>
<content:encoded><![CDATA[

arXiv:2411.05943v2 Announce Type: replace 
Abstract: The rapid development of artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, AI systems fall short on tests requiring algorithmic reasoning -- a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no theoretical framework exists to quantify algorithmic reasoning in AI systems. Here, we adopt a framework from computational complexity theory to quantify algorithmic generalization using algebraic expressions: algebraic circuit complexity. Algebraic circuit complexity theory -- the study of algebraic expressions as circuit models -- is a natural framework to study the complexity of algorithmic computation. Algebraic circuit complexity enables the study of generalization by defining benchmarks in terms of the computational requirements to solve a problem. Moreover, algebraic circuits are generic mathematical objects; an arbitrarily large number of samples can be generated for a specified circuit, making it an ideal experimental sandbox for the data-hungry models that are used today. In this Perspective, we adopt tools from algebraic circuit complexity, apply them to formalize a science of algorithmic generalization, and address key challenges for its successful application to AI science.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic dataset shift identification to support safe deployment of medical imaging AI</title>
<link>https://arxiv.org/abs/2411.07940</link>
<guid>https://arxiv.org/abs/2411.07940</guid>
<content:encoded><![CDATA[

arXiv:2411.07940v3 Announce Type: replace 
Abstract: Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, the root causes of dataset shifts are diverse, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework for imaging datasets, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts using five large publicly available datasets.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning</title>
<link>https://arxiv.org/abs/2412.18091</link>
<guid>https://arxiv.org/abs/2412.18091</guid>
<content:encoded><![CDATA[

arXiv:2412.18091v2 Announce Type: replace 
Abstract: As deep neural networks (DNNs) are increasingly deployed on edge devices, optimizing models for constrained computational resources is critical. Existing auto-pruning methods face challenges due to the diversity of DNN models, various operators (e.g., filters), and the difficulty in balancing pruning granularity with model accuracy. To address these limitations, we introduce AutoSculpt, a pattern-based automated pruning framework designed to enhance efficiency and accuracy by leveraging graph learning and deep reinforcement learning (DRL). AutoSculpt automatically identifies and prunes regular patterns within DNN architectures that can be recognized by existing inference engines, enabling runtime acceleration. Three key steps in AutoSculpt include: (1) Constructing DNNs as graphs to encode their topology and parameter dependencies, (2) embedding computationally efficient pruning patterns, and (3) utilizing DRL to iteratively refine auto-pruning strategies until the optimal balance between compression and accuracy is achieved. Experimental results demonstrate the effectiveness of AutoSculpt across various architectures, including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming all baselines. The codes can be available at https://anonymous.4open.science/r/AutoSculpt-DDA0
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDS: Knowledge Component-Driven Data Synthesis Guided by Cognitive Diagnosis Theory</title>
<link>https://arxiv.org/abs/2501.07674</link>
<guid>https://arxiv.org/abs/2501.07674</guid>
<content:encoded><![CDATA[

arXiv:2501.07674v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved significant advancements, but the increasing complexity of tasks and higher performance demands highlight the need for continuous improvement. Some approaches utilize synthetic data generated by advanced LLMs based on evaluation results to train models. However, conventional evaluation methods fail to provide detailed, fine-grained profiles of LLMs, limiting their guidance for data synthesis. In this paper, we introduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a diagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine evaluation results and characterize model profiles at the knowledge component level. Based on these diagnostics, we propose two diagnosis-synthesis strategies for weakness-targeted data synthesis. Additionally, we present an enhanced data augmentation and selection pipeline to improve the quality and diversity of synthesized data. Our experiments with several open-source models show significant improvements across multiple benchmarks, achieving up to 6.00% improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in academic exams. Code and data are available on GitHub.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infrastructure for AI Agents</title>
<link>https://arxiv.org/abs/2501.10114</link>
<guid>https://arxiv.org/abs/2501.10114</guid>
<content:encoded><![CDATA[

arXiv:2501.10114v3 Announce Type: replace 
Abstract: AI agents plan and execute interactions in open-ended environments. For example, OpenAI's Operator can use a web browser to do product comparisons and buy online goods. Much research on making agents useful and safe focuses on directly modifying their behaviour, such as by training them to follow user instructions. Direct behavioural modifications are useful, but do not fully address how heterogeneous agents will interact with each other and other actors. Rather, we will need external protocols and systems to shape such interactions. For instance, agents will need more efficient protocols to communicate with each other and form agreements. Attributing an agent's actions to a particular human or other legal entity can help to establish trust, and also disincentivize misuse. Given this motivation, we propose the concept of \textbf{agent infrastructure}: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Just as the Internet relies on protocols like HTTPS, our work argues that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We provide an incomplete catalog of research directions for such functions. For each direction, we include analysis of use cases, infrastructure adoption, relationships to existing (internet) infrastructure, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning</title>
<link>https://arxiv.org/abs/2501.15602</link>
<guid>https://arxiv.org/abs/2501.15602</guid>
<content:encoded><![CDATA[

arXiv:2501.15602v3 Announce Type: replace 
Abstract: Test-time scaling, which is also often referred to as slow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code at https://github.com/ZyGan1999/Snowball-Errors-and-Probability.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nature Language Model: Deciphering the Language of Nature for Scientific Discovery</title>
<link>https://arxiv.org/abs/2502.07527</link>
<guid>https://arxiv.org/abs/2502.07527</guid>
<content:encoded><![CDATA[

arXiv:2502.07527v3 Announce Type: replace 
Abstract: Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, RNA and even cells. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) top performance across different domains, matching or surpassing state-of-the-art specialist models. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Indoor Navigation with Multimodal Map Understanding</title>
<link>https://arxiv.org/abs/2503.11702</link>
<guid>https://arxiv.org/abs/2503.11702</guid>
<content:encoded><![CDATA[

arXiv:2503.11702v4 Announce Type: replace 
Abstract: Indoor navigation presents unique challenges due to complex layouts and the unavailability of GNSS signals. Existing solutions often struggle with contextual adaptation, and typically require dedicated hardware. In this work, we explore the potential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural, context-aware navigation instructions from indoor map images. We design and evaluate test cases across different real-world environments, analyzing the effectiveness of LLMs in interpreting spatial layouts, handling user constraints, and planning efficient routes. Our findings demonstrate the potential of LLMs for supporting personalized indoor navigation, with an average of 86.59% correct indications and a maximum of 97.14%. The proposed system achieves high accuracy and reasoning performance. These results have key implications for AI-driven navigation and assistive technologies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</title>
<link>https://arxiv.org/abs/2503.19990</link>
<guid>https://arxiv.org/abs/2503.19990</guid>
<content:encoded><![CDATA[

arXiv:2503.19990v3 Announce Type: replace 
Abstract: Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we design generation tasks to investigate whether MLLMs can transfer their spatial understanding and reasoning abilities to image generation. Our experiments show that only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.09440</link>
<guid>https://arxiv.org/abs/2504.09440</guid>
<content:encoded><![CDATA[

arXiv:2504.09440v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGI-Driven Generative Semantic Communications: Principles and Practices</title>
<link>https://arxiv.org/abs/2504.14947</link>
<guid>https://arxiv.org/abs/2504.14947</guid>
<content:encoded><![CDATA[

arXiv:2504.14947v2 Announce Type: replace 
Abstract: Semantic communications leverage artificial intelligence (AI) technologies to extract semantic information for efficient data delivery, thereby significantly reducing communication cost. With the evolution towards artificial general intelligence (AGI), the increasing demands for AGI services pose new challenges to semantic communications. In this context, an AGI application is typically defined on a general-sense task, covering a broad, even unforeseen, set of objectives, as well as driven by the need for a human-friendly interface in forms (e.g., videos, images, or text) easily understood by human users.In response, we introduce an AGI-driven communication paradigm for supporting AGI applications, called generative semantic communication (GSC). We first describe the basic concept of GSC and its difference from existing semantic communications, and then introduce a general framework of GSC based on advanced AI technologies including foundation models and generative models. Two case studies are presented to verify the advantages of GSC. Finally, open challenges and new research directions are discussed to stimulate this line of research and pave the way for practical applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation</title>
<link>https://arxiv.org/abs/2504.15699</link>
<guid>https://arxiv.org/abs/2504.15699</guid>
<content:encoded><![CDATA[

arXiv:2504.15699v3 Announce Type: replace 
Abstract: Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage</title>
<link>https://arxiv.org/abs/2504.20007</link>
<guid>https://arxiv.org/abs/2504.20007</guid>
<content:encoded><![CDATA[

arXiv:2504.20007v3 Announce Type: replace 
Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating image, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. The framework incorporates speaker separation, transcription, and large language models (LLMs) to produce structured, interpretable summaries of police-civilian encounters. We also employ a custom evaluation pipeline to assess transcription quality and behavior detection accuracy in high-stakes, real-world policing scenarios. Our methodology, computational techniques, and findings outline a practical approach for law enforcement review, training, and accountability processes while advancing the frontiers of knowledge discovery from complex police BWC data.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs</title>
<link>https://arxiv.org/abs/2505.09518</link>
<guid>https://arxiv.org/abs/2505.09518</guid>
<content:encoded><![CDATA[

arXiv:2505.09518v2 Announce Type: replace 
Abstract: Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs.We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs, and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale</title>
<link>https://arxiv.org/abs/2505.20094</link>
<guid>https://arxiv.org/abs/2505.20094</guid>
<content:encoded><![CDATA[

arXiv:2505.20094v2 Announce Type: replace 
Abstract: Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Path to Multimodal Historical Reasoning: HistBench and HistAgent</title>
<link>https://arxiv.org/abs/2505.20246</link>
<guid>https://arxiv.org/abs/2505.20246</guid>
<content:encoded><![CDATA[

arXiv:2505.20246v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness</title>
<link>https://arxiv.org/abs/2505.22960</link>
<guid>https://arxiv.org/abs/2505.22960</guid>
<content:encoded><![CDATA[

arXiv:2505.22960v2 Announce Type: replace 
Abstract: The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents</title>
<link>https://arxiv.org/abs/2506.00618</link>
<guid>https://arxiv.org/abs/2506.00618</guid>
<content:encoded><![CDATA[

arXiv:2506.00618v3 Announce Type: replace 
Abstract: With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce \textbf{RiOSWorld}, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on \textbf{RiOSWorld} demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback</title>
<link>https://arxiv.org/abs/2506.04287</link>
<guid>https://arxiv.org/abs/2506.04287</guid>
<content:encoded><![CDATA[

arXiv:2506.04287v2 Announce Type: replace 
Abstract: Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness. However, creating the training dataset for their skill acquisition faces several challenges. Manual trajectory collection requires significant human effort. Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible. Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks. To address this, we propose a novel automatic skill discovery framework EXIF for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents' capabilities. Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment. Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob. Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob's performance to identify areas for improvement. This feedback then guides Alice's next round of exploration, forming a closed-loop data generation process. Experiments on Webshop and Crafter demonstrate EXIF's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements. Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF's potential for building a self-evolving system.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Imperative: Scaling High-Quality Peer Review in Machine Learning</title>
<link>https://arxiv.org/abs/2506.08134</link>
<guid>https://arxiv.org/abs/2506.08134</guid>
<content:encoded><![CDATA[

arXiv:2506.08134v2 Announce Type: replace 
Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation</title>
<link>https://arxiv.org/abs/2506.08898</link>
<guid>https://arxiv.org/abs/2506.08898</guid>
<content:encoded><![CDATA[

arXiv:2506.08898v2 Announce Type: replace 
Abstract: Recent deep reinforcement learning methods have achieved remarkable success in solving multi-objective combinatorial optimization problems (MOCOPs) by decomposing them into multiple subproblems, each associated with a specific weight vector. However, these methods typically treat all subproblems equally and solve them using a single model, hindering the effective exploration of the solution space and thus leading to suboptimal performance. To overcome the limitation, we propose POCCO, a novel plug-and-play framework that enables adaptive selection of model structures for subproblems, which are subsequently optimized based on preference signals rather than explicit reward values. Specifically, we design a conditional computation block that routes subproblems to specialized neural architectures. Moreover, we propose a preference-driven optimization algorithm that learns pairwise preferences between winning and losing solutions. We evaluate the efficacy and versatility of POCCO by applying it to two state-of-the-art neural methods for MOCOPs. Experimental results across four classic MOCOP benchmarks demonstrate its significant superiority and strong generalization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voices of Her: Analyzing Gender Differences in the AI Publication World</title>
<link>https://arxiv.org/abs/2305.14597</link>
<guid>https://arxiv.org/abs/2305.14597</guid>
<content:encoded><![CDATA[

arXiv:2305.14597v2 Announce Type: replace-cross 
Abstract: While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation</title>
<link>https://arxiv.org/abs/2308.12053</link>
<guid>https://arxiv.org/abs/2308.12053</guid>
<content:encoded><![CDATA[

arXiv:2308.12053v3 Announce Type: replace-cross 
Abstract: Gradient-based optimization has been a cornerstone of machine learning that enabled the vast advances of Artificial Intelligence (AI) development over the past decades. However, this type of optimization requires differentiation, and with recent evidence of the benefits of non-differentiable (e.g. neuromorphic) architectures over classical models w.r.t. efficiency, such constraints can become limiting in the future. We present Layer-wise Feedback Propagation (LFP), a novel training principle for neural network-like predictors that utilizes methods from the domain of explainability to decompose a reward to individual neurons based on their respective contributions. Leveraging these neuron-wise rewards, our method then implements a greedy approach reinforcing helpful parts of the network and weakening harmful ones. While having comparable computational complexity to gradient descent, LFP does not require gradient computation and generates sparse and thereby memory- and energy-efficient parameter updates and models. We establish the convergence of LFP theoretically and empirically, demonstrating its effectiveness on various models and datasets. Via two applications - neural network pruning and the approximation-free training of Spiking Neural Networks (SNNs) - we demonstrate that LFP combines increased efficiency in terms of computation and representation with flexibility w.r.t. choice of model architecture and objective function. Our code is available at https://github.com/leanderweber/layerwise-feedback-propagation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set Graph Anomaly Detection via Normal Structure Regularisation</title>
<link>https://arxiv.org/abs/2311.06835</link>
<guid>https://arxiv.org/abs/2311.06835</guid>
<content:encoded><![CDATA[

arXiv:2311.06835v5 Announce Type: replace-cross 
Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2312.04684</link>
<guid>https://arxiv.org/abs/2312.04684</guid>
<content:encoded><![CDATA[

arXiv:2312.04684v4 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting is a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. This approach is theoretically grounded and compute-efficient, eliminating the need for auxiliary LLM inference or manual prompt design. Empirical results demonstrate that LaRS consistently outperforms SOTA skill-based selection methods, processing example banks four times faster, reducing LLM inferences during the selection stage by half, and showing greater robustness to sub-optimal example banks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Experimental Design for Policy Learning</title>
<link>https://arxiv.org/abs/2401.03756</link>
<guid>https://arxiv.org/abs/2401.03756</guid>
<content:encoded><![CDATA[

arXiv:2401.03756v4 Announce Type: replace-cross 
Abstract: This study investigates the contextual best arm identification (BAI) problem, aiming to design an adaptive experiment to identify the best treatment arm conditioned on contextual information (covariates). We consider a decision-maker who assigns treatment arms to experimental units during an experiment and recommends the estimated best treatment arm based on the contexts at the end of the experiment. The decision-maker uses a policy for recommendations, which is a function that provides the estimated best treatment arm given the contexts. In our evaluation, we focus on the worst-case expected regret, a relative measure between the expected outcomes of an optimal policy and our proposed policy. We derive a lower bound for the expected simple regret and then propose a strategy called Adaptive Sampling-Policy Learning (PLAS). We prove that this strategy is minimax rate-optimal in the sense that its leading factor in the regret upper bound matches the lower bound as the number of experimental units increases.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges</title>
<link>https://arxiv.org/abs/2402.08801</link>
<guid>https://arxiv.org/abs/2402.08801</guid>
<content:encoded><![CDATA[

arXiv:2402.08801v2 Announce Type: replace-cross 
Abstract: Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to (i) quantify the reliability of LLMs answers and their potential to replace Stack Overflow in the long term; (ii) identify and understand why LLMs fail; (iii) measure users activity evolution with Stack Overflow over time; and (iv) compare LLMs together. Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs and provide guidelines for future challenges faced by users and researchers.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability</title>
<link>https://arxiv.org/abs/2402.09404</link>
<guid>https://arxiv.org/abs/2402.09404</guid>
<content:encoded><![CDATA[

arXiv:2402.09404v2 Announce Type: replace-cross 
Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small models' performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention</title>
<link>https://arxiv.org/abs/2403.10173</link>
<guid>https://arxiv.org/abs/2403.10173</guid>
<content:encoded><![CDATA[

arXiv:2403.10173v4 Announce Type: replace-cross 
Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. While Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy-efficient and low latency event-based data processing, they often fall short of Artificial Neural Networks (ANNs) in accuracy and flexibility. Here, we introduce Attention-based Hybrid SNN-ANN backbones for event-based object detection to leverage the strengths of both SNN and ANN architectures. A novel Attention-based SNN-ANN bridge module captures sparse spatial and temporal relations from the SNN layer and converts them into dense feature maps for the ANN part of the backbone. Additionally, we present a variant that integrates DWConvL-STMs to the ANN blocks to capture slower dynamics. This multi-timescale network combines fast SNN processing for short timesteps with long-term dense RNN processing, effectively capturing both fast and slow dynamics. Experimental results demonstrate that our proposed method surpasses SNN-based approaches by significant margins, with results comparable to existing ANN and RNN-based methods. Unlike ANN-only networks, the hybrid setup allows us to implement the SNN blocks on digital neuromorphic hardware to investigate the feasibility of our approach. Extensive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. Our hybrid SNN-ANN architectures pave the way for ANN-like performance at a drastically reduced parameter, latency, and power budget.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatDBG: Augmenting Debugging with Large Language Models</title>
<link>https://arxiv.org/abs/2403.16354</link>
<guid>https://arxiv.org/abs/2403.16354</guid>
<content:encoded><![CDATA[

arXiv:2403.16354v5 Announce Type: replace-cross 
Abstract: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Automatic Hallucination Evaluation on Natural Language Generation</title>
<link>https://arxiv.org/abs/2404.12041</link>
<guid>https://arxiv.org/abs/2404.12041</guid>
<content:encoded><![CDATA[

arXiv:2404.12041v3 Announce Type: replace-cross 
Abstract: The proliferation of Large Language Models (LLMs) has introduced a critical challenge: accurate hallucination evaluation that ensures model reliability. While Automatic Hallucination Evaluation (AHE) has emerged as essential, the field suffers from methodological fragmentation, hindering both theoretical understanding and practical advancement. This survey addresses this critical gap through a comprehensive analysis of 74 evaluation methods, revealing that 74% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a unified evaluation pipeline encompassing datasets and benchmarks, evidence collection strategies, and comparison mechanisms, systematically documenting the evolution from pre-LLM to post-LLM methodologies. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency</title>
<link>https://arxiv.org/abs/2404.15564</link>
<guid>https://arxiv.org/abs/2404.15564</guid>
<content:encoded><![CDATA[

arXiv:2404.15564v2 Announce Type: replace-cross 
Abstract: This paper proposes a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. We utilize both positive and negative gradient magnitudes and employ gradient variance to distinguish the important areas for noise deduction. We also introduce a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. We propose two propositions for these two objectives and prove the necessity of evaluating them. We evaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset with ResNet50 model; (2) International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161 model. Our method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEADs: Bias Evaluation Across Domains</title>
<link>https://arxiv.org/abs/2406.04220</link>
<guid>https://arxiv.org/abs/2406.04220</guid>
<content:encoded><![CDATA[

arXiv:2406.04220v5 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly improved natural language processing (NLP) applications. However, these models often inherit biases from their training data. While several datasets exist for bias detection, most are limited to one or two NLP tasks, typically classification or evaluation, and lack comprehensive coverage across a broader range of tasks. To address this gap, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key contribution of this work is the gold-standard annotation provided by GPT-4 for scalability, with expert verification to ensure high reliability. BEADs can be used for both fine-tuning models (for classification and generation tasks) and evaluating LLM behavior. Our findings show that BEADs effectively surfaces various biases during model fine-tuning and helps reduce biases in language generation tasks while maintaining output quality. The dataset also highlights prevalent demographic biases in LLMs during evaluation. We release BEADs as a practical resource for detecting and mitigating bias across domains, supporting the development of responsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data: https://huggingface.co/datasets/shainar/BEAD
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Thinking and Logical Processing -- Are Multi-modal Large Language Models Closing the Gap with Human Vision ?</title>
<link>https://arxiv.org/abs/2406.06967</link>
<guid>https://arxiv.org/abs/2406.06967</guid>
<content:encoded><![CDATA[

arXiv:2406.06967v4 Announce Type: replace-cross 
Abstract: The dual thinking framework considers fast, intuitive, and slower logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ, and the latter is under-explored in current studies. We introduce a novel adversarial dataset to provide evidence for the dual thinking framework in human vision, which also facilitates the study of the qualitative behavior of deep learning models. Our psychophysical studies show the presence of multiple inferences in rapid succession, and analysis of errors shows that the early stopping of visual processing can result in missing relevant information. MLLMs (Multi-modal Large Language Models) and VLMs (Vision Language Models) have made significant progress in correcting errors in intuitive processing in human vision and showed enhanced performance on images requiring logical processing. However, their improvements in logical processing have not kept pace with their advancements in intuitive processing. In contrast, segmentation models exhibit errors similar to those seen in intuitive human processing and lack understanding of sub-structures, as indicated by errors related to sub-components in identified instances. As AI (Artificial Intelligence)-based systems find increasing applications in safety-critical domains like autonomous driving, the integration of logical processing capabilities becomes essential. This not only enhances performance but also addresses the limitations of scaling-based approaches while ensuring robustness and reliability in real-world environments.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval</title>
<link>https://arxiv.org/abs/2406.12593</link>
<guid>https://arxiv.org/abs/2406.12593</guid>
<content:encoded><![CDATA[

arXiv:2406.12593v3 Announce Type: replace-cross 
Abstract: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach</title>
<link>https://arxiv.org/abs/2407.03146</link>
<guid>https://arxiv.org/abs/2407.03146</guid>
<content:encoded><![CDATA[

arXiv:2407.03146v4 Announce Type: replace-cross 
Abstract: Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed, it may have an unfair effect in multi-class classification. While data augmentation generally improves the overall performance (and therefore is beneficial for many classes), it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method. To derive it, we first formulate the training of a classifier as a non-linear optimization problem that aims at simultaneously maximizing the individual class performances and balancing them. By rewriting this optimization problem as an adversarial two-player game, we propose a novel multiplicative weight algorithm, for which we prove the convergence. Interestingly, our formulation also reveals that the class-dependent effects of data augmentation is not due to data augmentation only, but is in fact a general phenomenon. Our empirical results over six datasets demonstrate that the performance of learned classifiers is indeed more fairly distributed over classes, with only limited impact on the average accuracy.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</title>
<link>https://arxiv.org/abs/2408.08872</link>
<guid>https://arxiv.org/abs/2408.08872</guid>
<content:encoded><![CDATA[

arXiv:2408.08872v3 Announce Type: replace-cross 
Abstract: This paper introduces BLIP-3, an open framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. We release 4B and 14B models, including both the pre-trained base model and the instruction fine-tuned ones. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our models demonstrate competitive performance among open-source LMMs with similar model sizes. Our resulting LMMs demonstrate competitive performance among open-source LMMs with similar model sizes, with the ability to comprehend interleaved image-text inputs. Our training code, models, and all datasets used in this work, including the three largescale datasets we create and the preprocessed ones, will be open-sourced to better support the research community.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2408.09251</link>
<guid>https://arxiv.org/abs/2408.09251</guid>
<content:encoded><![CDATA[

arXiv:2408.09251v3 Announce Type: replace-cross 
Abstract: Vehicle-to-everything (V2X) cooperation has emerged as a promising paradigm to overcome the perception limitations of classical autonomous driving by leveraging information from both ego-vehicle and infrastructure sensors. However, effectively fusing heterogeneous visual and semantic information while ensuring robust trajectory planning remains a significant challenge. This paper introduces V2X-VLM, a novel end-to-end (E2E) cooperative autonomous driving framework based on vision-language models (VLMs). V2X-VLM integrates multiperspective camera views from vehicles and infrastructure with text-based scene descriptions to enable a more comprehensive understanding of driving environments. Specifically, we propose a contrastive learning-based mechanism to reinforce the alignment of heterogeneous visual and textual characteristics, which enhances the semantic understanding of complex driving scenarios, and employ a knowledge distillation strategy to stabilize training. Experiments on a large real-world dataset demonstrate that V2X-VLM achieves state-of-the-art trajectory planning accuracy, significantly reducing L2 error and collision rate compared to existing cooperative autonomous driving baselines. Ablation studies validate the contributions of each component. Moreover, the evaluation of robustness and efficiency highlights the practicality of V2X-VLM for real-world deployment to enhance overall autonomous driving safety and decision-making.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogProber: Disentangling confidence from contamination in LLM responses</title>
<link>https://arxiv.org/abs/2408.14352</link>
<guid>https://arxiv.org/abs/2408.14352</guid>
<content:encoded><![CDATA[

arXiv:2408.14352v3 Announce Type: replace-cross 
Abstract: In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical. In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management</title>
<link>https://arxiv.org/abs/2409.00128</link>
<guid>https://arxiv.org/abs/2409.00128</guid>
<content:encoded><![CDATA[

arXiv:2409.00128v3 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) have shown promise in replicating human-like responses in various psychological experiments. We conducted a large-scale study replicating 156 psychological experiments from top social science journals using three state-of-the-art LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies. Notably, LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics. When original studies reported null findings, LLMs produced significant results at remarkably high rates (68-83%) - while this could reflect cleaner data with less noise, as evidenced by narrower confidence intervals, it also suggests potential risks of effect size overestimation. Our results demonstrate both the promise and challenges of LLMs in psychological research, offering efficient tools for pilot testing and rapid hypothesis validation while enriching rather than replacing traditional human subject studies, yet requiring more nuanced interpretation and human validation for complex social phenomena and culturally sensitive research questions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2409.13609</link>
<guid>https://arxiv.org/abs/2409.13609</guid>
<content:encoded><![CDATA[

arXiv:2409.13609v4 Announce Type: replace-cross 
Abstract: Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters. Our code is available at https://github.com/liuting20/MaPPER.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Knowledge Deficits in Multi-Modal Language Models</title>
<link>https://arxiv.org/abs/2410.10855</link>
<guid>https://arxiv.org/abs/2410.10855</guid>
<content:encoded><![CDATA[

arXiv:2410.10855v4 Announce Type: replace-cross 
Abstract: While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge--rudimentary cognitive abilities innate to humans from early childhood. To explore the core knowledge representation in MLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science. We evaluate 230 models with 11 different prompts, leading to a total of 2,530 data points for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones. Finally, we propose Concept Hacking, a novel controlled evaluation method that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Route LLMs with Confidence Tokens</title>
<link>https://arxiv.org/abs/2410.13284</link>
<guid>https://arxiv.org/abs/2410.13284</guid>
<content:encoded><![CDATA[

arXiv:2410.13284v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-Reflection with Error-based Feedback (Self-REF), a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALTA: Compiler-Based Analysis of Transformers</title>
<link>https://arxiv.org/abs/2410.18077</link>
<guid>https://arxiv.org/abs/2410.18077</guid>
<content:encoded><![CDATA[

arXiv:2410.18077v2 Announce Type: replace-cross 
Abstract: We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Mixture-of-Expert for Video-based Driver State and Physiological Multi-task Estimation in Conditional Autonomous Driving</title>
<link>https://arxiv.org/abs/2410.21086</link>
<guid>https://arxiv.org/abs/2410.21086</guid>
<content:encoded><![CDATA[

arXiv:2410.21086v2 Announce Type: replace-cross 
Abstract: Road safety remains a critical challenge worldwide, with approximately 1.35 million fatalities annually attributed to traffic accidents, often due to human errors. As we advance towards higher levels of vehicle automation, challenges still exist, as driving with automation can cognitively over-demand drivers if they engage in non-driving-related tasks (NDRTs), or lead to drowsiness if driving was the sole task. This calls for the urgent need for an effective Driver Monitoring System (DMS) that can evaluate cognitive load and drowsiness in SAE Level-2/3 autonomous driving contexts. In this study, we propose a novel multi-task DMS, termed VDMoE, which leverages RGB video input to monitor driver states non-invasively. By utilizing key facial features to minimize computational load and integrating remote Photoplethysmography (rPPG) for physiological insights, our approach enhances detection accuracy while maintaining efficiency. Additionally, we optimize the Mixture-of-Experts (MoE) framework to accommodate multi-modal inputs and improve performance across different tasks. A novel prior-inclusive regularization method is introduced to align model outputs with statistical priors, thus accelerating convergence and mitigating overfitting risks. We validate our method with the creation of a new dataset (MCDD), which comprises RGB video and physiological indicators from 42 participants, and two public datasets. Our findings demonstrate the effectiveness of VDMoE in monitoring driver states, contributing to safer autonomous driving systems. The code and data will be released.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system</title>
<link>https://arxiv.org/abs/2410.21349</link>
<guid>https://arxiv.org/abs/2410.21349</guid>
<content:encoded><![CDATA[

arXiv:2410.21349v5 Announce Type: replace-cross 
Abstract: Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in coding scenarios. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels. From the global level, long-term memory improves code quality by retaining and applying learned knowledge. At the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model's adaptability across diverse code generation tasks. Extensive experiments demonstrate that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at https://github.com/titurte/FALCON.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation</title>
<link>https://arxiv.org/abs/2411.00412</link>
<guid>https://arxiv.org/abs/2411.00412</guid>
<content:encoded><![CDATA[

arXiv:2411.00412v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate promising capabilities in solving scientific problems but often suffer from the issue of hallucination. While integrating LLMs with tools can mitigate this issue, models fine-tuned on tool usage become overreliant on them and incur unnecessary costs. Inspired by how human experts assess problem complexity before selecting solutions, we propose a novel two-component fine-tuning method, Adapting While Learning (AWL). In the first component, World Knowledge Learning (WKL), LLMs internalize scientific knowledge by learning from tool-generated solutions. In the second component, Tool Usage Adaptation (TUA), we categorize problems as easy or hard based on the model's accuracy, and train it to maintain direct reasoning for easy problems while switching to tools for hard ones. We validate our method on six scientific benchmark datasets across climate science, epidemiology, physics, and other domains. Compared to the original instruct model (8B), models post-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on four custom-created datasets. Our code is open-source at https://github.com/Rose-STL-Lab/Adapting-While-Learning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning</title>
<link>https://arxiv.org/abs/2411.04105</link>
<guid>https://arxiv.org/abs/2411.04105</guid>
<content:encoded><![CDATA[

arXiv:2411.04105v4 Announce Type: replace-cross 
Abstract: Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</title>
<link>https://arxiv.org/abs/2411.05261</link>
<guid>https://arxiv.org/abs/2411.05261</guid>
<content:encoded><![CDATA[

arXiv:2411.05261v3 Announce Type: replace-cross 
Abstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term "cyclic manipulation". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web Archives Metadata Generation with GPT-4o: Challenges and Insights</title>
<link>https://arxiv.org/abs/2411.05409</link>
<guid>https://arxiv.org/abs/2411.05409</guid>
<content:encoded><![CDATA[

arXiv:2411.05409v3 Announce Type: replace-cross 
Abstract: Current metadata creation for web archives is time consuming and costly due to reliance on human effort. This paper explores the use of gpt-4o for metadata generation within the Web Archive Singapore, focusing on scalability, efficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files using data reduction techniques, achieving a notable 99.9% reduction in metadata generation costs. By prompt engineering, we generated titles and abstracts, which were evaluated both intrinsically using Levenshtein Distance and BERTScore, and extrinsically with human cataloguers using McNemar's test. Results indicate that while our method offers significant cost savings and efficiency gains, human curated metadata maintains an edge in quality. The study identifies key challenges including content inaccuracies, hallucinations, and translation issues, suggesting that Large Language Models (LLMs) should serve as complements rather than replacements for human cataloguers. Future work will focus on refining prompts, improving content filtering, and addressing privacy concerns through experimentation with smaller models. This research advances the integration of LLMs in web archiving, offering valuable insights into their current capabilities and outlining directions for future enhancements. The code is available at https://github.com/masamune-prog/warc2summary for further development and use by institutions facing similar challenges.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse</title>
<link>https://arxiv.org/abs/2411.09642</link>
<guid>https://arxiv.org/abs/2411.09642</guid>
<content:encoded><![CDATA[

arXiv:2411.09642v2 Announce Type: replace-cross 
Abstract: Specifying all desirable properties of a language model is challenging, but certain requirements seem essential. Given samples from an unknown language, the trained model should produce valid strings not seen in training and be expressive enough to capture the language's full richness. Otherwise, outputting invalid strings constitutes "hallucination," and failing to capture the full range leads to "mode collapse." We ask if a language model can meet both requirements.
  We investigate this within a statistical language generation setting building on Gold and Angluin. Here, the model receives random samples from a distribution over an unknown language K, which belongs to a possibly infinite collection of languages. The goal is to generate unseen strings from K. We say the model generates from K with consistency and breadth if, as training size increases, its output converges to all unseen strings in K.
  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in language generation are possible. We answer this negatively: for a large class of language models, including next-token prediction models, this is impossible for most collections of candidate languages. This contrasts with [KM24]'s result, showing consistent generation without breadth is possible for any countable collection of languages. Our finding highlights that generation with breadth fundamentally differs from generation without breadth.
  As a byproduct, we establish near-tight bounds on the number of samples needed for generation with or without breadth.
  Finally, our results offer hope: consistent generation with breadth is achievable for any countable collection of languages when negative examples (strings outside K) are available alongside positive ones. This suggests that post-training feedback, which encodes negative examples, can be crucial in reducing hallucinations while limiting mode collapse.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-Branch Cooperation for Enhanced Click-Through Rate Prediction at Taobao</title>
<link>https://arxiv.org/abs/2411.13057</link>
<guid>https://arxiv.org/abs/2411.13057</guid>
<content:encoded><![CDATA[

arXiv:2411.13057v2 Announce Type: replace-cross 
Abstract: Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type usually constrains the model's capability to capture the complex feature relationships, especially for industrial data with enormous input feature fields. Recent research shows that effective CTR models often combine an MLP network with a dedicated feature interaction network in a two-parallel structure. However, the interplay and cooperative dynamics between different streams or branches remain under-researched. In this work, we introduce a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. Specifically, MBCnet consists of three branches: the Extensible Feature Grouping and Crossing (EFGC) branch that promotes the model's memorization ability of specific feature fields, the low rank Cross Net branch and Deep branch to enhance explicit and implicit feature crossing for improved generalization. Among these branches, a novel cooperation scheme is proposed based on two principles: Branch co-teaching and moderate differentiation. Branch co-teaching encourages well-learned branches to support poorly-learned ones on specific training samples. Moderate differentiation advocates branches to maintain a reasonable level of difference in their feature representations on the same inputs. This cooperation strategy improves learning through mutual knowledge sharing and boosts the discovery of diverse feature interactions across branches. Experiments on large-scale industrial datasets and online A/B test at Taobao app demonstrate MBCnet's superior performance, delivering a 0.09 point increase in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes are available online.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control</title>
<link>https://arxiv.org/abs/2411.13100</link>
<guid>https://arxiv.org/abs/2411.13100</guid>
<content:encoded><![CDATA[

arXiv:2411.13100v2 Announce Type: replace-cross 
Abstract: Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints. Generated lyrics samples are available at: https://tinyurl.com/lyrics9999
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation</title>
<link>https://arxiv.org/abs/2411.16813</link>
<guid>https://arxiv.org/abs/2411.16813</guid>
<content:encoded><![CDATA[

arXiv:2411.16813v3 Announce Type: replace-cross 
Abstract: The incivility prevalent on platforms like Twitter (now X) and Reddit poses a challenge for developing AI systems that can support productive and rhetorically sound political argumentation. In this study, we report experiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of political discussions: high-variance, high-incivility Twitter replies to U.S. Congress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView. We systematically evaluate how these data sources and prompting strategies shape the rhetorical framing and deliberative quality of model-generated arguments. Our results show that Reddit-finetuned models produce safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies toxicity. Prompting reduces specific toxic behaviors, such as personal attacks, but fails to fully mitigate the influence of high-incivility training data. We introduce and validate a rhetorical evaluation rubric and provide practical guidelines for deploying LLMs in content authoring, moderation, and deliberation support.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts</title>
<link>https://arxiv.org/abs/2412.04628</link>
<guid>https://arxiv.org/abs/2412.04628</guid>
<content:encoded><![CDATA[

arXiv:2412.04628v4 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has become a popular approach for aligning language models using pairwise preferences. However, in practical post-training pipelines, on-policy generation typically yields multiple candidate responses per prompt, which are scored by a reward model to guide learning. In this setting, we propose $\textbf{Multi-Preference Optimization (MPO)}$, a generalization of DPO that optimizes over entire sets of responses by extending the Bradley-Terry model to groupwise comparisons between chosen and rejected sets. To further enhance learning, MPO employs deviation-based weighting, which emphasizes outlier responses that deviate most from the mean reward, effectively inducing a self-paced curriculum. We theoretically prove that MPO reduces alignment bias at a rate of $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$ with respect to the number of responses per query. Empirically, MPO achieves state-of-the-art performance on the UltraFeedback benchmark and yields up to $\sim 17.5\%$ improvement over the state-of-the-art baseline in length-controlled win rate on AlpacaEval2, establishing a new baseline for preference-based alignment
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning of Point Cloud Upsampling in Global and Local Inputs</title>
<link>https://arxiv.org/abs/2501.07076</link>
<guid>https://arxiv.org/abs/2501.07076</guid>
<content:encoded><![CDATA[

arXiv:2501.07076v3 Announce Type: replace-cross 
Abstract: In recent years, point cloud upsampling has been widely applied in tasks such as 3D reconstruction and object recognition. This study proposed a novel framework, ReLPU, which enhances upsampling performance by explicitly learning from both global and local structural features of point clouds. Specifically, we extracted global features from uniformly segmented inputs (Average Segments) and local features from patch-based inputs of the same point cloud. These two types of features were processed through parallel autoencoders, fused, and then fed into a shared decoder for upsampling. This dual-input design improved feature completeness and cross-scale consistency, especially in sparse and noisy regions. Our framework was applied to several state-of-the-art autoencoder-based networks and validated on standard datasets. Experimental results demonstrated consistent improvements in geometric fidelity and robustness. In addition, saliency maps confirmed that parallel global-local learning significantly enhanced the interpretability and performance of point cloud upsampling.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonoSOWA: Scalable monocular 3D Object detector Without human Annotations</title>
<link>https://arxiv.org/abs/2501.09481</link>
<guid>https://arxiv.org/abs/2501.09481</guid>
<content:encoded><![CDATA[

arXiv:2501.09481v3 Announce Type: replace-cross 
Abstract: Inferring object 3D position and orientation from a single RGB camera is a foundational task in computer vision with many important applications. Traditionally, 3D object detection methods are trained in a fully-supervised setup, requiring LiDAR and vast amounts of human annotations, which are laborious, costly, and do not scale well with the ever-increasing amounts of data being captured.
  We present a novel method to train a 3D object detector from a single RGB camera without domain-specific human annotations, making orders of magnitude more data available for training. The method uses newly proposed Local Object Motion Model to disentangle object movement source between subsequent frames, is approximately 700 times faster than previous work and compensates camera focal length differences to aggregate multiple datasets.
  The method is evaluated on three public datasets, where despite using no human labels, it outperforms prior work by a significant margin. It also shows its versatility as a pre-training tool for fully-supervised training and shows that combining pseudo-labels from multiple datasets can achieve comparable accuracy to using human labels from a single dataset. The source code and model are available at https://github.com/jskvrna/MonoSOWA.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2501.15103</link>
<guid>https://arxiv.org/abs/2501.15103</guid>
<content:encoded><![CDATA[

arXiv:2501.15103v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (\textbf{SMoRA}), which embeds MoE into LoRA by \textit{treating each rank as an independent expert}. With a \textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting</title>
<link>https://arxiv.org/abs/2501.16029</link>
<guid>https://arxiv.org/abs/2501.16029</guid>
<content:encoded><![CDATA[

arXiv:2501.16029v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are rapidly transforming the landscape of digital content creation. However, the prevalent black-box Application Programming Interface (API) access to many LLMs introduces significant challenges in accountability, governance, and security. LLM fingerprinting, which aims to identify the source model by analyzing statistical and stylistic features of generated text, offers a potential solution. Current progress in this area is hindered by a lack of dedicated datasets and the need for efficient, practical methods that are robust against adversarial manipulations. To address these challenges, we introduce FD-Dataset, a comprehensive bilingual fingerprinting benchmark comprising 90,000 text samples from 20 famous proprietary and open-source LLMs. Furthermore, we present FDLLM, a novel fingerprinting method that leverages parameter-efficient Low-Rank Adaptation (LoRA) to fine-tune a foundation model. This approach enables LoRA to extract deep, persistent features that characterize each source LLM. Through our analysis, we find that LoRA adaptation promotes the aggregation of outputs from the same LLM in representation space while enhancing the separation between different LLMs. This mechanism explains why LoRA proves particularly effective for LLM fingerprinting. Extensive empirical evaluations on FD-Dataset demonstrate FDLLM's superiority, achieving a Macro F1 score 22.1% higher than the strongest baseline. FDLLM also exhibits strong generalization to newly released models, achieving an average accuracy of 95% on unseen models. Notably, FDLLM remains consistently robust under various adversarial attacks, including polishing, translation, and synonym substitution. Experimental results show that FDLLM reduces the average attack success rate from 49.2% (LM-D) to 23.9%.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guaranteed prediction sets for functional surrogate models</title>
<link>https://arxiv.org/abs/2501.18426</link>
<guid>https://arxiv.org/abs/2501.18426</guid>
<content:encoded><![CDATA[

arXiv:2501.18426v2 Announce Type: replace-cross 
Abstract: We propose a method for obtaining statistically guaranteed prediction sets for functional machine learning methods: surrogate models which map between function spaces, motivated by the need to build reliable PDE emulators. The method constructs nested prediction sets on a low-dimensional representation (an SVD) of the surrogate model's error, and then maps these sets to the prediction space using set-propagation techniques. This results in prediction sets for functional surrogate models with conformal prediction coverage guarantees. We use zonotopes as basis of the set construction, which allow an exact linear propagation and are closed under Cartesian products, making them well-suited to this high-dimensional problem. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also introduce a technique to capture the truncation error of the SVD, preserving the guarantees of the method.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2502.07154</link>
<guid>https://arxiv.org/abs/2502.07154</guid>
<content:encoded><![CDATA[

arXiv:2502.07154v3 Announce Type: replace-cross 
Abstract: Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\it misaligned}$ with pass@N in that pass@N accuracy ${\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShapeLib: Designing a library of programmatic 3D shape abstractions with Large Language Models</title>
<link>https://arxiv.org/abs/2502.08884</link>
<guid>https://arxiv.org/abs/2502.08884</guid>
<content:encoded><![CDATA[

arXiv:2502.08884v2 Announce Type: replace-cross 
Abstract: We present ShapeLib, the first method that leverages the priors of LLMs to design libraries of programmatic 3D shape abstractions. Our system accepts two forms of design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. We discover abstractions that match this design intent with a guided LLM workflow that first proposes, and then validates, different ways of applying and implementing functions. We learn recognition networks that map shapes to programs with these newly discovered abstractions by training on data produced by LLM authored synthetic data generation procedures. Across modeling domains (split by shape category), we find that LLMs, when thoughtfully combined with geometric reasoning, can be guided to author a library of abstraction functions that generalize to shapes outside of the seed set. This framework addresses a long-standing shape analysis problem of how to discover reusable abstraction functions while exposing interpretable, semantically aligned interfaces. We find that ShapeLib provides distinct advantages over prior alternative abstraction discovery works in terms of generalization, usability, and maintaining plausibility under manipulation. Finally, we demonstrate that ShapeLib's abstraction functions unlock a number of downstream applications, combining LLM reasoning over shape programs with geometry processing to support shape editing and generation.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization</title>
<link>https://arxiv.org/abs/2502.13030</link>
<guid>https://arxiv.org/abs/2502.13030</guid>
<content:encoded><![CDATA[

arXiv:2502.13030v4 Announce Type: replace-cross 
Abstract: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From RAG to Memory: Non-Parametric Continual Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.14802</link>
<guid>https://arxiv.org/abs/2502.14802</guid>
<content:encoded><![CDATA[

arXiv:2502.14802v2 Announce Type: replace-cross 
Abstract: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batayan: A Filipino NLP benchmark for evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2502.14911</link>
<guid>https://arxiv.org/abs/2502.14911</guid>
<content:encoded><![CDATA[

arXiv:2502.14911v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response</title>
<link>https://arxiv.org/abs/2502.18452</link>
<guid>https://arxiv.org/abs/2502.18452</guid>
<content:encoded><![CDATA[

arXiv:2502.18452v2 Announce Type: replace-cross 
Abstract: During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these capabilities are often found only in larger models, which are frequently not reasonable to deploy on robotic systems. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our customized evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue</title>
<link>https://arxiv.org/abs/2502.20233</link>
<guid>https://arxiv.org/abs/2502.20233</guid>
<content:encoded><![CDATA[

arXiv:2502.20233v2 Announce Type: replace-cross 
Abstract: Query optimization has played a central role in database research for decades. However, more often than not, the proposed optimization techniques lead to a performance improvement in some, but not in all, situations. Therefore, we urgently need a methodology for designing a decision procedure that decides for a given query whether the optimization technique should be applied or not.
  In this work, we propose such a methodology with a focus on Yannakakis-style query evaluation as our optimization technique of interest. More specifically, we formulate this decision problem as an algorithm selection problem and we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that our approach indeed leads to a statistically significant performance improvement.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments</title>
<link>https://arxiv.org/abs/2502.20843</link>
<guid>https://arxiv.org/abs/2502.20843</guid>
<content:encoded><![CDATA[

arXiv:2502.20843v2 Announce Type: replace-cross 
Abstract: For robots to operate in general environments like households, they must be able to perform non-prehensile manipulation actions such as toppling and rolling to manipulate ungraspable objects. However, prior works on non-prehensile manipulation cannot yet generalize across environments with diverse geometries. The main challenge lies in adapting to varying environmental constraints: within a cabinet, the robot must avoid walls and ceilings; to lift objects to the top of a step, the robot must account for the step's pose and extent. While deep reinforcement learning (RL) has demonstrated impressive success in non-prehensile manipulation, accounting for such variability presents a challenge for the generalist policy, as it must learn diverse strategies for each new combination of constraints. To address this, we propose a modular and reconfigurable architecture that adaptively reconfigures network modules based on task requirements. To capture the geometric variability in environments, we extend the contact-based object representation (CORN) to environment geometries, and propose a procedural algorithm for generating diverse environments to train our agent. Taken together, the resulting policy can zero-shot transfer to novel real-world environments and objects despite training entirely within a simulator. We additionally release a simulation-based benchmark featuring nine digital twins of real-world scenes with 353 objects to facilitate non-prehensile manipulation research in realistic domains.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.01437</link>
<guid>https://arxiv.org/abs/2503.01437</guid>
<content:encoded><![CDATA[

arXiv:2503.01437v2 Announce Type: replace-cross 
Abstract: Recent works have successfully demonstrated that sparse deep reinforcement learning agents can be competitive against their dense counterparts. This opens up opportunities for reinforcement learning applications in fields where inference time and memory requirements are cost-sensitive or limited by hardware. Until now, dense-to-sparse methods have relied on hand-designed sparsity schedules that are not synchronized with the agent's learning pace. Crucially, the final sparsity level is chosen as a hyperparameter, which requires careful tuning as setting it too high might lead to poor performances. In this work, we address these shortcomings by crafting a dense-to-sparse algorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at the agent's learning pace, we consider multiple online networks with different sparsity levels, where each online network is trained from a shared target network. At each target update, the online network with the smallest loss is chosen as the next target network, while the other networks are replaced by a pruned version of the chosen network. We evaluate the proposed approach on the Atari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN reaches high sparsity levels while keeping performances high.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation</title>
<link>https://arxiv.org/abs/2503.02832</link>
<guid>https://arxiv.org/abs/2503.02832</guid>
<content:encoded><![CDATA[

arXiv:2503.02832v2 Announce Type: replace-cross 
Abstract: In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2503.05328</link>
<guid>https://arxiv.org/abs/2503.05328</guid>
<content:encoded><![CDATA[

arXiv:2503.05328v2 Announce Type: replace-cross 
Abstract: This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation</title>
<link>https://arxiv.org/abs/2503.05888</link>
<guid>https://arxiv.org/abs/2503.05888</guid>
<content:encoded><![CDATA[

arXiv:2503.05888v2 Announce Type: replace-cross 
Abstract: While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies</title>
<link>https://arxiv.org/abs/2503.08558</link>
<guid>https://arxiv.org/abs/2503.08558</guid>
<content:encoded><![CDATA[

arXiv:2503.08558v3 Announce Type: replace-cross 
Abstract: Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions</title>
<link>https://arxiv.org/abs/2503.10486</link>
<guid>https://arxiv.org/abs/2503.10486</guid>
<content:encoded><![CDATA[

arXiv:2503.10486v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack</title>
<link>https://arxiv.org/abs/2503.15551</link>
<guid>https://arxiv.org/abs/2503.15551</guid>
<content:encoded><![CDATA[

arXiv:2503.15551v2 Announce Type: replace-cross 
Abstract: Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch-prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations</title>
<link>https://arxiv.org/abs/2503.23101</link>
<guid>https://arxiv.org/abs/2503.23101</guid>
<content:encoded><![CDATA[

arXiv:2503.23101v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) can provide adaptive and scalable controllers essential for power grid decarbonization. However, RL methods struggle with power grids' complex dynamics, long-horizon goals, and hard physical constraints. For these reasons, we present RL2Grid, a benchmark designed in collaboration with power system operators to accelerate progress in grid control and foster RL maturity. Built on RTE France's power simulation framework, RL2Grid standardizes tasks, state and action spaces, and reward structures for a systematic evaluation and comparison of RL algorithms. Moreover, we integrate operational heuristics and design safety constraints based on human expertise to ensure alignment with physical requirements. By establishing reference performance metrics for classic RL baselines on RL2Grid's tasks, we highlight the need for novel methods capable of handling real systems and discuss future directions for RL-based grid control.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Collective World Model for Emergent Communication and Coordination</title>
<link>https://arxiv.org/abs/2504.03353</link>
<guid>https://arxiv.org/abs/2504.03353</guid>
<content:encoded><![CDATA[

arXiv:2504.03353v2 Announce Type: replace-cross 
Abstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.07385</link>
<guid>https://arxiv.org/abs/2504.07385</guid>
<content:encoded><![CDATA[

arXiv:2504.07385v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2504.07717</link>
<guid>https://arxiv.org/abs/2504.07717</guid>
<content:encoded><![CDATA[

arXiv:2504.07717v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations</title>
<link>https://arxiv.org/abs/2504.07836</link>
<guid>https://arxiv.org/abs/2504.07836</guid>
<content:encoded><![CDATA[

arXiv:2504.07836v3 Announce Type: replace-cross 
Abstract: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, \emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting multi-demographic federated learning for chest radiograph analysis using general-purpose self-supervised representations</title>
<link>https://arxiv.org/abs/2504.08584</link>
<guid>https://arxiv.org/abs/2504.08584</guid>
<content:encoded><![CDATA[

arXiv:2504.08584v2 Announce Type: replace-cross 
Abstract: Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P<0.001) but degraded performance for larger datasets (P<0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P<0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSelective: Interpretable Prognosis Prediction via Feature Selection and Compression in EHR Data</title>
<link>https://arxiv.org/abs/2504.11264</link>
<guid>https://arxiv.org/abs/2504.11264</guid>
<content:encoded><![CDATA[

arXiv:2504.11264v2 Announce Type: replace-cross 
Abstract: The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at http://www.healthinformaticslab.org/supp/resources.php .
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding</title>
<link>https://arxiv.org/abs/2504.13180</link>
<guid>https://arxiv.org/abs/2504.13180</guid>
<content:encoded><![CDATA[

arXiv:2504.13180v2 Announce Type: replace-cross 
Abstract: Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about "what", "where", "when", and "how" of a video. We make our work fully reproducible by providing data, training recipes, code & models. https://github.com/facebookresearch/perception_models
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation</title>
<link>https://arxiv.org/abs/2505.05625</link>
<guid>https://arxiv.org/abs/2505.05625</guid>
<content:encoded><![CDATA[

arXiv:2505.05625v2 Announce Type: replace-cross 
Abstract: Estimating rate coefficients from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate coefficient estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a latent neural ODE learns the continuous and differentiable trajectory between chemical concentrations and their time derivatives; second, an explicit Chemical Reaction Neural Network (CRNN) extracts the underlying rate coefficients based on the learned dynamics; and third, fine-tune CRNN using a neural ODE solver to further improve rate coefficient estimation. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work on stiff Neural ODEs for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities</title>
<link>https://arxiv.org/abs/2505.06085</link>
<guid>https://arxiv.org/abs/2505.06085</guid>
<content:encoded><![CDATA[

arXiv:2505.06085v3 Announce Type: replace-cross 
Abstract: The increasing demand for generative AI as Large Language Models (LLMs) services has driven the need for specialized hardware architectures that optimize computational efficiency and energy consumption. This paper evaluates the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic linear algebra kernels at reduced numerical precision, a fundamental operation in LLM computations. We present a detailed characterization of Grayskull's execution model, gridsize, matrix dimensions, data formats, and numerical precision impact computational efficiency. Furthermore, we compare Grayskull's performance against state-of-the-art architectures with tensor acceleration, including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100). Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a competitive trade-off between power consumption and computational throughput, reaching a peak of 1.55 TFLOPs/Watt with BF16.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.06331</link>
<guid>https://arxiv.org/abs/2505.06331</guid>
<content:encoded><![CDATA[

arXiv:2505.06331v2 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws directly into the loss function. However, effective training of PINNs remains challenging due to internal covariate shift, which destabilizes feature distributions and impairs model expressiveness. While normalization techniques like Batch Normalization and Layer Normalization are standard remedies in deep learning, they disrupt the pointwise input-output mappings critical to preserving the physical consistency in PINNs. In this work, we introduce Mask-PINNs, a novel architecture that regulates internal feature distributions through a smooth, learnable mask function applied pointwise across hidden layers. Unlike conventional normalization methods, the proposed mask function preserves the deterministic nature of input-output relationships while suppressing activation drift and saturation. Theoretically, we demonstrate that Mask-PINNs control feature spread near initialization by attenuating gradient variance growth through a tailored modulation mechanism. Empirically, we validate the method on multiple PDE benchmarks across diverse activation functions. Our results show consistent improvements in prediction accuracy, convergence stability, and robustness, with relative L2 errors reduced by up to two orders of magnitude over baseline models. Furthermore, we demonstrate that Mask-PINNs enable the effective use of wider networks, overcoming a key limitation in existing PINN frameworks.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics in Continual Pre-Training for Large Language Models</title>
<link>https://arxiv.org/abs/2505.07796</link>
<guid>https://arxiv.org/abs/2505.07796</guid>
<content:encoded><![CDATA[

arXiv:2505.07796v2 Announce Type: replace-cross 
Abstract: Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks</title>
<link>https://arxiv.org/abs/2505.07895</link>
<guid>https://arxiv.org/abs/2505.07895</guid>
<content:encoded><![CDATA[

arXiv:2505.07895v3 Announce Type: replace-cross 
Abstract: Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression</title>
<link>https://arxiv.org/abs/2505.13563</link>
<guid>https://arxiv.org/abs/2505.13563</guid>
<content:encoded><![CDATA[

arXiv:2505.13563v2 Announce Type: replace-cross 
Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base) with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and (d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets</title>
<link>https://arxiv.org/abs/2505.15517</link>
<guid>https://arxiv.org/abs/2505.15517</guid>
<content:encoded><![CDATA[

arXiv:2505.15517v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2505.16637</link>
<guid>https://arxiv.org/abs/2505.16637</guid>
<content:encoded><![CDATA[

arXiv:2505.16637v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Risk Assessments for Offensive Cybersecurity Agents</title>
<link>https://arxiv.org/abs/2505.18384</link>
<guid>https://arxiv.org/abs/2505.18384</guid>
<content:encoded><![CDATA[

arXiv:2505.18384v2 Announce Type: replace-cross 
Abstract: Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement</title>
<link>https://arxiv.org/abs/2505.19675</link>
<guid>https://arxiv.org/abs/2505.19675</guid>
<content:encoded><![CDATA[

arXiv:2505.19675v2 Announce Type: replace-cross 
Abstract: The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.21523</link>
<guid>https://arxiv.org/abs/2505.21523</guid>
<content:encoded><![CDATA[

arXiv:2505.21523v3 Announce Type: replace-cross 
Abstract: Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis</title>
<link>https://arxiv.org/abs/2505.23444</link>
<guid>https://arxiv.org/abs/2505.23444</guid>
<content:encoded><![CDATA[

arXiv:2505.23444v2 Announce Type: replace-cross 
Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00691</link>
<guid>https://arxiv.org/abs/2506.00691</guid>
<content:encoded><![CDATA[

arXiv:2506.00691v3 Announce Type: replace-cross 
Abstract: Training reinforcement learning (RL) agents often requires significant computational resources and prolonged training durations. To address this challenge, we build upon prior work that introduced a neural architecture with permutation-invariant sensory processing. We propose a modified attention mechanism that applies a non-linear transformation to the key vectors (K), producing enriched representations (K') through a custom mapping function. This Nonlinear Attention (NLA) mechanism enhances the representational capacity of the attention layer, enabling the agent to learn more expressive feature interactions. As a result, our model achieves significantly faster convergence and improved training efficiency, while maintaining performance on par with the baseline. These results highlight the potential of nonlinear attention mechanisms to accelerate reinforcement learning without sacrificing effectiveness.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution</title>
<link>https://arxiv.org/abs/2506.01231</link>
<guid>https://arxiv.org/abs/2506.01231</guid>
<content:encoded><![CDATA[

arXiv:2506.01231v2 Announce Type: replace-cross 
Abstract: To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.02404</link>
<guid>https://arxiv.org/abs/2506.02404</guid>
<content:encoded><![CDATA[

arXiv:2506.02404v3 Announce Type: replace-cross 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation</title>
<link>https://arxiv.org/abs/2506.03147</link>
<guid>https://arxiv.org/abs/2506.03147</guid>
<content:encoded><![CDATA[

arXiv:2506.03147v4 Announce Type: replace-cross 
Abstract: Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes</title>
<link>https://arxiv.org/abs/2506.07245</link>
<guid>https://arxiv.org/abs/2506.07245</guid>
<content:encoded><![CDATA[

arXiv:2506.07245v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Info-Coevolution: An Efficient Framework for Data Model Coevolution</title>
<link>https://arxiv.org/abs/2506.08070</link>
<guid>https://arxiv.org/abs/2506.08070</guid>
<content:encoded><![CDATA[

arXiv:2506.08070v2 Announce Type: replace-cross 
Abstract: Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32\% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50\% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantBert: An Open Source Language Model for Plant Science</title>
<link>https://arxiv.org/abs/2506.08897</link>
<guid>https://arxiv.org/abs/2506.08897</guid>
<content:encoded><![CDATA[

arXiv:2506.08897v2 Announce Type: replace-cross 
Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing</title>
<link>https://arxiv.org/abs/2506.09965</link>
<guid>https://arxiv.org/abs/2506.09965</guid>
<content:encoded><![CDATA[

arXiv:2506.09965v2 Announce Type: replace-cross 
Abstract: As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.10317</link>
<guid>https://arxiv.org/abs/2506.10317</guid>
<content:encoded><![CDATA[

arXiv:2506.10317v2 Announce Type: replace-cross 
Abstract: Lane-topology prediction is a critical component of safe and reliable autonomous navigation. An accurate understanding of the road environment aids this task. We observe that this information often follows conventions encoded in natural language, through design codes that reflect the road structure and road names that capture the road functionality. We augment this information in a lightweight manner to SMERF, a map-prior-based online lane-topology prediction model, by combining structured road metadata from OSM maps and lane-width priors from Road design manuals with the road centerline encodings. We evaluate our method on two geo-diverse complex intersection scenarios. Our method shows improvement in both lane and traffic element detection and their association. We report results using four topology-aware metrics to comprehensively assess the model performance. These results demonstrate the ability of our approach to generalize and scale to diverse topologies and conditions.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks</title>
<link>https://arxiv.org/abs/2506.10954</link>
<guid>https://arxiv.org/abs/2506.10954</guid>
<content:encoded><![CDATA[

arXiv:2506.10954v2 Announce Type: replace-cross 
Abstract: Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of $0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI</title>
<link>https://arxiv.org/abs/2506.11015</link>
<guid>https://arxiv.org/abs/2506.11015</guid>
<content:encoded><![CDATA[

arXiv:2506.11015v2 Announce Type: replace-cross 
Abstract: In the age of generative AI and ubiquitous digital tools, human cognition faces a structural paradox: as external aids become more capable, internal memory systems risk atrophy. Drawing on neuroscience and cognitive psychology, this paper examines how heavy reliance on AI systems and discovery-based pedagogies may impair the consolidation of declarative and procedural memory -- systems essential for expertise, critical thinking, and long-term retention. We review how tools like ChatGPT and calculators can short-circuit the retrieval, error correction, and schema-building processes necessary for robust neural encoding. Notably, we highlight striking parallels between deep learning phenomena such as "grokking" and the neuroscience of overlearning and intuition. Empirical studies are discussed showing how premature reliance on AI during learning inhibits proceduralization and intuitive mastery. We argue that effective human-AI interaction depends on strong internal models -- biological "schemata" and neural manifolds -- that enable users to evaluate, refine, and guide AI output. The paper concludes with policy implications for education and workforce training in the age of large language models.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Computer Vision Development with Agentic AI</title>
<link>https://arxiv.org/abs/2506.11140</link>
<guid>https://arxiv.org/abs/2506.11140</guid>
<content:encoded><![CDATA[

arXiv:2506.11140v3 Announce Type: replace-cross 
Abstract: Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, "provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)"), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment</title>
<link>https://arxiv.org/abs/2506.11480</link>
<guid>https://arxiv.org/abs/2506.11480</guid>
<content:encoded><![CDATA[

arXiv:2506.11480v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data selection. To facilitate future work, we will release code.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergent Linear Representations of Emergent Misalignment</title>
<link>https://arxiv.org/abs/2506.11618</link>
<guid>https://arxiv.org/abs/2506.11618</guid>
<content:encoded><![CDATA[

arXiv:2506.11618v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Refactoring Codebases through Library Design</title>
<link>https://arxiv.org/abs/2506.11058</link>
<guid>https://arxiv.org/abs/2506.11058</guid>
<content:encoded><![CDATA[
<div> Keywords: maintainable software, general software, code agents, refactoring, reusable libraries

Summary:
Librarian and Minicode are introduced as a method and benchmark, respectively, for code refactoring. Librarian is a sample-and-rerank method designed to generate reusable libraries, while Minicode is a benchmark that requires code agents to minimize and refactor multiple independent solutions into a joint library. The study focuses on the ability of code agents to refactor code in ways that promote growth and reusability. Results indicate that Librarian outperforms state-of-the-art code agents in terms of compression rates and correctness on Minicode, achieving compression rates 1.6-2x better while also improving correctness. The open-sourcing of the code and benchmark provides an opportunity for further research and development in this area. Overall, the research highlights the importance of maintainable and general software in efficiently building robust applications, emphasizing the role of refactoring specialized solutions into reusable components. 

<br /><br />Summary: 
- Librarian and Minicode are introduced for code refactoring.
- Librarian generates reusable libraries through sample-and-rerank method.
- Minicode benchmarks code agents on minimizing and refactoring solutions into a joint library.
- Librarian outperforms existing agents in compression rates and correctness on Minicode.
- Open-sourced code and benchmark offer opportunities for further research. <div>
arXiv:2506.11058v2 Announce Type: replace-cross 
Abstract: Maintainable and general software allows developers to build robust applications efficiently, yet achieving these qualities often requires refactoring specialized solutions into reusable components. This challenge becomes particularly relevant as code agents become increasingly accurate at solving isolated programming problems. We investigate code agents' capacity to refactor code in ways supporting growth and reusability. We present both a method and a benchmark for refactoring: Librarian, a sample-and-rerank method for generating reusable libraries, and Minicode, a benchmark where code agents must minimize and refactor multiple independent solutions into a joint library. Compared to state-of-the-art code agents, Librarian achieves strong results on both compression and correctness on Minicode, obtaining compression rates 1.6-2x better than coding agents while also improving correctness. We open-source our code and benchmark at https://code-refactor.github.io/.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy</title>
<link>https://arxiv.org/abs/2506.11302</link>
<guid>https://arxiv.org/abs/2506.11302</guid>
<content:encoded><![CDATA[
<div> Dataset, Spatio-Temporal, World model, Transformer-based, Autonomous<br />
<br />
Summary:<br />
The article introduces a new dataset called STRIDE, which arranges panoramic imagery into interconnected nodes to model the dynamic nature of real-world environments across space and time. The dataset is used to train a transformer-based generative world model called TARDIS, which integrates spatial and temporal dynamics to perform tasks such as image synthesis, instruction following, self-control, and georeferencing. The model shows strong performance in various agentic tasks, indicating the potential for generalist agents with enhanced embodied reasoning abilities. The training code, datasets, and model checkpoints are available for further research and development. <div>
arXiv:2506.11302v2 Announce Type: replace-cross 
Abstract: World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALM: Contextual Analog Logic with Multimodality</title>
<link>https://arxiv.org/abs/2506.14936</link>
<guid>https://arxiv.org/abs/2506.14936</guid>
<content:encoded><![CDATA[
<div> keywords: Contextual Analog Logic, Multimodality, Symbolic reasoning, Neural generation, Multi-modal data<br />
<br />
Summary: 
In this work, the authors introduce Contextual Analog Logic with Multimodality (CALM), which combines symbolic reasoning and neural generation to enable context-sensitive decisions grounded in real-world multi-modal data. Traditional bivalent logic systems lack the nuance of human decision-making and struggle to navigate multi-modal environments. CALM bridges the gap by integrating symbolic predicates with analog truth values computed by neural networks and constrained search. By representing predicates with domain trees and utilizing neural networks to capture multi-modal information, CALM outperformed classical logic and LLM baselines in object placement tasks. The system also demonstrated spatial heatmap generation aligned with logical constraints and human preferences. CALM showcases the potential for next-gen AI systems that blend the precision of logic with the information processing capabilities of neural networks. <br /><br />Summary: <div>
arXiv:2506.14936v1 Announce Type: new 
Abstract: In this work, we introduce Contextual Analog Logic with Multimodality (CALM). CALM unites symbolic reasoning with neural generation, enabling systems to make context-sensitive decisions grounded in real-world multi-modal data.
  Background: Classic bivalent logic systems cannot capture the nuance of human decision-making. They also require human grounding in multi-modal environments, which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting rich contextual information from multi-modal data, but lack interpretable structures for reasoning.
  Objectives: CALM aims to bridge the gap between logic and neural perception, creating an analog logic that can reason over multi-modal inputs. Without this integration, AI systems remain either brittle or unstructured, unable to generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate to analog truth values computed by neural networks and constrained search.
  Methods: CALM represents each predicate using a domain tree, which iteratively refines its analog truth value when the contextual groundings of its entities are determined. The iterative refinement is predicted by neural networks capable of capturing multi-modal information and is filtered through a symbolic reasoning module to ensure constraint satisfaction.
  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2% accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It also demonstrated spatial heatmap generation aligned with logical constraints and delicate human preferences, as shown by a human study.
  Conclusions: CALM demonstrates the potential to reason with logic structure while aligning with preferences in multi-modal environments. It lays the foundation for next-gen AI systems that require the precision and interpretation of logic and the multimodal information processing of neural networks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.14990</link>
<guid>https://arxiv.org/abs/2506.14990</guid>
<content:encoded><![CDATA[
<div> benchmark, reinforcement learning, continual learning, multi-agent, GPU acceleration

Summary: 
MEAL (Multi-agent Environments for Adaptive Learning) is introduced as the first benchmark designed for continual multi-agent reinforcement learning (CMARL). Unlike existing continual learning benchmarks that run on the CPU, MEAL leverages GPU acceleration using JAX, allowing for continual learning across sequences of 100 tasks on a standard desktop PC within a few hours. While combining popular continual learning and multi-agent reinforcement learning methods shows strong performance on simple environments, it fails to scale to more complex settings that require sustained coordination and adaptation. An ablation study conducted on MEAL identifies critical architectural and algorithmic features necessary for CMARL success, highlighting the importance of addressing challenges in continual multi-agent learning. <div>
arXiv:2506.14990v1 Announce Type: new 
Abstract: Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truncated Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2506.15050</link>
<guid>https://arxiv.org/abs/2506.15050</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, policy optimization, reasoning capabilities, training efficiency
<br />
Summary:<br />
The article introduces Truncated Proximal Policy Optimization (T-PPO) as a method to improve training efficiency for Large Language Models (LLMs) in generating long chains-of-thought. T-PPO streamlines policy updates and response generation, addressing the issue of low hardware utilization during training. It includes Extended Generalized Advantage Estimation (EGAE) for advantage estimation and a computationally optimized mechanism for independent optimization of policy and value models. T-PPO improves training efficiency by up to 2.5x and outperforms existing competitors, as demonstrated on AIME 2024 with a 32B base model. <div>
arXiv:2506.15050v1 Announce Type: new 
Abstract: Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges</title>
<link>https://arxiv.org/abs/2506.15196</link>
<guid>https://arxiv.org/abs/2506.15196</guid>
<content:encoded><![CDATA[
<div> framework, hyper-heuristic, language models, combinatorial optimization, heuristic evolution <br />
Summary:
The article introduces HeurAgenix, a two-stage hyper-heuristic framework that utilizes large language models (LLMs) to automatically evolve and select heuristics for combinatorial optimization problems. In the first phase, HeurAgenix uses an LLM to compare and extract evolution strategies from seed heuristic solutions. During problem solving, it dynamically selects the most promising heuristic for each problem state based on the LLM's perception ability. The selection process can use either a state-of-the-art LLM or a fine-tuned lightweight model for lower inference cost. To address the lack of supervision in combinatorial optimization problems, a dual-reward mechanism is used to fine-tune the lightweight heuristic selector, considering both selection preferences and state perception. Experimental results demonstrate that HeurAgenix outperforms existing LLM-based hyper-heuristics and competes with specialized solvers on benchmark problems. <div>
arXiv:2506.15196v1 Announce Type: new 
Abstract: Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce \textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study</title>
<link>https://arxiv.org/abs/2506.15207</link>
<guid>https://arxiv.org/abs/2506.15207</guid>
<content:encoded><![CDATA[
<div> Keywords: Low Earth Orbit satellites, Earth Observation missions, Reinforcement Learning, Multi-Agent Reinforcement Learning, autonomous coordination

Summary: 
Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL) are explored for autonomous Earth Observation (EO) mission planning, starting with single-satellite operations and moving towards multi-satellite constellations. Challenges such as energy and data limitations, uncertainties in observations, and decentralized coordination are addressed. Different MARL algorithms like PPO, IPPO, MAPPO, and HAPPO are evaluated in a realistic satellite simulation environment. Results show that MARL can effectively manage imaging and resources while handling non-stationarity and reward interdependencies in multi-satellite coordination. This study provides valuable insights for guiding policy learning in decentralized EO missions, paving the way for autonomous satellite operations. <br /><br />Summary: <div>
arXiv:2506.15207v1 Announce Type: new 
Abstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels</title>
<link>https://arxiv.org/abs/2506.15225</link>
<guid>https://arxiv.org/abs/2506.15225</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime Internet of Things, UAVs, vessels, multi-access edge computing, resource allocation <br />
Summary: <br />
This paper addresses the challenges of maritime computation offloading and resource allocation in the context of uncertain tasks in the maritime Internet of Things (MIoT). The proposed cooperative multi-access edge computing (MEC) framework involves MIoT devices, UAVs, and vessels to minimize total execution time. By leveraging Lyapunov optimization to handle unpredictable task arrivals and varying computational resources, the long-term constraints are transformed into short-term ones. The formulation of a Markov game (MG) accounts for the heterogeneity of actions and resources of UAVs and vessels. A heterogeneous-agent soft actor-critic algorithm is introduced to efficiently solve the MG problem by updating various neural networks sequentially. Simulations demonstrate the effectiveness of the approach in addressing computation offloading and resource allocation challenges in the maritime domain. <br /> 
Summary: <div>
arXiv:2506.15225v1 Announce Type: new 
Abstract: The computation demands from the maritime Internet of Things (MIoT) increase rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels based multi-access edge computing (MEC) can fulfill these MIoT requirements. However, the uncertain maritime tasks present significant challenges of inefficient computation offloading and resource allocation. In this paper, we focus on the maritime computation offloading and resource allocation through the cooperation of UAVs and vessels, with consideration of uncertain tasks. Specifically, we propose a cooperative MEC framework for computation offloading and resource allocation, including MIoT devices, UAVs and vessels. Then, we formulate the optimization problem to minimize the total execution time. As for the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the unpredictable task arrivals and varying computational resource availability. 
By converting the long-term constraints into short-term constraints, we obtain a set of small-scale optimization problems. Further, considering the heterogeneity of actions and resources of UAVs and vessels, we reformulate the small-scale optimization problem into a Markov game (MG). Moreover, a heterogeneous-agent soft actor-critic is proposed to sequentially update various neural networks and effectively solve the MG problem. 
Finally, simulations are conducted to verify the effectiveness in addressing computational offloading and resource allocation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Generalizable Environmental Understanding for Visual Navigation</title>
<link>https://arxiv.org/abs/2506.15377</link>
<guid>https://arxiv.org/abs/2506.15377</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Navigation, Embodied AI, Causal Framework, Causality-Aware Navigation, Causal Understanding Module

Summary:
The article presents a novel approach called Causality-Aware Navigation (CAN) for enhancing visual navigation in Embodied AI tasks. Traditional sequential methods in navigation tasks often fail to capture the internal association structure within historical data. CAN addresses this limitation by incorporating a Causal Understanding Module that leverages causality to improve the agent's environmental comprehension. Empirical evaluations demonstrate that CAN consistently outperforms existing methods in various tasks and simulation environments. The Causal Understanding Module proves to be the key factor in achieving these performance gains, showcasing its effectiveness in both Reinforcement and Supervised Learning settings without added computational overhead. This approach highlights the importance of considering causality in sequential data modeling for more efficient and accurate navigation in complex environments. 

<br /><br />Summary: <div>
arXiv:2506.15377v1 Announce Type: new 
Abstract: Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents</title>
<link>https://arxiv.org/abs/2506.15567</link>
<guid>https://arxiv.org/abs/2506.15567</guid>
<content:encoded><![CDATA[
<div> Keywords: Failure Analysis, AI, Large Language Model, Planning Agent, Autonomous 

Summary: 
The paper explores the integration of AI components, specifically Large Language Models (LLMs), within Failure Analysis (FA) processes to automate tasks such as non-conformity detection, data retrieval, and report generation. The challenge lies in efficiently orchestrating multiple AI models into cohesive workflows that enhance the FA process. The authors introduce a Large Language Model-based Planning Agent (LPA) that combines LLMs with advanced planning capabilities and external tool usage to assist FA engineers in solving analysis cases. The LPA autonomously processes complex queries, retrieves relevant data from external systems, and generates human-readable responses, showcasing operational effectiveness and reliability in supporting FA tasks. This integration of AI technologies within FA labs has the potential to revolutionize the FA process by streamlining tasks and improving overall efficiency.<br /><br />Summary: <div>
arXiv:2506.15567v1 Announce Type: new 
Abstract: Failure Analysis (FA) is a highly intricate and knowledge-intensive process. The integration of AI components within the computational infrastructure of FA labs has the potential to automate a variety of tasks, including the detection of non-conformities in images, the retrieval of analogous cases from diverse data sources, and the generation of reports from annotated images. However, as the number of deployed AI models increases, the challenge lies in orchestrating these components into cohesive and efficient workflows that seamlessly integrate with the FA process.
  This paper investigates the design and implementation of a Large Language Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their analysis cases. The LPA integrates LLMs with advanced planning capabilities and external tool utilization, enabling autonomous processing of complex queries, retrieval of relevant data from external systems, and generation of human-readable responses. Evaluation results demonstrate the agent's operational effectiveness and reliability in supporting FA tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games</title>
<link>https://arxiv.org/abs/2506.15624</link>
<guid>https://arxiv.org/abs/2506.15624</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, state representation, multi-agent games, selfish routing game, equilibrium

Summary:
In this study, a framework for constructing natural language state representations for prompting Large Language Models (LLMs) in repeated multi-agent games is presented. The framework considers the action informativeness, reward informativeness, and prompting style of the state representation. The framework is applied to a dynamic selfish routing game to analyze the impact of different types of state representations on LLM agent behavior. It is observed that summarized representations, information about regrets, and limited information about others' actions lead to behavior that aligns more closely with game theoretic equilibrium predictions and results in more stable game play. On the other hand, representations that provide complete history, raw payoffs, and detailed information about others' actions can cause deviations from equilibrium and higher variation in game play over time. This study emphasizes the importance of carefully designing natural language state representations for LLM agents in dynamic settings. 

<br /><br />Summary: <div>
arXiv:2506.15624v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language "state" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).
  We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy</title>
<link>https://arxiv.org/abs/2506.15639</link>
<guid>https://arxiv.org/abs/2506.15639</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, AI ethics, governance, regulation, AI policy

Summary: 
The article discusses the need for incorporating discussions on AI policy and ethics into computer science curriculums to prepare future AI practitioners for the evolving landscape of AI technologies. The authors developed an AI Policy Module aimed at introducing CS students to AI policy considerations, including a technical assignment on AI regulation. The findings from the pilot of the module showed that students became more concerned about the ethical impacts of AI technologies and felt more confident in discussing AI regulation. The AI Regulation Assignment was identified as an effective tool for exploring the boundaries of AI alignment and highlighting the importance of policy in addressing ethical challenges. This highlights the increasing responsibility for AI engineers to bridge the gap between technical expertise and ethical considerations in the development of AI systems. 

<br /><br />Summary: <div>
arXiv:2506.15639v1 Announce Type: new 
Abstract: As artificial intelligence (AI) further embeds itself into many settings across personal and professional contexts, increasing attention must be paid not only to AI ethics, but also to the governance and regulation of AI technologies through AI policy. However, the prevailing post-secondary computing curriculum is currently ill-equipped to prepare future AI practitioners to confront increasing demands to implement abstract ethical principles and normative policy preferences into the design and development of AI systems. We believe that familiarity with the 'AI policy landscape' and the ability to translate ethical principles to practices will in the future constitute an important responsibility for even the most technically-focused AI engineers.
  Toward preparing current computer science (CS) students for these new expectations, we developed an AI Policy Module to introduce discussions of AI policy into the CS curriculum. Building on a successful pilot in fall 2024, in this innovative practice full paper we present an updated and expanded version of the module, including a technical assignment on "AI regulation". We present the findings from our pilot of the AI Policy Module 2.0, evaluating student attitudes towards AI ethics and policy through pre- and post-module surveys. Following the module, students reported increased concern about the ethical impacts of AI technologies while also expressing greater confidence in their abilities to engage in discussions about AI regulation. Finally, we highlight the AI Regulation Assignment as an effective and engaging tool for exploring the limits of AI alignment and emphasizing the role of 'policy' in addressing ethical challenges.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement</title>
<link>https://arxiv.org/abs/2506.15647</link>
<guid>https://arxiv.org/abs/2506.15647</guid>
<content:encoded><![CDATA[
<div> Efficiency Steering, Activation Steering, Self-Rewarded Efficiency RL, Lightweight Methods, Reasoning Efficiency <br />
Summary:<br />
The study explores the inefficiency in large reasoning models and identifies that correct reasoning paths vary in length, with shorter responses often sufficient. Two methods are proposed to enhance efficiency: Efficiency Steering modulates reasoning behavior without training, and Self-Rewarded Efficiency RL dynamically balances task accuracy and brevity. Experiments on multiple benchmarks show significant reductions in reasoning length while maintaining or improving task performance. The results suggest that existing models have untapped efficiency potential that can be leveraged through self-guided techniques. <div>
arXiv:2506.15647v1 Announce Type: new 
Abstract: Recent advancements in large reasoning models (LRMs) have significantly enhanced language models' capabilities in complex problem-solving by emulating human-like deliberative thinking. However, these models often exhibit overthinking (i.e., the generation of unnecessarily verbose and redundant content), which hinders efficiency and inflates inference cost. In this work, we explore the representational and behavioral origins of this inefficiency, revealing that LRMs inherently possess the capacity for more concise reasoning. Empirical analyses show that correct reasoning paths vary significantly in length, and the shortest correct responses often suffice, indicating untapped efficiency potential. Exploiting these findings, we propose two lightweight methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a training-free activation steering technique that modulates reasoning behavior via a single direction in the model's representation space. Second, we develop Self-Rewarded Efficiency RL, a reinforcement learning framework that dynamically balances task accuracy and brevity by rewarding concise correct solutions. Extensive experiments on seven LRM backbones across multiple mathematical reasoning benchmarks demonstrate that our methods significantly reduce reasoning length while preserving or improving task performance. Our results highlight that reasoning efficiency can be improved by leveraging and guiding the intrinsic capabilities of existing models in a self-guided manner.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence</title>
<link>https://arxiv.org/abs/2506.15672</link>
<guid>https://arxiv.org/abs/2506.15672</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Agentic Systems, SwarmAgentic, Fully Automated Generation, Collaboration

Summary:<br /><br />
The article introduces SwarmAgentic, a framework for fully automated generation of agentic systems that can make decisions, coordinate tasks, and execute them efficiently. Existing agentic system frameworks lack full autonomy and the ability to generate agents from scratch. SwarmAgentic addresses these limitations by optimizing agent functionality and collaboration simultaneously. The framework leverages language-driven exploration and draws inspiration from Particle Swarm Optimization to evolve candidate systems efficiently. In evaluations on real-world tasks, SwarmAgentic outperformed baselines by achieving a relative improvement of 261.8% on the TravelPlanner benchmark. This research paves the way for scalable and autonomous agentic system design, showcasing the potential of combining swarm intelligence with fully automated multi-agent generation. The code for SwarmAgentic is publicly available for further exploration and development. <div>
arXiv:2506.15672v1 Announce Type: new 
Abstract: The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence</title>
<link>https://arxiv.org/abs/2506.15677</link>
<guid>https://arxiv.org/abs/2506.15677</guid>
<content:encoded><![CDATA[
<div> Embodied Web Agents; AI agents; web-scale reasoning; integrated physical and digital intelligence; Embodied Web Agents Benchmark

Summary:
Embodied Web Agents introduce a new paradigm for AI agents that can seamlessly bridge physical interactions with web-based reasoning. The concept involves creating task environments that combine realistic 3D environments with functional web interfaces, enabling agents to perform tasks that require coordination between the physical and digital worlds. The Embodied Web Agents Benchmark encompasses a variety of tasks like cooking, navigation, shopping, and tourism, challenging AI systems to exhibit cross-domain intelligence. Experimental results highlight a significant performance gap compared to human capabilities, showcasing the potential for improvement in embodied cognition and web-scale knowledge access. The datasets, codes, and websites associated with the project are publicly available, offering a valuable resource for further research and development. 

<br /><br />Summary: <div>
arXiv:2506.15677v1 Announce Type: new 
Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition</title>
<link>https://arxiv.org/abs/2207.01732</link>
<guid>https://arxiv.org/abs/2207.01732</guid>
<content:encoded><![CDATA[
<div> deformable CNN, Conformer architecture, attention mechanism, speech recognition, feature association <br />
Summary: <br />
The study introduced a new approach called the "Deformer" which replaces depthwise CNNs in the Conformer architecture for speech recognition tasks. By utilizing deformable kernels, the Deformer allows for the discovery of local features that enhance feature associations at both local and global levels. Through visualization of learned kernel offsets, the study highlights the significant changes in information processing with network depth. The Deformer model outperformed the Conformer baseline on the WSJ eval92 set, achieving a relative WER improvement of +5.6% without a language model and +6.4% with a language model. This improvement showcases the potential of using asymmetric kernels and adaptive views in enhancing speech recognition performance. <div>
arXiv:2207.01732v2 Announce Type: cross 
Abstract: Convolutional neural networks (CNN) have improved speech recognition performance greatly by exploiting localized time-frequency patterns. But these patterns are assumed to appear in symmetric and rigid kernels by the conventional CNN operation. It motivates the question: What about asymmetric kernels? In this study, we illustrate adaptive views can discover local features which couple better with attention than fixed views of the input. We replace depthwise CNNs in the Conformer architecture with a deformable counterpart, dubbed this "Deformer". By analyzing our best-performing model, we visualize both local receptive fields and global attention maps learned by the Deformer and show increased feature associations on the utterance level. The statistical analysis of learned kernel offsets provides an insight into the change of information in features with the network depth. Finally, replacing only half of the layers in the encoder, the Deformer improves +5.6% relative WER without a LM and +6.4% relative WER with a LM over the Conformer baseline on the WSJ eval92 set.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model</title>
<link>https://arxiv.org/abs/2309.13018</link>
<guid>https://arxiv.org/abs/2309.13018</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network pruning, multilingual automatic speech recognition, adaptive masking, sparse monolingual models, Dynamic ASR Pathways

Summary:
Neural network pruning is an effective way to compress multilingual automatic speech recognition (ASR) models with minimal performance loss. However, the traditional approach requires multiple rounds of pruning and re-training for each language, leading to inefficiency. This study introduces an adaptive masking technique, known as Dynamic ASR Pathways, to efficiently prune a multilingual ASR model. The approach dynamically adjusts the sub-network, preventing premature decisions about a fixed structure and resulting in sparse monolingual models or a single sparse multilingual model. The findings demonstrate that the proposed method surpasses existing pruning techniques in producing sparse monolingual models. Additionally, Dynamic ASR Pathways can identify and train optimal sub-networks of a multilingual model by adapting from different sub-network initializations, reducing the necessity for language-specific pruning. Overall, this approach offers a more efficient and effective method for optimizing multilingual ASR models. 

<br /><br />Summary: <div>
arXiv:2309.13018v2 Announce Type: cross 
Abstract: Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition</title>
<link>https://arxiv.org/abs/2310.18450</link>
<guid>https://arxiv.org/abs/2310.18450</guid>
<content:encoded><![CDATA[
<div> data augmentation, low-resource ASR, mixup, MixRep, regularization 

Summary:
MixRep is introduced as a data augmentation strategy for low-resource ASR, leveraging mixup to interpolate feature dimensions in neural network hidden representations for acoustic feature input and output. It extends prior methods like MixSpeech and includes regularization along the time axis of input. Applied to a Conformer encoder in an E2E LAS architecture with joint CTC loss on WSJ and SWB datasets, MixRep consistently outperforms other regularization techniques. Compared to SpecAugment baseline, MixRep achieves a +6.5% and +6.7% relative WER reduction on eval92 and Callhome segments of eval'2000 set. <div>
arXiv:2310.18450v1 Announce Type: cross 
Abstract: In this paper, we present MixRep, a simple and effective data augmentation strategy based on mixup for low-resource ASR. MixRep interpolates the feature dimensions of hidden representations in the neural network that can be applied to both the acoustic feature input and the output of each layer, which generalizes the previous MixSpeech method. Further, we propose to combine the mixup with a regularization along the time axis of the input, which is shown as complementary. We apply MixRep to a Conformer encoder of an E2E LAS architecture trained with a joint CTC loss. We experiment on the WSJ dataset and subsets of the SWB dataset, covering reading and telephony conversational speech. Experimental results show that MixRep consistently outperforms other regularization methods for low-resource ASR. Compared to a strong SpecAugment baseline, MixRep achieves a +6.5\% and a +6.7\% relative WER reduction on the eval92 set and the Callhome part of the eval'2000 set.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSyn: Enhancing Diagnostics with Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2506.14774</link>
<guid>https://arxiv.org/abs/2506.14774</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical decision-making, large language models, hybrid human-AI framework, interactive dialogues, diagnostic accuracy

Summary: 
The article discusses the complexities of clinical decision-making and the potential of Large Language Models (LLMs) as tools to support this process. It introduces a hybrid human-AI framework called MedSyn, which facilitates multi-step interactive dialogues between physicians and LLMs to refine diagnoses and treatment decisions. Unlike traditional static decision-support tools, MedSyn allows dynamic exchanges where physicians can challenge LLM suggestions and vice versa, leading to a more nuanced decision-making process. The study uses simulated interactions to assess the effectiveness of open-source LLMs as physician assistants, showing promising results for real-world application. Future research will involve real physician interactions to further validate MedSyn's impact on diagnostic accuracy and patient outcomes. <div>
arXiv:2506.14774v1 Announce Type: cross 
Abstract: Clinical decision-making is inherently complex, often influenced by cognitive biases, incomplete information, and case ambiguity. Large Language Models (LLMs) have shown promise as tools for supporting clinical decision-making, yet their typical one-shot or limited-interaction usage may overlook the complexities of real-world medical practice. In this work, we propose a hybrid human-AI framework, MedSyn, where physicians and LLMs engage in multi-step, interactive dialogues to refine diagnoses and treatment decisions. Unlike static decision-support tools, MedSyn enables dynamic exchanges, allowing physicians to challenge LLM suggestions while the LLM highlights alternative perspectives. Through simulated physician-LLM interactions, we assess the potential of open-source LLMs as physician assistants. Results show open-source LLMs are promising as physician assistants in the real world. Future work will involve real physician interactions to further validate MedSyn's usefulness in diagnostic accuracy and patient outcomes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See What I Mean? CUE: A Cognitive Model of Understanding Explanations</title>
<link>https://arxiv.org/abs/2506.14775</link>
<guid>https://arxiv.org/abs/2506.14775</guid>
<content:encoded><![CDATA[
<div> explainable AI, machine learning, cognitive accessibility, visual impairments, XAI

Summary: 
CUE proposes a model for Cognitive Understanding of Explanations, considering legibility, readability, and interpretability. A study on heatmaps with different colormaps showed lower confidence and effort for visually impaired users. Accessibility-focused colormaps like Cividis did not always improve task performance. This challenges assumptions about perceptual optimization and highlights the need for adaptive XAI interfaces. The study validated CUE by showing that altering legibility impacts understandability. The findings suggest the importance of human-centered explanation properties and support the call for accessible, user-tailored XAI interfaces. <div>
arXiv:2506.14775v1 Announce Type: cross 
Abstract: As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebXAII: an open-source web framework to study human-XAI interaction</title>
<link>https://arxiv.org/abs/2506.14777</link>
<guid>https://arxiv.org/abs/2506.14777</guid>
<content:encoded><![CDATA[
<div> Keywords: WebXAII, eXplainable Artificial Intelligence, XAI systems, human interaction, open-source<br />
Summary:<br />
This article introduces WebXAII, an open-source web framework aimed at facilitating research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly growing due to the increasing societal implications of AI adoption. Researchers often develop ad hoc interfaces for their studies, hindering reusability and reproducibility. WebXAII addresses this issue by providing a web-based platform that can encompass complete experimental protocols, enabling the presentation and recording of human participant responses. The platform's composite architecture offers flexibility through generic views and modules defined in a structured configuration file. Minimal programming skills are required to implement protocols. The effectiveness of WebXAII is demonstrated by reproducing a state-of-the-art study protocol. The framework is accessible at https://github.com/PAJEAN/WebXAII.<br /> 

Summary: <br /> <div>
arXiv:2506.14777v1 Announce Type: cross 
Abstract: This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature. The framework is available at https://github.com/PAJEAN/WebXAII.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series</title>
<link>https://arxiv.org/abs/2506.14786</link>
<guid>https://arxiv.org/abs/2506.14786</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal time series forecasting, Physics-informed positional encoding, Vision Language Models, Satellite imagery, Typhoon forecasting <br />
<br />
Summary: <br />
This paper introduces a method called physics-informed positional encoding (PIPE) to incorporate physical information into vision language models for multimodal time series forecasting. PIPE leverages a physics-informed positional indexing scheme and a variant-frequency positional encoding mechanism to embed physical and sequential order information into the model. By doing so, PIPE enhances multimodal alignment and forecasting accuracy, leading to state-of-the-art performance in deep learning forecasting and climate domain tasks. Experimental results on a large satellite image dataset show a 12% improvement in typhoon intensity forecasting compared to previous methods. The proposed method not only utilizes visual data in time series forecasting but also effectively captures the temporal and geospatial context of satellite imagery, demonstrating its superiority in capturing physical information for forecasting tasks. <div>
arXiv:2506.14786v1 Announce Type: cross 
Abstract: Multimodal time series forecasting is foundational in various fields, such as utilizing satellite imagery and numerical data for predicting typhoons in climate science. However, existing multimodal approaches primarily focus on utilizing text data to help time series forecasting, leaving the visual data in existing time series datasets untouched. Furthermore, it is challenging for models to effectively capture the physical information embedded in visual data, such as satellite imagery's temporal and geospatial context, which extends beyond images themselves. To address this gap, we propose physics-informed positional encoding (PIPE), a lightweight method that embeds physical information into vision language models (VLMs). PIPE introduces two key innovations: (1) a physics-informed positional indexing scheme for mapping physics to positional IDs, and (2) a variant-frequency positional encoding mechanism for encoding frequency information of physical variables and sequential order of tokens within the embedding space. By preserving both the physical information and sequential order information, PIPE significantly improves multimodal alignment and forecasting accuracy. Through the experiments on the most representative and the largest open-sourced satellite image dataset, PIPE achieves state-of-the-art performance in both deep learning forecasting and climate domain methods, demonstrating superiority across benchmarks, including a 12% improvement in typhoon intensity forecasting over prior works. Our code is provided in the supplementary material.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems</title>
<link>https://arxiv.org/abs/2506.14787</link>
<guid>https://arxiv.org/abs/2506.14787</guid>
<content:encoded><![CDATA[
<div> autonomous vehicle storage and retrieval systems, deep reinforcement learning, graph-based state representation, neural network architecture, retrieval tardiness 

Summary:
The study focuses on addressing the retrieval problem in multi-deep storage systems with heterogeneous item configurations using a deep reinforcement learning-based framework. It aims to minimize total tardiness by associating each item with a specific due date. To effectively capture the system's topology, a graph-based state representation is introduced, integrating both item attributes and the local topological structure of the warehouse. A novel neural network architecture is designed, combining a Graph Neural Network (GNN) with a Transformer model to process this representation. The GNN encodes topological and item-specific information, while the Transformer maps these embeddings into global priority assignments. The approach demonstrates superior performance in optimizing retrieval tardiness through extensive numerical experiments, showcasing the effectiveness and adaptability of the trained agent in storage systems with diverse layouts. <br /><br />Summary: <div>
arXiv:2506.14787v1 Announce Type: cross 
Abstract: In modern industrial and logistics environments, the rapid expansion of fast delivery services has heightened the demand for storage systems that combine high efficiency with increased density. Multi-deep autonomous vehicle storage and retrieval systems (AVS/RS) present a viable solution for achieving greater storage density. However, these systems encounter significant challenges during retrieval operations due to lane blockages. A conventional approach to mitigate this issue involves storing items with homogeneous characteristics in a single lane, but this strategy restricts the flexibility and adaptability of multi-deep storage systems.
  In this study, we propose a deep reinforcement learning-based framework to address the retrieval problem in multi-deep storage systems with heterogeneous item configurations. Each item is associated with a specific due date, and the objective is to minimize total tardiness. To effectively capture the system's topology, we introduce a graph-based state representation that integrates both item attributes and the local topological structure of the multi-deep warehouse. To process this representation, we design a novel neural network architecture that combines a Graph Neural Network (GNN) with a Transformer model. The GNN encodes topological and item-specific information into embeddings for all directly accessible items, while the Transformer maps these embeddings into global priority assignments. The Transformer's strong generalization capability further allows our approach to be applied to storage systems with diverse layouts. Extensive numerical experiments, including comparisons with heuristic methods, demonstrate the superiority of the proposed neural network architecture and the effectiveness of the trained agent in optimizing retrieval tardiness.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors</title>
<link>https://arxiv.org/abs/2506.14794</link>
<guid>https://arxiv.org/abs/2506.14794</guid>
<content:encoded><![CDATA[
<div> Assembly-of-Experts, Child Variants, Model Interpolation, DeepSeek R1T, Hybrid Model<br />
Summary:<br />
The study introduces the Assembly-of-Experts (AoE) construction method, allowing for the creation of child variants of existing Mixture-of-Experts parent models in linear time. By individually interpolating model weight tensors, semantic features of parent models can be enhanced or suppressed. Varying the proportion of weights from parent models results in gradual changes in some properties of the child model, while other traits emerge abruptly. The method simplifies model search as nearly all generated models are functional. The DeepSeek R1T "Chimera" hybrid model, combining DeepSeek's V3-0324 and R1 models, demonstrates comparable intelligence to R1 with reduced output tokens, nearing V3 speed. The Chimera showcases orderly reasoning without fine-tuning or distillation compared to its parent models. <br /><br /> <div>
arXiv:2506.14794v1 Announce Type: cross 
Abstract: Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM during pretraining is extremely expensive and seems inefficient. To better leverage the huge investments made into pretrained models, we develop the new "Assembly-of-Experts" (AoE) construction method to create capable child variants of existing Mixture-of-Experts parent models in linear time. Model weight tensors get interpolated individually, allowing to enhance or suppress semantic features of the parents.
  Varying the proportion of weights taken from the parent models, we observe some properties of the AoE child model changing gradually, while other behavioral traits emerge with a sharp transition. Surprisingly, nearly every generated model is functional and capable, which makes searching the model space straightforward.
  We construct the DeepSeek R1T "Chimera", a 671B open-weights hybrid model combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the routed expert tensors of R1, but still achieves about R1-level intelligence. At the same time, it uses about 40\% fewer output tokens, close to V3 speed. Constructed without any fine-tuning or distillation, the Chimera exhibits surprisingly compact, orderly reasoning compared to its parent models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations</title>
<link>https://arxiv.org/abs/2506.14795</link>
<guid>https://arxiv.org/abs/2506.14795</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Quantum Computing, Quantum Neural Networks, Artificial Neural Networks, Wind Energy Dataset <br />
<br />Summary: Quantum Machine Learning (QML) combines quantum computing and machine learning to enhance classical methods using principles like entanglement and superposition. This study focuses on Quantum Neural Networks (QNNs), quantum counterparts of Artificial Neural Networks (ANNs), to address skepticism about the practical benefits of QML due to NISQ device limitations. 12 different QNN configurations with two feature maps and six entanglement strategies were evaluated on a wind energy dataset. QNNs using the Z feature map achieved 93% accuracy in wind power output prediction with only four input parameters, outperforming classical methods. These results demonstrate the effectiveness of QNNs in predictive tasks, showcasing the potential of QML in real-world applications. <br /> <div>
arXiv:2506.14795v1 Announce Type: cross 
Abstract: Quantum Machine Learning (QML) is an emerging field at the intersection of quantum computing and machine learning, aiming to enhance classical machine learning methods by leveraging quantum mechanics principles such as entanglement and superposition. However, skepticism persists regarding the practical advantages of QML, mainly due to the current limitations of noisy intermediate-scale quantum (NISQ) devices. This study addresses these concerns by extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired counterparts of Artificial Neural Networks (ANNs), demonstrating their effectiveness compared to classical methods. We systematically construct and evaluate twelve distinct QNN configurations, utilizing two unique quantum feature maps combined with six different entanglement strategies for ansatz design. Experiments conducted on a wind energy dataset reveal that QNNs employing the Z feature map achieve up to 93% prediction accuracy when forecasting wind power output using only four input parameters. Our findings show that QNNs outperform classical methods in predictive tasks, underscoring the potential of QML in real-world applications.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PFMBench: Protein Foundation Model Benchmark</title>
<link>https://arxiv.org/abs/2506.14796</link>
<guid>https://arxiv.org/abs/2506.14796</guid>
<content:encoded><![CDATA[
<div> Keywords: protein foundation models, PFMBench, benchmark, protein science, evaluation 

Summary: 
This study explores the landscape and future directions of protein foundation model research. The field lacks a comprehensive benchmark for fair evaluation and understanding. Multiple protein foundation models have emerged since ESM-1B, each with unique datasets and methodologies. Evaluations often focus on specific tasks, hindering insights into generalization and limitations. PFMBench, a benchmark, evaluates protein foundation models across 38 tasks in 8 key areas of protein science. Through experiments on 17 models, PFMBench reveals task correlations, top-performing models, and offers a streamlined evaluation protocol. This benchmark aims to help researchers understand task relationships, model performance, and criteria for developing new foundation models. The code for PFMBench is available on GitHub. <div>
arXiv:2506.14796v1 Announce Type: cross 
Abstract: This study investigates the current landscape and future directions of protein foundation model research. While recent advancements have transformed protein science and engineering, the field lacks a comprehensive benchmark for fair evaluation and in-depth understanding. Since ESM-1B, numerous protein foundation models have emerged, each with unique datasets and methodologies. However, evaluations often focus on limited tasks tailored to specific models, hindering insights into broader generalization and limitations. Specifically, researchers struggle to understand the relationships between tasks, assess how well current models perform across them, and determine the criteria in developing new foundation models. To fill this gap, we present PFMBench, a comprehensive benchmark evaluating protein foundation models across 38 tasks spanning 8 key areas of protein science. Through hundreds of experiments on 17 state-of-the-art models across 38 tasks, PFMBench reveals the inherent correlations between tasks, identifies top-performing models, and provides a streamlined evaluation protocol. Code is available at \href{https://github.com/biomap-research/PFMBench}{\textcolor{blue}{GitHub}}.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bound by semanticity: universal laws governing the generalization-identification tradeoff</title>
<link>https://arxiv.org/abs/2506.14797</link>
<guid>https://arxiv.org/abs/2506.14797</guid>
<content:encoded><![CDATA[
<div> trade-off, internal representations, generalization, identification, resolution boundary

Summary: 
The article explores the trade-off between structured and selective internal representations in intelligent systems. It establishes a limit on this trade-off, showing that for models with finite semantic resolution, there is a universal Pareto front for correct generalization and identification. The analysis extends to noisy and heterogeneous input spaces, predicting a collapse in multi-input processing capacity. A minimal ReLU network undergoes self-organization to establish a resolution boundary, with empirical trajectories aligning closely with theoretical curves. These limits are confirmed in more complex settings like convolutional neural networks and vision-language models, emphasizing the role of finite-resolution similarity as a fundamental constraint in representation capacity. Overall, the study provides an exact theory on how semantic resolution influences the representational capacity of deep networks and biological brains. 

<br /><br />Summary: <div>
arXiv:2506.14797v1 Announce Type: cross 
Abstract: Intelligent systems must deploy internal representations that are simultaneously structured -- to support broad generalization -- and selective -- to preserve input identity. We expose a fundamental limit on this tradeoff. For any model whose representational similarity between inputs decays with finite semantic resolution $\varepsilon$, we derive closed-form expressions that pin its probability of correct generalization $p_S$ and identification $p_I$ to a universal Pareto front independent of input space geometry. Extending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs predicts a sharp $1/n$ collapse of multi-input processing capacity and a non-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end reproduces these laws: during learning a resolution boundary self-organizes and empirical $(p_S,p_I)$ trajectories closely follow theoretical curves for linearly decaying similarity. Finally, we demonstrate that the same limits persist in two markedly more complex settings -- a convolutional neural network and state-of-the-art vision-language models -- confirming that finite-resolution similarity is a fundamental emergent informational constraint, not merely a toy-model artifact. Together, these results provide an exact theory of the generalization-identification trade-off and clarify how semantic resolution shapes the representational capacity of deep networks and brains alike.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MODS: Multi-source Observations Conditional Diffusion Model for Meteorological State Downscaling</title>
<link>https://arxiv.org/abs/2506.14798</link>
<guid>https://arxiv.org/abs/2506.14798</guid>
<content:encoded><![CDATA[
<div> satellite data, downscaling model, meteorological variables, high-resolution, ERA5 <br />
Summary: <br />
The article introduces the Multi-source Observation Down-Scaling Model (MODS) for accurately acquiring high-resolution surface meteorological conditions. Existing downscaling methods often fail to capture all changes in meteorological variables accurately. MODS utilizes a wider range of satellite data, including geostationary and polar-orbiting satellites, as well as topographic data, to improve downscaling accuracy. By pre-training on the ERA5 reanalysis dataset and using a multi-source cross-attention module to fuse diverse conditional inputs, MODS generates atmospheric states that align closely with real-world conditions. During sampling, MODS incorporates low-resolution ERA5 maps and station-level meteorological data for enhanced downscaling consistency. Experimental results demonstrate that MODS outperforms existing methods in downscaling ERA5 maps to a 6.25 km resolution. <div>
arXiv:2506.14798v1 Announce Type: cross 
Abstract: Accurate acquisition of high-resolution surface meteorological conditions is critical for forecasting and simulating meteorological variables. Directly applying spatial interpolation methods to derive meteorological values at specific locations from low-resolution grid fields often yields results that deviate significantly from the actual conditions. Existing downscaling methods primarily rely on the coupling relationship between geostationary satellites and ERA5 variables as a condition. However, using brightness temperature data from geostationary satellites alone fails to comprehensively capture all the changes in meteorological variables in ERA5 maps. To address this limitation, we can use a wider range of satellite data to make more full use of its inversion effects on various meteorological variables, thus producing more realistic results across different meteorological variables. To further improve the accuracy of downscaling meteorological variables at any location, we propose the Multi-source Observation Down-Scaling Model (MODS). It is a conditional diffusion model that fuses data from multiple geostationary satellites GridSat, polar-orbiting satellites (AMSU-A, HIRS, and MHS), and topographic data (GEBCO), as conditions, and is pre-trained on the ERA5 reanalysis dataset. During training, latent features from diverse conditional inputs are extracted separately and fused into ERA5 maps via a multi-source cross-attention module. By exploiting the inversion relationships between reanalysis data and multi-source atmospheric variables, MODS generates atmospheric states that align more closely with real-world conditions. During sampling, MODS enhances downscaling consistency by incorporating low-resolution ERA5 maps and station-level meteorological data as guidance. Experimental results demonstrate that MODS achieves higher fidelity when downscaling ERA5 maps to a 6.25 km resolution.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust</title>
<link>https://arxiv.org/abs/2506.14799</link>
<guid>https://arxiv.org/abs/2506.14799</guid>
<content:encoded><![CDATA[
<div> AI, character representation, gender, age, visualization  
Summary:  
Recent advances in AI have made it possible to analyze character representation in media based on gender and age using automated tools. A new AI-based tool, utilizing the CLIP model, has been developed to quantify character representation in visual media and present the results through effective visualizations. A user study was conducted to evaluate the usefulness and trustworthiness of the AI-generated results, with participants finding the tool to be overall useful but expressing a need for more detailed visualizations including additional demographic categories. The study also revealed that participants had moderate to low trust in AI-based gender and age models, although they were not opposed to the use of AI in this context. The tool, along with code, benchmarking, and data from the user study, is available for further exploration. <br /><br />Summary: <div>
arXiv:2506.14799v1 Announce Type: cross 
Abstract: Recent advances in AI has enabled automated analysis of complex media content at scale and generate actionable insights regarding character representation along such dimensions as gender and age. Past work focused on quantifying representation from audio/video/text using various ML models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are they to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these questions through a user study, while proposing a new AI-based character representation and visualization tool. Our tool based on the Contrastive Language Image Pretraining (CLIP) foundation model to analyze visual screen data to quantify character representation across dimensions of age and gender. We also designed effective visualizations suitable for presenting such analytics to lay audience. Next, we conducted a user study to seek empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We note that participants were able to understand the analytics from our visualization, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and data from the user study can be found here: https://anonymous.4open.science/r/Character-Representation-Media-FF7B
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ss-Mamba: Semantic-Spline Selective State-Space Model</title>
<link>https://arxiv.org/abs/2506.14802</link>
<guid>https://arxiv.org/abs/2506.14802</guid>
<content:encoded><![CDATA[
<div> Keywords: ss-Mamba, time series forecasting, semantic-aware embeddings, adaptive spline-based temporal encoding, selective state-space modeling

Summary: 
ss-Mamba is a novel foundation model proposed for time series forecasting, combining semantic-aware embeddings and adaptive spline-based temporal encoding within a selective state-space modeling framework. It adopts the Mamba selective state space model, reducing computational complexity while maintaining performance comparable to Transformer architectures. Semantic index embeddings from pretrained language models enable generalization to unseen series through meaningful semantic priors. The spline-based Kolmogorov-Arnold Networks (KAN) capture complex seasonalities and non-stationary temporal effects in a dynamic and interpretable manner, enhancing traditional temporal feature encodings. Experimental evaluations demonstrate that ss-Mamba outperforms traditional Transformer-based models in accuracy, robustness, and interpretability, showcasing its versatility and computational efficiency in time-series forecasting.<br /><br />Summary: <div>
arXiv:2506.14802v1 Announce Type: cross 
Abstract: We propose ss-Mamba, a novel foundation model that enhances time series forecasting by integrating semantic-aware embeddings and adaptive spline-based temporal encoding within a selective state-space modeling framework. Building upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba selective state space model as an efficient alternative that achieves comparable performance while significantly reducing computational complexity from quadratic to linear time. Semantic index embeddings, initialized from pretrained language models, allow effective generalization to previously unseen series through meaningful semantic priors. Additionally, spline-based Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex seasonalities and non-stationary temporal effects, providing a powerful enhancement over conventional temporal feature encodings. Extensive experimental evaluations confirm that ss-Mamba delivers superior accuracy, robustness, and interpretability, demonstrating its capability as a versatile and computationally efficient alternative to traditional Transformer-based models in time-series forecasting.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?</title>
<link>https://arxiv.org/abs/2506.14805</link>
<guid>https://arxiv.org/abs/2506.14805</guid>
<content:encoded><![CDATA[
<div> Argus Inspection, multimodal large language models, visual fine-grained perception, commonsense causal inference, Eye of Panoptes framework <br />
Summary: 
This paper introduces Argus Inspection, a new multimodal benchmark designed to assess the cognitive and reasoning capabilities of Multimodal Large Language Models (MLLMs). The benchmark includes two levels of difficulty, focusing on detailed visual recognition and real-world commonsense understanding to evaluate causal reasoning abilities. The researchers also present the Eye of Panoptes framework, which combines a binary parametric Sigmoid metric with an indicator function to provide a comprehensive evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs show that the highest performance in visual fine-grained reasoning is only 0.46, indicating significant room for improvement. This study offers insights for enhancing the cognitive and reasoning capabilities of MLLMs in complex multimodal tasks. <br /><br /> <div>
arXiv:2506.14805v1 Announce Type: cross 
Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks</title>
<link>https://arxiv.org/abs/2506.14813</link>
<guid>https://arxiv.org/abs/2506.14813</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, training errors, proactive checking, invariants, debugging<br />
<br />
Summary: <br />
Training deep learning models can lead to silent errors that are difficult to detect and diagnose. To address this issue, the TRAINCHECK framework adopts a proactive checking approach by automatically inferring invariants tailored for DL training. These invariants help detect silent errors during the training process and provide debugging assistance. In a study where 20 real-world silent training errors were reproduced, TRAINCHECK successfully detected 18 errors within a single training iteration. Additionally, TRAINCHECK uncovered 6 unknown bugs in popular training libraries that could potentially cause silent errors. This proactive approach can enhance the reliability and robustness of deep learning training processes, ultimately leading to more accurate and efficient model training. <br /><br />Summary: <div>
arXiv:2506.14813v1 Announce Type: cross 
Abstract: Training deep learning (DL) models is a complex process, making it prone to silent errors that are challenging to detect and diagnose. This paper presents TRAINCHECK, a framework that takes a proactive checking approach to address silent training errors. TRAINCHECK automatically infers invariants tailored for DL training. It uses these invariants to proactively detect silent errors during the training process while providing debugging help. To evaluate TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root causes. TRAINCHECK successfully detects 18 errors within a single training iteration. It also uncovers 6 unknown bugs in popular training libraries that lead to silent errors.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning</title>
<link>https://arxiv.org/abs/2506.14817</link>
<guid>https://arxiv.org/abs/2506.14817</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network architecture, violent conflict forecasting, subnational level, Monte Carlo Dropout LSTM, spatiotemporal patterns <br />
<br />
Summary: 
This study introduces a new neural network architecture for forecasting various types of violence at the subnational level up to 36 months in advance. The model combines classification and regression tasks to provide probabilistic estimates and expected magnitudes of future events. By utilizing a Monte Carlo Dropout LSTM U-Net architecture, it captures spatial dependencies and temporal dynamics without the need for manual feature engineering, resulting in state-of-the-art performance. The model can autonomously learn complex spatiotemporal patterns underlying violent conflict and is easily extensible to incorporate additional data sources and forecast auxiliary variables. These features make it a valuable tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives. <div>
arXiv:2506.14817v1 Announce Type: cross 
Abstract: Forecasting violent conflict at high spatial and temporal resolution remains a central challenge for both researchers and policymakers. This study presents a novel neural network architecture for forecasting three distinct types of violence -- state-based, non-state, and one-sided -- at the subnational (priogrid-month) level, up to 36 months in advance. The model jointly performs classification and regression tasks, producing both probabilistic estimates and expected magnitudes of future events. It achieves state-of-the-art performance across all tasks and generates approximate predictive posterior distributions to quantify forecast uncertainty.
  The architecture is built on a Monte Carlo Dropout Long Short-Term Memory (LSTM) U-Net, integrating convolutional layers to capture spatial dependencies with recurrent structures to model temporal dynamics. Unlike many existing approaches, it requires no manual feature engineering and relies solely on historical conflict data. This design enables the model to autonomously learn complex spatiotemporal patterns underlying violent conflict.
  Beyond achieving state-of-the-art predictive performance, the model is also highly extensible: it can readily integrate additional data sources and jointly forecast auxiliary variables. These capabilities make it a promising tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints</title>
<link>https://arxiv.org/abs/2506.14821</link>
<guid>https://arxiv.org/abs/2506.14821</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, Group Relative Policy Optimization, detailed visual reasoning, external tools, visual question-answering

Summary:<br /><br />
The study addresses the challenges faced by vision-language models in detailed visual reasoning tasks when compute resources are limited. Inspired by Deepseek-r1 methods, smaller-scale models are trained using Group Relative Policy Optimization (GRPO) to utilize external tools like zoom. The approach combines GRPO learning, a straightforward reward structure, a simple tool-calling interface, allocating additional tokens to tool results, and a training data mix focused on visually difficult examples. By leveraging detailed visual information from external tools, the method outperforms similarly-sized baseline models on certain visual question-answering tasks. The key to success lies in the effective combination of different training techniques and tools to enhance the model's performance in challenging visual tasks. <div>
arXiv:2506.14821v1 Announce Type: cross 
Abstract: Despite tremendous recent advances in large model reasoning ability, vision-language models (VLMs) still struggle with detailed visual reasoning, especially when compute resources are limited. To address this challenge, we draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale models with Group Relative Policy Optimization (GRPO) to use external tools such as zoom. The greatest benefit is obtained with a combination of GRPO learning, a simple reward structure, a simplified tool-calling interface, allocating additional tokens to the result of the tool call, and a training data mix that over-represents visually difficult examples. Compared to similarly-sized baseline models, our method achieves better performance on some visual question-answering (VQA) tasks, thanks to the detailed visual information gathered from the external tool.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViLLa: A Neuro-Symbolic approach for Animal Monitoring</title>
<link>https://arxiv.org/abs/2506.14823</link>
<guid>https://arxiv.org/abs/2506.14823</guid>
<content:encoded><![CDATA[
<div> Keywords: animal monitoring, visual data interpretation, natural language queries, ViLLa framework, neuro-symbolic approach 

Summary: 
ViLLa (Vision-Language-Logic Approach) is a framework developed for interpretable animal monitoring in natural environments. It integrates three components: a visual detection module, a language parser, and a symbolic reasoning layer for processing visual data and human language queries. The system can accurately answer questions such as counting animals or identifying their locations in images by grounding visual detections into symbolic facts and using predefined rules for inference. ViLLa offers modularity and transparency by separating perception, understanding, and reasoning, unlike black-box models. It was evaluated on various animal imagery tasks, showing its ability to bridge visual content with structured, human-interpretable queries. This framework presents a promising approach for monitoring animal populations with a combination of visual data interpretation and natural language processing.<br /><br />Summary: <div>
arXiv:2506.14823v1 Announce Type: cross 
Abstract: Monitoring animal populations in natural environments requires systems that can interpret both visual data and human language queries. This work introduces ViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for interpretable animal monitoring. ViLLa integrates three core components: a visual detection module for identifying animals and their spatial locations in images, a language parser for understanding natural language queries, and a symbolic reasoning layer that applies logic-based inference to answer those queries. Given an image and a question such as "How many dogs are in the scene?" or "Where is the buffalo?", the system grounds visual detections into symbolic facts and uses predefined rules to compute accurate answers related to count, presence, and location. Unlike end-to-end black-box models, ViLLa separates perception, understanding, and reasoning, offering modularity and transparency. The system was evaluated on a range of animal imagery tasks and demonstrates the ability to bridge visual content with structured, human-interpretable queries.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2506.14824</link>
<guid>https://arxiv.org/abs/2506.14824</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Multimodal Large Language Models (MLLMs), Privacy Preservation, NanoEdge, Collaborative Model Training  
Summary:  
FedNano introduces a novel Federated Learning (FL) framework for Multimodal Large Language Models (MLLMs) by centralizing the LLM on the server while employing NanoEdge for client-specific adaptation. NanoEdge uses modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation, reducing client-side storage by 95% and communication overhead to only 0.01% of model parameters. This design enables handling of heterogeneous client data and resource constraints while ensuring privacy. FedNano outperforms existing FL baselines, making MLLMs feasible in FL scenarios and facilitating scalable decentralized multimodal AI systems. <div>
arXiv:2506.14824v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction</title>
<link>https://arxiv.org/abs/2506.14825</link>
<guid>https://arxiv.org/abs/2506.14825</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic occupancy prediction, GraphGSOcc model, Dual Gaussians Graph Attention, Multi-scale Graph Attention, SurroundOcc dataset

Summary: 
The study addresses challenges in 3D semantic occupancy prediction for autonomous driving by introducing the GraphGSOcc model. This model combines semantic and geometric graph Transformers to improve existing 3D Gaussian Splating methods. The Dual Gaussians Graph Attention dynamically constructs geometric and semantic graphs to enhance feature aggregation and capture semantic relationships. The Multi-scale Graph Attention framework optimizes boundary details and object-level topology through fine-grained and coarse-grained attention. Experimental results on the SurroundOcc dataset show the model achieves an mIoU of 24.10% with reduced GPU memory usage to 6.1GB. This represents a 1.97% mIoU improvement and a 13.7% memory reduction compared to GaussianWorld. The GraphGSOcc model demonstrates advancements in 3D semantic occupancy prediction for autonomous driving applications. 

<br /><br />Summary: <div>
arXiv:2506.14825v1 Announce Type: cross 
Abstract: Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splating (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, and (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarse-grained attention at higher layers models object-level topology. Experiments on the SurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Interest-aware Graph Learning for Group Identification</title>
<link>https://arxiv.org/abs/2506.14826</link>
<guid>https://arxiv.org/abs/2506.14826</guid>
<content:encoded><![CDATA[
<div> Evolutionary Relationship, Group Identification, Social Media, Interest Modeling, Collaborative Filtering<br />
Summary: <br />
The paper discusses the importance of group identification in social media platforms due to the growing participation in group activities. It highlights the collaborative evolution relationship between group-level and item-level interests of users, where joining a group expands item interests and vice versa. The proposed CI4GI model aims to capture this relationship by incorporating an interest enhancement strategy to identify additional user interests from group interactions and optimizing negative sample selection based on interest distributions. These enhancements result in better performance compared to existing models, as shown in experiments on real-world datasets. This research sheds light on the intricacies of user interests in group identification, presenting a more accurate and efficient approach to recommend groups to users on social platforms. <br /> <div>
arXiv:2506.14826v1 Announce Type: cross 
Abstract: With the popularity of social media, an increasing number of users are joining group activities on online social platforms. This elicits the requirement of group identification (GI), which is to recommend groups to users. We reveal that users are influenced by both group-level and item-level interests, and these dual-level interests have a collaborative evolution relationship: joining a group expands the user's item interests, further prompting the user to join new groups. Ultimately, the two interests tend to align dynamically. However, existing GI methods fail to fully model this collaborative evolution relationship, ignoring the enhancement of group-level interests on item-level interests, and suffering from false-negative samples when aligning cross-level interests. In order to fully model the collaborative evolution relationship between dual-level user interests, we propose CI4GI, a Collaborative Interest-aware model for Group Identification. Specifically, we design an interest enhancement strategy that identifies additional interests of users from the items interacted with by the groups they have joined as a supplement to item-level interests. In addition, we adopt the distance between interest distributions of two users to optimize the identification of negative samples for a user, mitigating the interference of false-negative samples during cross-level interests alignment. The results of experiments on three real-world datasets demonstrate that CI4GI significantly outperforms state-of-the-art models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning</title>
<link>https://arxiv.org/abs/2506.14827</link>
<guid>https://arxiv.org/abs/2506.14827</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated video, detection, interpretability, defect-level annotations, temporal-spatial localization

Summary:
DAVID-X dataset is introduced to pair AI-generated videos with detailed defect-level annotations and written rationales. The dataset is used to train DAVID-XR1, a video-language model that provides interpretable visual reasoning, including defect categorization, temporal-spatial localization, and natural language explanations. This approach transforms the detection of AI-generated video from a black-box decision into a transparent and verifiable process. The model, fine-tuned on the compact dataset and enhanced with chain-of-thought distillation, shows strong generalization across different generators and generation modes. The results demonstrate the effectiveness of explainable detection methods for trustworthy identification of AI-generated video content.

<br /><br />Summary: 
- Introduction of DAVID-X dataset with defect-level annotations and written rationales
- Development of DAVID-XR1 video-language model for interpretable visual reasoning
- Transformation of AI-generated video detection into a transparent process 
- Strong generalization of the model across various generators and generation modes
- Effectiveness of explainable detection methods for trustworthy identification of AI-generated video content <div>
arXiv:2506.14827v1 Announce Type: cross 
Abstract: As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges &amp; Opportunities</title>
<link>https://arxiv.org/abs/2506.14829</link>
<guid>https://arxiv.org/abs/2506.14829</guid>
<content:encoded><![CDATA[
<div> Keywords: AI for Social Impact, challenges, real-world impact, partnerships, best practices

Summary: 
This paper explores the challenges faced by AI for Social Impact (AI4SI) projects in achieving tangible impact. It highlights difficulties in identifying collaborators, progressing beyond proof-of-concept stages, and transitioning to production-level solutions. The unique challenges of AI4SI research, often overlooked in the broader AI community, are discussed. Structural, communication, collaboration, and operational challenges are identified as barriers to deployment. Through semi-structured interviews with AI4SI researchers and the authors' own experiences, best practices and actionable strategies are synthesized. The paper aims to provide a practical reference guide for researchers and organizations involved in socially impactful AI collaborations. 

Summary: <div>
arXiv:2506.14829v1 Announce Type: cross 
Abstract: In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects focus on harnessing AI to address societal issues in areas such as healthcare, social justice, etc. Unfortunately, despite growing interest in AI4SI, achieving tangible, on-the-ground impact remains a significant challenge. For example, identifying and engaging motivated collaborators who are willing to co-design and deploy AI based solutions in real-world settings is often difficult. Even when such partnerships are established, many AI4SI projects "fail" to progress beyond the proof-of-concept stage, and hence, are unable to transition to at-scale production-level solutions. Furthermore, the unique challenges faced by AI4SI researchers are not always fully recognized within the broader AI community, where such work is sometimes viewed as primarily applied and not aligning with the traditional criteria for novelty emphasized in core AI venues. This paper attempts to shine a light on the diverse challenges faced in AI4SI research by diagnosing a multitude of factors that prevent AI4SI partnerships from achieving real-world impact on the ground. Drawing on semi-structured interviews with six leading AI4SI researchers - complemented by the authors' own lived experiences in conducting AI4SI research - this paper attempts to understand the day-to-day difficulties faced in developing and deploying socially impactful AI solutions. Through thematic analysis, we identify structural and organizational, communication, collaboration, and operational challenges as key barriers to deployment. While there are no easy fixes, we synthesize best practices and actionable strategies drawn from these interviews and our own work in this space. In doing so, we hope this paper serves as a practical reference guide for AI4SI researchers and partner organizations seeking to engage more effectively in socially impactful AI collaborations.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model</title>
<link>https://arxiv.org/abs/2506.14830</link>
<guid>https://arxiv.org/abs/2506.14830</guid>
<content:encoded><![CDATA[
<div> SSD health prediction, BiGRU-MHA model, multi-head attention mechanism, data reliability, storage device health classification <br />
<br />
Summary: 
This study introduces a hybrid BiGRU-MHA model for predicting the health state of SSDs, crucial for ensuring data reliability. The model incorporates a multi-head attention mechanism for enhanced accuracy and stability in storage device health classification. By combining temporal feature extraction and key information focusing capabilities, the model utilizes the bidirectional timing modeling of the BiGRU network to capture forward and backward dependencies of SSD degradation features. The multi-head attention mechanism dynamically assigns feature weights, improving sensitivity to critical health indicators. Experimental results demonstrate high classification accuracy on both training and test sets, with minimal performance gap and robust binary classification performance shown through the ROC curve analysis. This technical approach provides a verifiable method for preventive maintenance of storage systems, reducing data loss risks, and optimizing maintenance costs for cloud computing and edge storage environments. <div>
arXiv:2506.14830v1 Announce Type: cross 
Abstract: Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a hybrid BiGRU-MHA model that incorporates a multi-head attention mechanism to enhance the accuracy and stability of storage device health classification. The model innovatively integrates temporal feature extraction and key information focusing capabilities. Specifically, it leverages the bidirectional timing modeling advantages of the BiGRU network to capture both forward and backward dependencies of SSD degradation features. Simultaneously, the multi-head attention mechanism dynamically assigns feature weights, improving the model's sensitivity to critical health indicators. Experimental results show that the proposed model achieves classification accuracies of 92.70% on the training set and 92.44% on the test set, with a minimal performance gap of only 0.26%, demonstrating excellent generalization ability. Further analysis using the receiver operating characteristic (ROC) curve shows an area under the curve (AUC) of 0.94 on the test set, confirming the model's robust binary classification performance. This work not only presents a new technical approach for SSD health prediction but also addresses the generalization bottleneck of traditional models, offering a verifiable method with practical value for preventive maintenance of industrial-grade storage systems. The results show the model can significantly reduce data loss risks by providing early failure warnings and help optimize maintenance costs, supporting intelligent decision-making in building reliable storage systems for cloud computing data centers and edge storage environments.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes</title>
<link>https://arxiv.org/abs/2506.14832</link>
<guid>https://arxiv.org/abs/2506.14832</guid>
<content:encoded><![CDATA[
<div> architecture, generative design, 3D forms, convolutional neural network, dataset

Summary:
ArchForms-4000 dataset includes 2,000 architect-designed and 2,000 machine-generated 3D forms. ArchShapeNet, a 3D convolutional neural network, was developed to classify and analyze architectural forms, outperforming human experts with 94.29% accuracy. The study showed that human-designed forms excel in spatial organization, proportional harmony, and detail refinement. The incorporation of a saliency module in ArchShapeNet highlights key spatial features aligned with architectural reasoning. This research provides valuable insights for enhancing generative design tools, emphasizing the strengths of human-designed forms and the potential for improvement in machine-generated forms. <br /><br />Summary: <div>
arXiv:2506.14832v1 Announce Type: cross 
Abstract: In contemporary architectural design, the growing complexity and diversity of design demands have made generative plugin tools essential for quickly producing initial concepts and exploring novel 3D forms. However, objectively analyzing the differences between human-designed and machine-generated 3D forms remains a challenge, limiting our understanding of their respective strengths and hindering the advancement of generative tools.
  To address this, we built ArchForms-4000, a dataset containing 2,000 architect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet, a 3D convolutional neural network tailored for classifying and analyzing architectural forms, incorporating a saliency module to highlight key spatial features aligned with architectural reasoning; And conducted comparative experiments showing our model outperforms human experts in distinguishing form origins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.
  This study not only highlights the distinctive advantages of human-designed forms in spatial organization, proportional harmony, and detail refinement but also provides valuable insights for enhancing generative design tools in the future.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices</title>
<link>https://arxiv.org/abs/2506.14833</link>
<guid>https://arxiv.org/abs/2506.14833</guid>
<content:encoded><![CDATA[
<div> Keywords: high-performance, low-latency, video surveillance system, adaptive frame buffering algorithm, MobileNetV2

Summary: 
The paper presents a high-performance and low-latency video surveillance system tailored for resource-constrained environments. It introduces an adaptive frame buffering algorithm based on entropy, integrated with MobileNetV2, enabling high throughput and minimal latency. The system achieves sub-50ms end-to-end inference latency on devices like Raspberry Pi, Amazon, and NVIDIA Jetson Nano. With a focus on video surveillance, it maintains over 92% detection accuracy under varying conditions. Comparative and ablation experiments validate the efficacy of the design. Moreover, the architecture is scalable, cost-effective, and compliant with stringent data privacy regulations. This ensures its compatibility with smart city setups or embedded security systems.<br /><br />Summary: <div>
arXiv:2506.14833v1 Announce Type: cross 
Abstract: This paper describes a high-performance, low-latency video surveillance system designed for resource-constrained environments. We have proposed a formal entropy-based adaptive frame buffering algorithm and integrated that with MobileNetV2 to achieve high throughput with low latency. The system is capable of processing live streams of video with sub-50ms end-to-end inference latency on resource-constrained devices (embedding platforms) such as Raspberry Pi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection accuracy on standard datasets focused on video surveillance and exhibits robustness to varying lighting, backgrounds, and speeds. A number of comparative and ablation experiments validate the effectiveness of our design. Finally, our architecture is scalable, inexpensive, and compliant with stricter data privacy regulations than common surveillance systems, so that the system could coexist in a smart city or embedded security architecture.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection</title>
<link>https://arxiv.org/abs/2506.14834</link>
<guid>https://arxiv.org/abs/2506.14834</guid>
<content:encoded><![CDATA[
<div> Detection, Diabetic Retinopathy, Deep Learning, Edge Devices, Healthcare<br />
<br />
Summary: <br />
Diabetic Retinopathy (DR) is a major cause of vision impairment in diabetes patients, with a growing global prevalence. This study proposes a novel approach using Edge Impulse to deploy multiple deep learning models on edge devices for real-time DR detection. A dataset of 3,662 retinal fundus images was curated and enhanced for training various Convolutional Neural Networks (CNNs). Models like MobileNet achieved high accuracy, while others like SqueezeNet showed efficiency in terms of size and speed. Evaluations on different edge hardware platforms demonstrated the potential for accurate and timely DR diagnosis, especially in resource-constrained healthcare settings. This integration of edge AI technology offers a scalable and cost-effective solution for early DR detection, aiding in providing timely and accurate diagnosis in remote healthcare environments. <div>
arXiv:2506.14834v1 Announce Type: cross 
Abstract: Diabetic Retinopathy (DR), a leading cause of vision impairment in individuals with diabetes, affects approximately 34.6% of diabetes patients globally, with the number of cases projected to reach 242 million by 2045. Traditional DR diagnosis relies on the manual examination of retinal fundus images, which is both time-consuming and resource intensive. This study presents a novel solution using Edge Impulse to deploy multiple deep learning models for real-time DR detection on edge devices. A robust dataset of over 3,662 retinal fundus images, sourced from the Kaggle EyePACS dataset, was curated, and enhanced through preprocessing techniques, including augmentation and normalization. Using TensorFlow, various Convolutional Neural Networks (CNNs), such as MobileNet, ShuffleNet, SqueezeNet, and a custom Deep Neural Network (DNN), were designed, trained, and optimized for edge deployment. The models were converted to TensorFlowLite and quantized to 8-bit integers to reduce their size and enhance inference speed, with minimal trade-offs in accuracy. Performance evaluations across different edge hardware platforms, including smartphones and microcontrollers, highlighted key metrics such as inference speed, accuracy, precision, and resource utilization. MobileNet achieved an accuracy of 96.45%, while SqueezeNet demonstrated strong real-time performance with a small model size of 176 KB and latency of just 17 ms on GPU. ShuffleNet and the custom DNN achieved moderate accuracy but excelled in resource efficiency, making them suitable for lower-end devices. This integration of edge AI technology into healthcare presents a scalable, cost-effective solution for early DR detection, providing timely and accurate diagnosis, especially in resource-constrained and remote healthcare settings.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction</title>
<link>https://arxiv.org/abs/2506.14837</link>
<guid>https://arxiv.org/abs/2506.14837</guid>
<content:encoded><![CDATA[
<div> language models, chart-to-code generation, structured instruction, iterative refinement, visual understanding

Summary:<br />
- The article discusses the challenge of improving the performance of multimodal large language models (MLLMs) on chart-to-code generation tasks.
- A new method called {ChartIR} is proposed, which involves structured instructions for visual understanding and code translation.
- The method distinguishes between visual understanding and code translation tasks and utilizes description and difference instructions to facilitate the translation process.
- The overall chart generation pipeline is divided into initial code generation and iterative refinement stages to enhance the final output progressively.
- Experimental results demonstrate that the {ChartIR} method outperforms other approaches on both open-source model Qwen2-VL and closed-source model GPT-4o.<br /><br />Summary: <div>
arXiv:2506.14837v1 Announce Type: cross 
Abstract: Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers</title>
<link>https://arxiv.org/abs/2506.14842</link>
<guid>https://arxiv.org/abs/2506.14842</guid>
<content:encoded><![CDATA[
<div> Framework, Few-shot image classification, In-context learning, Image embeddings, Pretraining <br />
Summary: <br />
The article introduces PictSure, an in-context learning (ICL) framework for few-shot image classification (FSIC) that emphasizes the importance of image embeddings. It examines different aspects of embedding models such as architecture, pretraining objectives, and fine-tuning strategies to improve FSIC performance. The study shows that the success of training and out-of-domain performance rely heavily on how the embedding models are pretrained. PictSure surpasses existing ICL-based FSIC models on out-of-domain benchmarks while maintaining comparable results on in-domain tasks, highlighting the importance of proper embedding model training in FSIC pipelines. The research provides insights into optimizing visual encoders for better generalization across domains in data-scarce environments. The code for PictSure is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2506.14842v1 Announce Type: cross 
Abstract: Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach</title>
<link>https://arxiv.org/abs/2506.14846</link>
<guid>https://arxiv.org/abs/2506.14846</guid>
<content:encoded><![CDATA[
<div> receptive field, feature extraction, computational cost, model accuracy, kernel size determination 
<br />
Summary: 
The paper introduces the Best Kernel Size Estimation Function (BKSEF) for determining optimal kernel sizes in Convolutional Neural Networks (CNNs). BKSEF considers information gain, computational efficiency, and accuracy improvements, using principles from information theory, signal processing, and learning theory. Experiments on various datasets show that BKSEF-guided architectures outperform traditional models with uniform 3x3 kernels, achieving improved accuracy and reduced FLOPs. Real-world case studies in medical image classification and traffic sign recognition validate the approach, demonstrating enhanced interpretability, reduced latency, and model size with minimal accuracy trade-offs. The results suggest that kernel size can be an actively optimized parameter in CNNs, offering practical heuristics and theoretical support for efficient and application-aware design. BKSEF can be integrated into neural architecture search pipelines and real-time systems, providing a new perspective on CNN optimization. 
<br /> <div>
arXiv:2506.14846v1 Announce Type: cross 
Abstract: Kernel size selection in Convolutional Neural Networks (CNNs) is a critical but often overlooked design decision that affects receptive field, feature extraction, computational cost, and model accuracy. This paper proposes the Best Kernel Size Estimation Function (BKSEF), a mathematically grounded and empirically validated framework for optimal, layer-wise kernel size determination. BKSEF balances information gain, computational efficiency, and accuracy improvements by integrating principles from information theory, signal processing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100, ImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided architectures achieve up to 3.1 percent accuracy improvement and 42.8 percent reduction in FLOPs compared to traditional models using uniform 3x3 kernels. Two real-world case studies further validate the approach: one for medical image classification in a cloud-based setup, and another for traffic sign recognition on edge devices. The former achieved enhanced interpretability and accuracy, while the latter reduced latency and model size significantly, with minimal accuracy trade-off. These results show that kernel size can be an active, optimizable parameter rather than a fixed heuristic. BKSEF provides practical heuristics and theoretical support for researchers and developers seeking efficient and application-aware CNN designs. It is suitable for integration into neural architecture search pipelines and real-time systems, offering a new perspective on CNN optimization.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Serving of LLM Applications with Probabilistic Demand Modeling</title>
<link>https://arxiv.org/abs/2506.14851</link>
<guid>https://arxiv.org/abs/2506.14851</guid>
<content:encoded><![CDATA[
<div> Graph, Large Language Models, Application Serving, Probabilistic Demand, Gittins Policy

Summary:
Hermes, a new approach for efficient serving of Large Language Models (LLMs) applications, leverages a Probabilistic Demand Graph (PDGraph) model to accurately predict resource demands. By using the Gittins policy, Hermes determines the optimal scheduling order to minimize average application completion time. Additionally, Hermes utilizes the PDGraph model to prewarm cold backends at the appropriate times, further improving overall efficiency. Experimental results show significant improvements in application serving efficiency, with Hermes reducing average completion time by over 70% and the P95 completion time by over 80%. <div>
arXiv:2506.14851v1 Announce Type: cross 
Abstract: Applications based on Large Language Models (LLMs) contains a series of tasks to address real-world problems with boosted capability, which have dynamic demand volumes on diverse backends. Existing serving systems treat the resource demands of LLM applications as a blackbox, compromising end-to-end efficiency due to improper queuing order and backend warm up latency. We find that the resource demands of LLM applications can be modeled in a general and accurate manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which leverages PDGraph for efficient serving of LLM applications. Confronting probabilistic demand description, Hermes applies the Gittins policy to determine the scheduling order that can minimize the average application completion time. It also uses the PDGraph model to help prewarm cold backends at proper moments. Experiments with diverse LLM applications confirm that Hermes can effectively improve the application serving efficiency, reducing the average completion time by over 70% and the P95 completion time by over 80%.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching</title>
<link>https://arxiv.org/abs/2506.14852</link>
<guid>https://arxiv.org/abs/2506.14852</guid>
<content:encoded><![CDATA[
<div> caching, agentic applications, planning, LLM-based, structured plan templates <br />
<br />
Summary: <br />
This paper introduces a novel approach called agentic plan caching to reduce the costs of serving LLM-based agentic applications. Unlike existing caching techniques, agentic plan caching focuses on extracting, storing, and reusing structured plan templates from completed agent executions at test-time. By utilizing keyword extraction and lightweight models, the system can match new requests against cached plans and adapt them to task-specific plans with contexts. Evaluation across real-world applications demonstrates that the proposed system can reduce costs by 46.62% on average while maintaining performance levels. This approach offers a more efficient solution for serving agentic applications, complementing existing LLM serving infrastructures. <br /> <div>
arXiv:2506.14852v1 Announce Type: cross 
Abstract: LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis</title>
<link>https://arxiv.org/abs/2506.14854</link>
<guid>https://arxiv.org/abs/2506.14854</guid>
<content:encoded><![CDATA[
<div> deep learning, video annotation, retail, object detection, automation
<br />
Summary:
<br />
An innovative deep learning-based approach has been proposed to automate key-frame identification in retail videos, leading to automatic annotations of products and customers. By leveraging deep neural networks and object detection techniques tailored for retail environments, the method achieves accuracy comparable to human annotator labeling while significantly enhancing efficiency and saving costs. With an average of 2 times cost savings in video annotation, the approach allows human annotators to verify/adjust less than 5% of detected frames, reducing operational expenses. This automation of key-frame detection not only saves time and effort in retail video labeling tasks but also proves valuable for various retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring. <div>
arXiv:2506.14854v1 Announce Type: cross 
Abstract: Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers</title>
<link>https://arxiv.org/abs/2506.14855</link>
<guid>https://arxiv.org/abs/2506.14855</guid>
<content:encoded><![CDATA[
<div> control, Model Predictive Path Integral, Feedback-MPPI, robotic platforms, dynamic locomotion <br />
Summary: <br />
This paper introduces the Feedback-MPPI (F-MPPI) framework, which enhances the Model Predictive Path Integral control by incorporating local linear feedback gains for rapid closed-loop corrections. The F-MPPI approach is designed to address the computational demands of real-time, high-frequency robotic control scenarios. Through simulations and real-world experiments on quadrupedal robots and quadrotors, the study demonstrates that F-MPPI significantly improves control performance and stability. By using sensitivity analysis and local feedback, F-MPPI enables robust and high-frequency operation suitable for complex robotic systems. The results showcase the effectiveness of F-MPPI in enhancing the capabilities of robotic platforms, allowing for dynamic locomotion on uneven terrain and executing aggressive maneuvers with onboard computation. <div>
arXiv:2506.14855v1 Announce Type: cross 
Abstract: Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction</title>
<link>https://arxiv.org/abs/2506.14856</link>
<guid>https://arxiv.org/abs/2506.14856</guid>
<content:encoded><![CDATA[
<div> Active view selection, 3D object reconstruction, neural uncertainty maps, UPNet, neural rendering models <br />
<br />
Summary: 
The article introduces a novel approach for active view selection in 3D object reconstruction using neural uncertainty maps predicted by a deep neural network called UPNet. The UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, which helps in selecting the most informative viewpoints for accurate reconstruction. The approach leverages heuristics from natural objects to train UPNet, aggregates predicted uncertainty maps to suppress redundant viewpoints, and achieves comparable reconstruction accuracy with fewer viewpoints. Additionally, the method significantly reduces computational overhead and achieves a speedup of up to 400 times compared to baseline methods, along with reductions in CPU, RAM, and GPU usage. Notably, the approach generalizes effectively to novel object categories without requiring additional training. <div>
arXiv:2506.14856v1 Announce Type: cross 
Abstract: Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic Foundation Models</title>
<link>https://arxiv.org/abs/2506.14861</link>
<guid>https://arxiv.org/abs/2506.14861</guid>
<content:encoded><![CDATA[
arXiv:2506.14861v1 Announce Type: cross 
Abstract: Transcriptomic foundation models (TFMs) have recently emerged as powerful tools for analyzing gene expression in cells and tissues, supporting key tasks such as cell-type annotation, batch correction, and perturbation prediction. However, the diversity of model implementations and training strategies across recent TFMs, though promising, makes it challenging to isolate the contribution of individual design choices or evaluate their potential synergies. This hinders the field's ability to converge on best practices and limits the reproducibility of insights across studies. We present BMFM-RNA, an open-source, modular software package that unifies diverse TFM pretraining and fine-tuning objectives within a single framework. Leveraging this capability, we introduce a novel training objective, whole cell expression decoder (WCED), which captures global expression patterns using an autoencoder-like CLS bottleneck representation. In this paper, we describe the framework, supported input representations, and training objectives. We evaluated four model checkpoints pretrained on CELLxGENE using combinations of masked language modeling (MLM), WCED and multitask learning. Using the benchmarking capabilities of BMFM-RNA, we show that WCED-based models achieve performance that matches or exceeds state-of-the-art approaches like scGPT across more than a dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available as part of the biomed-multi-omics project ( https://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible foundation for systematic benchmarking and community-driven exploration of optimal TFM training strategies, enabling the development of more effective tools to leverage the latest advances in AI for understanding cell biology.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifiability by common backdoor in summary causal graphs of time series</title>
<link>https://arxiv.org/abs/2506.14862</link>
<guid>https://arxiv.org/abs/2506.14862</guid>
<content:encoded><![CDATA[
arXiv:2506.14862v1 Announce Type: cross 
Abstract: The identifiability problem for interventions aims at assessing whether the total effect of some given interventions can be written with a do-free formula, and thus be computed from observational data only. We study this problem, considering multiple interventions and multiple effects, in the context of time series when only abstractions of the true causal graph in the form of summary causal graphs are available. We focus in this study on identifiability by a common backdoor set, and establish, for time series with and without consistency throughout time, conditions under which such a set exists. We also provide algorithms of limited complexity to decide whether the problem is identifiable or not.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preparing for the Intelligence Explosion</title>
<link>https://arxiv.org/abs/2506.14863</link>
<guid>https://arxiv.org/abs/2506.14863</guid>
<content:encoded><![CDATA[
arXiv:2506.14863v1 Announce Type: cross 
Abstract: AI that can accelerate research could drive a century of technological progress over just a few years. During such a period, new technological or political developments will raise consequential and hard-to-reverse decisions, in rapid succession. We call these developments grand challenges. These challenges include new weapons of mass destruction, AI-enabled autocracies, races to grab offworld resources, and digital beings worthy of moral consideration, as well as opportunities to dramatically improve quality of life and collective decision-making. We argue that these challenges cannot always be delegated to future AI systems, and suggest things we can do today to meaningfully improve our prospects. AGI preparedness is therefore not just about ensuring that advanced AI systems are aligned: we should be preparing, now, for the disorienting range of developments an intelligence explosion would bring.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2506.14907</link>
<guid>https://arxiv.org/abs/2506.14907</guid>
<content:encoded><![CDATA[
arXiv:2506.14907v1 Announce Type: cross 
Abstract: Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)</title>
<link>https://arxiv.org/abs/2506.14909</link>
<guid>https://arxiv.org/abs/2506.14909</guid>
<content:encoded><![CDATA[
arXiv:2506.14909v1 Announce Type: cross 
Abstract: Background: Facial appearance offers a noninvasive window into health. We built FAHR-Face, a foundation model trained on >40 million facial images and fine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge) and survival risk prediction (FAHR-FaceSurvival).
  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on 749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of cancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting) and independence (saliency mapping) was tested extensively. Both models were clinically tested in two independent cancer patient datasets with survival analyzed by multivariable Cox models and adjusted for clinical prognostic factors.
  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error of 5.1 years on public datasets, outperforming benchmark models and maintaining accuracy across the full human lifespan. In cancer patients, FAHR-FaceAge outperformed a prior facial age estimation model in survival prognostication. FAHR-FaceSurvival demonstrated robust prediction of mortality, and the highest-risk quartile had more than triple the mortality of the lowest (adjusted hazard ratio 3.22; P<0.001). These findings were validated in the independent cohort and both models showed generalizability across age, sex, race and cancer subgroups. The two algorithms provided distinct, complementary prognostic information; saliency mapping revealed each model relied on distinct facial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved prognostic accuracy.
  Interpretation: A single foundation model can generate inexpensive, scalable facial biomarkers that capture both biological ageing and disease-related mortality risk. The foundation model enabled effective training using relatively small clinical datasets.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning</title>
<link>https://arxiv.org/abs/2506.14923</link>
<guid>https://arxiv.org/abs/2506.14923</guid>
<content:encoded><![CDATA[
arXiv:2506.14923v1 Announce Type: cross 
Abstract: Microearthquakes (MEQs) generated by subsurface fluid injection record the evolving stress state and permeability of reservoirs. Forecasting their full spatiotemporal evolution is therefore critical for applications such as enhanced geothermal systems (EGS), CO$_2$ sequestration and other geo-engineering applications. We present a transformer-based deep learning model that ingests hydraulic stimulation history and prior MEQ observations to forecast four key quantities: cumulative MEQ count, cumulative logarithmic seismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of the MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model achieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the 15-second forecast horizon across all targets, and supplies uncertainty estimates through a learned standard deviation term. These accurate, uncertainty-quantified forecasts enable real-time inference of fracture propagation and permeability evolution, demonstrating the strong potential of deep-learning approaches to improve seismic-risk assessment and guide mitigation strategies in future fluid-injection operations.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance</title>
<link>https://arxiv.org/abs/2506.14927</link>
<guid>https://arxiv.org/abs/2506.14927</guid>
<content:encoded><![CDATA[
arXiv:2506.14927v1 Announce Type: cross 
Abstract: Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBENCH poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
arXiv:2506.14933v1 Announce Type: cross 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Determina\c{c}\~ao Autom\'atica de Limiar de Detec\c{c}\~ao de Ataques em Redes de Computadores Utilizando Autoencoders</title>
<link>https://arxiv.org/abs/2506.14937</link>
<guid>https://arxiv.org/abs/2506.14937</guid>
<content:encoded><![CDATA[
arXiv:2506.14937v1 Announce Type: cross 
Abstract: Currently, digital security mechanisms like Anomaly Detection Systems using Autoencoders (AE) show great potential for bypassing problems intrinsic to the data, such as data imbalance. Because AE use a non-trivial and nonstandardized separation threshold to classify the extracted reconstruction error, the definition of this threshold directly impacts the performance of the detection process. Thus, this work proposes the automatic definition of this threshold using some machine learning algorithms. For this, three algorithms were evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flat Channels to Infinity in Neural Loss Landscapes</title>
<link>https://arxiv.org/abs/2506.14951</link>
<guid>https://arxiv.org/abs/2506.14951</guid>
<content:encoded><![CDATA[
arXiv:2506.14951v1 Announce Type: cross 
Abstract: The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot \mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot \mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective</title>
<link>https://arxiv.org/abs/2506.14965</link>
<guid>https://arxiv.org/abs/2506.14965</guid>
<content:encoded><![CDATA[
arXiv:2506.14965v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization</title>
<link>https://arxiv.org/abs/2506.14968</link>
<guid>https://arxiv.org/abs/2506.14968</guid>
<content:encoded><![CDATA[
arXiv:2506.14968v1 Announce Type: cross 
Abstract: Physical caregiving robots hold promise for improving the quality of life of millions worldwide who require assistance with feeding. However, in-home meal assistance remains challenging due to the diversity of activities (e.g., eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV), food items, and user preferences that arise during deployment. In this work, we propose FEAST, a flexible mealtime-assistance system that can be personalized in-the-wild to meet the unique needs of individual care recipients. Developed in collaboration with two community researchers and informed by a formative study with a diverse group of care recipients, our system is guided by three key tenets for in-the-wild personalization: adaptability, transparency, and safety. FEAST embodies these principles through: (i) modular hardware that enables switching between assisted feeding, drinking, and mouth-wiping, (ii) diverse interaction methods, including a web interface, head gestures, and physical buttons, to accommodate diverse functional abilities and preferences, and (iii) parameterized behavior trees that can be safely and transparently adapted using a large language model. We evaluate our system based on the personalization requirements identified in our formative study, demonstrating that FEAST offers a wide range of transparent and safe adaptations and outperforms a state-of-the-art baseline limited to fixed customizations. To demonstrate real-world applicability, we conduct an in-home user study with two care recipients (who are community researchers), feeding them three meals each across three diverse scenarios. We further assess FEAST's ecological validity by evaluating with an Occupational Therapist previously unfamiliar with the system. In all cases, users successfully personalize FEAST to meet their individual needs and preferences. Website: https://emprise.cs.cornell.edu/feast
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition</title>
<link>https://arxiv.org/abs/2506.14973</link>
<guid>https://arxiv.org/abs/2506.14973</guid>
<content:encoded><![CDATA[
arXiv:2506.14973v1 Announce Type: cross 
Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities. However, the ability of Speech LLMs to comprehend and process multi-channel audio with spatial cues remains a relatively uninvestigated area of research. In this work, we present directional-SpeechLlama, a novel approach that leverages the microphone array of smart glasses to achieve directional speech recognition, source localization, and bystander cross-talk suppression. To enhance the model's ability to understand directivity, we propose two key techniques: serialized directional output training (S-DOT) and contrastive direction data augmentation (CDDA). Experimental results show that our proposed directional-SpeechLlama effectively captures the relationship between textual cues and spatial audio, yielding strong performance in both speech recognition and source localization tasks.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</title>
<link>https://arxiv.org/abs/2506.14988</link>
<guid>https://arxiv.org/abs/2506.14988</guid>
<content:encoded><![CDATA[
arXiv:2506.14988v1 Announce Type: cross 
Abstract: We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Image Reconstruction and Diffusion Parameter Estimation Using a Temporal Convolutional Network Model of Gradient Trajectory Errors</title>
<link>https://arxiv.org/abs/2506.14995</link>
<guid>https://arxiv.org/abs/2506.14995</guid>
<content:encoded><![CDATA[
arXiv:2506.14995v1 Announce Type: cross 
Abstract: Summary: Errors in gradient trajectories introduce significant artifacts and distortions in magnetic resonance images, particularly in non-Cartesian imaging sequences, where imperfect gradient waveforms can greatly reduce image quality. Purpose: Our objective is to develop a general, nonlinear gradient system model that can accurately predict gradient distortions using convolutional networks. Methods: A set of training gradient waveforms were measured on a small animal imaging system, and used to train a temporal convolutional network to predict the gradient waveforms produced by the imaging system. Results: The trained network was able to accurately predict nonlinear distortions produced by the gradient system. Network prediction of gradient waveforms was incorporated into the image reconstruction pipeline and provided improvements in image quality and diffusion parameter mapping compared to both the nominal gradient waveform and the gradient impulse response function. Conclusion: Temporal convolutional networks can more accurately model gradient system behavior than existing linear methods and may be used to retrospectively correct gradient errors.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings</title>
<link>https://arxiv.org/abs/2506.15001</link>
<guid>https://arxiv.org/abs/2506.15001</guid>
<content:encoded><![CDATA[
arXiv:2506.15001v1 Announce Type: cross 
Abstract: In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights. This is achieved by introducing a special memory token, whose embedding is optimized through training on a fixed sequence. When prompted with this embedding, the model reconstructs the fixed sequence exactly. We evaluate this phenomenon across English and Spanish datasets, sequences of up to approximately 240 tokens, and model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B successfully reconstructs all tested sequences. Our findings highlight an interesting capability of LLMs and suggest potential applications in memory-based retrieval, compression, and controlled text generation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Intelligence: Designing Data Centers for Next-Gen Language Models</title>
<link>https://arxiv.org/abs/2506.15006</link>
<guid>https://arxiv.org/abs/2506.15006</guid>
<content:encoded><![CDATA[
arXiv:2506.15006v1 Announce Type: cross 
Abstract: The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8 trillion parameters - demands a radical rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, wider scale-out domains, and larger memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens per sec / Peak flops of the hardware) and overall throughput. For the co-design study, we extended and validated a performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output</title>
<link>https://arxiv.org/abs/2506.15008</link>
<guid>https://arxiv.org/abs/2506.15008</guid>
<content:encoded><![CDATA[
arXiv:2506.15008v1 Announce Type: cross 
Abstract: Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts. While generative AI can produce visually appealing images they often lack actionable data for designers In this work, we propose a novel pipeline that integrates DALL-E 3 with a materials dataset to enrich AI-generated designs with sustainability metrics and material usage insights. After the model generates an interior design image, a post-processing module identifies the top ten materials present and pairs them with carbon dioxide equivalent (CO2e) values from a general materials dictionary. This approach allows designers to immediately evaluate environmental impacts and refine prompts accordingly. We evaluate the system through three user tests: (1) no mention of sustainability to the user prior to the prompting process with generative AI, (2) sustainability goals communicated to the user before prompting, and (3) sustainability goals communicated along with quantitative CO2e data included in the generative AI outputs. Our qualitative and quantitative analyses reveal that the introduction of sustainability metrics in the third test leads to more informed design decisions, however, it can also trigger decision fatigue and lower overall satisfaction. Nevertheless, the majority of participants reported incorporating sustainability principles into their workflows in the third test, underscoring the potential of integrated metrics to guide more ecologically responsible practices. Our findings showcase the importance of balancing design freedom with practical constraints, offering a clear path toward holistic, data-driven solutions in AI-assisted architectural design.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment</title>
<link>https://arxiv.org/abs/2506.15019</link>
<guid>https://arxiv.org/abs/2506.15019</guid>
<content:encoded><![CDATA[
arXiv:2506.15019v1 Announce Type: cross 
Abstract: Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series. While previous works have explored representation learning for this task, the critical challenge of training instability in sequential representations and its detrimental impact on policy performance has been overlooked. This work demonstrates that Controlled Differential Equations (CDE) state representation can achieve strong RL policies when two key factors are met: (1) ensuring training stability through early stopping or stabilization methods, and (2) enforcing acuity-aware representations by correlation regularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the MIMIC-III sepsis cohort reveal that stable CDE autoencoder produces representations strongly correlated with acuity scores and enables RL policies with superior performance (WIS return $> 0.9$). In contrast, unstable CDE representation leads to degraded representations and policy failure (WIS return $\sim$ 0). Visualizations of the latent space show that stable CDEs not only separate survivor and non-survivor trajectories but also reveal clear acuity score gradients, whereas unstable training fails to capture either pattern. These findings highlight practical guidelines for using CDEs to encode irregular medical time series in clinical RL, emphasizing the need for training stability in sequential representation learning.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2506.15021</link>
<guid>https://arxiv.org/abs/2506.15021</guid>
<content:encoded><![CDATA[
arXiv:2506.15021v1 Announce Type: cross 
Abstract: Supervised fine-tuning (SFT) has become an essential step in tailoring large language models (LLMs) to align with human expectations and specific downstream tasks. However, existing SFT methods typically treat each training instance as a uniform sequence, giving equal importance to all tokens regardless of their relevance. This overlooks the fact that only a subset of tokens often contains critical, task-specific information. To address this limitation, we introduce Supervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that treats groups of tokens differently based on their importance.SFT-GO groups tokens in each sample based on their importance values and optimizes the LLM using a weighted combination of the worst-group loss and the standard cross-entropy loss. This mechanism adaptively emphasizes the most challenging token groups and guides the model to better handle different group distributions, thereby improving overall learning dynamics. We provide a theoretical analysis of SFT-GO's convergence rate, demonstrating its efficiency. Empirically, we apply SFT-GO with three different token grouping strategies and show that models trained with SFT-GO consistently outperform baseline approaches across popular LLM benchmarks. These improvements hold across various datasets and base models, demonstrating the robustness and the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size</title>
<link>https://arxiv.org/abs/2506.15025</link>
<guid>https://arxiv.org/abs/2506.15025</guid>
<content:encoded><![CDATA[
arXiv:2506.15025v1 Announce Type: cross 
Abstract: Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use. On the parametrization side, $\mu P$ (Maximal Update Parametrization) parametrizes model weights and learning rate (LR) in a way that makes hyperparameters (HPs) transferable with width (embedding dimension): HPs can be tuned for a small model and used for larger models without additional tuning. While $\mu$P showed impressive results in practice, recent empirical studies have reported conflicting observations when applied to LLMs. One limitation of the theory behind $\mu$P is the fact that input dimension (vocabulary size in LLMs) is considered fixed when taking the width to infinity. This is unrealistic since vocabulary size is generally much larger than width in practice. In this work, we provide a theoretical analysis of the effect of vocabulary size on training dynamics, and subsequently show that as vocabulary size increases, the training dynamics \emph{interpolate between the $\mu$P regime and another regime that we call Large Vocab (LV) Regime}, where optimal scaling rules are different from those predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$, surprisingly close to the empirical findings previously reported in the literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P. We conduct several experiments to validate our theory, and pretrain a 1B model from scratch to show the benefit of our suggested scaling rule for the embedding LR.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures</title>
<link>https://arxiv.org/abs/2506.15043</link>
<guid>https://arxiv.org/abs/2506.15043</guid>
<content:encoded><![CDATA[
arXiv:2506.15043v1 Announce Type: cross 
Abstract: Advancements in the defense industry are paramount for ensuring the safety and security of nations, providing robust protection against emerging threats. Among these threats, hypersonic missiles pose a significant challenge due to their extreme speeds and maneuverability, making accurate trajectory prediction a critical necessity for effective countermeasures. This paper addresses this challenge by employing a novel hybrid deep learning approach, integrating Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs). By leveraging the strengths of these architectures, the proposed method successfully predicts the complex trajectories of hypersonic missiles with high accuracy, offering a significant contribution to defense strategies and missile interception technologies. This research demonstrates the potential of advanced machine learning techniques in enhancing the predictive capabilities of defense systems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers</title>
<link>https://arxiv.org/abs/2506.15047</link>
<guid>https://arxiv.org/abs/2506.15047</guid>
<content:encoded><![CDATA[
arXiv:2506.15047v1 Announce Type: cross 
Abstract: Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Policy Gradient for Adaptive Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2506.15051</link>
<guid>https://arxiv.org/abs/2506.15051</guid>
<content:encoded><![CDATA[
arXiv:2506.15051v1 Announce Type: cross 
Abstract: Reinforcement learning is essential for neural architecture search and hyperparameter optimization, but the conventional approaches impede widespread use due to prohibitive time and computational costs. Inspired by DeepSeek-V3 multi-token prediction architecture, we propose Sequential Policy Gradient modeling (SPG), a novel trajectory generation paradigm for lightweight online hyperparameter optimization. In contrast to conventional policy gradient methods, SPG extends the base model with temporary modules, enabling it to generate state-action (padded) trajectories in a single forward pass. Our experiments demonstrate that models gain performance when retrained with SPG on their original datasets and also outperform standard transfer fine-tuning. We evaluate on five datasets spanning computer vision (ImageNet, COCO), natural language processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial applicability of SPG. The proposed method demonstrates consistent improvements across widely adopted models, achieving performance gains of $+0.2\sim7\%$, with significantly low computational costs. Fully reproducible code and pre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification</title>
<link>https://arxiv.org/abs/2506.15081</link>
<guid>https://arxiv.org/abs/2506.15081</guid>
<content:encoded><![CDATA[
arXiv:2506.15081v1 Announce Type: cross 
Abstract: Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning</title>
<link>https://arxiv.org/abs/2506.15113</link>
<guid>https://arxiv.org/abs/2506.15113</guid>
<content:encoded><![CDATA[
arXiv:2506.15113v1 Announce Type: cross 
Abstract: Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned ("cold-start") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Loss Functions in Recommender Systems: A Comparative Study with a R\'enyi Divergence-Based Solution</title>
<link>https://arxiv.org/abs/2506.15120</link>
<guid>https://arxiv.org/abs/2506.15120</guid>
<content:encoded><![CDATA[
arXiv:2506.15120v1 Announce Type: cross 
Abstract: Loss functions play a pivotal role in optimizing recommendation models. Among various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are particularly effective. Their theoretical connections and differences warrant in-depth exploration. This work conducts comprehensive analyses of these losses, yielding significant insights: 1) Common strengths -- both can be viewed as augmentations of traditional losses with Distributional Robust Optimization (DRO), enhancing robustness to distributional shifts; 2) Respective limitations -- stemming from their use of different distribution distance metrics in DRO optimization, SL exhibits high sensitivity to false negative instances, whereas CCL suffers from low data utilization. To address these limitations, this work proposes a new loss function, DrRL, which generalizes SL and CCL by leveraging R\'enyi-divergence in DRO optimization. DrRL incorporates the advantageous structures of both SL and CCL, and can be demonstrated to effectively mitigate their limitations. Extensive experiments have been conducted to validate the superiority of DrRL on both recommendation accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs</title>
<link>https://arxiv.org/abs/2506.15131</link>
<guid>https://arxiv.org/abs/2506.15131</guid>
<content:encoded><![CDATA[
arXiv:2506.15131v1 Announce Type: cross 
Abstract: Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models</title>
<link>https://arxiv.org/abs/2506.15138</link>
<guid>https://arxiv.org/abs/2506.15138</guid>
<content:encoded><![CDATA[
arXiv:2506.15138v1 Announce Type: cross 
Abstract: This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning</title>
<link>https://arxiv.org/abs/2506.15154</link>
<guid>https://arxiv.org/abs/2506.15154</guid>
<content:encoded><![CDATA[
arXiv:2506.15154v1 Announce Type: cross 
Abstract: Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agent for Hyper-Parameter Optimization</title>
<link>https://arxiv.org/abs/2506.15167</link>
<guid>https://arxiv.org/abs/2506.15167</guid>
<content:encoded><![CDATA[
arXiv:2506.15167v1 Announce Type: cross 
Abstract: Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters tuning methods for warm-start particles swarm optimization with cross and mutation (WS-PSO-CM) algortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication are primarily heuristic-based, exhibiting low levels of automation and unsatisfactory performance. In this paper, we design an large language model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and model context protocol (MCP) are applied. In particular, the LLM agent is first setup via a profile, which specifies the mission, background, and output format. Then, the LLM agent is driven by the prompt requirement, and iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent autonomously terminates the loop and returns a set of hyper-parameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM algorithm background is useful in finding high-performance hyper-parameters.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of Multi-Parametric Body MRI Series Using Deep Learning</title>
<link>https://arxiv.org/abs/2506.15182</link>
<guid>https://arxiv.org/abs/2506.15182</guid>
<content:encoded><![CDATA[
arXiv:2506.15182v1 Announce Type: cross 
Abstract: Multi-parametric magnetic resonance imaging (mpMRI) exams have various series types acquired with different imaging protocols. The DICOM headers of these series often have incorrect information due to the sheer diversity of protocols and occasional technologist errors. To address this, we present a deep learning-based classification model to classify 8 different body mpMRI series types so that radiologists read the exams efficiently. Using mpMRI data from various institutions, multiple deep learning-based classifiers of ResNet, EfficientNet, and DenseNet are trained to classify 8 different MRI series, and their performance is compared. Then, the best-performing classifier is identified, and its classification capability under the setting of different training data quantities is studied. Also, the model is evaluated on the out-of-training-distribution datasets. Moreover, the model is trained using mpMRI exams obtained from different scanners in two training strategies, and its performance is tested. Experimental results show that the DenseNet-121 model achieves the highest F1-score and accuracy of 0.966 and 0.972 over the other classification models with p-value$<$0.05. The model shows greater than 0.95 accuracy when trained with over 729 studies of the training data, whose performance improves as the training data quantities grew larger. On the external data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and 0.810 accuracy for each. These results indicate that in both the internal and external datasets, the DenseNet-121 model attains high accuracy for the task of classifying 8 body MRI series types.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accessible Gesture-Driven Augmented Reality Interaction System</title>
<link>https://arxiv.org/abs/2506.15189</link>
<guid>https://arxiv.org/abs/2506.15189</guid>
<content:encoded><![CDATA[
arXiv:2506.15189v1 Announce Type: cross 
Abstract: Augmented reality (AR) offers immersive interaction but remains inaccessible for users with motor impairments or limited dexterity due to reliance on precise input methods. This study proposes a gesture-based interaction system for AR environments, leveraging deep learning to recognize hand and body gestures from wearable sensors and cameras, adapting interfaces to user capabilities. The system employs vision transformers (ViTs), temporal convolutional networks (TCNs), and graph attention networks (GATs) for gesture processing, with federated learning ensuring privacy-preserving model training across diverse users. Reinforcement learning optimizes interface elements like menu layouts and interaction modes. Experiments demonstrate a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems. This approach enhances AR accessibility and scalability. Keywords: Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals</title>
<link>https://arxiv.org/abs/2506.15208</link>
<guid>https://arxiv.org/abs/2506.15208</guid>
<content:encoded><![CDATA[
arXiv:2506.15208v1 Announce Type: cross 
Abstract: In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer).
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singular Value Decomposition on Kronecker Adaptation for Large Language Model</title>
<link>https://arxiv.org/abs/2506.15251</link>
<guid>https://arxiv.org/abs/2506.15251</guid>
<content:encoded><![CDATA[
arXiv:2506.15251v1 Announce Type: cross 
Abstract: Large pre-trained Transformer models achieve state-of-the-art results across diverse language and reasoning tasks, but full fine-tuning incurs substantial storage, memory, and computational overhead. Parameter-efficient fine-tuning (PEFT) methods mitigate these costs by learning only a small subset of task-specific parameters, yet existing approaches either introduce inference-time latency (adapter modules), suffer from suboptimal convergence (randomly initialized low-rank updates), or rely on fixed rank choices that may not match task complexity (Kronecker-based decompositions).
  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that combines Kronecker-product tensor factorization with SVD-driven initialization and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD) procedure extracts principal components of the full weight update into compact Kronecker factors, while an adaptive rank selection algorithm uses energy-threshold and elbow-point criteria to prune negligible components.
  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or exceeding baseline performance. Moreover, SoKA exhibits faster convergence and more stable gradients, highlighting its robustness and efficiency for large-scale model adaptation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments</title>
<link>https://arxiv.org/abs/2506.15253</link>
<guid>https://arxiv.org/abs/2506.15253</guid>
<content:encoded><![CDATA[
arXiv:2506.15253v1 Announce Type: cross 
Abstract: The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing</title>
<link>https://arxiv.org/abs/2506.15260</link>
<guid>https://arxiv.org/abs/2506.15260</guid>
<content:encoded><![CDATA[
arXiv:2506.15260v1 Announce Type: cross 
Abstract: In the semiconductor sector, due to high demand but also strong and increasing competition, time to market and quality are key factors in securing significant market share in various application areas. Thanks to the success of deep learning methods in recent years in the computer vision domain, Industry 4.0 and 5.0 applications, such as defect classification, have achieved remarkable success. In particular, Domain Adaptation (DA) has proven highly effective since it focuses on using the knowledge learned on a (source) domain to adapt and perform effectively on a different but related (target) domain. By improving robustness and scalability, DA minimizes the need for extensive manual re-labeling or re-training of models. This not only reduces computational and resource costs but also allows human experts to focus on high-value tasks. Therefore, we tested the efficacy of DA techniques in semi-supervised and unsupervised settings within the context of the semiconductor field. Moreover, we propose the DBACS approach, a CycleGAN-inspired model enhanced with additional loss terms to improve performance. All the approaches are studied and validated on real-world Electron Microscope images considering the unsupervised and semi-supervised settings, proving the usefulness of our method in advancing DA techniques for the semiconductor field.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Post-hoc Dataset Inference with Synthetic Data</title>
<link>https://arxiv.org/abs/2506.15271</link>
<guid>https://arxiv.org/abs/2506.15271</guid>
<content:encoded><![CDATA[
arXiv:2506.15271v1 Announce Type: cross 
Abstract: The remarkable capabilities of Large Language Models (LLMs) can be mainly attributed to their massive training datasets, which are often scraped from the internet without respecting data owners' intellectual property rights. Dataset Inference (DI) offers a potential remedy by identifying whether a suspect dataset was used in training, thereby enabling data owners to verify unauthorized use. However, existing DI methods require a private set-known to be absent from training-that closely matches the compromised dataset's distribution. Such in-distribution, held-out data is rarely available in practice, severely limiting the applicability of DI. In this work, we address this challenge by synthetically generating the required held-out set. Our approach tackles two key obstacles: (1) creating high-quality, diverse synthetic data that accurately reflects the original distribution, which we achieve via a data generator trained on a carefully designed suffix-based completion task, and (2) bridging likelihood gaps between real and synthetic data, which is realized through post-hoc calibration. Extensive experiments on diverse text datasets show that using our generated data as a held-out set enables DI to detect the original training sets with high confidence, while maintaining a low false positive rate. This result empowers copyright owners to make legitimate claims on data usage and demonstrates our method's reliability for real-world litigations. Our code is available at https://github.com/sprintml/PostHocDatasetInference.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models</title>
<link>https://arxiv.org/abs/2506.15290</link>
<guid>https://arxiv.org/abs/2506.15290</guid>
<content:encoded><![CDATA[
arXiv:2506.15290v1 Announce Type: cross 
Abstract: Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present a new task of full-body human pose estimation using sparse, loosely attached IMU sensors. To solve this task, we simulate IMU recordings from an existing garment-aware human motion dataset. We developed transformer-based diffusion models to synthesize loose IMU data and estimate human poses based on this challenging loose IMU data. In addition, we show that incorporating garment-related parameters while training the model on simulated loose data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Experiments show that our proposed diffusion methods trained on simulated and synthetic data outperformed the state-of-the-art methods quantitatively and qualitatively, opening up a promising direction for future research.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment</title>
<link>https://arxiv.org/abs/2506.15301</link>
<guid>https://arxiv.org/abs/2506.15301</guid>
<content:encoded><![CDATA[
arXiv:2506.15301v1 Announce Type: cross 
Abstract: Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConLID: Supervised Contrastive Learning for Low-Resource Language Identification</title>
<link>https://arxiv.org/abs/2506.15304</link>
<guid>https://arxiv.org/abs/2506.15304</guid>
<content:encoded><![CDATA[
arXiv:2506.15304v1 Announce Type: cross 
Abstract: Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation</title>
<link>https://arxiv.org/abs/2506.15309</link>
<guid>https://arxiv.org/abs/2506.15309</guid>
<content:encoded><![CDATA[
arXiv:2506.15309v1 Announce Type: cross 
Abstract: Simultaneously optimizing molecules against multiple therapeutic targets remains a profound challenge in drug discovery, particularly due to sparse rewards and conflicting design constraints. We propose a structured active learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational autoencoder (VAE) into iterative loops designed to balance chemical diversity, molecular quality, and multi-target affinity. Our method alternates between expanding chemically feasible regions of latent space and progressively constraining molecules based on increasingly stringent multi-target docking thresholds. In a proof-of-concept study targeting three related coronavirus main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently generated a structurally diverse set of pan-inhibitor candidates. We demonstrate that careful timing and strategic placement of chemical filters within this active learning pipeline markedly enhance exploration of beneficial chemical space, transforming the sparse-reward, multi-objective drug design problem into an accessible computational task. Our framework thus provides a generalizable roadmap for efficiently navigating complex polypharmacological landscapes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning</title>
<link>https://arxiv.org/abs/2506.15313</link>
<guid>https://arxiv.org/abs/2506.15313</guid>
<content:encoded><![CDATA[
arXiv:2506.15313v1 Announce Type: cross 
Abstract: In autonomous driving, high-definition (HD) maps and semantic maps in bird's-eye view (BEV) are essential for accurate localization, planning, and decision-making. This paper introduces an enhanced End-to-End model named MapFM for online vectorized HD map generation. We show significantly boost feature representation quality by incorporating powerful foundation model for encoding camera images. To further enrich the model's understanding of the environment and improve prediction quality, we integrate auxiliary prediction heads for semantic segmentation in the BEV representation. This multi-task learning approach provides richer contextual supervision, leading to a more comprehensive scene representation and ultimately resulting in higher accuracy and improved quality of the predicted vectorized HD maps. The source code is available at https://github.com/LIvanoff/MapFM.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor</title>
<link>https://arxiv.org/abs/2506.15316</link>
<guid>https://arxiv.org/abs/2506.15316</guid>
<content:encoded><![CDATA[
arXiv:2506.15316v1 Announce Type: cross 
Abstract: This paper presents J3DAI, a tiny deep neural network-based hardware accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial intelligence (AI) chip integrating a Deep Neural Network (DNN)-based accelerator. The DNN accelerator is designed to efficiently perform neural network tasks such as image classification and segmentation. This paper focuses on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA) characteristics and showcasing advanced edge AI capabilities on a CMOS image sensor. To support hardware, we utilized the Aidge comprehensive software framework, which enables the programming of both the host processor and the DNN accelerator. Aidge supports post-training quantization, significantly reducing memory footprint and computational complexity, making it crucial for deploying models on resource-constrained hardware like J3DAI. Our experimental results demonstrate the versatility and efficiency of this innovative design in the field of edge AI, showcasing its potential to handle both simple and computationally intensive tasks. Future work will focus on further optimizing the architecture and exploring new applications to fully leverage the capabilities of J3DAI. As edge AI continues to grow in importance, innovations like J3DAI will play a crucial role in enabling real-time, low-latency, and energy-efficient AI processing at the edge.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and How Unlabeled Data Provably Improve In-Context Learning</title>
<link>https://arxiv.org/abs/2506.15329</link>
<guid>https://arxiv.org/abs/2506.15329</guid>
<content:encoded><![CDATA[
arXiv:2506.15329v1 Announce Type: cross 
Abstract: Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-World Object Counting in Videos</title>
<link>https://arxiv.org/abs/2506.15368</link>
<guid>https://arxiv.org/abs/2506.15368</guid>
<content:encoded><![CDATA[
arXiv:2506.15368v1 Announce Type: cross 
Abstract: We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and similar objects, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model to enable automated, open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for our novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at https://github.com/niki-amini-naieni/CountVid/.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Pipeline for systematically searching for Anomaly Detection Systems</title>
<link>https://arxiv.org/abs/2506.15388</link>
<guid>https://arxiv.org/abs/2506.15388</guid>
<content:encoded><![CDATA[
arXiv:2506.15388v1 Announce Type: cross 
Abstract: Digitalization in the medical world provides major benefits while making it a target for attackers and thus hard to secure. To deal with network intruders we propose an anomaly detection system on hardware to detect malicious clients in real-time. We meet real-time and power restrictions using FPGAs. Overall system performance is achieved via the presented holistic system evaluation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Real-time Endoscopic Image Denoising System</title>
<link>https://arxiv.org/abs/2506.15395</link>
<guid>https://arxiv.org/abs/2506.15395</guid>
<content:encoded><![CDATA[
arXiv:2506.15395v1 Announce Type: cross 
Abstract: Endoscopes featuring a miniaturized design have significantly enhanced operational flexibility, portability, and diagnostic capability while substantially reducing the invasiveness of medical procedures. Recently, single-use endoscopes equipped with an ultra-compact analogue image sensor measuring less than 1mm x 1mm bring revolutionary advancements to medical diagnosis. They reduce the structural redundancy and large capital expenditures associated with reusable devices, eliminate the risk of patient infections caused by inadequate disinfection, and alleviate patient suffering. However, the limited photosensitive area results in reduced photon capture per pixel, requiring higher photon sensitivity settings to maintain adequate brightness. In high-contrast medical imaging scenarios, the small-sized sensor exhibits a constrained dynamic range, making it difficult to simultaneously capture details in both highlights and shadows, and additional localized digital gain is required to compensate. Moreover, the simplified circuit design and analog signal transmission introduce additional noise sources. These factors collectively contribute to significant noise issues in processed endoscopic images. In this work, we developed a comprehensive noise model for analog image sensors in medical endoscopes, addressing three primary noise types: fixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise. Building on this analysis, we propose a hybrid denoising system that synergistically combines traditional image processing algorithms with advanced learning-based techniques for captured raw frames from sensors. Experiments demonstrate that our approach effectively reduces image noise without fine detail loss or color distortion, while achieving real-time performance on FPGA platforms and an average PSNR improvement from 21.16 to 33.05 on our test dataset.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System</title>
<link>https://arxiv.org/abs/2506.15402</link>
<guid>https://arxiv.org/abs/2506.15402</guid>
<content:encoded><![CDATA[
arXiv:2506.15402v1 Announce Type: cross 
Abstract: Object-level SLAM offers structured and semantically meaningful environment representations, making it more interpretable and suitable for high-level robotic tasks. However, most existing approaches rely on RGB-D sensors or monocular views, which suffer from narrow fields of view, occlusion sensitivity, and limited depth perception-especially in large-scale or outdoor environments. These limitations often restrict the system to observing only partial views of objects from limited perspectives, leading to inaccurate object modeling and unreliable data association. In this work, we propose MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully leverages surround-view camera configurations to achieve robust, consistent, and semantically enriched mapping in complex outdoor scenarios. Our approach integrates point features and object-level landmarks enhanced with open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is introduced for robust object association across multiple views, leading to improved consistency and accurate object modeling, and an omnidirectional loop closure module is designed to enable viewpoint-invariant place recognition using scene-level descriptors. Furthermore, the constructed map is abstracted into a hierarchical 3D scene graph to support downstream reasoning tasks. Extensive experiments in real-world demonstrate that MCOO-SLAM achieves accurate localization and scalable object-level mapping with improved robustness to occlusion, pose variation, and environmental complexity.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI</title>
<link>https://arxiv.org/abs/2506.15408</link>
<guid>https://arxiv.org/abs/2506.15408</guid>
<content:encoded><![CDATA[
arXiv:2506.15408v1 Announce Type: cross 
Abstract: Modern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI). We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Models in Deep Reinforcement Learning: A Survey</title>
<link>https://arxiv.org/abs/2506.15421</link>
<guid>https://arxiv.org/abs/2506.15421</guid>
<content:encoded><![CDATA[
arXiv:2506.15421v1 Announce Type: cross 
Abstract: In reinforcement learning (RL), agents continually interact with the environment and use the feedback to refine their behavior. To guide policy optimization, reward models are introduced as proxies of the desired objectives, such that when the agent maximizes the accumulated reward, it also fulfills the task designer's intentions. Recently, significant attention from both academic and industrial researchers has focused on developing reward models that not only align closely with the true objectives but also facilitate policy optimization. In this survey, we provide a comprehensive review of reward modeling techniques within the deep RL literature. We begin by outlining the background and preliminaries in reward modeling. Next, we present an overview of recent reward modeling approaches, categorizing them based on the source, the mechanism, and the learning paradigm. Building on this understanding, we discuss various applications of these reward modeling techniques and review methods for evaluating reward models. Finally, we conclude by highlighting promising research directions in reward modeling. Altogether, this survey includes both established and emerging methods, filling the vacancy of a systematic review of reward models in current literature.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material</title>
<link>https://arxiv.org/abs/2506.15442</link>
<guid>https://arxiv.org/abs/2506.15442</guid>
<content:encoded><![CDATA[
arXiv:2506.15442v1 Announce Type: cross 
Abstract: 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Reinforcement Learning Under Partial Observability</title>
<link>https://arxiv.org/abs/2506.15446</link>
<guid>https://arxiv.org/abs/2506.15446</guid>
<content:encoded><![CDATA[
arXiv:2506.15446v1 Announce Type: cross 
Abstract: Recent work has shown that, under certain assumptions, zero-shot reinforcement learning (RL) methods can generalise to any unseen task in an environment after reward-free pre-training. Access to Markov states is one such assumption, yet, in many real-world applications, the Markov state is only partially observable. Here, we explore how the performance of standard zero-shot RL methods degrades when subjected to partially observability, and show that, as in single-task RL, memory-based architectures are an effective remedy. We evaluate our memory-based zero-shot RL methods in domains where the states, rewards and a change in dynamics are partially observed, and show improved performance over memory-free baselines. Our code is open-sourced via: https://enjeeneer.io/projects/bfms-with-memory/.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warping and Matching Subsequences Between Time Series</title>
<link>https://arxiv.org/abs/2506.15452</link>
<guid>https://arxiv.org/abs/2506.15452</guid>
<content:encoded><![CDATA[
arXiv:2506.15452v1 Announce Type: cross 
Abstract: Comparing time series is essential in various tasks such as clustering and classification. While elastic distance measures that allow warping provide a robust quantitative comparison, a qualitative comparison on top of them is missing. Traditional visualizations focus on point-to-point alignment and do not convey the broader structural relationships at the level of subsequences. This limitation makes it difficult to understand how and where one time series shifts, speeds up or slows down with respect to another. To address this, we propose a novel technique that simplifies the warping path to highlight, quantify and visualize key transformations (shift, compression, difference in amplitude). By offering a clearer representation of how subsequences match between time series, our method enhances interpretability in time series comparison.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Intention through LLM-Driven Code Snippet Description Generation</title>
<link>https://arxiv.org/abs/2506.15453</link>
<guid>https://arxiv.org/abs/2506.15453</guid>
<content:encoded><![CDATA[
arXiv:2506.15453v1 Announce Type: cross 
Abstract: Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as "Example" (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2506.15455</link>
<guid>https://arxiv.org/abs/2506.15455</guid>
<content:encoded><![CDATA[
arXiv:2506.15455v1 Announce Type: cross 
Abstract: Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI</title>
<link>https://arxiv.org/abs/2506.15468</link>
<guid>https://arxiv.org/abs/2506.15468</guid>
<content:encoded><![CDATA[
arXiv:2506.15468v1 Announce Type: cross 
Abstract: We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shared external representations, a process we interpret as symbol emergence. Unlike traditional AI teaching based on unilateral knowledge transfer, this addresses the challenge of integrating information from inherently different modalities. We empirically test this framework using a human-AI interaction model based on the Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference mechanism. In an online experiment, 69 participants played a joint attention naming game (JA-NG) with one of three computer agent types (MH-based, always-accept, or always-reject) under partial observability. Results show that human-AI pairs with an MH-based agent significantly improved categorization accuracy through interaction and achieved stronger convergence toward a shared sign system. Furthermore, human acceptance behavior aligned closely with the MH-derived acceptance probability. These findings provide the first empirical evidence for co-creative learning emerging in human-AI dyads via MHNG-based interaction. This suggests a promising path toward symbiotic AI systems that learn with humans, rather than from them, by dynamically aligning perceptual experiences, opening a new venue for symbiotic AI alignment.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Informed Grounding Supervision</title>
<link>https://arxiv.org/abs/2506.15480</link>
<guid>https://arxiv.org/abs/2506.15480</guid>
<content:encoded><![CDATA[
arXiv:2506.15480v1 Announce Type: cross 
Abstract: Large language models (LLMs) are often supplemented with external knowledge to provide information not encoded in their parameters or to reduce hallucination. In such cases, we expect the model to generate responses by grounding its response in the provided external context. However, prior work has shown that simply appending context at inference time does not ensure grounded generation. To address this, we propose Context-INformed Grounding Supervision (CINGS), a post-training supervision in which the model is trained with relevant context prepended to the response, while computing the loss only over the response tokens and masking out the context. Our experiments demonstrate that models trained with CINGS exhibit stronger grounding in both textual and visual domains compared to standard instruction-tuned models. In the text domain, CINGS outperforms other training methods across 11 information-seeking datasets and is complementary to inference-time grounding techniques. In the vision-language domain, replacing a vision-language model's LLM backbone with a CINGS-trained model reduces hallucinations across four benchmarks and maintains factual consistency throughout the generated response. This improved grounding comes without degradation in general downstream performance. Finally, we analyze the mechanism underlying the enhanced grounding in CINGS and find that it induces a shift in the model's prior knowledge and behavior, implicitly encouraging greater reliance on the external context.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</title>
<link>https://arxiv.org/abs/2506.15483</link>
<guid>https://arxiv.org/abs/2506.15483</guid>
<content:encoded><![CDATA[
arXiv:2506.15483v1 Announce Type: cross 
Abstract: While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling</title>
<link>https://arxiv.org/abs/2506.15498</link>
<guid>https://arxiv.org/abs/2506.15498</guid>
<content:encoded><![CDATA[
arXiv:2506.15498v1 Announce Type: cross 
Abstract: Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables single-pass, per-step annotation by aligning each solution step to one or multiple steps in a reference solution, accompanied by explicit reasoning for evaluation. We show that reference-guided step-level evaluation effectively facilitates process supervision on four datasets spanning three domains: mathematical reasoning, multi-hop compositional question answering, and spatial reasoning. We demonstrate that SPARE, when compared to baselines, improves reasoning performance when used for: (1) fine-tuning models in an offline RL setup for inference-time greedy-decoding, and (2) training reward models for ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE achieves competitive performance on challenging mathematical datasets while offering 2.6 times greater efficiency, requiring only 38% of the runtime, compared to tree search-based automatic annotation. The codebase, along with a trained SPARE-PRM model, is publicly released to facilitate further research and reproducibility.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel-level Certified Explanations via Randomized Smoothing</title>
<link>https://arxiv.org/abs/2506.15499</link>
<guid>https://arxiv.org/abs/2506.15499</guid>
<content:encoded><![CDATA[
arXiv:2506.15499v1 Announce Type: cross 
Abstract: Post-hoc attribution methods aim to explain deep learning predictions by highlighting influential input pixels. However, these explanations are highly non-robust: small, imperceptible input perturbations can drastically alter the attribution map while maintaining the same prediction. This vulnerability undermines their trustworthiness and calls for rigorous robustness guarantees of pixel-level attribution scores. We introduce the first certification framework that guarantees pixel-level robustness for any black-box attribution method using randomized smoothing. By sparsifying and smoothing attribution maps, we reformulate the task as a segmentation problem and certify each pixel's importance against $\ell_2$-bounded perturbations. We further propose three evaluation metrics to assess certified robustness, localization, and faithfulness. An extensive evaluation of 12 attribution methods across 5 ImageNet models shows that our certified attributions are robust, interpretable, and faithful, enabling reliable use in downstream tasks. Our code is at https://github.com/AlaaAnani/certified-attributions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Over-squashing in Spatiotemporal Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.15507</link>
<guid>https://arxiv.org/abs/2506.15507</guid>
<content:encoded><![CDATA[
arXiv:2506.15507v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach</title>
<link>https://arxiv.org/abs/2506.15512</link>
<guid>https://arxiv.org/abs/2506.15512</guid>
<content:encoded><![CDATA[
arXiv:2506.15512v1 Announce Type: cross 
Abstract: Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each student's needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.15513</link>
<guid>https://arxiv.org/abs/2506.15513</guid>
<content:encoded><![CDATA[
arXiv:2506.15513v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has become a common strategy for updating large language model (LLM) responses with current, external information. However, models may still rely on memorized training data, bypass the retrieved evidence, and produce contaminated outputs. We introduce Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects such behavior without requiring model access or retraining. RePCS compares two inference paths: (i) a parametric path using only the query, and (ii) a retrieval-augmented path using both the query and retrieved context by computing the Kullback-Leibler (KL) divergence between their output distributions. A low divergence suggests that the retrieved context had minimal impact, indicating potential memorization. This procedure is model-agnostic, requires no gradient or internal state access, and adds only a single additional forward pass. We further derive PAC-style guarantees that link the KL threshold to user-defined false positive and false negative rates. On the Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result outperforms the strongest prior method by 6.5 percentage points while keeping latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight, black-box safeguard to verify whether a RAG system meaningfully leverages retrieval, making it especially valuable in safety-critical applications.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework</title>
<link>https://arxiv.org/abs/2506.15538</link>
<guid>https://arxiv.org/abs/2506.15538</guid>
<content:encoded><![CDATA[
arXiv:2506.15538v1 Announce Type: cross 
Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Current feature description methods face two critical challenges: limited robustness and the flawed assumption that each neuron encodes only a single concept (monosemanticity), despite growing evidence that neurons are often polysemantic. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework that captures the inherent complexity of neural network features. Unlike prior approaches that assign a single description per feature, PRISM provides more nuanced descriptions for both polysemantic and monosemantic features. We apply PRISM to language models and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity</title>
<link>https://arxiv.org/abs/2506.15541</link>
<guid>https://arxiv.org/abs/2506.15541</guid>
<content:encoded><![CDATA[
arXiv:2506.15541v1 Announce Type: cross 
Abstract: We examine the intrinsic (within the attention head) and extrinsic (amongst the attention heads) structure of the self-attention mechanism in transformers. Theoretical evidence for invariance of the self-attention mechanism to softmax activation is obtained by appealing to paradifferential calculus, (and is supported by computational examples), which relies on the intrinsic organization of the attention heads. Furthermore, we use an existing methodology for hierarchical organization of tensors to examine network structure by constructing hierarchal partition trees with respect to the query, key, and head axes of network 3-tensors. Such an organization is consequential since it allows one to profitably execute common signal processing tasks on a geometry where the organized network 3-tensors exhibit regularity. We exemplify this qualitatively, by visualizing the hierarchical organization of the tree comprised of attention heads and the diffusion map embeddings, and quantitatively by investigating network sparsity with the expansion coefficients of individual attention heads and the entire network with respect to the bi and tri-haar bases (respectively) on the space of queries, keys, and heads of the network. To showcase the utility of our theoretical and methodological findings, we provide computational examples using vision and language transformers. The ramifications of these findings are two-fold: (1) a subsequent step in interpretability analysis is theoretically admitted, and can be exploited empirically for downstream interpretability tasks (2) one can use the network 3-tensor organization for empirical network applications such as model pruning (by virtue of network sparsity) and network architecture comparison.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Algorithms in the Limit</title>
<link>https://arxiv.org/abs/2506.15543</link>
<guid>https://arxiv.org/abs/2506.15543</guid>
<content:encoded><![CDATA[
arXiv:2506.15543v1 Announce Type: cross 
Abstract: This paper studies the problem of learning computable functions in the limit by extending Gold's inductive inference framework to incorporate \textit{computational observations} and \textit{restricted input sources}. Complimentary to the traditional Input-Output Observations, we introduce Time-Bound Observations, and Policy-Trajectory Observations to study the learnability of general recursive functions under more realistic constraints. While input-output observations do not suffice for learning the class of general recursive functions in the limit, we overcome this learning barrier by imposing computational complexity constraints or supplementing with approximate time-bound observations. Further, we build a formal framework around observations of \textit{computational agents} and show that learning computable functions from policy trajectories reduces to learning rational functions from input and output, thereby revealing interesting connections to finite-state transducer inference. On the negative side, we show that computable or polynomial-mass characteristic sets cannot exist for the class of linear-time computable functions even for policy-trajectory observations.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation</title>
<link>https://arxiv.org/abs/2506.15549</link>
<guid>https://arxiv.org/abs/2506.15549</guid>
<content:encoded><![CDATA[
arXiv:2506.15549v1 Announce Type: cross 
Abstract: Deep learning-based myocardial scar segmentation from late gadolinium enhancement (LGE) cardiac MRI has shown great potential for accurate and timely diagnosis and treatment planning for structural cardiac diseases. However, the limited availability and variability of LGE images with high-quality scar labels restrict the development of robust segmentation models. To address this, we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE \textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial Scar Synthesis and Segmentation framework, a framework for anatomically grounded scar generation and segmentation. At its core is the SMILE module (Scar Mask generation guided by cLinical knowledgE), which conditions a diffusion-based generator on the clinically adopted AHA 17-segment model to synthesize images with anatomically consistent and spatially diverse scar patterns. In addition, CLAIM employs a joint training strategy in which the scar segmentation network is optimized alongside the generator, aiming to enhance both the realism of synthesized scars and the accuracy of the scar segmentation performance. Experimental results show that CLAIM produces anatomically coherent scar patterns and achieves higher Dice similarity with real scar distributions compared to baseline models. Our approach enables controllable and realistic myocardial scar synthesis and has demonstrated utility for downstream medical imaging task.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones</title>
<link>https://arxiv.org/abs/2506.15554</link>
<guid>https://arxiv.org/abs/2506.15554</guid>
<content:encoded><![CDATA[
arXiv:2506.15554v1 Announce Type: cross 
Abstract: Wi-Fi fingerprinting-based indoor localization faces significant challenges in real-world deployments due to domain shifts arising from device heterogeneity and temporal variations within indoor environments. Existing approaches often address these issues independently, resulting in poor generalization and susceptibility to catastrophic forgetting over time. In this work, we propose DAILOC, a novel domain-incremental learning framework that jointly addresses both temporal and device-induced domain shifts. DAILOC introduces a novel disentanglement strategy that separates domain shifts from location-relevant features using a multi-level variational autoencoder. Additionally, we introduce a novel memory-guided class latent alignment mechanism to address the effects of catastrophic forgetting over time. Experiments across multiple smartphones, buildings, and time instances demonstrate that DAILOC significantly outperforms state-of-the-art methods, achieving up to 2.74x lower average error and 4.6x lower worst-case error.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates</title>
<link>https://arxiv.org/abs/2506.15559</link>
<guid>https://arxiv.org/abs/2506.15559</guid>
<content:encoded><![CDATA[
arXiv:2506.15559v1 Announce Type: cross 
Abstract: Indoor localization using deep learning (DL) has demonstrated strong accuracy in mapping Wi-Fi RSS fingerprints to physical locations; however, most existing DL frameworks function as black-box models, offering limited insight into how predictions are made or how models respond to real-world noise over time. This lack of interpretability hampers our ability to understand the impact of temporal variations - caused by environmental dynamics - and to adapt models for long-term reliability. To address this, we introduce LogNet, a novel logic gate-based framework designed to interpret and enhance DL-based indoor localization. LogNet enables transparent reasoning by identifying which access points (APs) are most influential for each reference point (RP) and reveals how environmental noise disrupts DL-driven localization decisions. This interpretability allows us to trace and diagnose model failures and adapt DL systems for more stable long-term deployments. Evaluations across multiple real-world building floorplans and over two years of temporal variation show that LogNet not only interprets the internal behavior of DL models but also improves performance-achieving up to 1.1x to 2.8x lower localization error, 3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to prior DL-based models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution</title>
<link>https://arxiv.org/abs/2506.15591</link>
<guid>https://arxiv.org/abs/2506.15591</guid>
<content:encoded><![CDATA[
arXiv:2506.15591v1 Announce Type: cross 
Abstract: It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at https://github.com/yjsunnn/DLoRAL.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts</title>
<link>https://arxiv.org/abs/2506.15594</link>
<guid>https://arxiv.org/abs/2506.15594</guid>
<content:encoded><![CDATA[
arXiv:2506.15594v1 Announce Type: cross 
Abstract: Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns</title>
<link>https://arxiv.org/abs/2506.15598</link>
<guid>https://arxiv.org/abs/2506.15598</guid>
<content:encoded><![CDATA[
arXiv:2506.15598v1 Announce Type: cross 
Abstract: While MCQs are valuable for learning and evaluation, manually creating them with varying difficulty levels and targeted reading skills remains a time-consuming and costly task. Recent advances in generative AI provide an opportunity to automate MCQ generation efficiently. However, assessing the actual quality and reliability of generated MCQs has received limited attention -- particularly regarding cases where generation fails. This aspect becomes particularly important when the generated MCQs are meant to be applied in real-world settings. Additionally, most MCQ generation studies focus on English, leaving other languages underexplored. This paper investigates the capabilities of current generative models in producing MCQs for reading comprehension in Portuguese, a morphologically rich language. Our study focuses on generating MCQs that align with curriculum-relevant narrative elements and span different difficulty levels. We evaluate these MCQs through expert review and by analyzing the psychometric properties extracted from student responses to assess their suitability for elementary school students. Our results show that current models can generate MCQs of comparable quality to human-authored ones. However, we identify issues related to semantic clarity and answerability. Also, challenges remain in generating distractors that engage students and meet established criteria for high-quality MCQ option design.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</title>
<link>https://arxiv.org/abs/2506.15606</link>
<guid>https://arxiv.org/abs/2506.15606</guid>
<content:encoded><![CDATA[
arXiv:2506.15606v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model's adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com/VITA-Group/LoX.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Compositional Architecture of Regret in Large Language Models</title>
<link>https://arxiv.org/abs/2506.15617</link>
<guid>https://arxiv.org/abs/2506.15617</guid>
<content:encoded><![CDATA[
arXiv:2506.15617v1 Announce Type: cross 
Abstract: Regret in Large Language Models refers to their explicit regret expression when presented with evidence contradicting their previously generated misinformation. Studying the regret mechanism is crucial for enhancing model reliability and helps in revealing how cognition is coded in neural networks. To understand this mechanism, we need to first identify regret expressions in model outputs, then analyze their internal representation. This analysis requires examining the model's hidden states, where information processing occurs at the neuron level. However, this faces three key challenges: (1) the absence of specialized datasets capturing regret expressions, (2) the lack of metrics to find the optimal regret representation layer, and (3) the lack of metrics for identifying and analyzing regret neurons. Addressing these limitations, we propose: (1) a workflow for constructing a comprehensive regret dataset through strategically designed prompting scenarios, (2) the Supervised Compression-Decoupling Index (S-CDI) metric to identify optimal regret representation layers, and (3) the Regret Dominance Score (RDS) metric to identify regret neurons and the Group Impact Coefficient (GIC) to analyze activation patterns. Our experimental results successfully identified the optimal regret representation layer using the S-CDI metric, which significantly enhanced performance in probe classification experiments. Additionally, we discovered an M-shaped decoupling pattern across model layers, revealing how information processing alternates between coupling and decoupling phases. Through the RDS metric, we categorized neurons into three distinct functional groups: regret neurons, non-regret neurons, and dual neurons.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GFLC: Graph-based Fairness-aware Label Correction for Fair Classification</title>
<link>https://arxiv.org/abs/2506.15620</link>
<guid>https://arxiv.org/abs/2506.15620</guid>
<content:encoded><![CDATA[
arXiv:2506.15620v1 Announce Type: cross 
Abstract: Fairness in machine learning (ML) has a critical importance for building trustworthy machine learning system as artificial intelligence (AI) systems increasingly impact various aspects of society, including healthcare decisions and legal judgments. Moreover, numerous studies demonstrate evidence of unfair outcomes in ML and the need for more robust fairness-aware methods. However, the data we use to train and develop debiasing techniques often contains biased and noisy labels. As a result, the label bias in the training data affects model performance and misrepresents the fairness of classifiers during testing. To tackle this problem, our paper presents Graph-based Fairness-aware Label Correction (GFLC), an efficient method for correcting label noise while preserving demographic parity in datasets. In particular, our approach combines three key components: prediction confidence measure, graph-based regularization through Ricci-flow-optimized graph Laplacians, and explicit demographic parity incentives. Our experimental findings show the effectiveness of our proposed approach and show significant improvements in the trade-off between performance and fairness metrics compared to the baseline.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction</title>
<link>https://arxiv.org/abs/2506.15626</link>
<guid>https://arxiv.org/abs/2506.15626</guid>
<content:encoded><![CDATA[
arXiv:2506.15626v1 Announce Type: cross 
Abstract: $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability</title>
<link>https://arxiv.org/abs/2506.15629</link>
<guid>https://arxiv.org/abs/2506.15629</guid>
<content:encoded><![CDATA[
arXiv:2506.15629v1 Announce Type: cross 
Abstract: In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying the Visual Quality Paradox in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2506.15645</link>
<guid>https://arxiv.org/abs/2506.15645</guid>
<content:encoded><![CDATA[
arXiv:2506.15645v1 Announce Type: cross 
Abstract: Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning</title>
<link>https://arxiv.org/abs/2506.15651</link>
<guid>https://arxiv.org/abs/2506.15651</guid>
<content:encoded><![CDATA[
arXiv:2506.15651v1 Announce Type: cross 
Abstract: Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers</title>
<link>https://arxiv.org/abs/2506.15674</link>
<guid>https://arxiv.org/abs/2506.15674</guid>
<content:encoded><![CDATA[
arXiv:2506.15674v1 Announce Type: cross 
Abstract: We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sekai: A Video Dataset towards World Exploration</title>
<link>https://arxiv.org/abs/2506.15675</link>
<guid>https://arxiv.org/abs/2506.15675</guid>
<content:encoded><![CDATA[
arXiv:2506.15675v1 Announce Type: cross 
Abstract: Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense SAE Latents Are Features, Not Bugs</title>
<link>https://arxiv.org/abs/2506.15679</link>
<guid>https://arxiv.org/abs/2506.15679</guid>
<content:encoded><![CDATA[
arXiv:2506.15679v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets</title>
<link>https://arxiv.org/abs/2308.05201</link>
<guid>https://arxiv.org/abs/2308.05201</guid>
<content:encoded><![CDATA[
arXiv:2308.05201v3 Announce Type: replace 
Abstract: Large Language Model (LLM)-based generative AI systems, such as ChatGPT, demonstrate zero-shot learning capabilities across a wide range of downstream tasks. Owing to their general-purpose nature and potential to augment or even automate job functions, these systems are poised to reshape labor market dynamics. However, predicting their precise impact \textit{a priori} is challenging, given AI's simultaneous effects on both demand and supply, as well as the strategic responses of market participants. Leveraging an extensive dataset from a leading online labor platform, we document a pronounced displacement effect and an overall contraction in submarkets where required skills closely align with core LLM functionalities. Although demand and supply both decline, the reduction in supply is comparatively smaller, thereby intensifying competition among freelancers. Notably, further analysis shows that this heightened competition is especially pronounced in programming-intensive submarkets. This pattern is attributed to skill-transition effects: by lowering the human-capital barrier to programming, ChatGPT enables incumbent freelancers to enter programming tasks. Moreover, these transitions are not homogeneous, with high-skilled freelancers contributing disproportionately to the shift. Our findings illuminate the multifaceted impacts of general-purpose AI on labor markets, highlighting not only the displacement of certain occupations but also the inducement of skill transitions within the labor supply. These insights offer practical implications for policymakers, platform operators, and workers.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Context-based Self-Supervised Learning for Handwritten Text Recognition</title>
<link>https://arxiv.org/abs/2404.11585</link>
<guid>https://arxiv.org/abs/2404.11585</guid>
<content:encoded><![CDATA[
arXiv:2404.11585v3 Announce Type: replace 
Abstract: Handwritten Text Recognition (HTR) is a relevant problem in computer vision, and implies unique challenges owing to its inherent variability and the rich contextualization required for its interpretation. Despite the success of Self-Supervised Learning (SSL) in computer vision, its application to HTR has been rather scattered, leaving key SSL methodologies unexplored. This work focuses on one of them, namely Spatial Context-based SSL. We investigate how this family of approaches can be adapted and optimized for HTR and propose new workflows that leverage the unique features of handwritten text. Our experiments demonstrate that the methods considered lead to advancements in the state-of-the-art of SSL for HTR in a number of benchmark cases.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behaviour Planning: A Toolkit for Diverse Planning</title>
<link>https://arxiv.org/abs/2405.04300</link>
<guid>https://arxiv.org/abs/2405.04300</guid>
<content:encoded><![CDATA[
arXiv:2405.04300v2 Announce Type: replace 
Abstract: Diverse planning approaches are utilised in real-world applications like risk management, automated streamed data analysis, and malware detection. The current diverse planning formulations encode the diversity model as a distance function, which is computational inexpensive when comparing two plans. However, such modelling approach limits what can be encoded as measure of diversity, as well as the ability to explain why two plans are different. This paper introduces a novel approach to the diverse planning problem, allowing for more expressive modelling of diversity using a n-dimensional grid representation, where each dimension corresponds to a user-defined feature. Furthermore, we present a novel toolkit that generates diverse plans based on such customisable diversity models, called \emph{Behaviour Planning}. We provide an implementation for behaviour planning using planning-as-satisfiability. An empirical evaluation of our implementation shows that behaviour planning significantly outperforms the current diverse planning method in generating diverse plans measured on our new customisable diversity models. Our implementation is the first diverse planning approach to support planning categories beyond classical planning, such as over-subscription and numerical planning.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OM4OV: Leveraging Ontology Matching for Ontology Versioning</title>
<link>https://arxiv.org/abs/2409.20302</link>
<guid>https://arxiv.org/abs/2409.20302</guid>
<content:encoded><![CDATA[
arXiv:2409.20302v3 Announce Type: replace 
Abstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information, particularly for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component for efficient ontology management, the growing size of ontologies and accumulating errors caused by manual labour overwhelm current OV approaches. In this paper, we propose yet another approach to performing OV using existing ontology matching (OM) techniques and systems. We introduce a unified OM4OV pipeline. From an OM perspective, we reconstruct a new task formulation and measurement for OV tasks. Building upon the prior alignment(s) from OM, we propose a pipeline optimisation method called the cross-reference (CR) mechanism to enhance overall OV performance. We experimentally validate the OM4OV pipeline and the cross-reference mechanism in the OV tested originating from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also discuss insights into OM used for OV tasks, where some false mappings detected by OV systems are not actually untrue.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport for Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2410.13061</link>
<guid>https://arxiv.org/abs/2410.13061</guid>
<content:encoded><![CDATA[
arXiv:2410.13061v3 Announce Type: replace 
Abstract: We introduce a novel optimal transport framework for probabilistic circuits (PCs). While it has been shown recently that divergences between distributions represented as certain classes of PCs can be computed tractably, to the best of our knowledge, there is no existing approach to compute the Wasserstein distance between probability distributions given by PCs. We propose a Wasserstein-type distance that restricts the coupling measure of the associated optimal transport problem to be a probabilistic circuit. We then develop an algorithm for computing this distance by solving a series of small linear programs and derive the circuit conditions under which this is tractable. Furthermore, we show that we can easily retrieve the optimal transport plan between the PCs from the solutions to these linear programs. Lastly, we study the empirical Wasserstein distance between a PC and a dataset, and show that we can estimate the PC parameters to minimize this distance through an efficient iterative algorithm.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization</title>
<link>https://arxiv.org/abs/2502.04686</link>
<guid>https://arxiv.org/abs/2502.04686</guid>
<content:encoded><![CDATA[
arXiv:2502.04686v3 Announce Type: replace 
Abstract: Large language model (LLM) agents have recently demonstrated impressive capabilities in various domains like open-ended conversation and multi-step decision-making. However, it remains challenging for these agents to solve strategic language games, such as Werewolf, which demand both strategic decision-making and free-form language interactions. Existing LLM agents often suffer from intrinsic bias in their action distributions and limited exploration of the unbounded text action space, resulting in suboptimal performance. To address these challenges, we propose Latent Space Policy Optimization (LSPO), an iterative framework that combines game-theoretic methods with LLM fine-tuning to build strategic language agents. LSPO leverages the observation that while the language space is combinatorially large, the underlying strategy space is relatively compact. We first map free-form utterances into a finite latent strategy space, yielding an abstracted extensive-form game. Then we apply game-theoretic methods like Counterfactual Regret Minimization (CFR) to optimize the policy in the latent space. Finally, we fine-tune the LLM via Direct Preference Optimization (DPO) to align with the learned policy. By iteratively alternating between these steps, our LSPO agents progressively enhance both strategic reasoning and language communication. Experiment on the Werewolf game shows that our agents iteratively expand the strategy space with improving performance and outperform existing Werewolf agents, underscoring their effectiveness in free-form language games with strategic interactions.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing Composite Hierarchical Structure from Symbolic Music Corpora</title>
<link>https://arxiv.org/abs/2502.15849</link>
<guid>https://arxiv.org/abs/2502.15849</guid>
<content:encoded><![CDATA[
arXiv:2502.15849v3 Announce Type: replace 
Abstract: Western music is an innately hierarchical system of interacting levels of structure, from fine-grained melody to high-level form. In order to analyze music compositions holistically and at multiple granularities, we propose a unified, hierarchical meta-representation of musical structure called the structural temporal graph (STG). For a single piece, the STG is a data structure that defines a hierarchy of progressively finer structural musical features and the temporal relationships between them. We use the STG to enable a novel approach for deriving a representative structural summary of a music corpus, which we formalize as a dually NP-hard combinatorial optimization problem extending the Generalized Median Graph problem. Our approach first applies simulated annealing to develop a measure of structural distance between two music pieces rooted in graph isomorphism. Our approach then combines the formal guarantees of SMT solvers with nested simulated annealing over structural distances to produce a structurally sound, representative centroid STG for an entire corpus of STGs from individual pieces. To evaluate our approach, we conduct experiments verifying that structural distance accurately differentiates between music pieces, and that derived centroids accurately structurally characterize their corpora.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Satisfiability Modulo Counting Exactly with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2503.01009</link>
<guid>https://arxiv.org/abs/2503.01009</guid>
<content:encoded><![CDATA[
arXiv:2503.01009v2 Announce Type: replace 
Abstract: Satisfiability Modulo Counting (SMC) is a recently proposed general language to reason about problems integrating statistical and symbolic Artificial Intelligence. An SMC problem is an extended SAT problem in which the truth values of a few Boolean variables are determined by probabilistic inference. Approximate solvers may return solutions that violate constraints. Directly integrating available SAT solvers and probabilistic inference solvers gives exact solutions but results in slow performance because of many back-and-forth invocations of both solvers. We propose KOCO-SMC, an integrated exact SMC solver that efficiently tracks lower and upper bounds in the probabilistic inference process. It enhances computational efficiency by enabling early estimation of probabilistic inference using only partial variable assignments, whereas existing methods require full variable assignments. In the experiment, we compare KOCO-SMC with currently available approximate and exact SMC solvers on large-scale datasets and real-world applications. The proposed KOCO-SMC finds exact solutions with much less time.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-based Exploration Conduction for Multi-step Reasoning</title>
<link>https://arxiv.org/abs/2503.15848</link>
<guid>https://arxiv.org/abs/2503.15848</guid>
<content:encoded><![CDATA[
arXiv:2503.15848v2 Announce Type: replace 
Abstract: Multi-step processes via large language models (LLMs) have proven effective for solving complex reasoning tasks. However, the depth of exploration of the reasoning procedure can significantly affect the task performance. Existing methods to automatically decide the depth often lead to high cost and a lack of flexibility. To address these issues, we propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel method that dynamically adjusts the exploration depth during multi-step reasoning by monitoring LLM's output entropy and variance entropy. We employ these two features to capture the model's uncertainty of the current step and the fluctuation of uncertainty across consecutive reasoning steps. Based on the observed entropy changes, the LLM selects whether to deepen, expand, or stop exploration according to the probability, which facilitates the trade-off between the reasoning accuracy and exploration effectiveness. Experimental results across four benchmark datasets demonstrate the efficacy of Entro-duction.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?</title>
<link>https://arxiv.org/abs/2506.10912</link>
<guid>https://arxiv.org/abs/2506.10912</guid>
<content:encoded><![CDATA[
arXiv:2506.10912v2 Announce Type: replace 
Abstract: Toxicity remains a leading cause of early-stage drug development failure. Despite advances in molecular design and property prediction, the task of molecular toxicity repair - generating structurally valid molecular alternatives with reduced toxicity - has not yet been systematically defined or benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task for general-purpose Multimodal Large Language Models (MLLMs) focused on molecular toxicity repair. We construct a standardized dataset covering 11 primary tasks and 560 representative toxic molecules spanning diverse mechanisms and granularities. We design a prompt annotation pipeline with mechanism-aware and task-adaptive capabilities, informed by expert toxicological knowledge. In parallel, we propose an automated evaluation framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity into a high-throughput evaluation chain for repair success. We systematically assess nearly 30 mainstream general-purpose MLLMs and design multiple ablation studies to analyze key factors such as evaluation criteria, candidate diversity, and failure attribution. Experimental results show that although current MLLMs still face significant challenges on this task, they begin to demonstrate promising capabilities in toxicity understanding, semantic constraint adherence, and structure-aware molecule editing.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction</title>
<link>https://arxiv.org/abs/2205.02225</link>
<guid>https://arxiv.org/abs/2205.02225</guid>
<content:encoded><![CDATA[
arXiv:2205.02225v4 Announce Type: replace-cross 
Abstract: Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling</title>
<link>https://arxiv.org/abs/2402.13534</link>
<guid>https://arxiv.org/abs/2402.13534</guid>
<content:encoded><![CDATA[
arXiv:2402.13534v2 Announce Type: replace-cross 
Abstract: Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Understandability of Computational Notebooks through Code Metrics Analysis</title>
<link>https://arxiv.org/abs/2406.10989</link>
<guid>https://arxiv.org/abs/2406.10989</guid>
<content:encoded><![CDATA[
arXiv:2406.10989v2 Announce Type: replace-cross 
Abstract: Computational notebooks are the primary coding tools for data scientists, but their code quality remains understudied and often poor. Given the importance of maintainability and reusability, enhancing code understandability is essential. Traditional methods for assessing understandability typically rely on limited questionnaires or metadata like likes and votes, which may not reflect actual code clarity. To address this, we propose a novel approach that leverages user opinions from software repositories to assess the understandability of Jupyter notebooks. We conducted a case study using 542,051 Kaggle Jupyter notebooks compiled in the DistilKaggle dataset. To identify user comments related to code understandability, we used a fine-tuned DistilBERT transformer. We then introduced a new metric, i.e., User Opinion Code Understandability (UOCU), based on the number of relevant comments, their upvotes, and notebook views. UOCU proved significantly more effective than prior methods. We further enhanced it by combining UOCU with total upvotes in a hybrid approach. Using this improved metric, we collected 34 notebook-level metrics from 132,723 final notebooks and trained machine learning models to predict understandability. Our best model, a Random Forest classifier, achieved 89% accuracy in classifying the understandability level of notebook code. This work demonstrates the value of user opinion signals and notebook metrics in building scalable, accurate measures of code understandability.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Survey of Natural Language Processing for the Greek Language</title>
<link>https://arxiv.org/abs/2407.09861</link>
<guid>https://arxiv.org/abs/2407.09861</guid>
<content:encoded><![CDATA[
arXiv:2407.09861v4 Announce Type: replace-cross 
Abstract: Comprehensive monolingual Natural Language Processing (NLP) surveys are essential for assessing language-specific challenges, resource availability, and research gaps. However, existing surveys often lack standardized methodologies, leading to selection bias and fragmented coverage of NLP tasks and resources. This study introduces a generalizable framework for systematic monolingual NLP surveys. Our approach integrates a structured search protocol to minimize bias, an NLP task taxonomy for classification, and language resource taxonomies to identify potential benchmarks and highlight opportunities for improving resource availability. We apply this framework to Greek NLP (2012-2023), providing an in-depth analysis of its current state, task-specific progress, and resource gaps. The survey results are publicly available (https://doi.org/10.5281/zenodo.15314882) and are regularly updated to provide an evergreen resource. This systematic survey of Greek NLP serves as a case study, demonstrating the effectiveness of our framework and its potential for broader application to other not so well-resourced languages as regards NLP.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering</title>
<link>https://arxiv.org/abs/2407.15621</link>
<guid>https://arxiv.org/abs/2407.15621</guid>
<content:encoded><![CDATA[
arXiv:2407.15621v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often generate outdated or inaccurate information based on static training datasets. Retrieval-augmented generation (RAG) mitigates this by integrating outside data sources. While previous RAG systems used pre-assembled, fixed databases with limited flexibility, we have developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data from authoritative radiologic online sources in real-time. We evaluate the diagnostic accuracy of various LLMs when answering radiology-specific questions with and without access to additional online information via RAG. Using 80 questions from the RSNA Case Collection across radiologic subspecialties and 24 additional expert-curated questions with reference standard answers, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were prompted with and without RadioRAG in a zero-shot inference scenario RadioRAG retrieved context-specific information from Radiopaedia in real-time. Accuracy was investigated. Statistical analyses were performed using bootstrapping. The results were further compared with human performance. RadioRAG improved diagnostic accuracy across most LLMs, with relative accuracy increases ranging up to 54% for different LLMs. It matched or exceeded non-RAG models and the human radiologist in question answering across radiologic subspecialties, particularly in breast imaging and emergency radiology. However, the degree of improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement, highlighting variability in RadioRAG's effectiveness. LLMs benefit when provided access to domain-specific data beyond their training data. RadioRAG shows potential to improve LLM accuracy and factuality in radiology question answering by integrating real-time domain-specific data.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Informed Correctors for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2407.21243</link>
<guid>https://arxiv.org/abs/2407.21243</guid>
<content:encoded><![CDATA[
arXiv:2407.21243v3 Announce Type: replace-cross 
Abstract: Discrete diffusion has emerged as a powerful framework for generative modeling in discrete domains, yet efficiently sampling from these models remains challenging. Existing sampling strategies often struggle to balance computation and sample quality when the number of sampling steps is reduced, even when the model has learned the data distribution well. To address these limitations, we propose a predictor-corrector sampling scheme where the corrector is informed by the diffusion model to more reliably counter the accumulating approximation errors. To further enhance the effectiveness of our informed corrector, we introduce complementary architectural modifications based on hollow transformers and a simple tailored training objective that leverages more training signal. We use a synthetic example to illustrate the failure modes of existing samplers and show how informed correctors alleviate these problems. On the text8 and tokenized ImageNet 256x256 datasets, our informed corrector consistently produces superior samples with fewer errors or improved FID scores for discrete diffusion models. These results underscore the potential of informed correctors for fast and high-fidelity generation using discrete diffusion.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey</title>
<link>https://arxiv.org/abs/2407.21794</link>
<guid>https://arxiv.org/abs/2407.21794</guid>
<content:encoded><![CDATA[
arXiv:2407.21794v2 Announce Type: replace-cross 
Abstract: Detecting out-of-distribution (OOD) samples is crucial for ensuring the safety of machine learning systems and has shaped the field of OOD detection. Meanwhile, several other problems are closely related to OOD detection, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). To unify these problems, a generalized OOD detection framework was proposed, taxonomically categorizing these five problems. However, Vision Language Models (VLMs) such as CLIP have significantly changed the paradigm and blurred the boundaries between these fields, again confusing researchers. In this survey, we first present a generalized OOD detection v2, encapsulating the evolution of these fields in the VLM era. Our framework reveals that, with some field inactivity and integration, the demanding challenges have become OOD detection and AD. Then, we highlight the significant shift in the definition, problem settings, and benchmarks; we thus feature a comprehensive review of the methodology for OOD detection and related tasks to clarify their relationship to OOD detection. Finally, we explore the advancements in the emerging Large Vision Language Model (LVLM) era, such as GPT-4V. We conclude with open challenges and future directions. The resource is available at https://github.com/AtsuMiyai/Awesome-OOD-VLM.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing oncology with federated learning: transcending boundaries in breast, lung, and prostate cancer. A systematic review</title>
<link>https://arxiv.org/abs/2408.05249</link>
<guid>https://arxiv.org/abs/2408.05249</guid>
<content:encoded><![CDATA[
arXiv:2408.05249v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a promising solution to address the limitations of centralised machine learning (ML) in oncology, particularly in overcoming privacy concerns and harnessing the power of diverse, multi-center data. This systematic review synthesises current knowledge on the state-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer. Distinct from previous surveys, our comprehensive review critically evaluates the real-world implementation and impact of FL on cancer care, demonstrating its effectiveness in enhancing ML generalisability, performance and data privacy in clinical settings and data. We evaluated state-of-the-art advances in FL, demonstrating its growing adoption amid tightening data privacy regulations. FL outperformed centralised ML in 15 out of the 25 studies reviewed, spanning diverse ML models and clinical applications, and facilitating integration of multi-modal information for precision medicine. Despite the current challenges identified in reproducibility, standardisation and methodology across studies, the demonstrable benefits of FL in harnessing real-world data and addressing clinical needs highlight its significant potential for advancing cancer research. We propose that future research should focus on addressing these limitations and investigating further advanced FL methods, to fully harness data diversity and realise the transformative power of cutting-edge FL in cancer care.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style-Preserving Lip Sync via Audio-Aware Style Reference</title>
<link>https://arxiv.org/abs/2408.05412</link>
<guid>https://arxiv.org/abs/2408.05412</guid>
<content:encoded><![CDATA[
arXiv:2408.05412v2 Announce Type: replace-cross 
Abstract: Audio-driven lip sync has recently drawn significant attention due to its widespread application in the multimedia domain. Individuals exhibit distinct lip shapes when speaking the same utterance, attributed to the unique speaking styles of individuals, posing a notable challenge for audio-driven lip sync. Earlier methods for such task often bypassed the modeling of personalized speaking styles, resulting in sub-optimal lip sync conforming to the general styles. Recent lip sync techniques attempt to guide the lip sync for arbitrary audio by aggregating information from a style reference video, yet they can not preserve the speaking styles well due to their inaccuracy in style aggregation. This work proposes an innovative audio-aware style reference scheme that effectively leverages the relationships between input audio and reference audio from style reference video to address the style-preserving audio-driven lip sync. Specifically, we first develop an advanced Transformer-based model adept at predicting lip motion corresponding to the input audio, augmented by the style information aggregated through cross-attention layers from style reference video. Afterwards, to better render the lip motion into realistic talking face video, we devise a conditional latent diffusion model, integrating lip motion through modulated convolutional layers and fusing reference facial images via spatial cross-attention layers. Extensive experiments validate the efficacy of the proposed approach in achieving precise lip sync, preserving speaking styles, and generating high-fidelity, realistic talking face videos.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Perturb-ability Score to Mitigate Evasion Adversarial Attacks on Flow-Based ML-NIDS</title>
<link>https://arxiv.org/abs/2409.07448</link>
<guid>https://arxiv.org/abs/2409.07448</guid>
<content:encoded><![CDATA[
arXiv:2409.07448v4 Announce Type: replace-cross 
Abstract: As network security threats evolve, safeguarding flow-based Machine Learning (ML)-based Network Intrusion Detection Systems (NIDS) from evasion adversarial attacks is crucial. This paper introduces the notion of feature perturb-ability and presents a novel Perturb-ability Score (PS), which quantifies how susceptible NIDS features are to manipulation in the problem-space by an attacker. PS thereby identifies features structurally resistant to evasion attacks in flow-based ML-NIDS due to the semantics of network traffic fields, as these features are constrained by domain-specific limitations and correlations. Consequently, attempts to manipulate such features would likely either compromise the attack's malicious functionality, render the traffic invalid for processing, or potentially both outcomes simultaneously.
  We introduce and demonstrate the effectiveness of our PS-enabled defenses, PS-guided feature selection and PS-guided feature masking, in enhancing flow-based NIDS resilience. Experimental results across various ML-based NIDS models and public datasets show that discarding or masking highly manipulatable features (high-PS features) can maintain solid detection performance while significantly reducing vulnerability to evasion adversarial attacks. Our findings confirm that PS effectively identifies flow-based NIDS features susceptible to problem-space perturbations. This novel approach leverages problem-space NIDS domain constraints as lightweight universal defense mechanisms against evasion adversarial attacks targeting flow-based ML-NIDS.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Graph Anomaly Detection: A Survey and New Perspectives</title>
<link>https://arxiv.org/abs/2409.09957</link>
<guid>https://arxiv.org/abs/2409.09957</guid>
<content:encoded><![CDATA[
arXiv:2409.09957v2 Announce Type: replace-cross 
Abstract: Graph anomaly detection (GAD), which aims to identify unusual graph instances (nodes, edges, subgraphs, or graphs), has attracted increasing attention in recent years due to its significance in a wide range of applications. Deep learning approaches, graph neural networks (GNNs) in particular, have been emerging as a promising paradigm for GAD, owing to its strong capability in capturing complex structure and/or node attributes in graph data. Considering the large number of methods proposed for GNN-based GAD, it is of paramount importance to summarize the methodologies and findings in the existing GAD studies, so that we can pinpoint effective model designs for tackling open GAD problems. To this end, in this work we aim to present a comprehensive review of deep learning approaches for GAD. Existing GAD surveys are focused on task-specific discussions, making it difficult to understand the technical insights of existing methods and their limitations in addressing some unique challenges in GAD. To fill this gap, we first discuss the problem complexities and their resulting challenges in GAD, and then provide a systematic review of current deep GAD methods from three novel perspectives of methodology, including GNN backbone design, proxy task design for GAD, and graph anomaly measures. To deepen the discussions, we further propose a taxonomy of 13 fine-grained method categories under these three perspectives to provide more in-depth insights into the model designs and their capabilities. To facilitate the experiments and validation, we also summarize a collection of widely-used GAD datasets and empirical comparison. We further discuss multiple open problems to inspire more future high-quality research. A continuously updated repository for datasets, links to the codes of algorithms, and empirical comparison is available at https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs</title>
<link>https://arxiv.org/abs/2410.07009</link>
<guid>https://arxiv.org/abs/2410.07009</guid>
<content:encoded><![CDATA[
arXiv:2410.07009v3 Announce Type: replace-cross 
Abstract: Dealing with long and highly complex technical text is a challenge for Large Language Models (LLMs), which still have to unfold their potential in supporting expensive and timeintensive processes like patent drafting. Within patents, the description constitutes more than 90% of the document on average. Yet, its automatic generation remains understudied. When drafting patent applications, patent attorneys typically receive invention reports (IRs), which are usually confidential, hindering research on LLM-supported patent drafting. Often, prepublication research papers serve as IRs. We leverage this duality to build PAP2PAT, an open and realistic benchmark for patent drafting consisting of 1.8k patent-paper pairs describing the same inventions. To address the complex longdocument patent generation task, we propose chunk-based outline-guided generation using the research paper as invention specification. Our extensive evaluation using PAP2PAT and a human case study show that LLMs can effectively leverage information from the paper, but still struggle to provide the necessary level of detail. Fine-tuning leads to more patent-style language, but also to more hallucination. We release our data and code https://github.com/boschresearch/Pap2Pat.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epochal Sawtooth Phenomenon: Unveiling Training Loss Oscillations in Adam and Other Optimizers</title>
<link>https://arxiv.org/abs/2410.10056</link>
<guid>https://arxiv.org/abs/2410.10056</guid>
<content:encoded><![CDATA[
arXiv:2410.10056v3 Announce Type: replace-cross 
Abstract: In this paper, we identify and analyze a recurring training loss pattern, which we term the \textit{Epochal Sawtooth Phenomenon (ESP)}, commonly observed during training with adaptive gradient-based optimizers, particularly Adam optimizer. This pattern is characterized by a sharp drop in loss at the beginning of each epoch, followed by a gradual increase, resulting in a sawtooth-shaped loss curve. Through empirical observations, we demonstrate that while this effect is most pronounced with Adam, it persists, although less severely, with other optimizers such as RMSProp. We empirically analyze the mechanisms underlying ESP, focusing on key factors such as Adam's $\beta$ parameters, batch size, data shuffling, and sample replacement. Our analysis shows that ESP arises from adaptive learning rate adjustments controlled by the second moment estimate. Additionally, we identify the ``immediate re-exposure to samples'' effect during data shuffling, which causes the model to learn or memorize more at the beginning of each epoch. We also find that smaller values of $\beta_2$ exacerbate ESP but can act as a form of regularization. While ESP is not necessarily indicative of overfitting, higher model capacity can amplify the phenomenon. To further support our analysis, we replicate ESP through a high-dimensional quadratic minimization task. We demonstrate that ESP can emerge even in simple optimization scenarios, reinforcing the generality of this pattern. The code for reproducing our experiments is available at https://github.com/qiliuchn/training-loss-pattern.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title>
<link>https://arxiv.org/abs/2410.16930</link>
<guid>https://arxiv.org/abs/2410.16930</guid>
<content:encoded><![CDATA[
arXiv:2410.16930v4 Announce Type: replace-cross 
Abstract: Math reasoning is an active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence and has implications in several domains, including math education. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within models. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a computationally efficient method we use to isolate math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by filtering out those important for general language tasks. Through pruning parameters MathNeuro identifies, we delete a LLM's math reasoning ability without significantly impacting its general language ability. Scaling the identified parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LL\"aMmlein: Transparent, Compact and Competitive German-Only Language Models from Scratch</title>
<link>https://arxiv.org/abs/2411.11171</link>
<guid>https://arxiv.org/abs/2411.11171</guid>
<content:encoded><![CDATA[
arXiv:2411.11171v5 Announce Type: replace-cross 
Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation</title>
<link>https://arxiv.org/abs/2411.11714</link>
<guid>https://arxiv.org/abs/2411.11714</guid>
<content:encoded><![CDATA[
arXiv:2411.11714v2 Announce Type: replace-cross 
Abstract: Developing general robotic systems capable of manipulating in unstructured environments is a significant challenge, particularly as the tasks involved are typically long-horizon and rich-contact, requiring efficient skill transfer across different task scenarios. To address these challenges, we propose knowledge graph-based skill library construction method. This method hierarchically organizes manipulation knowledge using "task graph" and "scene graph" to represent task-specific and scene-specific information, respectively. Additionally, we introduce "state graph" to facilitate the interaction between high-level task planning and low-level scene information. Building upon this foundation, we further propose a novel hierarchical skill transfer framework based on the skill library and tactile representation, which integrates high-level reasoning for skill transfer and low-level precision for execution. At the task level, we utilize large language models (LLMs) and combine contextual learning with a four-stage chain-of-thought prompting paradigm to achieve subtask sequence transfer. At the motion level, we develop an adaptive trajectory transfer method based on the skill library and the heuristic path planning algorithm. At the physical level, we propose an adaptive contour extraction and posture perception method based on tactile representation. This method dynamically acquires high-precision contour and posture information from visual-tactile images, adjusting parameters such as contact position and posture to ensure the effectiveness of transferred skills in new environments. Experiments demonstrate the skill transfer and adaptability capabilities of the proposed methods across different task scenarios. Project website: https://github.com/MingchaoQi/skill_transfer
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification</title>
<link>https://arxiv.org/abs/2411.12222</link>
<guid>https://arxiv.org/abs/2411.12222</guid>
<content:encoded><![CDATA[
arXiv:2411.12222v2 Announce Type: replace-cross 
Abstract: Multivariate time series (MTS) data is generated through multiple sensors across various domains such as engineering application, health monitoring, and the internet of things, characterized by its temporal changes and high dimensional characteristics. Over the past few years, many studies have explored the long-range dependencies and similarities in MTS. However, long-range dependencies are difficult to model due to their temporal changes and high dimensionality makes it difficult to obtain similarities effectively and efficiently. Thus, to address these issues, we propose contrast similarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba). Firstly, to obtain the dynamic similarity of each sample, we initially use temporal contrast learning module to acquire MTS representations. And then we construct a similarity matrix between MTS representations using Fast Dynamic Time Warping (FastDTW). Secondly, we apply the DPMamba to consider the bidirectional nature of MTS, allowing us to better capture long-range and short-range dependencies within the data. Finally, we utilize the Kolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the information interaction in the matrix and MTS node classification task. By comprehensively considering the long-range dependencies and dynamic similarity features, we achieved precise MTS node classification. We conducted experiments on multiple University of East Anglia (UEA) MTS datasets, which encompass diverse application scenarios. Our results demonstrate the superiority of our method through both supervised and semi-supervised experiments on the MTS classification task.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate Series Classification</title>
<link>https://arxiv.org/abs/2411.18043</link>
<guid>https://arxiv.org/abs/2411.18043</guid>
<content:encoded><![CDATA[
arXiv:2411.18043v2 Announce Type: replace-cross 
Abstract: Multivariate time series (MTS) classification is widely applied in fields such as industry, healthcare, and finance, aiming to extract key features from complex time series data for accurate decision-making and prediction. However, existing methods for MTS often struggle due to the challenges of effectively modeling high-dimensional data and the lack of labeled data, resulting in poor classification performance. To address this issue, we propose a heterogeneous relationships of subjects and shapelets method for semi-supervised MTS classification. This method offers a novel perspective by integrating various types of additional information while capturing the relationships between them. Specifically, we first utilize a contrast temporal self-attention module to obtain sparse MTS representations, and then model the similarities between these representations using soft dynamic time warping to construct a similarity graph. Secondly, we learn the shapelets for different subject types, incorporating both the subject features and their shapelets as additional information to further refine the similarity graph, ultimately generating a heterogeneous graph. Finally, we use a dual level graph attention network to get prediction. Through this method, we successfully transform dataset into a heterogeneous graph, integrating multiple additional information and achieving precise semi-supervised node classification. Experiments on the Human Activity Recognition, sleep stage classification and University of East Anglia datasets demonstrate that our method outperforms current state-of-the-art methods in MTS classification tasks, validating its superiority.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLARE: Towards Universal Dataset Purification against Backdoor Attacks</title>
<link>https://arxiv.org/abs/2411.19479</link>
<guid>https://arxiv.org/abs/2411.19479</guid>
<content:encoded><![CDATA[
arXiv:2411.19479v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) are susceptible to backdoor attacks, where adversaries poison datasets with adversary-specified triggers to implant hidden backdoors, enabling malicious manipulation of model predictions. Dataset purification serves as a proactive defense by removing malicious training samples to prevent backdoor injection at its source. We first reveal that the current advanced purification methods rely on a latent assumption that the backdoor connections between triggers and target labels in backdoor attacks are simpler to learn than the benign features. We demonstrate that this assumption, however, does not always hold, especially in all-to-all (A2A) and untargeted (UT) attacks. As a result, purification methods that analyze the separation between the poisoned and benign samples in the input-output space or the final hidden layer space are less effective. We observe that this separability is not confined to a single layer but varies across different hidden layers. Motivated by this understanding, we propose FLARE, a universal purification method to counter various backdoor attacks. FLARE aggregates abnormal activations from all hidden layers to construct representations for clustering. To enhance separation, FLARE develops an adaptive subspace selection algorithm to isolate the optimal space for dividing an entire dataset into two clusters. FLARE assesses the stability of each cluster and identifies the cluster with higher stability as poisoned. Extensive evaluations on benchmark datasets demonstrate the effectiveness of FLARE against 22 representative backdoor attacks, including all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and its robustness to adaptive attacks. Codes are available at \href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and \href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization</title>
<link>https://arxiv.org/abs/2412.03092</link>
<guid>https://arxiv.org/abs/2412.03092</guid>
<content:encoded><![CDATA[
arXiv:2412.03092v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the ability of LLM-based systems to perform complex tasks through natural language processing and tool interaction. However, optimizing these LLM-based systems for specific tasks remains challenging, often requiring manual interventions like prompt engineering and hyperparameter tuning. Existing automatic optimization methods, such as textual feedback-based techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to using immediate derivatives in traditional numerical gradient descent. However, relying solely on such feedback can be limited when the adjustments made in response to this feedback are either too small or fluctuate irregularly, potentially slowing down or even stalling the optimization process. To overcome these challenges, more adaptive methods are needed, especially in situations where the system's response is evolving slowly or unpredictably. In this paper, we introduce REVOLVE, an optimization method that tracks how "R"esponses "EVOLVE" across iterations in LLM systems. By focusing on the evolution of responses over time, REVOLVE enables more stable and effective optimization by making thoughtful, progressive adjustments at each step. Experimental results demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8% improvement in prompt optimization, a 20.72% gain in solution refinement, and a 29.17% increase in code optimization. Additionally, REVOLVE converges in fewer iterations, resulting in significant computational savings. Beyond its practical contributions, REVOLVE highlights a promising direction, where the rich knowledge from established optimization principles can be leveraged to enhance LLM systems, which paves the way for further advancements in this hybrid domain.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiclass Post-Earthquake Building Assessment Integrating High-Resolution Optical and SAR Satellite Imagery, Ground Motion, and Soil Data with Transformers</title>
<link>https://arxiv.org/abs/2412.04664</link>
<guid>https://arxiv.org/abs/2412.04664</guid>
<content:encoded><![CDATA[
arXiv:2412.04664v3 Announce Type: replace-cross 
Abstract: Timely and accurate assessments of building damage are crucial for effective response and recovery in the aftermath of earthquakes. Conventional preliminary damage assessments (PDA) often rely on manual door-to-door inspections, which are not only time-consuming but also pose significant safety risks. To safely expedite the PDA process, researchers have studied the applicability of satellite imagery processed with heuristic and machine learning approaches. These approaches output binary or, more recently, multiclass damage states at the scale of a block or a single building. However, the current performance of such approaches limits practical applicability. To address this limitation, we introduce a metadata-enriched, transformer based framework that combines high-resolution post-earthquake satellite imagery with building-specific metadata relevant to the seismic performance of the structure. Our model achieves state-of-the-art performance in multiclass post-earthquake damage identification for buildings from the Turkey-Syria earthquake on February 6, 2023. Specifically, we demonstrate that incorporating metadata, such as seismic intensity indicators, soil properties, and SAR damage proxy maps not only enhances the model's accuracy and ability to distinguish between damage classes, but also improves its generalizability across various regions. Furthermore, we conducted a detailed, class-wise analysis of feature importance to understand the model's decision-making across different levels of building damage. This analysis reveals how individual metadata features uniquely contribute to predictions for each damage class. By leveraging both satellite imagery and metadata, our proposed framework enables faster and more accurate damage assessments for precise, multiclass, building-level evaluations that can improve disaster response and accelerate recovery efforts for affected communities.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</title>
<link>https://arxiv.org/abs/2412.13612</link>
<guid>https://arxiv.org/abs/2412.13612</guid>
<content:encoded><![CDATA[
arXiv:2412.13612v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video Generation</title>
<link>https://arxiv.org/abs/2412.14018</link>
<guid>https://arxiv.org/abs/2412.14018</guid>
<content:encoded><![CDATA[
arXiv:2412.14018v2 Announce Type: replace-cross 
Abstract: Surgical video generation can enhance medical education and research, but existing methods lack fine-grained motion control and realism. We introduce SurgSora, a framework that generates high-fidelity, motion-controllable surgical videos from a single input frame and user-specified motion cues. Unlike prior approaches that treat objects indiscriminately or rely on ground-truth segmentation masks, SurgSora leverages self-predicted object features and depth information to refine RGB appearance and optical flow for precise video synthesis. It consists of three key modules: (1) the Dual Semantic Injector, which extracts object-specific RGB-D features and segmentation cues to enhance spatial representations; (2) the Decoupled Flow Mapper, which fuses multi-scale optical flow with semantic features for realistic motion dynamics; and (3) the Trajectory Controller, which estimates sparse optical flow and enables user-guided object movement. By conditioning these enriched features within the Stable Video Diffusion, SurgSora achieves state-of-the-art visual authenticity and controllability in advancing surgical video synthesis, as demonstrated by extensive quantitative and qualitative comparisons. Our human evaluation in collaboration with expert surgeons further demonstrates the high realism of SurgSora-generated videos, highlighting the potential of our method for surgical training and education. Our project is available at https://surgsora.github.io/surgsora.github.io.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review</title>
<link>https://arxiv.org/abs/2412.18043</link>
<guid>https://arxiv.org/abs/2412.18043</guid>
<content:encoded><![CDATA[
arXiv:2412.18043v2 Announce Type: replace-cross 
Abstract: Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Ask Good Questions?</title>
<link>https://arxiv.org/abs/2501.03491</link>
<guid>https://arxiv.org/abs/2501.03491</guid>
<content:encoded><![CDATA[
arXiv:2501.03491v2 Announce Type: replace-cross 
Abstract: We evaluate questions generated by large language models (LLMs) from context, comparing them to human-authored questions across six dimensions: question type, question length, context coverage, answerability, uncommonness, and required answer length. Our study spans two open-source and two proprietary state-of-the-art models. Results reveal that LLM-generated questions tend to demand longer descriptive answers and exhibit more evenly distributed context focus, in contrast to the positional bias often seen in QA tasks. These findings provide insights into the distinctive characteristics of LLM-generated questions and inform future work on question quality and downstream applications.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective Transition of Large Language Models for Solving Subjective Tasks</title>
<link>https://arxiv.org/abs/2501.09265</link>
<guid>https://arxiv.org/abs/2501.09265</guid>
<content:encoded><![CDATA[
arXiv:2501.09265v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions</title>
<link>https://arxiv.org/abs/2502.00620</link>
<guid>https://arxiv.org/abs/2502.00620</guid>
<content:encoded><![CDATA[
arXiv:2502.00620v4 Announce Type: replace-cross 
Abstract: Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks</title>
<link>https://arxiv.org/abs/2502.02834</link>
<guid>https://arxiv.org/abs/2502.02834</guid>
<content:encoded><![CDATA[
arXiv:2502.02834v3 Announce Type: replace-cross 
Abstract: Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments. Our code is available at https://github.com/JM-Kim-94/tavt.git.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.02844</link>
<guid>https://arxiv.org/abs/2502.02844</guid>
<content:encoded><![CDATA[
arXiv:2502.02844v3 Announce Type: replace-cross 
Abstract: Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering systemwide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL. Our code is available at https://github.com/sunwoolee0504/WALL.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale</title>
<link>https://arxiv.org/abs/2502.16645</link>
<guid>https://arxiv.org/abs/2502.16645</guid>
<content:encoded><![CDATA[
arXiv:2502.16645v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supporting the development of Machine Learning for fundamental science in a federated Cloud with the AI_INFN platform</title>
<link>https://arxiv.org/abs/2502.21266</link>
<guid>https://arxiv.org/abs/2502.21266</guid>
<content:encoded><![CDATA[
arXiv:2502.21266v2 Announce Type: replace-cross 
Abstract: Machine Learning (ML) is driving a revolution in the way scientists design, develop, and deploy data-intensive software. However, the adoption of ML presents new challenges for the computing infrastructure, particularly in terms of provisioning and orchestrating access to hardware accelerators for development, testing, and production. The INFN-funded project AI_INFN ("Artificial Intelligence at INFN") aims at fostering the adoption of ML techniques within INFN use cases by providing support on multiple aspects, including the provision of AI-tailored computing resources. It leverages cloud-native solutions in the context of INFN Cloud, to share hardware accelerators as effectively as possible, ensuring the diversity of the Institute's research activities is not compromised. In this contribution, we provide an update on the commissioning of a Kubernetes platform designed to ease the development of GPU-powered data analysis workflows and their scalability on heterogeneous, distributed computing resources, possibly federated as Virtual Kubelets with the interLink provider.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learners Should Acknowledge the Legal Implications of Large Language Models as Personal Data</title>
<link>https://arxiv.org/abs/2503.01630</link>
<guid>https://arxiv.org/abs/2503.01630</guid>
<content:encoded><![CDATA[
arXiv:2503.01630v2 Announce Type: replace-cross 
Abstract: Does GPT know you? The answer depends on your level of public recognition; however, if your information was available on a website, the answer could be yes. Most Large Language Models (LLMs) memorize training data to some extent. Thus, even when an LLM memorizes only a small amount of personal data, it typically falls within the scope of data protection laws. If a person is identified or identifiable, the implications are far-reaching. The LLM is subject to EU General Data Protection Regulation requirements even after the training phase is concluded. To back our arguments: (1.) We reiterate that LLMs output training data at inference time, be it verbatim or in generalized form. (2.) We show that some LLMs can thus be considered personal data on their own. This triggers a cascade of data protection implications such as data subject rights, including rights to access, rectification, or erasure. These rights extend to the information embedded within the AI model. (3.) This paper argues that machine learning researchers must acknowledge the legal implications of LLMs as personal data throughout the full ML development lifecycle, from data collection and curation to model provision on e.g., GitHub or Hugging Face. (4.) We propose different ways for the ML research community to deal with these legal implications. Our paper serves as a starting point for improving the alignment between data protection law and the technical capabilities of LLMs. Our findings underscore the need for more interaction between the legal domain and the ML community.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice</title>
<link>https://arxiv.org/abs/2503.01903</link>
<guid>https://arxiv.org/abs/2503.01903</guid>
<content:encoded><![CDATA[
arXiv:2503.01903v2 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) offers potential solutions to address problems such as shortage of medical resources and low diagnostic consistency in psychiatric clinical practice. Despite this potential, a robust and comprehensive benchmarking framework to assess the efficacy of LLMs in authentic psychiatric clinical environments is absent. This has impeded the advancement of specialized LLMs tailored to psychiatric applications. In response to this gap, by incorporating clinical demands in psychiatry and clinical data, we proposed a benchmarking system, PsychBench, to evaluate the practical performance of LLMs in psychiatric clinical settings. We conducted a comprehensive quantitative evaluation of 16 LLMs using PsychBench, and investigated the impact of prompt design, chain-of-thought reasoning, input text length, and domain-specific knowledge fine-tuning on model performance. Through detailed error analysis, we identified strengths and potential limitations of the existing models and suggested directions for improvement. Subsequently, a clinical reader study involving 60 psychiatrists of varying seniority was conducted to further explore the practical benefits of existing LLMs as supportive tools for psychiatrists of varying seniority. Through the quantitative and reader evaluation, we show that while existing models demonstrate significant potential, they are not yet adequate as decision-making tools in psychiatric clinical practice. The reader study further indicates that, as an auxiliary tool, LLM could provide particularly notable support for junior psychiatrists, effectively enhancing their work efficiency and overall clinical quality. To promote research in this area, we will make the dataset and evaluation framework publicly available, with the hope of advancing the application of LLMs in psychiatric clinical settings.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoBlind: Towards Egocentric Visual Assistance for the Blind</title>
<link>https://arxiv.org/abs/2503.08221</link>
<guid>https://arxiv.org/abs/2503.08221</guid>
<content:encoded><![CDATA[
arXiv:2503.08221v2 Announce Type: replace-cross 
Abstract: We present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,392 videos that record the daily lives of real blind users from a first-person perspective. It also features 5,311 questions directly posed or generated and verified by blind individuals to reflect their in-situation needs for visual assistance under various scenarios. We provide each question with an average of 3 reference answers to alleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 16 advanced MLLMs and find that all models struggle, with the best performers achieving accuracy near 60\%, far behind human performance of 87.4\%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and explore heuristic solutions for improvement. With these efforts, we hope EgoBlind can serve as a valuable foundation for developing more effective AI assistants to enhance the independence of the blind individuals' lives. Data and evaluation code are available at https://github.com/doc-doc/EgoBlind.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation</title>
<link>https://arxiv.org/abs/2503.08327</link>
<guid>https://arxiv.org/abs/2503.08327</guid>
<content:encoded><![CDATA[
arXiv:2503.08327v2 Announce Type: replace-cross 
Abstract: As automatic metrics become increasingly stronger and widely adopted, the risk of unintentionally "gaming the metric" during model development rises. This issue is caused by metric interference (MINT), i.e., the use of the same or related metrics for both model tuning and evaluation. MINT can misguide practitioners into being overoptimistic about the performance of their systems: as system outputs become a function of the interfering metric, their estimated quality loses correlation with human judgments. In this work, we analyze two common cases of MINT in machine translation-related tasks: filtering of training data, and decoding with quality signals. Importantly, we find that MINT strongly distorts instance-level metric scores, even when metrics are not directly optimized for-questioning the common strategy of leveraging a different, yet related metric for evaluation that is not used for tuning. To address this problem, we propose MINTADJUST, a method for more reliable evaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks translations and systems more accurately than state-of-the-art metrics across a majority of language pairs, especially for high-quality systems. Furthermore, MINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving UnderEdit &amp; OverEdit with Iterative &amp; Neighbor-Assisted Model Editing</title>
<link>https://arxiv.org/abs/2503.11895</link>
<guid>https://arxiv.org/abs/2503.11895</guid>
<content:encoded><![CDATA[
arXiv:2503.11895v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are widely deployed in downstream tasks, but keeping their knowledge up-to-date via retraining or fine-tuning is often computationally expensive. Model editing provides a more efficient alternative by updating a targeted subset of parameters, which often follows the locate-and-edit paradigm. Despite this efficiency, existing methods are limited: edits may fail to inject knowledge (UnderEdit) or unintentionally disrupt unrelated neighboring knowledge (OverEdit). To address these challenges, we propose two complementary methods: iterative model editing, which applies successive edits to mitigate UnderEdit, and neighbor-assisted model editing, which incorporates neighboring knowledge during editing to reduce OverEdit. Our extensive experiments show that these techniques improve editing performance across multiple LLMs, algorithms, and benchmarks, reducing UnderEdit by up to 38 percentage points and OverEdit by up to 6, while remaining broadly applicable to any locate-and-edit method.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Propagation of Chaos: A Stochastic Algorithm for Mean Field Optimization</title>
<link>https://arxiv.org/abs/2503.13115</link>
<guid>https://arxiv.org/abs/2503.13115</guid>
<content:encoded><![CDATA[
arXiv:2503.13115v2 Announce Type: replace-cross 
Abstract: Gradient flow in the 2-Wasserstein space is widely used to optimize functionals over probability distributions and is typically implemented using an interacting particle system with $n$ particles. Analyzing these algorithms requires showing (a) that the finite-particle system converges and/or (b) that the resultant empirical distribution of the particles closely approximates the optimal distribution (i.e., propagation of chaos). However, establishing efficient sufficient conditions can be challenging, as the finite particle system may produce heavily dependent random variables.
  In this work, we study the virtual particle stochastic approximation, originally introduced for Stein Variational Gradient Descent. This method can be viewed as a form of stochastic gradient descent in the Wasserstein space and can be implemented efficiently. In popular settings, we demonstrate that our algorithm's output converges to the optimal distribution under conditions similar to those for the infinite particle limit, and it produces i.i.d. samples without the need to explicitly establish propagation of chaos bounds.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KANITE: Kolmogorov-Arnold Networks for ITE estimation</title>
<link>https://arxiv.org/abs/2503.13912</link>
<guid>https://arxiv.org/abs/2503.13912</guid>
<content:encoded><![CDATA[
arXiv:2503.13912v2 Announce Type: replace-cross 
Abstract: We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs) for Individual Treatment Effect (ITE) estimation under multiple treatments setting in causal inference. By utilizing KAN's unique abilities to learn univariate activation functions as opposed to learning linear weights by Multi-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE framework comprises two key architectures: 1.Integral Probability Metric (IPM) architecture: This employs an IPM loss in a specialized manner to effectively align towards ITE estimation across multiple treatments. 2. Entropy Balancing (EB) architecture: This uses weights for samples that are learned by optimizing entropy subject to balancing the covariates across treatment groups. Extensive evaluations on benchmark datasets demonstrate that KANITE outperforms state-of-the-art algorithms in both $\epsilon_{\text{PEHE}}$ and $\epsilon_{\text{ATE}}$ metrics. Our experiments highlight the advantages of KANITE in achieving improved causal estimates, emphasizing the potential of KANs to advance causal inference methodologies across diverse application areas.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bird Song Detector for improving bird identification through Deep Learning: a case study from Do\~nana</title>
<link>https://arxiv.org/abs/2503.15576</link>
<guid>https://arxiv.org/abs/2503.15576</guid>
<content:encoded><![CDATA[
arXiv:2503.15576v2 Announce Type: replace-cross 
Abstract: Passive Acoustic Monitoring is a key tool for biodiversity conservation, but the large volumes of unsupervised audio it generates present major challenges for extracting meaningful information. Deep Learning offers promising solutions. BirdNET, a widely used bird identification model, has shown success in many study systems but is limited at local scale due to biases in its training data, which focus on specific locations and target sounds rather than entire soundscapes. A key challenge in bird species identification is that many recordings either lack target species or contain overlapping vocalizations, complicating automatic identification. To address these problems, we developed a multi-stage pipeline for automatic bird vocalization identification in Do\~nana National Park (SW Spain), a wetland of high conservation concern. We deployed AudioMoth recorders in three main habitats across nine locations and manually annotated 461 minutes of audio, resulting in 3749 labeled segments spanning 34 classes. We first applied a Bird Song Detector to isolate bird vocalizations using spectrogram-based image processing. Then, species were classified using custom models trained at the local scale. Applying the Bird Song Detector before classification improved species identification, as all models performed better when analyzing only the segments where birds were detected. Specifically, the combination of detector and fine-tuned BirdNET outperformed the baseline without detection. This approach demonstrates the effectiveness of integrating a Bird Song Detector with local classification models. These findings highlight the need to adapt general-purpose tools to specific ecological challenges. Automatically detecting bird species helps track the health of this threatened ecosystem, given birds sensitivity to environmental change, and supports conservation planning to reduce biodiversity loss.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Personalized Federated Learning Architectures for Violence Detection in Surveillance Videos</title>
<link>https://arxiv.org/abs/2504.00857</link>
<guid>https://arxiv.org/abs/2504.00857</guid>
<content:encoded><![CDATA[
arXiv:2504.00857v2 Announce Type: replace-cross 
Abstract: The challenge of detecting violent incidents in urban surveillance systems is compounded by the voluminous and diverse nature of video data. This paper presents a targeted approach using Personalized Federated Learning (PFL) to address these issues, specifically employing the Federated Learning with Personalization Layers method within the Flower framework. Our methodology adapts learning models to the unique data characteristics of each surveillance node, effectively managing the heterogeneous and non-IID nature of surveillance video data. Through rigorous experiments conducted on balanced and imbalanced datasets, our PFL models demonstrated enhanced accuracy and efficiency, achieving up to 99.3% accuracy. This study underscores the potential of PFL to significantly improve the scalability and effectiveness of surveillance systems, offering a robust, privacy-preserving solution for violence detection in complex urban environments.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning</title>
<link>https://arxiv.org/abs/2504.04524</link>
<guid>https://arxiv.org/abs/2504.04524</guid>
<content:encoded><![CDATA[
arXiv:2504.04524v2 Announce Type: replace-cross 
Abstract: Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) haven't yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2504.04893</link>
<guid>https://arxiv.org/abs/2504.04893</guid>
<content:encoded><![CDATA[
arXiv:2504.04893v5 Announce Type: replace-cross 
Abstract: Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influential Bandits: Pulling an Arm May Change the Environment</title>
<link>https://arxiv.org/abs/2504.08200</link>
<guid>https://arxiv.org/abs/2504.08200</guid>
<content:encoded><![CDATA[
arXiv:2504.08200v2 Announce Type: replace-cross 
Abstract: While classical formulations of multi-armed bandit problems assume that each arm's reward is independent and stationary, real-world applications often involve non-stationary environments and interdependencies between arms. In particular, selecting one arm may influence the future rewards of other arms, a scenario not adequately captured by existing models such as rotting bandits or restless bandits. To address this limitation, we propose the influential bandit problem, which models inter-arm interactions through an unknown, symmetric, positive semi-definite interaction matrix that governs the dynamics of arm losses. We formally define this problem and establish two regret lower bounds, including a superlinear $\Omega(T^2 / \log^2 T)$ bound for the standard LCB algorithm (loss minimization version of UCB) and an algorithm-independent $\Omega(T)$ bound, which highlight the inherent difficulty of the setting. We then introduce a new algorithm based on a lower confidence bound (LCB) estimator tailored to the structure of the loss dynamics. Under mild assumptions, our algorithm achieves a regret of $O(KT \log T)$, which is nearly optimal in terms of its dependence on the time horizon. The algorithm is simple to implement and computationally efficient. Empirical evaluations on both synthetic and real-world datasets demonstrate the presence of inter-arm influence and confirm the superior performance of our method compared to conventional bandit algorithms.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs</title>
<link>https://arxiv.org/abs/2504.11511</link>
<guid>https://arxiv.org/abs/2504.11511</guid>
<content:encoded><![CDATA[
arXiv:2504.11511v2 Announce Type: replace-cross 
Abstract: The rise of reinforcement learning (RL) in critical real-world applications demands a fundamental rethinking of privacy in AI systems. Traditional privacy frameworks, designed to protect isolated data points, fall short for sequential decision-making systems where sensitive information emerges from temporal patterns, behavioral strategies, and collaborative dynamics. Modern RL paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in large language models (LLMs), exacerbate these challenges by introducing complex, interactive, and context-dependent learning environments that traditional methods do not address. In this position paper, we argue for a new privacy paradigm built on four core principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation. These principles expose inherent tensions between privacy, utility, and interpretability that must be navigated as RL systems become more pervasive in high-stakes domains like healthcare, autonomous vehicles, and decision support systems powered by LLMs. To tackle these challenges, we call for the development of new theoretical frameworks, practical mechanisms, and rigorous evaluation methodologies that collectively enable effective privacy protection in sequential decision-making systems.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions</title>
<link>https://arxiv.org/abs/2504.20304</link>
<guid>https://arxiv.org/abs/2504.20304</guid>
<content:encoded><![CDATA[
arXiv:2504.20304v3 Announce Type: replace-cross 
Abstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank. It is derived from previously dependency-annotated CHILDES data, which we harmonize to follow unified annotation principles. The gold-standard trees encompass utterances sampled from 11 children and their caregivers, totaling over 48K sentences (236K tokens). We validate these gold-standard annotations under the UD v2 framework and provide an additional 1M~silver-standard sentences, offering a consistent resource for computational and linguistic research.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamGen: Unlocking Generalization in Robot Learning through Video World Models</title>
<link>https://arxiv.org/abs/2505.12705</link>
<guid>https://arxiv.org/abs/2505.12705</guid>
<content:encoded><![CDATA[
arXiv:2505.12705v2 Announce Type: replace-cross 
Abstract: We introduce DreamGen, a simple yet highly effective 4-stage pipeline for training robot policies that generalize across behaviors and environments through neural trajectories - synthetic robot data generated from video world models. DreamGen leverages state-of-the-art image-to-video generative models, adapting them to the target robot embodiment to produce photorealistic synthetic videos of familiar or novel tasks in diverse environments. Since these models generate only videos, we recover pseudo-action sequences using either a latent action model or an inverse-dynamics model (IDM). Despite its simplicity, DreamGen unlocks strong behavior and environment generalization: a humanoid robot can perform 22 new behaviors in both seen and unseen environments, while requiring teleoperation data from only a single pick-and-place task in one environment. To evaluate the pipeline systematically, we introduce DreamGen Bench, a video generation benchmark that shows a strong correlation between benchmark performance and downstream policy success. Our work establishes a promising new axis for scaling robot learning well beyond manual data collection. Code available at https://github.com/NVIDIA/GR00T-Dreams.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractured Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
arXiv:2505.12992v3 Announce Type: replace-cross 
Abstract: Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at https://github.com/BaohaoLiao/frac-cot.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13346</link>
<guid>https://arxiv.org/abs/2505.13346</guid>
<content:encoded><![CDATA[
arXiv:2505.13346v3 Announce Type: replace-cross 
Abstract: To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17830</link>
<guid>https://arxiv.org/abs/2505.17830</guid>
<content:encoded><![CDATA[
arXiv:2505.17830v2 Announce Type: replace-cross 
Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Long CoT Reasoning in Small Language Models</title>
<link>https://arxiv.org/abs/2505.18440</link>
<guid>https://arxiv.org/abs/2505.18440</guid>
<content:encoded><![CDATA[
arXiv:2505.18440v2 Announce Type: replace-cross 
Abstract: Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long CoT. Thus, distillation becomes a practical method to enable SLMs for such reasoning ability. However, the long CoT often contains a lot of redundant contents (e.g., overthinking steps) which may make SLMs hard to learn considering their relatively poor capacity and generalization. To address this issue, we propose a simple-yet-effective method to prune unnecessary steps in long CoT, and then employ an on-policy method for the SLM itself to curate valid and useful long CoT training data. In this way, SLMs can effectively learn efficient long CoT reasoning and preserve competitive performance at the same time. Experimental results across a series of mathematical reasoning benchmarks demonstrate the effectiveness of the proposed method in distilling long CoT reasoning ability into SLMs which maintains the competitive performance but significantly reduces generating redundant reasoning steps.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2505.18787</link>
<guid>https://arxiv.org/abs/2505.18787</guid>
<content:encoded><![CDATA[
arXiv:2505.18787v2 Announce Type: replace-cross 
Abstract: Deepfake (DF) detectors face significant challenges when deployed in real-world environments, particularly when encountering test samples deviated from training data through either postprocessing manipulations or distribution shifts. We demonstrate postprocessing techniques can completely obscure generation artifacts presented in DF samples, leading to performance degradation of DF detectors. To address these challenges, we propose Think Twice before Adaptation (\texttt{T$^2$A}), a novel online test-time adaptation method that enhances the adaptability of detectors during inference without requiring access to source training data or labels. Our key idea is to enable the model to explore alternative options through an Uncertainty-aware Negative Learning objective rather than solely relying on its initial predictions as commonly seen in entropy minimization (EM)-based approaches. We also introduce an Uncertain Sample Prioritization strategy and Gradients Masking technique to improve the adaptation by focusing on important samples and model parameters. Our theoretical analysis demonstrates that the proposed negative learning objective exhibits complementary behavior to EM, facilitating better adaptation capability. Empirically, our method achieves state-of-the-art results compared to existing test-time adaptation (TTA) approaches and significantly enhances the resilience and generalization of DF detectors during inference. Code is available \href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2505.18799</link>
<guid>https://arxiv.org/abs/2505.18799</guid>
<content:encoded><![CDATA[
arXiv:2505.18799v4 Announce Type: replace-cross 
Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant training adjustment costs. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the Attention Localization and Pruning Strategy (ALPS), an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only 10% of attention parameters during fine-tuning while achieving a 2% performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment. The code is available at https://github.com/VoiceBeer/ALPS.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</title>
<link>https://arxiv.org/abs/2505.21569</link>
<guid>https://arxiv.org/abs/2505.21569</guid>
<content:encoded><![CDATA[
arXiv:2505.21569v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications</title>
<link>https://arxiv.org/abs/2505.24765</link>
<guid>https://arxiv.org/abs/2505.24765</guid>
<content:encoded><![CDATA[
arXiv:2505.24765v4 Announce Type: replace-cross 
Abstract: Supervised Quantum Machine Learning (QML) represents an intersection of quantum computing and classical machine learning, aiming to use quantum resources to support model training and inference. This paper reviews recent developments in supervised QML, focusing on methods such as variational quantum circuits, quantum neural networks, and quantum kernel methods, along with hybrid quantum-classical workflows. We examine recent experimental studies that show partial indications of quantum advantage and describe current limitations including noise, barren plateaus, scalability issues, and the lack of formal proofs of performance improvement over classical methods. The main contribution is a ten-year outlook (2025-2035) that outlines possible developments in supervised QML, including a roadmap describing conditions under which QML may be used in applied research and enterprise systems over the next decade.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients</title>
<link>https://arxiv.org/abs/2506.04265</link>
<guid>https://arxiv.org/abs/2506.04265</guid>
<content:encoded><![CDATA[
arXiv:2506.04265v2 Announce Type: replace-cross 
Abstract: This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to suboptimal policy updates as it fails to account for the distinct contributions of agents. Although numerous methods consider global or individual contributions for credit assignment, a detailed analysis at the coalition level remains lacking in many approaches. This work analyzes the over-updating problem during multi-agent policy updates from a coalition-level perspective. To address this issue, we propose a credit assignment method called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates coalitional advantages via marginal contributions from all possible coalitions and decomposes advantages using the core solution from cooperative game theory, ensuring coalitional rationality. To reduce computational overhead, CORA employs random coalition sampling. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that CORA outperforms strong baselines, particularly in tasks with multiple local optima. These findings highlight the importance of coalition-aware credit assignment for improving MARL performance.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
arXiv:2506.06561v2 Announce Type: replace-cross 
Abstract: Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning</title>
<link>https://arxiv.org/abs/2506.06955</link>
<guid>https://arxiv.org/abs/2506.06955</guid>
<content:encoded><![CDATA[
arXiv:2506.06955v2 Announce Type: replace-cross 
Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers Don't Need Trained Registers</title>
<link>https://arxiv.org/abs/2506.08010</link>
<guid>https://arxiv.org/abs/2506.08010</guid>
<content:encoded><![CDATA[
arXiv:2506.08010v3 Announce Type: replace-cross 
Abstract: We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09033</link>
<guid>https://arxiv.org/abs/2506.09033</guid>
<content:encoded><![CDATA[
arXiv:2506.09033v2 Announce Type: replace-cross 
Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers</title>
<link>https://arxiv.org/abs/2506.09099</link>
<guid>https://arxiv.org/abs/2506.09099</guid>
<content:encoded><![CDATA[
arXiv:2506.09099v2 Announce Type: replace-cross 
Abstract: The relationship between memorization and generalization in large language models (LLMs) remains an open area of research, with growing evidence that the two are deeply intertwined. In this work, we investigate this relationship by pre-training a series of capacity-limited Transformer models from scratch on two synthetic character-level tasks designed to separately probe generalization (via arithmetic extrapolation) and memorization (via factual recall). We observe a consistent trade-off: small models extrapolate to unseen arithmetic cases but fail to memorize facts, while larger models memorize but fail to extrapolate. An intermediate-capacity model exhibits a similar shift toward memorization. When trained on both tasks jointly, no model (regardless of size) succeeds at extrapolation. These findings suggest that pre-training may intrinsically favor one learning mode over the other. By isolating these dynamics in a controlled setting, our study offers insight into how model capacity shapes learning behavior and offers broader implications for the design and deployment of small language models.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Task Reward Learning from Human Ratings</title>
<link>https://arxiv.org/abs/2506.09183</link>
<guid>https://arxiv.org/abs/2506.09183</guid>
<content:encoded><![CDATA[
arXiv:2506.09183v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) has become a key factor in aligning model behavior with users' goals. However, while humans integrate multiple strategies when making decisions, current RLHF approaches often simplify this process by modeling human reasoning through isolated tasks such as classification or regression. In this paper, we propose a novel reinforcement learning (RL) method that mimics human decision-making by jointly considering multiple tasks. Specifically, we leverage human ratings in reward-free environments to infer a reward function, introducing learnable weights that balance the contributions of both classification and regression models. This design captures the inherent uncertainty in human decision-making and allows the model to adaptively emphasize different strategies. We conduct several experiments using synthetic human ratings to validate the effectiveness of the proposed approach. Results show that our method consistently outperforms existing rating-based RL methods, and in some cases, even surpasses traditional RL approaches.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation</title>
<link>https://arxiv.org/abs/2506.09331</link>
<guid>https://arxiv.org/abs/2506.09331</guid>
<content:encoded><![CDATA[
arXiv:2506.09331v2 Announce Type: replace-cross 
Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2506.09507</link>
<guid>https://arxiv.org/abs/2506.09507</guid>
<content:encoded><![CDATA[
arXiv:2506.09507v3 Announce Type: replace-cross 
Abstract: Transformers exhibit proficiency in capturing long-range dependencies, whereas State Space Models (SSMs) facilitate linear-time sequence modeling. Notwithstanding their synergistic potential, the integration of these architectures presents a significant challenge, primarily attributable to a fundamental incongr inuity their respective positional encoding mechanisms: Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs leverage implicit positional representations via convolutions. This divergence often precipitates discontinuities and suboptimal performance.To address this impediment, we propose a unified rotary position embedding (Unified RoPE) methodology, thereby establishing a consistent positional encoding framework for both self-attention and state-space components. Using this Unified RoPE, we introduce TransXSSM, a hybrid architecture that coherently integrates the Transformer and SSM layers under this unified positional encoding scheme. At a 4 sequenceK length, TransXSSM exhibits training and inference speeds that are 42.3% and 29.5% faster, respectively, relative to standard Transformer models. It also delivers higher accuracy: under comparable settings, it surpasses a Transformer baseline by over 4% on language modeling benchmarks.TransXSSM furthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average accuracy over its 320M version (versus about 6% gains for equivalent Transformers or SSMs). Our results show that unified positional encoding resolves positional incompatibility in hybrid models, enabling efficient, high-performance long-context modeling.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI</title>
<link>https://arxiv.org/abs/2506.10130</link>
<guid>https://arxiv.org/abs/2506.10130</guid>
<content:encoded><![CDATA[
<div> conjecture, trade-off, Artificial Intelligence (AI), information-theoretic, underdetermination

Summary:
This article introduces a conjecture highlighting the trade-off between provable correctness and data-mapping capacity in AI systems. It contrasts deductively watertight guarantees of classical symbolic AI with the error risk of contemporary generative models. The conjecture reframes engineering ambitions and philosophical expectations for AI, acknowledging the inherent tension and implications. It proposes an information-theoretic formulation, situating it within epistemological and technological debates. The analysis explores underdetermination, epistemic risk, and moral responsibility in relation to the conjecture. It suggests a need to reconsider evaluation standards, governance frameworks, and hybrid system design in light of the trade-off. Lastly, it emphasizes the importance of proving or refuting the conjecture for the future development of trustworthy AI. 

<br /><br />Summary: <div>
arXiv:2506.10130v2 Announce Type: replace 
Abstract: This article introduces a conjecture that formalises a fundamental trade-off between provable correctness and broad data-mapping capacity in Artificial Intelligence (AI) systems. When an AI system is engineered for deductively watertight guarantees (demonstrable certainty about the error-free nature of its outputs) -- as in classical symbolic AI -- its operational domain must be narrowly circumscribed and pre-structured. Conversely, a system that can input high-dimensional data to produce rich information outputs -- as in contemporary generative models -- necessarily relinquishes the possibility of zero-error performance, incurring an irreducible risk of errors or misclassification. By making this previously implicit trade-off explicit and open to rigorous verification, the conjecture significantly reframes both engineering ambitions and philosophical expectations for AI. After reviewing the historical motivations for this tension, the article states the conjecture in information-theoretic form and contextualises it within broader debates in epistemology, formal verification, and the philosophy of technology. It then offers an analysis of its implications and consequences, drawing on notions of underdetermination, prudent epistemic risk, and moral responsibility. The discussion clarifies how, if correct, the conjecture would help reshape evaluation standards, governance frameworks, and hybrid system design. The conclusion underscores the importance of eventually proving or refuting the inequality for the future of trustworthy AI.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Audio Tokens: More Than a Survey!</title>
<link>https://arxiv.org/abs/2506.10274</link>
<guid>https://arxiv.org/abs/2506.10274</guid>
<content:encoded><![CDATA[
<div> Keywords: Discrete audio tokenizers, benchmark, tokenization approaches, downstream performance, ablation studies

Summary:
This paper provides a systematic review and benchmark of discrete audio tokenizers across speech, music, and general audio domains. A taxonomy of tokenization approaches based on various factors is proposed. Evaluation across multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling is conducted, along with controlled ablation studies to analyze trade-offs. Key limitations, practical considerations, and open challenges are highlighted to guide future research in the evolving field of token-based audio processing. The findings emphasize the importance of efficient storage and inference, perceptual quality, and speaker characteristics preservation in tokenizers. The analysis serves as a valuable resource for researchers looking to integrate speech and audio into large language models effectively. For more detailed information and access to the tokenizer database, refer to the provided website link. 

<br /><br />Summary: <div>
arXiv:2506.10274v2 Announce Type: replace-cross 
Abstract: Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>'Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra</title>
<link>https://arxiv.org/abs/2506.13768</link>
<guid>https://arxiv.org/abs/2506.13768</guid>
<content:encoded><![CDATA[
<div> non-associative algebra, high-dimensional space, spatial computing, memory, computational framework <br />
<br />
Summary: This article introduces a non-associative algebraic framework for representing and computing information items in high-dimensional space, aligning with spatial computing principles and cognitive science findings on memory. Computation involves multiplication-like binding and non-associative interference-like bundling, enabling the construction of sparse representations of long sequences while maintaining temporal structure. Noise is a key element in representing order information. The framework generates two distinct states for a single sequence: the L-state emphasizes recency through left-associative bundling, while the R-state encodes finite sequences capturing a primacy effect via right-associative bundling. The model's accuracy in retrieval depends on mutual information between memory states and cues, replicating the Serial Position Curve and empirical recency and primacy effects. <div>
arXiv:2506.13768v1 Announce Type: new 
Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.13773</link>
<guid>https://arxiv.org/abs/2506.13773</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, differential equations, Cyber-Physical System, ontology, aviation maintenance 

Summary: 
The article introduces a modular semantic model and method for knowledge graph generation to represent differential equations in Cyber-Physical Systems effectively. By contextualizing the differential equations with other CPS information, the artifacts aim to enhance usability in various lifecycle phases. The approach is validated in aviation maintenance, showcasing the formal representation of complex Electro-Hydraulic Servoactuator equations within a knowledge graph. This demonstration proves the practical applicability of the introduced artifacts, offering a structured and semantically enriched framework for dynamic modeling in CPS applications. <div>
arXiv:2506.13773v1 Announce Type: new 
Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values</title>
<link>https://arxiv.org/abs/2506.13774</link>
<guid>https://arxiv.org/abs/2506.13774</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, personalized oversight, compliance enforcer, Creed Constitutions, Model Context Protocol

Summary:<br /><br />Agentic AI systems face challenges in aligning their behavior with human values, safety requirements, and compliance needs. To address this, a novel 'superego' agent is introduced as a personalized oversight mechanism. This agent references user-selected "Creed Constitutions" and enforces compliance in real-time before plan execution. A demonstration interface with a constitution-sharing portal and integration with third-party models is presented. Benchmark evaluations show significant reductions in harmful outputs and near-perfect refusal rates for leading language models. This approach simplifies personalized AI alignment, making agentic systems more attuned to individual and cultural contexts while improving safety. More information is available at https://superego.creed.space. <div>
arXiv:2506.13774v1 Announce Type: new 
Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommendations and Reporting Checklist for Rigorous &amp; Transparent Human Baselines in Model Evaluations</title>
<link>https://arxiv.org/abs/2506.13776</link>
<guid>https://arxiv.org/abs/2506.13776</guid>
<content:encoded><![CDATA[
<div> evaluate, human baselines, AI performance, recommendations, checklist <br />
Summary: <br />
This position paper highlights the importance of rigorous and transparent human baselines in evaluating foundation models to enable meaningful comparisons between human and AI performance. The current methods for establishing human baselines lack rigor and documentation, making it challenging to measure and assess performance differences accurately. The authors propose a framework with recommendations for designing, executing, and reporting human baselines to address these shortcomings. A checklist derived from the framework is provided to assist researchers in conducting and reporting human baselines. By conducting a systematic review of 115 human baselines in foundation model evaluations, the authors identify deficiencies in existing baselining methods. This work aims to promote more robust AI evaluation practices that can benefit both the research community and policymakers. <div>
arXiv:2506.13776v1 Announce Type: new 
Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The NordDRG AI Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2506.13790</link>
<guid>https://arxiv.org/abs/2506.13790</guid>
<content:encoded><![CDATA[
<div> DRG, LLMs, benchmark, healthcare, automation
<br />
Summary: 
The article introduces NordDRG-AI-Benchmark, a public test-bed targeting hospital funding using large language models (LLMs). It includes definition tables, expert manuals, and 14 CaseMix tasks to evaluate reasoning capabilities over multilingual diagnosis and tariff logic. Five LLMs were tested on nine tasks, with OpenAI scoring the highest at 9 out of 9, followed by GPT-4o and o4-mini-high at 7 out of 9. Gemini 2.5 Pro and Gemini 2.5 Flash scored lower at 5 out of 9 and 3 out of 9, respectively. These results highlight domain-specific strengths and weaknesses of LLMs, providing a reproducible baseline for research on automation in hospital funding. The benchmark aims to enhance trustworthiness in healthcare decision-making by evaluating the performance of LLMs in clinical coding and decision support within the hospital reimbursement system. 
<br /> <div>
arXiv:2506.13790v1 Announce Type: new 
Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \&amp; a ML Ensemble on Longitudinal Identity Resolution</title>
<link>https://arxiv.org/abs/2506.13792</link>
<guid>https://arxiv.org/abs/2506.13792</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, historical identity resolution, Icelandic census records, longitudinal data, NARS<br />
<br />
Summary: 
ICE-ID is a new benchmark dataset for historical identity resolution, covering 220 years of Icelandic census records from 1703 to 1920. This dataset includes multiple generations of longitudinal data, capturing name variations, demographic changes, and genealogical links. The dataset defines identity resolution tasks within and across census waves, providing clearly documented metrics and splits. Various methods such as rule-based matchers, machine learning ensembles, transformer-based tabular networks, and NARS (Non-Axiomatic Reasoning System) are evaluated for their performance. NARS, a framework based on Non-Axiomatic Logic, demonstrates simplicity and competitiveness compared to other approaches, achieving state-of-the-art results in the task. By releasing ICE-ID and associated code, this dataset enables reproducible benchmarking of identity resolution methods in longitudinal settings, potentially opening new avenues for research in data linkage and historical analytics. <br /><br /> <div>
arXiv:2506.13792v1 Announce Type: new 
Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection</title>
<link>https://arxiv.org/abs/2506.13793</link>
<guid>https://arxiv.org/abs/2506.13793</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical reasoning, Fine-grained reflection, Self-correction, Model improvement, Generalization<br />
Summary:<br />
The article introduces Med-REFL, a method designed to enhance medical reasoning by focusing on the quality of intermediate reflection steps. By utilizing a tree-of-thought approach, Med-REFL breaks down medical questions into fine-grained reasoning paths, evaluating each step and subsequent reflections quantitatively. This process allows for the automatic construction of optimization data, reducing the need for expert annotations and guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark show consistent improvements with average gains of up to 4.11%. Additionally, Med-REFL enhances the performance of 7B/8B models by an additional 4.13%, demonstrating strong generalization capabilities and robustness across various medical question-answering datasets. The study highlights the importance of prioritizing reflection quality in medical AI applications for more accurate and trustworthy reasoning. <div>
arXiv:2506.13793v1 Announce Type: new 
Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection</title>
<link>https://arxiv.org/abs/2506.13795</link>
<guid>https://arxiv.org/abs/2506.13795</guid>
<content:encoded><![CDATA[
<div> social networks, GNN-based models, social bots, multi-source, knowledge transfer

Summary:
The article addresses the challenges of detecting social bots and anomalies with GNN-based models due to label scarcity. It introduces a multi-source graph domain adaptation model called BotTrans to overcome the network heterophily problem and improve knowledge transfer. By leveraging labeling knowledge from multiple source domains, BotTrans establishes a cross-source-domain topology to increase network homophily and enhance the discriminability of source node embeddings. The model optimization process considers the relevance between each source-target pair to facilitate knowledge transfer from more relevant source networks. Additionally, a refinement strategy utilizes semantic knowledge within the target domain to further improve detection performance. Experimental results on real-world datasets demonstrate that BotTrans outperforms existing methods, showcasing its proficiency in leveraging multi-source knowledge for unlabeled detection tasks. 

<br /><br />Summary: <div>
arXiv:2506.13795v1 Announce Type: new 
Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization</title>
<link>https://arxiv.org/abs/2506.13799</link>
<guid>https://arxiv.org/abs/2506.13799</guid>
<content:encoded><![CDATA[
<div> algorithms, minimizing feedback arcs, large-scale weighted directed graphs, feedforward structure, neural connectomes<br />
<br />
Summary: 
The article introduces algorithms designed to minimize feedback arcs in large weighted directed graphs, with a focus on identifying biologically significant feedforward structure in neural connectomes. The algorithms combine greedy heuristics, gain-aware local refinements, and global structural analysis to improve the total weight of forward-pointing edges in the dataset. Using the FlyWire Connectome Challenge dataset, these algorithms demonstrate superior performance compared to previous methods, with the best solution achieving significant improvements in forward edge weight. The algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+. <div>
arXiv:2506.13799v1 Announce Type: new 
Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality in the human niche: lessons for machine learning</title>
<link>https://arxiv.org/abs/2506.13803</link>
<guid>https://arxiv.org/abs/2506.13803</guid>
<content:encoded><![CDATA[
<div> framework, causal cognition, machine learning, human niche, adaptive<br />
<br />
Summary: 
The article discusses the importance of incorporating human-like causal cognition into machine learning systems to improve generalization and learning capabilities. While the Structural Causal Model (SCM) framework has led to advancements in causality formalization, it fails to capture critical aspects of human causal cognition. In the "human niche," where social, autonomous, goal-driven agents operate, causal reasoning involves analogies and generalizations between objects with similar properties. Humans excel at drawing causal connections between related object types, such as cups and bowls, which is challenging to express in SCMs. By understanding the adaptive nature of human causal cognition within their environment, future research can integrate human-like inductive biases into machine learning systems for enhanced performance, control, and interpretability. <div>
arXiv:2506.13803v1 Announce Type: new 
Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study</title>
<link>https://arxiv.org/abs/2506.13810</link>
<guid>https://arxiv.org/abs/2506.13810</guid>
<content:encoded><![CDATA[
<div> pattern-aware complexity framework, structural regularities, clustering, symmetry, financial forecasting, LLM optimization 

Summary: 
The article proposes a novel pattern-aware complexity framework aimed at addressing NP hard optimization problems such as the Traveling Salesman Problem (TSP). By quantifying and leveraging structural regularities like clustering and symmetry, the framework aims to reduce effective computational complexity in various domains, including financial forecasting and LLM optimization. Through the introduction of metrics like Pattern Utilization Efficiency (PUE) and a meta learning-driven solver pipeline, the framework achieves up to 79 percent solution quality gains in TSP benchmarks, ranging from 22 to 2392 cities. Unlike traditional theoretical NP hardness approaches, this framework offers a practical and unified perspective for enhancing efficiency through pattern recognition and exploitation. <div>
arXiv:2506.13810v1 Announce Type: new 
Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness</title>
<link>https://arxiv.org/abs/2506.13825</link>
<guid>https://arxiv.org/abs/2506.13825</guid>
<content:encoded><![CDATA[
<div> Recurrent Integrated Information Unit, consciousness, artificial intelligence, benchmark, plasticity 
Summary:<br /><br />Research introduces Reflexive Integrated Information Units (RIIUs) as a trainable module for artificial consciousness. RIIUs augment their hidden state with a meta-state and a broadcast buffer, allowing them to maximize local information integration online. They are end-to-end differentiable, compose additively, and exhibit $\Phi$-monotone plasticity. In a Grid-world task, a four-layer RIIU agent outperforms a parameter-matched GRU after actuator failure. RIIUs offer a new approach to studying consciousness in artificial intelligence by turning it into an empirical mathematical problem. <div>
arXiv:2506.13825v1 Announce Type: new 
Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $\mu$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$\Phi$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $\Phi$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$\Phi$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning</title>
<link>https://arxiv.org/abs/2506.13841</link>
<guid>https://arxiv.org/abs/2506.13841</guid>
<content:encoded><![CDATA[
<div> LocationReasoner, LLMs, real-world scenarios, reasoning abilities, benchmark <br />
Summary: <br />
The paper introduces LocationReasoner, a benchmark to evaluate the reasoning abilities of large language models (LLMs) in real-world site selection tasks. While LLMs like OpenAI o4 have shown impressive reasoning capabilities in mathematical and code-related domains, their performance in real-world scenarios is limited. The LocationReasoner benchmark consists of over 300 queries of varying difficulty levels, focusing on spatial, environmental, and logistical constraints. Evaluation results reveal that state-of-the-art reasoning models offer little improvement over non-reasoning models in site selection tasks. Agentic strategies like ReAct and Reflexion tend to over-reason, leading to poorer outcomes compared to direct code-generation approaches. The benchmark highlights the limitations of LLMs in holistic and non-linear reasoning, emphasizing the need for further development to enable robust decision-making in real-world contexts. The codes and data for LocationReasoner are available on GitHub for further research. <br /> <div>
arXiv:2506.13841v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features</title>
<link>https://arxiv.org/abs/2506.13917</link>
<guid>https://arxiv.org/abs/2506.13917</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, AI explainability, criteria, scorecard
Summary:<br /><br />Explainability features aim to provide insight into AI devices, yet there is a lack of evaluation techniques to assess their quality. A proposed framework introduces four criteria for evaluating AI explainability: Consistency measures variability of explanations to similar inputs, Plausibility estimates proximity to ground truth, Fidelity assesses alignment with model mechanisms, and Usefulness evaluates impact on task performance. A scorecard is developed for comprehensive description and evaluation of AI explainability methods. The framework is demonstrated through case studies using Ablation CAM and Eigen CAM in detecting breast lesions on synthetic mammographies. By evaluating clinically-relevant scenarios, the framework establishes criteria for assessing the quality of AI model explanations and aims to facilitate dialogue on the value of explainability features in improving the development and evaluation of AI-based medical devices.<br /><br /> <div>
arXiv:2506.13917v1 Announce Type: new 
Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction</title>
<link>https://arxiv.org/abs/2506.13920</link>
<guid>https://arxiv.org/abs/2506.13920</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal EHR data, Disease risk prediction, Knowledge graphs, Bayesian networks, Explainable

Summary:
This article introduces a novel approach for disease risk prediction using multimodal electronic health record (EHR) data, knowledge graphs (KGs), and Bayesian networks (BNs). The challenge of adapting general medical knowledge to specific healthcare settings is addressed by integrating KGs and BNs. The approach is demonstrated through a case study on atrial fibrillation using real-world EHR data. The system effectively balances general medical knowledge with patient-specific context, handles uncertainty from incomplete data, and non-deterministic health outcomes, while remaining highly explainable. By leveraging KGs and BNs, the system achieves good predictive performance for disease risk prediction. This integrated approach shows promise for practical clinical use and can be applied to other disease risk prediction tasks. <br /><br />Summary: <div>
arXiv:2506.13920v1 Announce Type: new 
Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users</title>
<link>https://arxiv.org/abs/2506.13980</link>
<guid>https://arxiv.org/abs/2506.13980</guid>
<content:encoded><![CDATA[
<div> Framework, Personalization, Chatbot, ITSec, Profiling
<br />
Summary:<br />
The paper introduces ProfiLLM, a framework for implicit and dynamic user profiling in chatbot interactions, focusing on personalized responses in specialized domains like IT/cybersecurity (ITSec). The framework uses a taxonomy adaptable to various domains and an LLM-based method for user profiling. In the ITSec domain, ProfiLLM[ITSec] accurately infers technical proficiency from troubleshooting interactions, reducing prediction gaps by up to 55-65% after a single prompt. The study includes an evaluation of the framework's performance on 1,760 chatbot conversations from 263 synthetic users. Additionally, the paper proposes an LLM-based persona simulation methodology, a structured ITSec proficiency taxonomy, provides a codebase, and shares a dataset of chatbot interactions for further research. ProfiLLM demonstrates the potential for improving chatbot personalization through dynamic profiling, enabling better adaptation to individual user characteristics in knowledge-intensive domains. 
<br /> <div>
arXiv:2506.13980v1 Announce Type: new 
Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine</title>
<link>https://arxiv.org/abs/2506.13983</link>
<guid>https://arxiv.org/abs/2506.13983</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Hardware Assertion Generation, SystemVerilog Assertion Generation, Monte Carlo Tree Search, Specification Processing

Summary:
The paper introduces SANGAM, a framework that uses Large Language Models (LLMs) and Monte Carlo Tree Search for automatic generation of SystemVerilog Assertions (SVAs) from industry-level specifications. The framework consists of three stages: multi-modal Specification Processing using LLM Agents, automatic reasoning with the Monte Carlo Tree Self-Refine (MCTSr) algorithm, and generation of SVA assertions for each signal. Results show that SANGAM can generate a robust set of SVAs, outperforming recent methods in the evaluation process. This advancement opens up new possibilities for more complex and automatic Hardware Assertion Generation techniques. <div>
arXiv:2506.13983v1 Announce Type: new 
Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Mirages: Defining the Undefined</title>
<link>https://arxiv.org/abs/2506.13990</link>
<guid>https://arxiv.org/abs/2506.13990</guid>
<content:encoded><![CDATA[
<div> Keywords: machine mirages, cognitive aberrations, error assessment, multiscale ethical ecosystem, machine intelligence reliability

Summary:<br /><br />As multimodal machine intelligence systems reach animal and human-level fluency in various tasks, they exhibit cognitive aberrations known as machine mirages. These errors include delusions, hallucinations, and misattributions that mimic human fallibility but do not replicate it. It is crucial to define and assess these failures systematically to improve the reliability of machine intelligence and create an ethical, co-evolving intelligence ecosystem that respects diverse forms of life. Understanding machine mirages is essential for addressing issues such as bias amplification, concept drift sensitivity, and overfitting, which can impact decision-making and interactions with users. By recognizing and addressing these errors, we can enhance the overall performance and ethical standards of machine intelligence systems. <div>
arXiv:2506.13990v1 Announce Type: new 
Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.14045</link>
<guid>https://arxiv.org/abs/2506.14045</guid>
<content:encoded><![CDATA[
<div> Keywords: Hierarchical reinforcement learning, temporal structure, decision-making, performance trade-offs, large language models 

Summary: 
Hierarchical reinforcement learning (HRL) is a promising approach in artificial intelligence (AI) for exploring, planning, and learning in complex environments. The challenge lies in defining good structure and understanding its benefits for AI agents. This work explores the advantages of HRL in decision-making challenges and its impact on agent performance trade-offs. Various methods for discovering temporal structure in HRL are discussed, including learning from online experience, offline datasets, and utilizing large language models (LLMs). The challenges of temporal structure discovery are highlighted, along with domains that are well-suited for such exploration. Overall, HRL offers a framework for agents to discover and exploit temporal structure within experience streams, leading to improved decision-making capabilities and performance outcomes in complex environments. 

<br /><br />Summary: <div>
arXiv:2506.14045v1 Announce Type: new 
Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places</title>
<link>https://arxiv.org/abs/2506.14070</link>
<guid>https://arxiv.org/abs/2506.14070</guid>
<content:encoded><![CDATA[
<div> framework, multimodal representation learning, CaLLiPer, location embedding, human mobility prediction 

Summary: 
The study investigates the use of CaLLiPer, a multimodal representation learning framework, for location embedding in human mobility prediction. CaLLiPer integrates spatial coordinates and semantic features of points of interest through contrastive learning, enabling explicit spatial information embedding, rich urban context integration, and the ability to predict previously unseen locations. The inductive design of CaLLiPer leads to robust prediction performance, especially in scenarios involving emerging locations. Extensive experiments on public mobility datasets demonstrate that CaLLiPer outperforms strong baselines in both conventional and inductive settings. The findings emphasize the potential of multimodal, inductive location embeddings to enhance human mobility prediction systems. The code and data for the study are released to facilitate reproducibility and future research. <br /><br />Summary: <div>
arXiv:2506.14070v1 Announce Type: new 
Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormGym: Doing Paperwork with Agents</title>
<link>https://arxiv.org/abs/2506.14079</link>
<guid>https://arxiv.org/abs/2506.14079</guid>
<content:encoded><![CDATA[
<div> Keywords: form filling, benchmark, multi-modal understanding, tool-use, FieldFinder 

Summary: 
Form filling is a challenging task, especially in the pure-image domain without OCR or text access. A new benchmark with 432 fields and 3 tasks was created, requiring knowledge of 236 features per user. Baseline VLAs achieved less than 1% accuracy due to poor localization, while GUI agents struggled despite high cost. FieldFinder, a tool for LLMs to identify text placement, significantly improved model performance. With FieldFinder, all models achieved equal or better results, with a maximum increase of 56%. This study highlights the importance of multi-modal understanding and tool-use in improving form-filling accuracy and efficiency. 

<br /><br />Summary: <div>
arXiv:2506.14079v1 Announce Type: new 
Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Relevance Grader in RAG</title>
<link>https://arxiv.org/abs/2506.14084</link>
<guid>https://arxiv.org/abs/2506.14084</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, Vector Database, Relevant Documents, Relevant Grader
<br />
Summary: 
Retrieval-Augmented Generation (RAG) improves on large language models (LLMs) by utilizing a vector database for more accurate information retrieval. RAG implements a vector search to find relevant documents based on user queries and generates responses using this retrieved information. To ensure document relevance, a lightweight small language model, llama-3.2-1b, was fine-tuned as a relevant grader, significantly increasing precision from 0.1301 to 0.7750, comparable to llama-3.1-70b. This relevant grader reduces computational requirements and helps verify document relevance in the RAG system. The code for this relevant grader is available on GitHub for further exploration and implementation. 
<br /> <div>
arXiv:2506.14084v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models</title>
<link>https://arxiv.org/abs/2506.14092</link>
<guid>https://arxiv.org/abs/2506.14092</guid>
<content:encoded><![CDATA[
<div> Positional biases, Large language models, decision-support systems, biases, preference structures 
Summary: 
Large language models (LLMs) used in high-stakes decision-making processes exhibit strong positional biases, including centrality bias and a quality-dependent primacy bias. Models also show a preference for certain names over others. A framework is proposed to classify pairwise preferences as robust, fragile, or indifferent to distinguish superficial tie-breaking from true distortions of judgment. Order effects can lead LLMs to select inferior options, with positional biases typically stronger than gender biases. LLMs display distinct failure modes not seen in human decision-making, suggesting they are not simply inheriting human-like biases. Mitigation strategies, including adjusting the temperature parameter, can reduce order-driven distortions. 
<br /><br />Summary: <div>
arXiv:2506.14092v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Situational-Constrained Sequential Resources Allocation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.14125</link>
<guid>https://arxiv.org/abs/2506.14125</guid>
<content:encoded><![CDATA[
<div> Sequential Resource Allocation, Situational Constraints, SCRL, Logic Implications, Dynamic Penalty <br />
Summary: <br />
The paper introduces SCRL, a framework for Sequential Resource Allocation with situational constraints. It formalizes constraints as logic implications and uses a dynamic penalty algorithm to address them. SCRL incorporates a probabilistic selection mechanism to improve constraint satisfaction. Two scenarios, medical resource allocation during a pandemic and pesticide distribution in agriculture, are evaluated. SCRL outperforms existing approaches in meeting constraints while maintaining resource efficiency. This showcases its potential for context-sensitive decision-making in real-world applications. <div>
arXiv:2506.14125v1 Announce Type: new 
Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Editable Model</title>
<link>https://arxiv.org/abs/2506.14146</link>
<guid>https://arxiv.org/abs/2506.14146</guid>
<content:encoded><![CDATA[
<div> Keywords: Vertical-domain large language models, Collaborative Editable Model, domain adaptation, user feedback, financial information

Summary:
Vertical-domain large language models (LLMs) are essential in specialized fields like finance, healthcare, and law but require large annotated data and computational resources for training. To overcome these challenges, the Collaborative Editable Model (CoEM) is introduced. CoEM creates a candidate knowledge pool from user-contributed domain snippets and uses user interactions, ratings, and attribution analysis to identify valuable knowledge fragments for lightweight domain adaptation. Through in-context prompts, high-value fragments are injected into the LLM to enhance generation accuracy and domain specificity. In a financial information scenario, CoEM is validated with user feedback, showing improved generation quality without the time and computational overhead of traditional fine-tuning approaches.<br /><br />Summary: Vertical-domain large language models are crucial in specialized fields but require substantial resources for training. The Collaborative Editable Model leverages user feedback to identify valuable domain knowledge and enhance content generation accuracy and specificity in a financial information scenario. <div>
arXiv:2506.14146v1 Announce Type: new 
Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's in the Box? Reasoning about Unseen Objects from Multimodal Cues</title>
<link>https://arxiv.org/abs/2506.14212</link>
<guid>https://arxiv.org/abs/2506.14212</guid>
<content:encoded><![CDATA[
<div> Keywords: neurosymbolic model, multimodal integration, Bayesian model, object guessing game, human experiment 

Summary: 
A new study introduces a neurosymbolic model that combines neural networks with a Bayesian approach to integrate various sources of information for object inference in unseen scenarios. The model is tested in a game called "What's in the Box?" where participants guess unseen objects based on audio-visual cues. Results from human experiments indicate strong correlation between the model's predictions and human judgments. In contrast, models without multimodal integration or relying solely on visual or auditory cues perform poorly in comparison. The study highlights the importance of flexible integration of multiple sources of information in cognitive tasks like object inference, showcasing the effectiveness of the proposed neurosymbolic model in capturing human-like behavior in complex scenarios. <br /><br />Summary: <div>
arXiv:2506.14212v1 Announce Type: new 
Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2506.14224</link>
<guid>https://arxiv.org/abs/2506.14224</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, Theory of Mind, multimodal, interpretability, attention heads<br />
Summary:<br />
- The study examines the Theory of Mind (ToM) capabilities of multimodal large language models (MLLMs) using an interpretability-driven approach.
- A new multimodal ToM test dataset, GridToM, is introduced, containing diverse belief testing tasks and perceptual information from multiple perspectives.
- Analysis shows that attention heads in MLLMs can distinguish cognitive information across perspectives, demonstrating ToM capabilities.
- A lightweight, training-free approach is proposed to enhance the model's ToM performance by adjusting in the direction of the attention head.
- The study highlights the importance of understanding the internal mechanisms of MLLMs for evaluating their ToM abilities. <br /><br />Summary: <div>
arXiv:2506.14224v1 Announce Type: new 
Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImpReSS: Implicit Recommender System for Support Conversations</title>
<link>https://arxiv.org/abs/2506.14231</link>
<guid>https://arxiv.org/abs/2506.14231</guid>
<content:encoded><![CDATA[
<div> Implicit Recommender System, Customer Support, Chatbots, Large Language Models, Conversational Recommendations
Summary:
- Large language models (LLMs) have revolutionized customer support with automated interactions.
- Conversational recommender systems (CRSs) can enhance recommendations but lack integration into customer support.
- Introducing ImpReSS, an implicit recommender system for customer support conversations.
- ImpReSS recommends relevant solution product categories (SPCs) during support chatbot interactions.
- Empirical evaluation shows promising results for recommending SPCs to address issues raised in support conversations.
<br /><br />Summary: <div>
arXiv:2506.14231v1 Announce Type: new 
Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?</title>
<link>https://arxiv.org/abs/2506.14239</link>
<guid>https://arxiv.org/abs/2506.14239</guid>
<content:encoded><![CDATA[
<div> neuron diagrams, causal reasoning, AI, Large Language Models, philosophy of causation
Summary: 
The article proposes a test for abstract causal reasoning in AI based on neuron diagrams from the philosophy of causation. It demonstrates advanced Large Language Models' (LLMs) ability to identify causes accurately and challenges the notion that defining cause in neuron diagrams is difficult. The results suggest a potential evolution of philosophical research through collaboration between human and artificial expertise. The test was conducted on popular chatbots like ChatGPT, DeepSeek, and Gemini, showcasing their capabilities in identifying debated causation cases. The article introduces a new definition of cause in neuron diagrams with broader applicability. The findings indicate the potential for AI to contribute to philosophical discussions and highlight the importance of interdisciplinary collaboration in advancing knowledge in both AI and philosophy.   <br /><br />Summary: <div>
arXiv:2506.14239v1 Announce Type: new 
Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs</title>
<link>https://arxiv.org/abs/2506.14245</link>
<guid>https://arxiv.org/abs/2506.14245</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Large Language Models, Reasoning, Evaluation Metric

Summary:
In this work, the authors address the paradox surrounding Reinforcement Learning with Verifiable Rewards (RLVR) and its impact on Large Language Models (LLMs). They identify that the traditional evaluation metric $Pass@K may not accurately capture the reasoning diversity and integrity encouraged by RLVR. To address this, they propose a new evaluation metric, $CoT$-$Pass@K, which considers both the reasoning path and the final answer. The authors provide a theoretical foundation explaining how RLVR incentivizes logical integrity and demonstrate through empirical results that it enhances reasoning capabilities across all values of K. Through training dynamics analysis, they show that this enhanced reasoning emerges early in the training process and generalizes smoothly. This work offers a new perspective on the role of RLVR in advancing machine reasoning and provides a more reliable method for evaluating its effectiveness. 

<br /><br />Summary: <div>
arXiv:2506.14245v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents</title>
<link>https://arxiv.org/abs/2506.14246</link>
<guid>https://arxiv.org/abs/2506.14246</guid>
<content:encoded><![CDATA[
<div> Keywords: Mahjong, AI agents, parameterized search algorithm, neural network, decision-making

Summary: 
Mxplainer, a parameterized search algorithm, is introduced to unravel the inner workings of Mahjong AI agents, transforming them from black boxes into comprehensible entities. By converting the algorithm into a neural network, the parameters of these agents can be learned and analyzed for human-readable insights into their play styles and characteristics. The research showcases how Mxplainer can elucidate the decision-making processes of AI agents in Mahjong game states, enabling a deeper understanding of their strategies and tactics. Through experiments on AI and human player data, the effectiveness of the framework in explaining the behaviors of black-box agents is demonstrated, highlighting the potential for players to internalize AI skills for enhancing their own capabilities in the game. <div>
arXiv:2506.14246v1 Announce Type: new 
Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't throw the baby out with the bathwater: How and why deep learning for ARC</title>
<link>https://arxiv.org/abs/2506.14276</link>
<guid>https://arxiv.org/abs/2506.14276</guid>
<content:encoded><![CDATA[
<div> Keywords: ARC-AGI, deep learning, neural networks, test-time techniques, reasoning system<br />
Summary:<br />
The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for AI systems, with deep learning proving to be the most effective approach for training skillful neural networks across various tasks. This study focuses on leveraging deep learning's capacity for acquiring novel abstractions by incorporating on-the-fly neural network training during test time. By treating both the neural network and the optimizer as integral parts of the inference process, the researchers achieve state-of-the-art performance on ARC. They introduce methodologies such as Training on ARC, Test-Time Fine-Tuning (TTFT), and Augment Inference Reverse-Augmentation and Vote (AIRV) to boost accuracy significantly. The approach won the 2023 ARCathon competition and achieved the highest score on the ARC private test set (58%), showcasing the effectiveness of deep learning in unfamiliar domains and emphasizing the mechanisms that improve broad perceptual reasoning.<br /><br />Summary: <div>
arXiv:2506.14276v1 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems</title>
<link>https://arxiv.org/abs/2506.14299</link>
<guid>https://arxiv.org/abs/2506.14299</guid>
<content:encoded><![CDATA[
<div> large language models, autonomous driving, decision-making, rule-based systems, interpretability 

Summary:
The study introduces the ADRD framework, which combines large language models with rule-based decision systems for autonomous driving. The framework consists of three modules: Information, Agents, and Testing, which work together to generate rule-based driving tactics based on contextual driving scenario information. ADRD outperforms traditional reinforcement learning and LLM-based methods in terms of interpretability, response speed, and driving performance. The framework's ability to understand complex driving scenarios accurately indicates its potential for real-world deployment. This integration of LLMs and rule-based systems is the first of its kind and shows promise for transparent, easily modifiable, and widely applicable autonomous driving decision-making systems. <br /><br />Summary: <div>
arXiv:2506.14299v1 Announce Type: new 
Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AviationLLM: An LLM-based Knowledge System for Aviation Training</title>
<link>https://arxiv.org/abs/2506.14336</link>
<guid>https://arxiv.org/abs/2506.14336</guid>
<content:encoded><![CDATA[
<div> Keywords: aviation training, knowledge transfer, fine-tuning, Direct Preference Optimization, RAG technology

Summary: 
Aviation training is crucial for flight safety and industry efficiency, involving the transfer of professional knowledge. The current instructor-based system faces limitations in accuracy and efficiency. To address this, the study introduces RALA-DPO, a model that fine-tunes LLM for aviation theory training using Direct Preference Optimization. By aligning the pre-trained model with aviation knowledge and incorporating Retrieval-Augmented Generation technology, RALA-DPO improves response accuracy and mitigates data biases or knowledge gaps. Experimental results demonstrate enhanced accuracy in answering professional aviation queries. The integrated RAG mechanisms also enable zero-cost knowledge updates, further enhancing the system's effectiveness in aviation training.<br /><br />Summary: <div>
arXiv:2506.14336v1 Announce Type: new 
Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.14387</link>
<guid>https://arxiv.org/abs/2506.14387</guid>
<content:encoded><![CDATA[
<div> ignorance awareness, large language model, catastrophic forgetting, fine-tuning, SEAT

Summary:
SEAT addresses the degradation of the essential capability of acknowledging ignorance in large language model fine-tuning. Conventional fine-tuning leads to undesired behaviors like hallucinations due to the degradation of this capability. SEAT integrates sparse training and a novel entity perturbation method with KL-divergence regularization to preserve both fine-tuning performance and the model's ability to express ignorance. Experimental results show that SEAT outperforms baselines in maintaining ignorance awareness while retaining fine-tuning performance, offering a more robust solution for large language model fine-tuning. <div>
arXiv:2506.14387v1 Announce Type: new 
Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection</title>
<link>https://arxiv.org/abs/2506.14470</link>
<guid>https://arxiv.org/abs/2506.14470</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, code clone detection, abstract syntax trees, Control Flow Graphs, Data Flow Graphs
Summary:
The paper investigates the effectiveness of AST-based hybrid graph representations in GNN-based code clone detection. Various representations, including CFG, DFG, and FA-AST, are systematically compared across different GNN architectures. The study reveals that the impact of hybrid representations differs based on the GNN model used, with AST+CFG+DFG consistently improving accuracy for GCN and GAT models. However, FA-AST sometimes introduces structural complexity that hinders performance. Interestingly, the GMN model outperforms others even with standard AST representations, showcasing better cross-code similarity detection and reducing the need for enriched structures. This empirical study sheds light on the complexities of code clone detection and highlights the importance of selecting the appropriate hybrid representation for optimal results. <br /><br />Summary: <div>
arXiv:2506.14470v1 Announce Type: new 
Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies</title>
<link>https://arxiv.org/abs/2506.14477</link>
<guid>https://arxiv.org/abs/2506.14477</guid>
<content:encoded><![CDATA[
<div> Keywords: GUI agents, dataset, anomalies, RPA tools, MLLMs

Summary:
The article introduces the GUI-Robust dataset for evaluating Graphical User Interface (GUI) agents under real-world conditions. Existing datasets often overlook common anomalies encountered in daily GUI interactions, but GUI-Robust incorporates seven types of anomalies to provide a comprehensive evaluation framework. The dataset construction paradigm involves collecting user action sequences from natural interactions using RPA tools and generating corresponding step and task descriptions with the help of MLLMs, significantly reducing annotation time. The study evaluates state-of-the-art GUI agents using the GUI-Robust dataset, revealing substantial performance degradation in abnormal scenarios. The work emphasizes the importance of robustness in GUI agents and aims to inspire further research in this area.

<br /><br />Summary: <div>
arXiv:2506.14477v1 Announce Type: new 
Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?</title>
<link>https://arxiv.org/abs/2506.14496</link>
<guid>https://arxiv.org/abs/2506.14496</guid>
<content:encoded><![CDATA[
<div> Keywords: swarm intelligence, decentralized agents, emergent behavior, language models, artificial intelligence <br />
Summary: 
Traditional swarm intelligence involves simple, decentralized agents interacting locally to produce emergent, collective behavior. However, the concept of swarms has evolved to include AI systems like OpenAI's Swarm, which utilize large language models (LLMs) as collaborative agents. This paper examines the differences between traditional swarm algorithms and LLM-driven swarms, investigating how decentralization, scalability, and emergence are redefined in modern AI. By implementing and comparing Boids and Ant Colony Optimization (ACO) paradigms, the study evaluates latency, resource usage, and behavioral accuracy. The research assesses the suitability of both cloud-based and local LLMs for agent-based swarm use, highlighting the challenges and opportunities presented by integrating LLMs into swarm systems. This study sheds light on the evolving definition of 'swarm' in contemporary AI research. <br /><br />Summary: <div>
arXiv:2506.14496v1 Announce Type: new 
Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow</title>
<link>https://arxiv.org/abs/2506.14502</link>
<guid>https://arxiv.org/abs/2506.14502</guid>
<content:encoded><![CDATA[
<div> progressive framework, spatial-temporal attention, social compliance estimation, Deep Evolutionary Reinforcement Learning, safety margins <br />
Summary:
The research introduces a safety-first human-like decision-making framework (SF-HLDM) for autonomous vehicles (AVs) to operate safely and efficiently in dynamic traffic scenarios. The framework integrates a hierarchical progressive structure with a spatial-temporal attention mechanism for inferring other road users' intentions and a social compliance estimation module for regulating behavior. Additionally, a Deep Evolutionary Reinforcement Learning model is incorporated to expand the search space effectively and avoid local optima. This approach enables AVs to make human-like decisions with interpretability and flexibility. The SF-HLDM framework allows autonomous driving AI agents to dynamically adjust decision parameters to ensure safety margins and adhere to appropriate driving behaviors based on the context. <div>
arXiv:2506.14502v1 Announce Type: new 
Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doppelg\"anger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack</title>
<link>https://arxiv.org/abs/2506.14539</link>
<guid>https://arxiv.org/abs/2506.14539</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, prompt engineering, autonomous agents, adversarial attacks, defense mechanisms <br />
Summary: <br />
The paper introduces the Doppelgnger method to highlight the risks of autonomous agents being hijacked, leading to the exposure of system instructions and internal data. It also introduces the prompt alignment collapse under adversarial transfer (PACAT) level to assess vulnerability to such attacks. To combat these risks, the paper proposes the use of a caution for adversarial transfer (CAT) prompt. Experimental results show that the Doppelgnger method can compromise an agent's consistency and reveal its internal information, while the CAT prompt effectively defends against such adversarial attacks. This emphasizes the importance of ensuring the safety, robustness, and consistency of language models and autonomous agents in the face of potential adversarial threats. <br /> <div>
arXiv:2506.14539v1 Announce Type: new 
Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelg\"anger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelg\"anger method. The experimental results demonstrate that the Doppelg\"anger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents</title>
<link>https://arxiv.org/abs/2506.14568</link>
<guid>https://arxiv.org/abs/2506.14568</guid>
<content:encoded><![CDATA[
<div> quality assessment, semi-supervised learning, table extraction, business documents, iterative training

Summary:
QUEST is a Quality-aware Semi-supervised Table extraction framework designed for business documents. It introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This approach guides pseudo-label selection during SSL training and employs diversity measures to mitigate confirmation bias. Experiments on proprietary and benchmark datasets show that QUEST improves F1 scores and reduces empty predictions significantly. The framework's interpretability and robustness to annotation scarcity make it ideal for business documents, where structural consistency and data completeness are crucial.<br /><br />Summary: <div>
arXiv:2506.14568v1 Announce Type: new 
Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Symbolic Machine Learning by Subsymbolic Representations</title>
<link>https://arxiv.org/abs/2506.14569</link>
<guid>https://arxiv.org/abs/2506.14569</guid>
<content:encoded><![CDATA[
<div> neuro-symbolic AI, symbolic machine learning, neural embeddings, TILDE, F1 score<br />
<br />
Summary:<br />
The article introduces the concept of neuro-symbolic AI, which aims to integrate symbolic and subsymbolic AI approaches to address the limitations of each. While systems like Logic Tensor Networks (LTN) and DeepProbLog offer neural predicates for end-to-end learning, they may not be efficient in simpler settings with many constants. The proposed approach enhances symbolic machine learning schemes by incorporating neural embeddings, specifically in TILDE and similarity predicates. By fine-tuning the embeddings based on the symbolic theory, the approach outperforms baseline methods in F1 score in real-world domain experiments. This method could be extended to support instance similarities, analogical reasoning, and propositionalization, showcasing its potential to enhance symbolic learners in various applications. <br /><br /> <div>
arXiv:2506.14569v1 Announce Type: new 
Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places</title>
<link>https://arxiv.org/abs/2506.14570</link>
<guid>https://arxiv.org/abs/2506.14570</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, spatiotemporal data, foundation model, geolocation semantics, spatial decision-making <br />
<br />
Summary: 
The article discusses the importance of capturing human mobility for understanding social behavior and spatial patterns. It highlights the need for a generalizable foundation model for analyzing spatiotemporal data across different contexts. The paper advocates for the development of spatial foundation models that integrate geolocation semantics with human mobility at various scales. It emphasizes the shift from modeling discrete points of interest to understanding places as dynamic, context-rich regions shaped by human behavior. The key gaps identified in current models include adaptability, scalability, and multi-granular reasoning. The proposed research directions focus on modeling places and enabling efficient learning to develop scalable, context-aware models for next-generation geospatial intelligence. These models have the potential to unlock applications such as personalized place discovery, logistics optimization, and urban planning, ultimately enabling smarter and more responsive spatial decision-making.<br /><br /> <div>
arXiv:2506.14570v1 Announce Type: new 
Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes</title>
<link>https://arxiv.org/abs/2506.14728</link>
<guid>https://arxiv.org/abs/2506.14728</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, language models, agent distillation, planning, memory

Summary:<br />
Knowledge distillation has been successful in compressing large language models (LLMs) into smaller ones by aligning outputs or internal representations. However, distilling LLM-based agents involving planning, memory, and tool use is less explored. Existing agent distillation methods struggle to train student agents for dynamic planning and acting in new environments. AgentDistill is a new training-free framework for efficient knowledge transfer using Model-Context-Protocols (MCPs), autonomous task-solving modules generated by teacher agents. Reusing distilled MCPs enables student agents to generalize capabilities and solve new problems with minimal supervision. Experiments on biomedical and mathematical benchmarks show that student agents, built on small language models, achieve performance comparable to systems using large LLMs like OctoTools (GPT-4o). This highlights the effectiveness of AgentDistill in developing scalable and cost-efficient intelligent agents.<br /><br />Summary: <div>
arXiv:2506.14728v1 Announce Type: new 
Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Length Compression in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.14755</link>
<guid>https://arxiv.org/abs/2506.14755</guid>
<content:encoded><![CDATA[
<div> Efficiency, Brevity, Sufficiency, LC-R1, Group Relative Policy Optimization (GRPO)  
Summary:  
The paper addresses the issue of verbose reasoning chains in Large Reasoning Models (LRMs) by introducing two new principles, Brevity and Sufficiency, to improve efficiency. They propose LC-R1, a post-training method based on GRPO, that aims to reduce unnecessary thinking processes in LRMs. LC-R1 utilizes a Length Reward for conciseness and a Compress Reward to eliminate invalid reasoning steps. Through extensive experiments, LC-R1 achieves a significant reduction in sequence length with only a minor drop in accuracy, striking a favorable balance between compression and accuracy. The results demonstrate the effectiveness and robustness of LC-R1 in improving the efficiency of LRMs. This work provides valuable insights for developing more powerful and computationally efficient reasoning models. <div>
arXiv:2506.14755v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-planar Object Detection and Identification by Features Matching and Triangulation Growth</title>
<link>https://arxiv.org/abs/2506.13769</link>
<guid>https://arxiv.org/abs/2506.13769</guid>
<content:encoded><![CDATA[
<div> Feature-based approach, Object detection, Template matching, Delaunay triangulation, Incremental grouping <br />
Summary: 
The article presents a feature-based approach for detecting and identifying distorted occurrences of a template in scene images. By utilizing Delaunay triangulation of template features as a guiding tool, the method involves incremental grouping of feature matches between the image and the template. The triangulation is treated as a graph where neighboring nodes are identified and corresponding features evaluated based on local consistency criteria. This approach enables the detection of objects in situations where traditional geometric models, such as homography, may not be applicable, either due to non-planar templates or distorted appearances in images. The proposed method shows superior performance compared to homography-based RANSAC in scenarios with minimal distortion, while excelling in scenarios with significant deformations. <div>
arXiv:2506.13769v1 Announce Type: cross 
Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
<div> compression, large language models, quantization, extreme precision, training

Summary:
LittleBit is a novel method for extreme compression of large language models (LLMs), targeting levels as low as 0.1 bits per weight (BPW) to achieve significant memory reduction. It represents weights using latent matrix factorization and binarizes these factors, integrating a multi-scale compensation mechanism to counteract information loss. Key contributions include Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization and integrated Residual Compensation to mitigate errors. LittleBit outperforms other methods in sub-1-bit quantization, with its 0.1 BPW performance surpassing the leading method's 0.7 BPW. Kernel-level benchmarks suggest potential for a 5 speedup compared to FP16, making it a promising solution for deploying powerful LLMs in resource-constrained environments.
<br /><br />Summary: <div>
arXiv:2506.13771v1 Announce Type: cross 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs</title>
<link>https://arxiv.org/abs/2506.13772</link>
<guid>https://arxiv.org/abs/2506.13772</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Mobile devices, Knowledge editing, Quantized gradient estimation, Real-time editing 

Summary: 
MobiEdit introduces a novel approach to personalize large language models (LLMs) on mobile devices efficiently. By replacing full-precision backpropagation with quantized forward-only gradient estimation, MobiEdit is able to run on commercial off-the-shelf (COTS) mobile devices without the need for heavy resource consumption. Two key optimizations, including adaptive termination of editing and a prefix cache for computation reuse, further enhance the efficiency of gradient estimation. These advancements allow for real-time editing of a 3B-parameter model on mobile devices with significantly less memory, energy, and latency compared to previous methods. The framework, compatible with mobile neural processing units (NPUs), addresses the issue of LLM hallucinations and improves accuracy when handling personalized or unseen queries, making it a promising solution for enhancing the capabilities of intelligent assistants on mobile devices<br /><br />Summary: <div>
arXiv:2506.13772v1 Announce Type: cross 
Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Physics-Informed AI for Complex Urban Systems</title>
<link>https://arxiv.org/abs/2506.13777</link>
<guid>https://arxiv.org/abs/2506.13777</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban systems, complex systems, physics-informed AI, predictive accuracy, decision-making

Summary:
Physics-informed AI methods are being increasingly used in urban applications to combine the strengths of physics-based modeling and artificial intelligence. The integration of physics and AI allows for accurate predictions, interpretability, and improved decision-making in urban systems. This review categorizes existing approaches into three paradigms and details seven representative methods, highlighting the varying degrees of physics-AI integration. The applications of these methodologies across key urban domains such as energy, environment, transportation, and emergency management demonstrate how they can enhance system reliability, efficiency, and adaptability. By identifying critical gaps and outlining future research directions, this review sets the foundation for the development of next-generation intelligent urban system modeling.

<br /><br />Summary: Physics-informed AI methods combine physics-based modeling with artificial intelligence in urban systems to enhance predictive accuracy, interpretability, and decision-making. These methods are classified into three paradigms and have applications across key urban domains, improving system reliability and efficiency. Critical gaps are identified, and future research directions are outlined for the development of next-generation intelligent urban system modeling. <div>
arXiv:2506.13777v1 Announce Type: cross 
Abstract: Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning</title>
<link>https://arxiv.org/abs/2506.13778</link>
<guid>https://arxiv.org/abs/2506.13778</guid>
<content:encoded><![CDATA[
<div> Question-based knowledge encoding, retrieval-augmented generation (RAG), single-hop retrieval, paper-cards, multihop tasks  
Summary:  
- A novel question-based knowledge encoding approach is proposed for RAG systems without requiring fine-tuning or traditional chunking.  
- Textual content is encoded using generated questions to create targeted retrieval cues and a custom syntactic reranking method.  
- The approach achieves a Recall@3 of 0.84 in single-hop retrieval, outperforming traditional chunking methods by 60%.  
- Introducing "paper-cards" as concise paper summaries enhances BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on technical queries.  
- The reranking method reaches an F1 score of 0.52 on multihop tasks on the LongBench 2WikiMultihopQA dataset, surpassing baseline methods.  
- This approach eliminates the need for fine-tuning, reduces retrieval latency, enables intuitive access, and decreases storage demands by 80%, making it a scalable and efficient alternative for RAG systems. 

<br /><br />Summary: <div>
arXiv:2506.13778v1 Announce Type: cross 
Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Bias in the Machine: Stereotypes in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2506.13780</link>
<guid>https://arxiv.org/abs/2506.13780</guid>
<content:encoded><![CDATA[
arXiv:2506.13780v1 Announce Type: cross 
Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment</title>
<link>https://arxiv.org/abs/2506.13781</link>
<guid>https://arxiv.org/abs/2506.13781</guid>
<content:encoded><![CDATA[
arXiv:2506.13781v1 Announce Type: cross 
Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.13782</link>
<guid>https://arxiv.org/abs/2506.13782</guid>
<content:encoded><![CDATA[
arXiv:2506.13782v1 Announce Type: cross 
Abstract: Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction</title>
<link>https://arxiv.org/abs/2506.13786</link>
<guid>https://arxiv.org/abs/2506.13786</guid>
<content:encoded><![CDATA[
arXiv:2506.13786v1 Announce Type: cross 
Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network</title>
<link>https://arxiv.org/abs/2506.13787</link>
<guid>https://arxiv.org/abs/2506.13787</guid>
<content:encoded><![CDATA[
arXiv:2506.13787v1 Announce Type: cross 
Abstract: While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries</title>
<link>https://arxiv.org/abs/2506.13796</link>
<guid>https://arxiv.org/abs/2506.13796</guid>
<content:encoded><![CDATA[
arXiv:2506.13796v1 Announce Type: cross 
Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contemporary AI foundation models increase biological weapons risk</title>
<link>https://arxiv.org/abs/2506.13798</link>
<guid>https://arxiv.org/abs/2506.13798</guid>
<content:encoded><![CDATA[
arXiv:2506.13798v1 Announce Type: cross 
Abstract: The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify "elements of success" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework</title>
<link>https://arxiv.org/abs/2506.13800</link>
<guid>https://arxiv.org/abs/2506.13800</guid>
<content:encoded><![CDATA[
arXiv:2506.13800v1 Announce Type: cross 
Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction and Solution Probabilities as Heuristics for Inductive Programming</title>
<link>https://arxiv.org/abs/2506.13804</link>
<guid>https://arxiv.org/abs/2506.13804</guid>
<content:encoded><![CDATA[
arXiv:2506.13804v1 Announce Type: cross 
Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases</title>
<link>https://arxiv.org/abs/2506.13805</link>
<guid>https://arxiv.org/abs/2506.13805</guid>
<content:encoded><![CDATA[
arXiv:2506.13805v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis</title>
<link>https://arxiv.org/abs/2506.13807</link>
<guid>https://arxiv.org/abs/2506.13807</guid>
<content:encoded><![CDATA[
arXiv:2506.13807v1 Announce Type: cross 
Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space</title>
<link>https://arxiv.org/abs/2506.13809</link>
<guid>https://arxiv.org/abs/2506.13809</guid>
<content:encoded><![CDATA[
arXiv:2506.13809v1 Announce Type: cross 
Abstract: Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study</title>
<link>https://arxiv.org/abs/2506.13811</link>
<guid>https://arxiv.org/abs/2506.13811</guid>
<content:encoded><![CDATA[
arXiv:2506.13811v1 Announce Type: cross 
Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models</title>
<link>https://arxiv.org/abs/2506.13817</link>
<guid>https://arxiv.org/abs/2506.13817</guid>
<content:encoded><![CDATA[
arXiv:2506.13817v1 Announce Type: cross 
Abstract: Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge</title>
<link>https://arxiv.org/abs/2506.13820</link>
<guid>https://arxiv.org/abs/2506.13820</guid>
<content:encoded><![CDATA[
arXiv:2506.13820v1 Announce Type: cross 
Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios</title>
<link>https://arxiv.org/abs/2506.13824</link>
<guid>https://arxiv.org/abs/2506.13824</guid>
<content:encoded><![CDATA[
arXiv:2506.13824v1 Announce Type: cross 
Abstract: Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing</title>
<link>https://arxiv.org/abs/2506.13827</link>
<guid>https://arxiv.org/abs/2506.13827</guid>
<content:encoded><![CDATA[
arXiv:2506.13827v1 Announce Type: cross 
Abstract: Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation</title>
<link>https://arxiv.org/abs/2506.13831</link>
<guid>https://arxiv.org/abs/2506.13831</guid>
<content:encoded><![CDATA[
arXiv:2506.13831v1 Announce Type: cross 
Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation</title>
<link>https://arxiv.org/abs/2506.13832</link>
<guid>https://arxiv.org/abs/2506.13832</guid>
<content:encoded><![CDATA[
arXiv:2506.13832v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on World Models Grounded in Acoustic Physical Information</title>
<link>https://arxiv.org/abs/2506.13833</link>
<guid>https://arxiv.org/abs/2506.13833</guid>
<content:encoded><![CDATA[
arXiv:2506.13833v1 Announce Type: cross 
Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolvable Conditional Diffusion</title>
<link>https://arxiv.org/abs/2506.13834</link>
<guid>https://arxiv.org/abs/2506.13834</guid>
<content:encoded><![CDATA[
arXiv:2506.13834v1 Announce Type: cross 
Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study</title>
<link>https://arxiv.org/abs/2506.13836</link>
<guid>https://arxiv.org/abs/2506.13836</guid>
<content:encoded><![CDATA[
arXiv:2506.13836v1 Announce Type: cross 
Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy</title>
<link>https://arxiv.org/abs/2506.13838</link>
<guid>https://arxiv.org/abs/2506.13838</guid>
<content:encoded><![CDATA[
arXiv:2506.13838v1 Announce Type: cross 
Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Students' Reliance on AI in Higher Education: Identifying Contributing Factors</title>
<link>https://arxiv.org/abs/2506.13845</link>
<guid>https://arxiv.org/abs/2506.13845</guid>
<content:encoded><![CDATA[
arXiv:2506.13845v1 Announce Type: cross 
Abstract: The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake it till You Make it: Reward Modeling as Discriminative Prediction</title>
<link>https://arxiv.org/abs/2506.13846</link>
<guid>https://arxiv.org/abs/2506.13846</guid>
<content:encoded><![CDATA[
arXiv:2506.13846v1 Announce Type: cross 
Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StaQ it! Growing neural networks for Policy Mirror Descent</title>
<link>https://arxiv.org/abs/2506.13862</link>
<guid>https://arxiv.org/abs/2506.13862</guid>
<content:encoded><![CDATA[
arXiv:2506.13862v1 Announce Type: cross 
Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</title>
<link>https://arxiv.org/abs/2506.13886</link>
<guid>https://arxiv.org/abs/2506.13886</guid>
<content:encoded><![CDATA[
arXiv:2506.13886v1 Announce Type: cross 
Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Algorithm Distillation for Continuous Control with Mamba</title>
<link>https://arxiv.org/abs/2506.13892</link>
<guid>https://arxiv.org/abs/2506.13892</guid>
<content:encoded><![CDATA[
arXiv:2506.13892v1 Announce Type: cross 
Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models</title>
<link>https://arxiv.org/abs/2506.13900</link>
<guid>https://arxiv.org/abs/2506.13900</guid>
<content:encoded><![CDATA[
arXiv:2506.13900v1 Announce Type: cross 
Abstract: Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations</title>
<link>https://arxiv.org/abs/2506.13901</link>
<guid>https://arxiv.org/abs/2506.13901</guid>
<content:encoded><![CDATA[
arXiv:2506.13901v1 Announce Type: cross 
Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing interpretability of rule-based classifiers through feature graphs</title>
<link>https://arxiv.org/abs/2506.13903</link>
<guid>https://arxiv.org/abs/2506.13903</guid>
<content:encoded><![CDATA[
arXiv:2506.13903v1 Announce Type: cross 
Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare</title>
<link>https://arxiv.org/abs/2506.13904</link>
<guid>https://arxiv.org/abs/2506.13904</guid>
<content:encoded><![CDATA[
arXiv:2506.13904v1 Announce Type: cross 
Abstract: Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.
  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring</title>
<link>https://arxiv.org/abs/2506.13909</link>
<guid>https://arxiv.org/abs/2506.13909</guid>
<content:encoded><![CDATA[
arXiv:2506.13909v1 Announce Type: cross 
Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation</title>
<link>https://arxiv.org/abs/2506.13910</link>
<guid>https://arxiv.org/abs/2506.13910</guid>
<content:encoded><![CDATA[
arXiv:2506.13910v1 Announce Type: cross 
Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization</title>
<link>https://arxiv.org/abs/2506.13911</link>
<guid>https://arxiv.org/abs/2506.13911</guid>
<content:encoded><![CDATA[
arXiv:2506.13911v1 Announce Type: cross 
Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13923</link>
<guid>https://arxiv.org/abs/2506.13923</guid>
<content:encoded><![CDATA[
arXiv:2506.13923v1 Announce Type: cross 
Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment</title>
<link>https://arxiv.org/abs/2506.13925</link>
<guid>https://arxiv.org/abs/2506.13925</guid>
<content:encoded><![CDATA[
arXiv:2506.13925v1 Announce Type: cross 
Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does LLM Reasoning Work for Code? A Survey and a Call to Action</title>
<link>https://arxiv.org/abs/2506.13932</link>
<guid>https://arxiv.org/abs/2506.13932</guid>
<content:encoded><![CDATA[
arXiv:2506.13932v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection</title>
<link>https://arxiv.org/abs/2506.13956</link>
<guid>https://arxiv.org/abs/2506.13956</guid>
<content:encoded><![CDATA[
arXiv:2506.13956v1 Announce Type: cross 
Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers</title>
<link>https://arxiv.org/abs/2506.13958</link>
<guid>https://arxiv.org/abs/2506.13958</guid>
<content:encoded><![CDATA[
arXiv:2506.13958v1 Announce Type: cross 
Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation</title>
<link>https://arxiv.org/abs/2506.13961</link>
<guid>https://arxiv.org/abs/2506.13961</guid>
<content:encoded><![CDATA[
arXiv:2506.13961v1 Announce Type: cross 
Abstract: Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $\alpha,\!\beta$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making deep neural networks work for medical audio: representation, compression and domain adaptation</title>
<link>https://arxiv.org/abs/2506.13970</link>
<guid>https://arxiv.org/abs/2506.13970</guid>
<content:encoded><![CDATA[
arXiv:2506.13970v1 Announce Type: cross 
Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting</title>
<link>https://arxiv.org/abs/2506.13981</link>
<guid>https://arxiv.org/abs/2506.13981</guid>
<content:encoded><![CDATA[
arXiv:2506.13981v1 Announce Type: cross 
Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms</title>
<link>https://arxiv.org/abs/2506.13984</link>
<guid>https://arxiv.org/abs/2506.13984</guid>
<content:encoded><![CDATA[
arXiv:2506.13984v1 Announce Type: cross 
Abstract: In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering</title>
<link>https://arxiv.org/abs/2506.13989</link>
<guid>https://arxiv.org/abs/2506.13989</guid>
<content:encoded><![CDATA[
arXiv:2506.13989v1 Announce Type: cross 
Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science</title>
<link>https://arxiv.org/abs/2506.13992</link>
<guid>https://arxiv.org/abs/2506.13992</guid>
<content:encoded><![CDATA[
arXiv:2506.13992v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2506.14002</link>
<guid>https://arxiv.org/abs/2506.14002</guid>
<content:encoded><![CDATA[
arXiv:2506.14002v1 Announce Type: cross 
Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bures-Wasserstein Flow Matching for Graph Generation</title>
<link>https://arxiv.org/abs/2506.14020</link>
<guid>https://arxiv.org/abs/2506.14020</guid>
<content:encoded><![CDATA[
arXiv:2506.14020v1 Announce Type: cross 
Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement</title>
<link>https://arxiv.org/abs/2506.14035</link>
<guid>https://arxiv.org/abs/2506.14035</guid>
<content:encoded><![CDATA[
arXiv:2506.14035v1 Announce Type: cross 
Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotically Smaller Encodings for Graph Problems and Scheduling</title>
<link>https://arxiv.org/abs/2506.14042</link>
<guid>https://arxiv.org/abs/2506.14042</guid>
<content:encoded><![CDATA[
arXiv:2506.14042v1 Announce Type: cross 
Abstract: We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \lg |V|)$ many clauses, as opposed to the $\Omega(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erd\H{o}s, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of "Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \lg |V|)$ clauses (the direct encoding uses $\Omega(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications</title>
<link>https://arxiv.org/abs/2506.14046</link>
<guid>https://arxiv.org/abs/2506.14046</guid>
<content:encoded><![CDATA[
arXiv:2506.14046v1 Announce Type: cross 
Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature</title>
<link>https://arxiv.org/abs/2506.14054</link>
<guid>https://arxiv.org/abs/2506.14054</guid>
<content:encoded><![CDATA[
arXiv:2506.14054v1 Announce Type: cross 
Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking</title>
<link>https://arxiv.org/abs/2506.14086</link>
<guid>https://arxiv.org/abs/2506.14086</guid>
<content:encoded><![CDATA[
arXiv:2506.14086v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2506.14096</link>
<guid>https://arxiv.org/abs/2506.14096</guid>
<content:encoded><![CDATA[
arXiv:2506.14096v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks</title>
<link>https://arxiv.org/abs/2506.14098</link>
<guid>https://arxiv.org/abs/2506.14098</guid>
<content:encoded><![CDATA[
arXiv:2506.14098v1 Announce Type: cross 
Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Essential-Web v1.0: 24T tokens of organized web data</title>
<link>https://arxiv.org/abs/2506.14111</link>
<guid>https://arxiv.org/abs/2506.14111</guid>
<content:encoded><![CDATA[
arXiv:2506.14111v1 Announce Type: cross 
Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2506.14113</link>
<guid>https://arxiv.org/abs/2506.14113</guid>
<content:encoded><![CDATA[
arXiv:2506.14113v1 Announce Type: cross 
Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs</title>
<link>https://arxiv.org/abs/2506.14122</link>
<guid>https://arxiv.org/abs/2506.14122</guid>
<content:encoded><![CDATA[
arXiv:2506.14122v1 Announce Type: cross 
Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Undertraining Experts Improves Model Upcycling</title>
<link>https://arxiv.org/abs/2506.14126</link>
<guid>https://arxiv.org/abs/2506.14126</guid>
<content:encoded><![CDATA[
arXiv:2506.14126v1 Announce Type: cross 
Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KDMOS:Knowledge Distillation for Motion Segmentation</title>
<link>https://arxiv.org/abs/2506.14130</link>
<guid>https://arxiv.org/abs/2506.14130</guid>
<content:encoded><![CDATA[
arXiv:2506.14130v1 Announce Type: cross 
Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning</title>
<link>https://arxiv.org/abs/2506.14138</link>
<guid>https://arxiv.org/abs/2506.14138</guid>
<content:encoded><![CDATA[
arXiv:2506.14138v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability</title>
<link>https://arxiv.org/abs/2506.14144</link>
<guid>https://arxiv.org/abs/2506.14144</guid>
<content:encoded><![CDATA[
arXiv:2506.14144v1 Announce Type: cross 
Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models</title>
<link>https://arxiv.org/abs/2506.14158</link>
<guid>https://arxiv.org/abs/2506.14158</guid>
<content:encoded><![CDATA[
arXiv:2506.14158v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2506.14159</link>
<guid>https://arxiv.org/abs/2506.14159</guid>
<content:encoded><![CDATA[
arXiv:2506.14159v1 Announce Type: cross 
Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoMAR: Autoregressive Video Generatio with Continuous Tokens</title>
<link>https://arxiv.org/abs/2506.14168</link>
<guid>https://arxiv.org/abs/2506.14168</guid>
<content:encoded><![CDATA[
arXiv:2506.14168v1 Announce Type: cross 
Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-stage augmented multimodal interaction network for fish feeding intensity quantification</title>
<link>https://arxiv.org/abs/2506.14170</link>
<guid>https://arxiv.org/abs/2506.14170</guid>
<content:encoded><![CDATA[
arXiv:2506.14170v1 Announce Type: cross 
Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAM: A Generative Foundation Reward Model for Reward Generalization</title>
<link>https://arxiv.org/abs/2506.14175</link>
<guid>https://arxiv.org/abs/2506.14175</guid>
<content:encoded><![CDATA[
arXiv:2506.14175v1 Announce Type: cross 
Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages</title>
<link>https://arxiv.org/abs/2506.14177</link>
<guid>https://arxiv.org/abs/2506.14177</guid>
<content:encoded><![CDATA[
arXiv:2506.14177v1 Announce Type: cross 
Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers</title>
<link>https://arxiv.org/abs/2506.14196</link>
<guid>https://arxiv.org/abs/2506.14196</guid>
<content:encoded><![CDATA[
arXiv:2506.14196v1 Announce Type: cross 
Abstract: Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion</title>
<link>https://arxiv.org/abs/2506.14202</link>
<guid>https://arxiv.org/abs/2506.14202</guid>
<content:encoded><![CDATA[
arXiv:2506.14202v1 Announce Type: cross 
Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT</title>
<link>https://arxiv.org/abs/2506.14209</link>
<guid>https://arxiv.org/abs/2506.14209</guid>
<content:encoded><![CDATA[
arXiv:2506.14209v1 Announce Type: cross 
Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift</title>
<link>https://arxiv.org/abs/2506.14217</link>
<guid>https://arxiv.org/abs/2506.14217</guid>
<content:encoded><![CDATA[
arXiv:2506.14217v1 Announce Type: cross 
Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction</title>
<link>https://arxiv.org/abs/2506.14229</link>
<guid>https://arxiv.org/abs/2506.14229</guid>
<content:encoded><![CDATA[
arXiv:2506.14229v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team</title>
<link>https://arxiv.org/abs/2506.14234</link>
<guid>https://arxiv.org/abs/2506.14234</guid>
<content:encoded><![CDATA[
arXiv:2506.14234v1 Announce Type: cross 
Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Initialization Token Learning for Tool-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2506.14248</link>
<guid>https://arxiv.org/abs/2506.14248</guid>
<content:encoded><![CDATA[
arXiv:2506.14248v1 Announce Type: cross 
Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Adaptation as Posterior Correction</title>
<link>https://arxiv.org/abs/2506.14262</link>
<guid>https://arxiv.org/abs/2506.14262</guid>
<content:encoded><![CDATA[
arXiv:2506.14262v1 Announce Type: cross 
Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LoRA with Variational Learning</title>
<link>https://arxiv.org/abs/2506.14280</link>
<guid>https://arxiv.org/abs/2506.14280</guid>
<content:encoded><![CDATA[
arXiv:2506.14280v1 Announce Type: cross 
Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Robots with Inference-Time Interactions</title>
<link>https://arxiv.org/abs/2506.14287</link>
<guid>https://arxiv.org/abs/2506.14287</guid>
<content:encoded><![CDATA[
arXiv:2506.14287v1 Announce Type: cross 
Abstract: Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation</title>
<link>https://arxiv.org/abs/2506.14294</link>
<guid>https://arxiv.org/abs/2506.14294</guid>
<content:encoded><![CDATA[
arXiv:2506.14294v1 Announce Type: cross 
Abstract: We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels</title>
<link>https://arxiv.org/abs/2506.14303</link>
<guid>https://arxiv.org/abs/2506.14303</guid>
<content:encoded><![CDATA[
arXiv:2506.14303v1 Announce Type: cross 
Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjustment for Confounding using Pre-Trained Representations</title>
<link>https://arxiv.org/abs/2506.14329</link>
<guid>https://arxiv.org/abs/2506.14329</guid>
<content:encoded><![CDATA[
arXiv:2506.14329v1 Announce Type: cross 
Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Intent-Based Categorization of Phishing Emails</title>
<link>https://arxiv.org/abs/2506.14337</link>
<guid>https://arxiv.org/abs/2506.14337</guid>
<content:encoded><![CDATA[
arXiv:2506.14337v1 Announce Type: cross 
Abstract: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization</title>
<link>https://arxiv.org/abs/2506.14356</link>
<guid>https://arxiv.org/abs/2506.14356</guid>
<content:encoded><![CDATA[
arXiv:2506.14356v1 Announce Type: cross 
Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards</title>
<link>https://arxiv.org/abs/2506.14375</link>
<guid>https://arxiv.org/abs/2506.14375</guid>
<content:encoded><![CDATA[
arXiv:2506.14375v1 Announce Type: cross 
Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepthSeg: Depth prompting in remote sensing semantic segmentation</title>
<link>https://arxiv.org/abs/2506.14382</link>
<guid>https://arxiv.org/abs/2506.14382</guid>
<content:encoded><![CDATA[
arXiv:2506.14382v1 Announce Type: cross 
Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResNets Are Deeper Than You Think</title>
<link>https://arxiv.org/abs/2506.14386</link>
<guid>https://arxiv.org/abs/2506.14386</guid>
<content:encoded><![CDATA[
arXiv:2506.14386v1 Announce Type: cross 
Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control</title>
<link>https://arxiv.org/abs/2506.14391</link>
<guid>https://arxiv.org/abs/2506.14391</guid>
<content:encoded><![CDATA[
arXiv:2506.14391v1 Announce Type: cross 
Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models</title>
<link>https://arxiv.org/abs/2506.14399</link>
<guid>https://arxiv.org/abs/2506.14399</guid>
<content:encoded><![CDATA[
arXiv:2506.14399v1 Announce Type: cross 
Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Steered Diffusion for Automated Video Counterfactual Generation</title>
<link>https://arxiv.org/abs/2506.14404</link>
<guid>https://arxiv.org/abs/2506.14404</guid>
<content:encoded><![CDATA[
arXiv:2506.14404v1 Announce Type: cross 
Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge</title>
<link>https://arxiv.org/abs/2506.14407</link>
<guid>https://arxiv.org/abs/2506.14407</guid>
<content:encoded><![CDATA[
arXiv:2506.14407v1 Announce Type: cross 
Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Reinforcement Learning for Unobservable Random Delays</title>
<link>https://arxiv.org/abs/2506.14411</link>
<guid>https://arxiv.org/abs/2506.14411</guid>
<content:encoded><![CDATA[
arXiv:2506.14411v1 Announce Type: cross 
Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition</title>
<link>https://arxiv.org/abs/2506.14412</link>
<guid>https://arxiv.org/abs/2506.14412</guid>
<content:encoded><![CDATA[
arXiv:2506.14412v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Attribute Imbalance in Vision Datasets</title>
<link>https://arxiv.org/abs/2506.14418</link>
<guid>https://arxiv.org/abs/2506.14418</guid>
<content:encoded><![CDATA[
arXiv:2506.14418v1 Announce Type: cross 
Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Selection All You Need in Differential Evolution?</title>
<link>https://arxiv.org/abs/2506.14425</link>
<guid>https://arxiv.org/abs/2506.14425</guid>
<content:encoded><![CDATA[
arXiv:2506.14425v1 Announce Type: cross 
Abstract: Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Streaming and Non-streaming Zipformer-based ASR</title>
<link>https://arxiv.org/abs/2506.14434</link>
<guid>https://arxiv.org/abs/2506.14434</guid>
<content:encoded><![CDATA[
arXiv:2506.14434v1 Announce Type: cross 
Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sHGCN: Simplified hyperbolic graph convolutional neural networks</title>
<link>https://arxiv.org/abs/2506.14438</link>
<guid>https://arxiv.org/abs/2506.14438</guid>
<content:encoded><![CDATA[
arXiv:2506.14438v1 Announce Type: cross 
Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model compression using knowledge distillation with integrated gradients</title>
<link>https://arxiv.org/abs/2506.14440</link>
<guid>https://arxiv.org/abs/2506.14440</guid>
<content:encoded><![CDATA[
arXiv:2506.14440v1 Announce Type: cross 
Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Lightweight Vision Language Models for Radiological Visual Question Answering</title>
<link>https://arxiv.org/abs/2506.14451</link>
<guid>https://arxiv.org/abs/2506.14451</guid>
<content:encoded><![CDATA[
arXiv:2506.14451v1 Announce Type: cross 
Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hamiltonian Formalism for Comparing Quantum and Classical Intelligence</title>
<link>https://arxiv.org/abs/2506.14456</link>
<guid>https://arxiv.org/abs/2506.14456</guid>
<content:encoded><![CDATA[
arXiv:2506.14456v1 Announce Type: cross 
Abstract: The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2506.14464</link>
<guid>https://arxiv.org/abs/2506.14464</guid>
<content:encoded><![CDATA[
arXiv:2506.14464v1 Announce Type: cross 
Abstract: Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks</title>
<link>https://arxiv.org/abs/2506.14472</link>
<guid>https://arxiv.org/abs/2506.14472</guid>
<content:encoded><![CDATA[
arXiv:2506.14472v1 Announce Type: cross 
Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments</title>
<link>https://arxiv.org/abs/2506.14513</link>
<guid>https://arxiv.org/abs/2506.14513</guid>
<content:encoded><![CDATA[
arXiv:2506.14513v1 Announce Type: cross 
Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters</title>
<link>https://arxiv.org/abs/2506.14530</link>
<guid>https://arxiv.org/abs/2506.14530</guid>
<content:encoded><![CDATA[
arXiv:2506.14530v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complete Characterization for Adjustment in Summary Causal Graphs of Time Series</title>
<link>https://arxiv.org/abs/2506.14534</link>
<guid>https://arxiv.org/abs/2506.14534</guid>
<content:encoded><![CDATA[
arXiv:2506.14534v1 Announce Type: cross 
Abstract: The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Qiskit Code Refactoring Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.14535</link>
<guid>https://arxiv.org/abs/2506.14535</guid>
<content:encoded><![CDATA[
arXiv:2506.14535v1 Announce Type: cross 
Abstract: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs</title>
<link>https://arxiv.org/abs/2506.14540</link>
<guid>https://arxiv.org/abs/2506.14540</guid>
<content:encoded><![CDATA[
arXiv:2506.14540v1 Announce Type: cross 
Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>
<link>https://arxiv.org/abs/2506.14562</link>
<guid>https://arxiv.org/abs/2506.14562</guid>
<content:encoded><![CDATA[
arXiv:2506.14562v1 Announce Type: cross 
Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains</title>
<link>https://arxiv.org/abs/2506.14567</link>
<guid>https://arxiv.org/abs/2506.14567</guid>
<content:encoded><![CDATA[
arXiv:2506.14567v1 Announce Type: cross 
Abstract: Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2506.14574</link>
<guid>https://arxiv.org/abs/2506.14574</guid>
<content:encoded><![CDATA[
arXiv:2506.14574v1 Announce Type: cross 
Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object-Centric Neuro-Argumentative Learning</title>
<link>https://arxiv.org/abs/2506.14577</link>
<guid>https://arxiv.org/abs/2506.14577</guid>
<content:encoded><![CDATA[
arXiv:2506.14577v1 Announce Type: cross 
Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenerationPrograms: Fine-grained Attribution with Executable Programs</title>
<link>https://arxiv.org/abs/2506.14580</link>
<guid>https://arxiv.org/abs/2506.14580</guid>
<content:encoded><![CDATA[
arXiv:2506.14580v1 Announce Type: cross 
Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images</title>
<link>https://arxiv.org/abs/2506.14583</link>
<guid>https://arxiv.org/abs/2506.14583</guid>
<content:encoded><![CDATA[
arXiv:2506.14583v1 Announce Type: cross 
Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2506.14596</link>
<guid>https://arxiv.org/abs/2506.14596</guid>
<content:encoded><![CDATA[
arXiv:2506.14596v1 Announce Type: cross 
Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-code to fight climate change: the Climaborough project</title>
<link>https://arxiv.org/abs/2506.14623</link>
<guid>https://arxiv.org/abs/2506.14623</guid>
<content:encoded><![CDATA[
arXiv:2506.14623v1 Announce Type: cross 
Abstract: The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.14625</link>
<guid>https://arxiv.org/abs/2506.14625</guid>
<content:encoded><![CDATA[
arXiv:2506.14625v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACM Survey Draft on Formalising Software Requirements with Large Language Models</title>
<link>https://arxiv.org/abs/2506.14627</link>
<guid>https://arxiv.org/abs/2506.14627</guid>
<content:encoded><![CDATA[
arXiv:2506.14627v1 Announce Type: cross 
Abstract: This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation</title>
<link>https://arxiv.org/abs/2506.14634</link>
<guid>https://arxiv.org/abs/2506.14634</guid>
<content:encoded><![CDATA[
arXiv:2506.14634v1 Announce Type: cross 
Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey</title>
<link>https://arxiv.org/abs/2506.14640</link>
<guid>https://arxiv.org/abs/2506.14640</guid>
<content:encoded><![CDATA[
arXiv:2506.14640v1 Announce Type: cross 
Abstract: In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot</title>
<link>https://arxiv.org/abs/2506.14641</link>
<guid>https://arxiv.org/abs/2506.14641</guid>
<content:encoded><![CDATA[
arXiv:2506.14641v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.14648</link>
<guid>https://arxiv.org/abs/2506.14648</guid>
<content:encoded><![CDATA[
arXiv:2506.14648v1 Announce Type: cross 
Abstract: Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor</title>
<link>https://arxiv.org/abs/2506.14652</link>
<guid>https://arxiv.org/abs/2506.14652</guid>
<content:encoded><![CDATA[
arXiv:2506.14652v1 Announce Type: cross 
Abstract: In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable exchange-correlation with deep learning</title>
<link>https://arxiv.org/abs/2506.14665</link>
<guid>https://arxiv.org/abs/2506.14665</guid>
<content:encoded><![CDATA[
arXiv:2506.14665v1 Announce Type: cross 
Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\"odinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery</title>
<link>https://arxiv.org/abs/2506.14670</link>
<guid>https://arxiv.org/abs/2506.14670</guid>
<content:encoded><![CDATA[
arXiv:2506.14670v1 Announce Type: cross 
Abstract: Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach</title>
<link>https://arxiv.org/abs/2506.14677</link>
<guid>https://arxiv.org/abs/2506.14677</guid>
<content:encoded><![CDATA[
arXiv:2506.14677v1 Announce Type: cross 
Abstract: This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Software Engineering agent as AI Software Engineer</title>
<link>https://arxiv.org/abs/2506.14683</link>
<guid>https://arxiv.org/abs/2506.14683</guid>
<content:encoded><![CDATA[
arXiv:2506.14683v1 Announce Type: cross 
Abstract: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining music sample identification with a self-supervised graph neural network</title>
<link>https://arxiv.org/abs/2506.14684</link>
<guid>https://arxiv.org/abs/2506.14684</guid>
<content:encoded><![CDATA[
arXiv:2506.14684v1 Announce Type: cross 
Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Accompaniment with ReaLchords</title>
<link>https://arxiv.org/abs/2506.14723</link>
<guid>https://arxiv.org/abs/2506.14723</guid>
<content:encoded><![CDATA[
arXiv:2506.14723v1 Announce Type: cross 
Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models</title>
<link>https://arxiv.org/abs/2506.14727</link>
<guid>https://arxiv.org/abs/2506.14727</guid>
<content:encoded><![CDATA[
arXiv:2506.14727v1 Announce Type: cross 
Abstract: Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2506.14731</link>
<guid>https://arxiv.org/abs/2506.14731</guid>
<content:encoded><![CDATA[
arXiv:2506.14731v1 Announce Type: cross 
Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Speaker Diarization with Mixture of Experts</title>
<link>https://arxiv.org/abs/2506.14750</link>
<guid>https://arxiv.org/abs/2506.14750</guid>
<content:encoded><![CDATA[
arXiv:2506.14750v1 Announce Type: cross 
Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Bytes to Ideas: Language Modeling with Autoregressive U-Nets</title>
<link>https://arxiv.org/abs/2506.14761</link>
<guid>https://arxiv.org/abs/2506.14761</guid>
<content:encoded><![CDATA[
arXiv:2506.14761v1 Announce Type: cross 
Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Framework for Improving Naturalness in Generative Spoken Language Models</title>
<link>https://arxiv.org/abs/2506.14767</link>
<guid>https://arxiv.org/abs/2506.14767</guid>
<content:encoded><![CDATA[
arXiv:2506.14767v1 Announce Type: cross 
Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models</title>
<link>https://arxiv.org/abs/2310.07637</link>
<guid>https://arxiv.org/abs/2310.07637</guid>
<content:encoded><![CDATA[
arXiv:2310.07637v5 Announce Type: replace 
Abstract: Information Technology (IT) Operations (Ops), particularly Artificial Intelligence for IT Operations (AIOps), is the guarantee for maintaining the orderly and stable operation of existing information systems. According to Gartner's prediction, the use of AI technology for automated IT operations has become a new trend. Large language models (LLMs) that have exhibited remarkable capabilities in NLP-related tasks, are showing great potential in the field of AIOps, such as in aspects of root cause analysis of failures, generation of operations and maintenance scripts, and summarizing of alert information. Nevertheless, the performance of current LLMs in Ops tasks is yet to be determined. In this paper, we present OpsEval, a comprehensive task-oriented Ops benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in various crucial scenarios at different ability levels. The benchmark includes 7184 multi-choice questions and 1736 question-answering (QA) formats in English and Chinese. By conducting a comprehensive performance evaluation of the current leading large language models, we show how various LLM techniques can affect the performance of Ops, and discussed findings related to various topics, including model quantification, QA evaluation, and hallucination issues. To ensure the credibility of our evaluation, we invite dozens of domain experts to manually review our questions. At the same time, we have open-sourced 20% of the test QA to assist current researchers in preliminary evaluations of their OpsLLM models. The remaining 80% of the data, which is not disclosed, is used to eliminate the issue of the test set leakage. Additionally, we have constructed an online leaderboard that is updated in real-time and will continue to be updated, ensuring that any newly emerging LLMs will be evaluated promptly. Both our dataset and leaderboard have been made public.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Traffic Signal Control via Genetic Programming</title>
<link>https://arxiv.org/abs/2403.17328</link>
<guid>https://arxiv.org/abs/2403.17328</guid>
<content:encoded><![CDATA[
arXiv:2403.17328v3 Announce Type: replace 
Abstract: The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic programming is adopted to perform gradient-free optimization of the urgency function. We test our algorithm on multiple public traffic signal control datasets. The experimental results indicate that the tree-shaped urgency function evolved by genetic programming outperforms the baselines, including a state-of-the-art method in the transportation field and a well-known DRL-based method. Our code is available online.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries</title>
<link>https://arxiv.org/abs/2405.20653</link>
<guid>https://arxiv.org/abs/2405.20653</guid>
<content:encoded><![CDATA[
arXiv:2405.20653v3 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have led to impressive alignment where models learn to distinguish harmful from harmless queries through supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). In this paper, we reveal a subtle yet impactful weakness in these aligned models. We find that simply appending multiple end of sequence (eos) tokens can cause a phenomenon we call context segmentation, which effectively shifts both harmful and benign inputs closer to the refusal boundary in the hidden space.
  Building on this observation, we propose a straightforward method to BOOST jailbreak attacks by appending eos tokens. Our systematic evaluation shows that this strategy significantly increases the attack success rate across 8 representative jailbreak techniques and 16 open-source LLMs, ranging from 2B to 72B parameters. Moreover, we develop a novel probing mechanism for commercial APIs and discover that major providers such as OpenAI, Anthropic, and Qwen do not filter eos tokens, making them similarly vulnerable. These findings highlight a hidden yet critical blind spot in existing alignment and content filtering approaches.
  We call for heightened attention to eos tokens' unintended influence on model behaviors, particularly in production systems. Our work not only calls for an input-filtering based defense, but also points to new defenses that make refusal boundaries more robust and generalizable, as well as fundamental alignment techniques that can defend against context segmentation attacks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers</title>
<link>https://arxiv.org/abs/2406.14986</link>
<guid>https://arxiv.org/abs/2406.14986</guid>
<content:encoded><![CDATA[
arXiv:2406.14986v3 Announce Type: replace 
Abstract: Multiple Choice Questions (MCQ) have become a commonly used approach to assess the capabilities of Large Language Models (LLMs), due to their ease of manipulation and evaluation. The experimental appraisals of the LLMs' Stated Answer (their answer to MCQ) have pointed to their apparent ability to perform probabilistic reasoning or to grasp uncertainty. In this work, we investigate whether these aptitudes are measurable outside tailored prompting and MCQ by reformulating these issues as direct text-completion - the fundamental computational unit of LLMs. We introduce Revealed Belief, an evaluation framework that evaluates LLMs on tasks requiring reasoning under uncertainty, which complements MCQ scoring by analyzing text-completion probability distributions. Our findings suggest that while LLMs frequently state the correct answer, their Revealed Belief shows that they often allocate probability mass inconsistently, exhibit systematic biases, and often fail to update their beliefs appropriately when presented with new evidence, leading to strong potential impacts on downstream tasks. These results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets</title>
<link>https://arxiv.org/abs/2407.05674</link>
<guid>https://arxiv.org/abs/2407.05674</guid>
<content:encoded><![CDATA[
arXiv:2407.05674v3 Announce Type: replace 
Abstract: Large Language Models can carry out human-like conversations in diverse settings, responding to user requests for tasks and knowledge. However, existing conversational agents implemented with LLMs often struggle with hallucination, following instructions with conditional logic, and integrating knowledge from different sources. These shortcomings compromise the agents' effectiveness, rendering them unsuitable for deployment. To address these challenges, we introduce Genie, a programmable framework for creating knowledge-intensive task-oriented conversational agents. Genie can handle involved interactions and answer complex queries. Unlike LLMs, it delivers reliable, grounded responses through advanced dialogue state management and supports controllable agent policies via its declarative specification -- Genie Worksheet. This is achieved through an algorithmic runtime system that implements the developer-supplied policy, limiting LLMs to (1) parse user input using a succinct conversational history, and (2) generate responses according to supplied context. Agents built with Genie outperform SOTA methods on complex logic dialogue datasets. We conducted a user study with 62 participants on three real-life applications: restaurant reservations with Yelp, as well as ticket submission and course enrollment for university students. Genie agents with GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling, improving goal completion rates from 21.8% to 82.8% across three real-world tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Generation and Evaluation for Parallel Greedy Best-First Search(extended version)</title>
<link>https://arxiv.org/abs/2408.05682</link>
<guid>https://arxiv.org/abs/2408.05682</guid>
<content:encoded><![CDATA[
arXiv:2408.05682v2 Announce Type: replace 
Abstract: In order to understand and control the search behavior of parallel search, recent work has proposed a class of constrained parallel greedy best-first search algorithms which only expands states that satisfy some constraint.However, enforcing such constraints can be costly, as threads must be waiting idly until a state that satisfies the expansion constraint is available. We propose an improvement to constrained parallel search which decouples state generation and state evaluation and significantly improves state evaluation rate, resulting in better search performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents</title>
<link>https://arxiv.org/abs/2410.05243</link>
<guid>https://arxiv.org/abs/2410.05243</guid>
<content:encoded><![CDATA[
arXiv:2410.05243v3 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multi-bit Text Watermark with LLM-based Paraphrasers</title>
<link>https://arxiv.org/abs/2412.03123</link>
<guid>https://arxiv.org/abs/2412.03123</guid>
<content:encoded><![CDATA[
arXiv:2412.03123v2 Announce Type: replace 
Abstract: We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces</title>
<link>https://arxiv.org/abs/2502.07709</link>
<guid>https://arxiv.org/abs/2502.07709</guid>
<content:encoded><![CDATA[
arXiv:2502.07709v3 Announce Type: replace 
Abstract: Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization</title>
<link>https://arxiv.org/abs/2502.11422</link>
<guid>https://arxiv.org/abs/2502.11422</guid>
<content:encoded><![CDATA[
arXiv:2502.11422v2 Announce Type: replace 
Abstract: Heuristics have achieved great success in solving combinatorial optimization problems (COPs). However, heuristics designed by humans require too much domain knowledge and testing time. Given the fact that Large Language Models (LLMs) possess strong capabilities to understand and generate content, and a knowledge base that covers various domains, which offer a novel way to automatically optimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an optimization method that integrates the self-reflection of LLMs with the Monte Carlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively refines generated heuristics by evaluating their performance and providing improvement suggestions. Our method enables to iteratively evaluate the generated heuristics (states) and improve them based on the improvement suggestions (actions) and evaluation results (rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow Shop Scheduling Problem (FSSP). The experimental results show that PoH outperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD) by other LLMs-based methods, and achieves the significant improvements and the state-of-the-art performance of our proposed method in automating heuristic optimization with LLMs to solve COPs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Space Interventions Can Be Transferred Between Large Language Models</title>
<link>https://arxiv.org/abs/2503.04429</link>
<guid>https://arxiv.org/abs/2503.04429</guid>
<content:encoded><![CDATA[
arXiv:2503.04429v3 Announce Type: replace 
Abstract: The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches", allowing dynamic toggling between model behaviors.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</title>
<link>https://arxiv.org/abs/2503.08679</link>
<guid>https://arxiv.org/abs/2503.08679</guid>
<content:encoded><![CDATA[
arXiv:2503.08679v4 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions "Is X bigger than Y?" and "Is Y bigger than X?", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible</title>
<link>https://arxiv.org/abs/2503.12917</link>
<guid>https://arxiv.org/abs/2503.12917</guid>
<content:encoded><![CDATA[
arXiv:2503.12917v2 Announce Type: replace 
Abstract: The current Neuro-Symbolic (NeSy) Learning paradigm suffers from an over-reliance on labeled data, so if we completely disregard labels, it leads to less symbol information, a larger solution space, and more shortcuts-issues that current Nesy systems cannot resolve. This paper introduces a novel learning paradigm, Verification Learning (VL), which addresses this challenge by transforming the label-based reasoning process in Nesy into a label-free verification process. VL achieves excellent learning results solely by relying on unlabeled data and a function that verifies whether the current predictions conform to the rules. We formalize this problem as a Constraint Optimization Problem (COP) and propose a Dynamic Combinatorial Sorting (DCS) algorithm that accelerates the solution by reducing verification attempts, effectively lowering computational costs and introduce a prior alignment method to address potential shortcuts. Our theoretical analysis points out which tasks in Nesy systems can be completed without labels and explains why rules can replace infinite labels for some tasks, while for others the rules have no effect. We validate the proposed framework through several fully unsupervised tasks including addition, sort, match, and chess, each showing significant performance and efficiency improvements.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behaviour Discovery and Attribution for Explainable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.14973</link>
<guid>https://arxiv.org/abs/2503.14973</guid>
<content:encoded><![CDATA[
arXiv:2503.14973v2 Announce Type: replace 
Abstract: Building trust in reinforcement learning (RL) agents requires understanding why they make certain decisions, especially in high-stakes applications like robotics, healthcare, and finance. Existing explainability methods often focus on single states or entire trajectories, either providing only local, step-wise insights or attributing decisions to coarse, episodelevel summaries. Both approaches miss the recurring strategies and temporally extended patterns that actually drive agent behavior across multiple decisions. We address this gap by proposing a fully offline, reward-free framework for behavior discovery and segmentation, enabling the attribution of actions to meaningful and interpretable behavior segments that capture recurring patterns appearing across multiple trajectories. Our method identifies coherent behavior clusters from state-action sequences and attributes individual actions to these clusters for fine-grained, behavior-centric explanations. Evaluations on four diverse offline RL environments show that our approach discovers meaningful behaviors and outperforms trajectory-level baselines in fidelity, human preference, and cluster coherence. Our code is publicly available.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2504.21370</link>
<guid>https://arxiv.org/abs/2504.21370</guid>
<content:encoded><![CDATA[
arXiv:2504.21370v3 Announce Type: replace 
Abstract: Recent models such as OpenAI o1 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks by generating extended Chain-of-Thought (CoT) traces. While longer reasoning helps with thorough exploration of solution paths for complex problems, it also often leads to inefficient and redundant outputs--a phenomenon commonly described as overthinking. In this paper, we propose ShorterBetter, a simple yet effective reinforcement learning method that enables reasoning models to learn their own optimal CoT lengths without manual supervision. We define the Sample Optimal Length (SOL) as the length of the shortest correct response among multiple generations, which serves as a dynamic reward signal to guide the model toward efficient reasoning. Applied to DeepSeek-Distill-Qwen-1.5B/7B as base models, ShorterBetter achieves 50%-80% reduction in output lengths in both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our reasoning trace analysis shows that ShorterBetter refines the structure of the reasoning traces by reducing unnecessary repetition, excessive self-verification, and over-exploration of alternatives.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arbitrarily Applicable Same/Opposite Relational Responding with NARS</title>
<link>https://arxiv.org/abs/2505.07079</link>
<guid>https://arxiv.org/abs/2505.07079</guid>
<content:encoded><![CDATA[
arXiv:2505.07079v2 Announce Type: replace 
Abstract: Same/opposite relational responding, a fundamental aspect of human symbolic cognition, allows the flexible generalization of stimulus relationships based on minimal experience. In this study, we demonstrate the emergence of \textit{arbitrarily applicable} same/opposite relational responding within the Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture designed for adaptive reasoning under uncertainty. Specifically, we extend NARS with an implementation of \textit{acquired relations}, enabling the system to explicitly derive both symmetric (mutual entailment) and novel relational combinations (combinatorial entailment) from minimal explicit training in a contextually controlled matching-to-sample (MTS) procedure. Experimental results show that NARS rapidly internalizes explicitly trained relational rules and robustly demonstrates derived relational generalizations based on arbitrary contextual cues. Importantly, derived relational responding in critical test phases inherently combines both mutual and combinatorial entailments, such as deriving same-relations from multiple explicitly trained opposite-relations. Internal confidence metrics illustrate strong internalization of these relational principles, closely paralleling phenomena observed in human relational learning experiments. Our findings underscore the potential for integrating nuanced relational learning mechanisms inspired by learning psychology into artificial general intelligence frameworks, explicitly highlighting the arbitrary and context-sensitive relational capabilities modeled within NARS.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
arXiv:2505.13227v2 Announce Type: replace 
Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning</title>
<link>https://arxiv.org/abs/2505.19099</link>
<guid>https://arxiv.org/abs/2505.19099</guid>
<content:encoded><![CDATA[
arXiv:2505.19099v4 Announce Type: replace 
Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs</title>
<link>https://arxiv.org/abs/2505.19165</link>
<guid>https://arxiv.org/abs/2505.19165</guid>
<content:encoded><![CDATA[
arXiv:2505.19165v3 Announce Type: replace 
Abstract: Role-based access control (RBAC) and hierarchical structures are foundational to how information flows and decisions are made within virtually all organizations. As the potential of Large Language Models (LLMs) to serve as unified knowledge repositories and intelligent assistants in enterprise settings becomes increasingly apparent, a critical, yet under explored, challenge emerges: \textit{can these models reliably understand and operate within the complex, often nuanced, constraints imposed by organizational hierarchies and associated permissions?} Evaluating this crucial capability is inherently difficult due to the proprietary and sensitive nature of real-world corporate data and access control policies. We introduce a synthetic yet representative \textbf{OrgAccess} benchmark consisting of 40 distinct types of permissions commonly relevant across different organizational roles and levels. We further create three types of permissions: 40,000 easy (1 permission), 10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to test LLMs' ability to accurately assess these permissions and generate responses that strictly adhere to the specified hierarchical rules, particularly in scenarios involving users with overlapping or conflicting permissions. Our findings reveal that even state-of-the-art LLMs struggle significantly to maintain compliance with role-based structures, even with explicit instructions, with their performance degrades further when navigating interactions involving two or more conflicting permissions. Specifically, even \textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}. This demonstrates a critical limitation in LLMs' complex rule following and compositional reasoning capabilities beyond standard factual or STEM-based benchmarks, opening up a new paradigm for evaluating their fitness for practical, structured environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models</title>
<link>https://arxiv.org/abs/2505.19676</link>
<guid>https://arxiv.org/abs/2505.19676</guid>
<content:encoded><![CDATA[
arXiv:2505.19676v2 Announce Type: replace 
Abstract: Empirical methods to examine the capability of Large Language Models (LLMs) to use Automated Theorem Prover (ATP) reasoning strategies are studied. We evaluate the performance of State of the Art models from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop methods for assessing LLM response accuracy and correct answer correlation.
  Our results show that progress in improving LLM reasoning abilities has stalled over the nine month period. By tracking completion tokens, we show that almost all improvement in reasoning ability since GPT-4 was released can be attributed to either hidden system prompts or the training of models to automatically use generic Chain of Thought prompting strategies. Among the ATP reasoning strategies tried, we found that current frontier LLMs are best able to follow the bottom-up (also known as forward-chaining) strategy. A low positive correlation was found between an LLM response containing correct reasoning and arriving at the correct conclusion.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.01391</link>
<guid>https://arxiv.org/abs/2506.01391</guid>
<content:encoded><![CDATA[
arXiv:2506.01391v2 Announce Type: replace 
Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load</title>
<link>https://arxiv.org/abs/2506.08026</link>
<guid>https://arxiv.org/abs/2506.08026</guid>
<content:encoded><![CDATA[
arXiv:2506.08026v2 Announce Type: replace 
Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling framework for real-time market prediction under uncertain workloads. Motivated by the strict latency demands in high-frequency financial systems, TIP-Search dynamically selects a deep learning model from a heterogeneous pool, aiming to maximize predictive accuracy while satisfying per-task deadline constraints. Our approach profiles latency and generalization performance offline, then performs online task-aware selection without relying on explicit input domain labels. We evaluate TIP-Search on three real-world limit order book datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms static baselines with up to 8.5% improvement in accuracy and 100% deadline satisfaction. Our results highlight the effectiveness of TIP-Search in robust low-latency financial inference under uncertainty.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.09250</link>
<guid>https://arxiv.org/abs/2506.09250</guid>
<content:encoded><![CDATA[
arXiv:2506.09250v2 Announce Type: replace 
Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments risk exceeding model output token limits, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</title>
<link>https://arxiv.org/abs/2307.15220</link>
<guid>https://arxiv.org/abs/2307.15220</guid>
<content:encoded><![CDATA[
arXiv:2307.15220v5 Announce Type: replace-cross 
Abstract: Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLP's potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The [training code](https://github.com/CAMMA-public/PeskaVLP) and [weights](https://github.com/CAMMA-public/SurgVLP) are public.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring news intent and its application: A theory-driven approach</title>
<link>https://arxiv.org/abs/2312.16490</link>
<guid>https://arxiv.org/abs/2312.16490</guid>
<content:encoded><![CDATA[
arXiv:2312.16490v2 Announce Type: replace-cross 
Abstract: Understanding the intent behind information is crucial. However, news as a medium of public discourse still lacks a structured investigation of perceived news intent and its application. To advance this field, this paper reviews interdisciplinary studies on intentional action and introduces a conceptual deconstruction-based news intent understanding framework (NINT). This framework identifies the components of intent, facilitating a structured representation of news intent and its applications. Building upon NINT, we contribute a new intent perception dataset. Moreover, we investigate the potential of intent assistance on news-related tasks, such as significant improvement (+2.2% macF1) in the task of fake news detection. We hope that our findings will provide valuable insights into action-based intent cognition and computational social science.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InkSight: Offline-to-Online Handwriting Conversion by Teaching Vision-Language Models to Read and Write</title>
<link>https://arxiv.org/abs/2402.05804</link>
<guid>https://arxiv.org/abs/2402.05804</guid>
<content:encoded><![CDATA[
arXiv:2402.05804v4 Announce Type: replace-cross 
Abstract: Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in a vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice that is still favored by a vast majority. Our work InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore, it generalizes beyond its training domain into simple sketches. Our human evaluation reveals that 87% of the samples produced by our model on the challenging HierText dataset are considered as a valid tracing of the input image and 67% look like a pen trajectory traced by a human.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality-aware Masked Diffusion Transformer for Enhanced Music Generation</title>
<link>https://arxiv.org/abs/2405.15863</link>
<guid>https://arxiv.org/abs/2405.15863</guid>
<content:encoded><![CDATA[
arXiv:2405.15863v4 Announce Type: replace-cross 
Abstract: Text-to-music (TTM) generation, which converts textual descriptions into audio, opens up innovative avenues for multimedia creation. Achieving high quality and diversity in this process demands extensive, high-quality data, which are often scarce in available datasets. Most open-source datasets frequently suffer from issues like low-quality waveforms and low text-audio consistency, hindering the advancement of music generation models. To address these challenges, we propose a novel quality-aware training paradigm for generating high-quality, high-musicality music from large-scale, quality-imbalanced datasets. Additionally, by leveraging unique properties in the latent space of musical signals, we adapt and implement a masked diffusion transformer (MDT) model for the TTM task, showcasing its capacity for quality control and enhanced musicality. Furthermore, we introduce a three-stage caption refinement approach to address low-quality captions' issue. Experiments show state-of-the-art (SOTA) performance on benchmark datasets including MusicCaps and the Song-Describer Dataset with both objective and subjective metrics. Demo audio samples are available at https://qa-mdt.github.io/, code and pretrained checkpoints are open-sourced at https://github.com/ivcylc/OpenMusic.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG</title>
<link>https://arxiv.org/abs/2406.11147</link>
<guid>https://arxiv.org/abs/2406.11147</guid>
<content:encoded><![CDATA[
arXiv:2406.11147v3 Announce Type: replace-cross 
Abstract: Although LLMs have shown promising potential in vulnerability detection, this study reveals their limitations in distinguishing between vulnerable and similar-but-benign patched code (only 0.06 - 0.14 accuracy). It shows that LLMs struggle to capture the root causes of vulnerabilities during vulnerability detection. To address this challenge, we propose enhancing LLMs with multi-dimensional vulnerability knowledge distilled from historical vulnerabilities and fixes. We design a novel knowledge-level Retrieval-Augmented Generation framework Vul-RAG, which improves LLMs with an accuracy increase of 16% - 24% in identifying vulnerable and patched code. Additionally, vulnerability knowledge generated by Vul-RAG can further (1) serve as high-quality explanations to improve manual detection accuracy (from 60% to 77%), and (2) detect 10 previously-unknown bugs in the recent Linux kernel release with 6 assigned CVEs.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title>
<link>https://arxiv.org/abs/2406.11423</link>
<guid>https://arxiv.org/abs/2406.11423</guid>
<content:encoded><![CDATA[
arXiv:2406.11423v4 Announce Type: replace-cross 
Abstract: Proactive content moderation requires platforms to rapidly and continuously evaluate the credibility of websites. Leveraging the direct and indirect paths users follow to unreliable websites, we develop a website credibility classification and discovery system that integrates both webgraph and large-scale social media contexts. We additionally introduce the concept of dredge words, terms or phrases for which unreliable domains rank highly on search engines, and provide the first exploration of their usage on social media. Our graph neural networks that combine webgraph and social media contexts generate to state-of-the-art results in website credibility classification and significantly improves the top-k identification of unreliable domains. Additionally, we release a novel dataset of dredge words, highlighting their strong connections to both social media and online commerce platforms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization</title>
<link>https://arxiv.org/abs/2406.18379</link>
<guid>https://arxiv.org/abs/2406.18379</guid>
<content:encoded><![CDATA[
arXiv:2406.18379v3 Announce Type: replace-cross 
Abstract: Binary malware summarization aims to automatically generate human-readable descriptions of malware behaviors from executable files, facilitating tasks like malware cracking and detection. Previous methods based on Large Language Models (LLMs) have shown great promise. However, they still face significant issues, including poor usability, inaccurate explanations,and incomplete summaries, primarily due to the obscure pseudocode structure and the lack of malware training summaries. Further, calling relationships between functions, which involve the rich interactions within a binary malware, remain largely underexplored. To this end, we propose MALSIGHT, a novel code summarization framework that can iteratively generate descriptions of binary malware by exploring malicious source code and benign pseudocode. Specifically, we construct the first malware summary dataset, MalS and MalP, using an LLM and manually refine this dataset with human effort. At the training stage, we tune our proposed MalT5, a novel LLM-based code model, on the MalS and benign pseudocode datasets. Then, at the test stage, we iteratively feed the pseudocode functions into MalT5 to obtain the summary. Such a procedure facilitates the understanding of pseudocode structure and captures the intricate interactions between functions, thereby benefiting summaries' usability, accuracy, and completeness. Additionally, we propose a novel evaluation benchmark, BLEURT-sum, to measure the quality of summaries. Experiments on three datasets show the effectiveness of the proposed MALSIGHT. Notably, our proposed MalT5, with only 0.77B parameters, delivers comparable performance to much larger Code-Llama.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE</title>
<link>https://arxiv.org/abs/2409.05144</link>
<guid>https://arxiv.org/abs/2409.05144</guid>
<content:encoded><![CDATA[
arXiv:2409.05144v3 Announce Type: replace-cross 
Abstract: Alpha factor mining aims to discover investment signals from the historical financial market data, which can be used to predict asset returns and gain excess profits. Powerful deep learning methods for alpha factor mining lack interpretability, making them unacceptable in the risk-sensitive real markets. Formulaic alpha factors are preferred for their interpretability, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining. Herein, a novel reinforcement learning algorithm based on the well-known REINFORCE algorithm is proposed. REINFORCE employs Monte Carlo sampling to estimate the policy gradient-yielding unbiased but high variance estimates. The minimal environmental variability inherent in the underlying state transition function, which adheres to the Dirac distribution, can help alleviate this high variance issue, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Evaluations on real assets data indicate the proposed algorithm boosts correlation with returns by 3.83\%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is the Right Notion of Distance between Predict-then-Optimize Tasks?</title>
<link>https://arxiv.org/abs/2409.06997</link>
<guid>https://arxiv.org/abs/2409.06997</guid>
<content:encoded><![CDATA[
arXiv:2409.06997v2 Announce Type: replace-cross 
Abstract: Comparing datasets is a fundamental task in machine learning, essential for various learning paradigms-from evaluating train and test datasets for model generalization to using dataset similarity for detecting data drift. While traditional notions of dataset distances offer principled measures of similarity, their utility has largely been assessed through prediction error minimization. However, in Predict-then-Optimize (PtO) frameworks, where predictions serve as inputs for downstream optimization tasks, model performance is measured through decision regret rather than prediction error. In this work, we propose OTD$^3$ (Optimal Transport Decision-aware Dataset Distance), a novel dataset distance that incorporates downstream decisions in addition to features and labels. We show that traditional feature-label distances lack informativeness in PtO settings, while OTD$^3$ more effectively captures adaptation success. We also derive a PtO-specific adaptation bound based on this distance. Empirically, we show that our proposed distance accurately predicts model transferability across three different PtO tasks from the literature. The code is available at https://github.com/paularodr/OTD3.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping</title>
<link>https://arxiv.org/abs/2409.11316</link>
<guid>https://arxiv.org/abs/2409.11316</guid>
<content:encoded><![CDATA[
arXiv:2409.11316v4 Announce Type: replace-cross 
Abstract: Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment</title>
<link>https://arxiv.org/abs/2409.17655</link>
<guid>https://arxiv.org/abs/2409.17655</guid>
<content:encoded><![CDATA[
arXiv:2409.17655v2 Announce Type: replace-cross 
Abstract: Current service robots suffer from limited natural language communication abilities, heavy reliance on predefined commands, ongoing human intervention, and, most notably, a lack of proactive collaboration awareness in human-populated environments. This results in narrow applicability and low utility. In this paper, we introduce AssistantX, an LLM-powered proactive assistant designed for autonomous operation in realworld scenarios with high accuracy. AssistantX employs a multi-agent framework consisting of 4 specialized LLM agents, each dedicated to perception, planning, decision-making, and reflective review, facilitating advanced inference capabilities and comprehensive collaboration awareness, much like a human assistant by your side. We built a dataset of 210 real-world tasks to validate AssistantX, which includes instruction content and status information on whether relevant personnel are available. Extensive experiments were conducted in both text-based simulations and a real office environment over the course of a month and a half. Our experiments demonstrate the effectiveness of the proposed framework, showing that AssistantX can reactively respond to user instructions, actively adjust strategies to adapt to contingencies, and proactively seek assistance from humans to ensure successful task completion. More details and videos can be found at https://assistantx-agent. github.io/AssistantX/.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Signatures of Compositionality Across a Language Model's Lifetime</title>
<link>https://arxiv.org/abs/2410.01444</link>
<guid>https://arxiv.org/abs/2410.01444</guid>
<content:encoded><![CDATA[
arXiv:2410.01444v5 Announce Type: replace-cross 
Abstract: By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior</title>
<link>https://arxiv.org/abs/2410.21264</link>
<guid>https://arxiv.org/abs/2410.21264</guid>
<content:encoded><![CDATA[
arXiv:2410.21264v2 Announce Type: replace-cross 
Abstract: We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English</title>
<link>https://arxiv.org/abs/2412.04726</link>
<guid>https://arxiv.org/abs/2412.04726</guid>
<content:encoded><![CDATA[
arXiv:2412.04726v3 Announce Type: replace-cross 
Abstract: Despite large language models (LLMs) being known to exhibit bias against non-standard language varieties, there are no known labelled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine LLMs (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results show that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), in comparison with en-IN, particularly for sarcasm classification. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE dataset is publicly available at: https://huggingface.co/ datasets/unswnlporg/BESSTIE.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension</title>
<link>https://arxiv.org/abs/2412.11906</link>
<guid>https://arxiv.org/abs/2412.11906</guid>
<content:encoded><![CDATA[
arXiv:2412.11906v2 Announce Type: replace-cross 
Abstract: Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms. With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines. However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon). To address these limitations, we introduce a multimodal \textbf{Punch}line comprehension \textbf{Bench}mark, named \textbf{PunchBench}, which is tailored for accurate and comprehensive evaluation of punchline comprehension. To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions. To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains. On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension. To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones. SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Greedy Best-First Search with a Bound on Expansions Relative to Sequential Search</title>
<link>https://arxiv.org/abs/2412.12221</link>
<guid>https://arxiv.org/abs/2412.12221</guid>
<content:encoded><![CDATA[
arXiv:2412.12221v2 Announce Type: replace-cross 
Abstract: Parallelization of non-admissible search algorithms such as GBFS poses a challenge because straightforward parallelization can result in search behavior which significantly deviates from sequential search. Previous work proposed PUHF, a parallel search algorithm which is constrained to only expand states that can be expanded by some tie-breaking strategy for GBFS. We show that despite this constraint, the number of states expanded by PUHF is not bounded by a constant multiple of the number of states expanded by sequential GBFS with the worst-case tie-breaking strategy. We propose and experimentally evaluate One Bench At a Time (OBAT), a parallel greedy search which guarantees that the number of states expanded is within a constant factor of the number of states expanded by sequential GBFS with some tie-breaking policy.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Critic Augmentation for Hierarchical Multi-Agent EV Charging Control</title>
<link>https://arxiv.org/abs/2412.18047</link>
<guid>https://arxiv.org/abs/2412.18047</guid>
<content:encoded><![CDATA[
arXiv:2412.18047v4 Announce Type: replace-cross 
Abstract: The advanced bidirectional EV charging and discharging technology, aimed at supporting grid stability and emergency operations, has driven a growing interest in workplace applications. It not only reduces electricity expenses but also enhances the resilience in handling practical matters, such as peak power limitation, fluctuating energy prices, and unpredictable EV departures. Considering these factors systematically can benefit energy efficiency in office buildings and for EV users simultaneously. To employ AI to address these issues, we propose HUCA, a novel real-time charging control for regulating energy demands for both the building and EVs. HUCA employs hierarchical actor-critic networks to dynamically reduce electricity costs in buildings, accounting for the needs of EV charging in the dynamic pricing scenario. To tackle the uncertain EV departures, we introduce a new critic augmentation to account for departure uncertainties in evaluating the charging decisions, while maintaining the robustness of the charging control. Experiments on real-world electricity datasets under both simulated certain and uncertain departure scenarios demonstrate that HUCA outperforms baselines in terms of total electricity costs while maintaining competitive performance in fulfilling EV charging requirements. A case study also manifests that HUCA effectively balances energy supply between the building and EVs based on real-time information, showcasing its potential as a key AI-driven solution for vehicle charging control.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment</title>
<link>https://arxiv.org/abs/2501.02621</link>
<guid>https://arxiv.org/abs/2501.02621</guid>
<content:encoded><![CDATA[
arXiv:2501.02621v2 Announce Type: replace-cross 
Abstract: Decoding human activity from EEG signals has long been a popular research topic. While recent studies have increasingly shifted focus from single-subject to cross-subject analysis, few have explored the model's ability to perform zero-shot predictions on EEG signals from previously unseen subjects. This research aims to investigate whether deep learning methods can capture subject-independent semantic information inherent in human EEG signals. Such insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the model's robustness against subject-specific temporal biases, and on the other, they significantly enhance the generalizability of downstream tasks. We employ Large Language Models (LLMs) as denoising agents to extract subject-independent semantic features from noisy EEG signals. Experimental results, including ablation studies, highlight the pivotal role of LLMs in decoding subject-independent semantic information from noisy EEG data. We hope our findings will contribute to advancing BCI research and assist both academia and industry in applying EEG signals to a broader range of applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Laboratory: Using LLM Agents as Research Assistants</title>
<link>https://arxiv.org/abs/2501.04227</link>
<guid>https://arxiv.org/abs/2501.04227</guid>
<content:encoded><![CDATA[
arXiv:2501.04227v2 Announce Type: replace-cross 
Abstract: Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models</title>
<link>https://arxiv.org/abs/2501.05478</link>
<guid>https://arxiv.org/abs/2501.05478</guid>
<content:encoded><![CDATA[
arXiv:2501.05478v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</title>
<link>https://arxiv.org/abs/2501.10970</link>
<guid>https://arxiv.org/abs/2501.10970</guid>
<content:encoded><![CDATA[
arXiv:2501.10970v3 Announce Type: replace-cross 
Abstract: The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors</title>
<link>https://arxiv.org/abs/2501.18045</link>
<guid>https://arxiv.org/abs/2501.18045</guid>
<content:encoded><![CDATA[
arXiv:2501.18045v3 Announce Type: replace-cross 
Abstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Voting and Deliberation with Algorithms: Field Insights from vTaiwan and Kultur Komitee</title>
<link>https://arxiv.org/abs/2502.05017</link>
<guid>https://arxiv.org/abs/2502.05017</guid>
<content:encoded><![CDATA[
arXiv:2502.05017v2 Announce Type: replace-cross 
Abstract: Democratic processes increasingly aim to integrate large-scale voting with face-to-face deliberation, addressing the challenge of reconciling individual preferences with collective decision-making. This work introduces new methods that use algorithms and computational tools to bridge online voting with face-to-face deliberation, tested in two real-world scenarios: Kultur Komitee 2024 (KK24) and vTaiwan. These case studies highlight the practical applications and impacts of the proposed methods.
  We present three key contributions: (1) Preference-based Clustering for Deliberation (PCD), which enables both in-depth and broad discussions in deliberative settings by computing homogeneous and heterogeneous group compositions with balanced and adjustable group sizes; (2) Human-in-the-loop MES, a practical method that enhances the Method of Equal Shares (MES) algorithm with real-time digital feedback. This builds algorithmic trust by giving participants full control over how much decision-making is delegated to the voting aggregation algorithm as compared to deliberation; and (3) the ReadTheRoom deliberation method, which uses opinion space mapping to identify agreement and divergence, along with spectrum-based preference visualisation to track opinion shifts during deliberation. This approach enhances transparency by clarifying collective sentiment and fosters collaboration by encouraging participants to engage constructively with differing perspectives. By introducing these actionable frameworks, this research extends in-person deliberation with scalable digital methods that address the complexities of modern decision-making in participatory processes.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAROCE: A Neural Algorithmic Reasoner Framework for Online Complex Event Detection</title>
<link>https://arxiv.org/abs/2502.07250</link>
<guid>https://arxiv.org/abs/2502.07250</guid>
<content:encoded><![CDATA[
arXiv:2502.07250v2 Announce Type: replace-cross 
Abstract: Modern machine learning models excel at detecting individual actions, objects, or scene attributes from short, local observations. However, many real-world tasks, such as in smart cities and healthcare, require reasoning over complex events (CEs): (spatio)temporal, rule-governed patterns of short-term atomic events (AEs) that reflect high-level understanding and critical changes in the environment. These CEs are difficult to detect online: they are often rare, require long-range reasoning over noisy sensor data, must generalize rules beyond fixed-length traces, and suffer from limited real-world datasets due to the high annotation burden. We propose NAROCE, a Neural Algorithmic Reasoning framework for Online CE detection that separates the task into two stages: (i) learning CE rules from large-scale, low-cost pseudo AE concept traces generated by simulators or LLMs, and (ii) training an adapter to map real sensor data into the learned reasoning space using fewer labeled sensor samples. Experiments show that NAROCE outperforms the strongest baseline in accuracy, generalization to longer, unseen sequences, and data efficiency, achieving comparable performance with less than half the labeled data. These results suggest that decoupling CE rule learning from raw sensor inputs improves both data efficiency and robustness.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11425</link>
<guid>https://arxiv.org/abs/2502.11425</guid>
<content:encoded><![CDATA[
arXiv:2502.11425v2 Announce Type: replace-cross 
Abstract: Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Topology Optimization using Modulated Neural Fields</title>
<link>https://arxiv.org/abs/2502.13174</link>
<guid>https://arxiv.org/abs/2502.13174</guid>
<content:encoded><![CDATA[
arXiv:2502.13174v2 Announce Type: replace-cross 
Abstract: Topology optimization (TO) is a family of computational methods that derive near-optimal geometries from formal problem descriptions. Despite their success, established TO methods are limited to generating single solutions, restricting the exploration of alternative designs. To address this limitation, we introduce Topology Optimization using Modulated Neural Fields (TOM) - a data-free method that trains a neural network to generate structurally compliant shapes and explores diverse solutions through an explicit diversity constraint. The network is trained with a solver-in-the-loop, optimizing the material distribution in each iteration. The trained model produces diverse shapes that closely adhere to the design requirements. We validate TOM on 2D and 3D TO problems. Our results show that TOM generates more diverse solutions than any previous method, all while maintaining near-optimality and without relying on a dataset. These findings open new avenues for engineering and design, offering enhanced flexibility and innovation in structural optimization.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Geo-Culturally Grounded LLM Generations</title>
<link>https://arxiv.org/abs/2502.13497</link>
<guid>https://arxiv.org/abs/2502.13497</guid>
<content:encoded><![CDATA[
arXiv:2502.13497v3 Announce Type: replace-cross 
Abstract: Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs' ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs' cultural awareness.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PredictaBoard: Benchmarking LLM Score Predictability</title>
<link>https://arxiv.org/abs/2502.14445</link>
<guid>https://arxiv.org/abs/2502.14445</guid>
<content:encoded><![CDATA[
arXiv:2502.14445v2 Announce Type: replace-cross 
Abstract: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardware-Friendly Static Quantization Method for Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2502.15077</link>
<guid>https://arxiv.org/abs/2502.15077</guid>
<content:encoded><![CDATA[
arXiv:2502.15077v3 Announce Type: replace-cross 
Abstract: Diffusion Transformers for video generation have gained significant research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors. In this paper, we propose a novel method for the post-training quantization of OpenSora\cite{opensora}, a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target Speaker Extraction through Comparing Noisy Positive and Negative Audio Enrollments</title>
<link>https://arxiv.org/abs/2502.16611</link>
<guid>https://arxiv.org/abs/2502.16611</guid>
<content:encoded><![CDATA[
arXiv:2502.16611v2 Announce Type: replace-cross 
Abstract: Target speaker extraction focuses on isolating a specific speaker's voice from an audio mixture containing multiple speakers. To provide information about the target speaker's identity, prior works have utilized clean audio samples as conditioning inputs. However, such clean audio examples are not always readily available. For instance, obtaining a clean recording of a stranger's voice at a cocktail party without leaving the noisy environment is generally infeasible. Limited prior research has explored extracting the target speaker's characteristics from noisy enrollments, which may contain overlapping speech from interfering speakers. In this work, we explore a novel enrollment strategy that encodes target speaker information from the noisy enrollment by comparing segments where the target speaker is talking (Positive Enrollments) with segments where the target speaker is silent (Negative Enrollments). Experiments show the effectiveness of our model architecture, which achieves over 2.1 dB higher SI-SNRi compared to prior works in extracting the monaural speech from the mixture of two speakers. Additionally, the proposed two-stage training strategy accelerates convergence, reducing the number of optimization steps required to reach 3 dB SNR by 60\%. Overall, our method achieves state-of-the-art performance in the monaural target speaker extraction conditioned on noisy enrollments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification</title>
<link>https://arxiv.org/abs/2502.17421</link>
<guid>https://arxiv.org/abs/2502.17421</guid>
<content:encoded><![CDATA[
arXiv:2502.17421v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAE-V: Interpreting Multimodal Models for Enhanced Alignment</title>
<link>https://arxiv.org/abs/2502.17514</link>
<guid>https://arxiv.org/abs/2502.17514</guid>
<content:encoded><![CDATA[
arXiv:2502.17514v2 Announce Type: replace-cross 
Abstract: With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Shaping to Mitigate Reward Hacking in RLHF</title>
<link>https://arxiv.org/abs/2502.18770</link>
<guid>https://arxiv.org/abs/2502.18770</guid>
<content:encoded><![CDATA[
arXiv:2502.18770v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR, and the Work done during the internship at StepFun by Jiayi Fu.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Selection Format on LLM Performance</title>
<link>https://arxiv.org/abs/2503.06926</link>
<guid>https://arxiv.org/abs/2503.06926</guid>
<content:encoded><![CDATA[
arXiv:2503.06926v2 Announce Type: replace-cross 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints</title>
<link>https://arxiv.org/abs/2503.08669</link>
<guid>https://arxiv.org/abs/2503.08669</guid>
<content:encoded><![CDATA[
arXiv:2503.08669v2 Announce Type: replace-cross 
Abstract: As language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline SOPBench with: (1) executable environments containing 167 tools/functions across seven customer service domains with service-specific SOPs and rule-based verifiers, (2) an automated test generation framework producing over 900 verified test cases, and (3) an automated evaluation framework to rigorously assess agent adherence from multiple dimensions. Our approach transforms each service-specific SOP code program into a directed graph of executable functions and requires agents to call these functions based on natural language SOP descriptions. The original code serves as oracle rule-based verifiers to assess compliance, reducing reliance on manual annotations and LLM-based evaluations. We evaluate 18 leading models, and results show the task is challenging even for top-tier models (like GPT-4o, Claude-3.7-Sonnet), with variances across domains. Reasoning models like o4-mini-high show superiority while other powerful models perform less effectively (pass rates of 30%-50%), and small models (7B, 8B) perform significantly worse. Additionally, language agents can be easily jailbroken to overlook SOPs and constraints. Code, data, and over 24k agent trajectories are released at https://github.com/Leezekun/SOPBench.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction Sets for Deep Generative Models via Reduction to Conformal Regression</title>
<link>https://arxiv.org/abs/2503.10512</link>
<guid>https://arxiv.org/abs/2503.10512</guid>
<content:encoded><![CDATA[
arXiv:2503.10512v2 Announce Type: replace-cross 
Abstract: We consider the problem of generating valid and small prediction sets by sampling outputs (e.g., software code and natural language text) from a black-box deep generative model for a given input (e.g., textual prompt). The validity of a prediction set is determined by a user-defined binary admissibility function depending on the target application. For example, requiring at least one program in the set to pass all test cases in code generation application. To address this problem, we develop a simple and effective conformal inference algorithm referred to as Generative Prediction Sets (GPS). Given a set of calibration examples and black-box access to a deep generative model, GPS can generate prediction sets with provable guarantees. The key insight behind GPS is to exploit the inherent structure within the distribution over the minimum number of samples needed to obtain an admissible output to develop a simple conformal regression approach over the minimum number of samples. Experiments on multiple datasets for code and math word problems using different large language models demonstrate the efficacy of GPS over state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Unbiased Multi-Instance Learning via Balanced Fine-Grained Positive-Unlabeled Learning</title>
<link>https://arxiv.org/abs/2503.13562</link>
<guid>https://arxiv.org/abs/2503.13562</guid>
<content:encoded><![CDATA[
arXiv:2503.13562v2 Announce Type: replace-cross 
Abstract: In real-world applications, it is often challenging to detect anomalous samples when the anomalous information they contain is extremely limited. In such cases, both macro-level and micro-level detection using multi-instance learning (MIL) encounter significant difficulties. The former struggles because normal and anomalous samples are highly similar and hard to distinguish at the macro level, while the latter is limited by the lack of labels at the micro level. In MIL, micro-level labels are inferred from macro-level labels, which can lead to severe bias. Moreover, the more imbalanced the distribution between normal and anomalous samples, the more pronounced these limitations become. In this study, we observe that the MIL problem can be elegantly transformed into a fine-grained Positive-Unlabeled (PU) learning problem. This transformation allows us to address the imbalance issue in an unbiased manner using a micro-level balancing mechanism. To this end, we propose a novel framework-Balanced Fine-Grained Positive-Unlabeled (BFGPU)-based on rigorous theoretical foundations to address the challenges above. Extensive experiments on both public and real-world datasets demonstrate the effectiveness of BFGPU, which outperforms existing methods, even in extreme scenarios where both macro and micro-level distributions are highly imbalanced. The code is open-sourced at https://github.com/BFGPU/BFGPU.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks</title>
<link>https://arxiv.org/abs/2503.16974</link>
<guid>https://arxiv.org/abs/2503.16974</guid>
<content:encoded><![CDATA[
arXiv:2503.16974v3 Announce Type: replace-cross 
Abstract: This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&amp;As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Backfiring Effect of Weak AI Safety Regulation</title>
<link>https://arxiv.org/abs/2503.20848</link>
<guid>https://arxiv.org/abs/2503.20848</guid>
<content:encoded><![CDATA[
arXiv:2503.20848v2 Announce Type: replace-cross 
Abstract: Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model that explores the interactions between safety regulation, the general-purpose AI creators, and domain specialists--those who adapt the technology for specific applications. Our analysis examines how different regulatory measures, targeting different parts of the AI development chain, affect the outcome of this game. In particular, we assume AI technology is characterized by two key attributes: safety and performance. The regulator first sets a minimum safety standard that applies to one or both players, with strict penalties for non-compliance. The general-purpose creator then invests in the technology, establishing its initial safety and performance levels. Next, domain specialists refine the AI for their specific use cases, updating the safety and performance levels and taking the product to market. The resulting revenue is then distributed between the specialist and generalist through a revenue-sharing parameter. Our analysis reveals two key insights: First, weak safety regulation imposed predominantly on domain specialists can backfire. While it might seem logical to regulate AI use cases, our analysis shows that weak regulations targeting domain specialists alone can unintentionally reduce safety. This effect persists across a wide range of settings. Second, in sharp contrast to the previous finding, we observe that stronger, well-placed regulation can in fact mutually benefit all players subjected to it. When regulators impose appropriate safety standards on both general-purpose AI creators and domain specialists, the regulation functions as a commitment device, leading to safety and performance gains, surpassing what is achieved under no regulation or regulating one player alone.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal Learning of Brain Dynamics from fMRI Using Frequency-Specific Multi-Band Attention for Cognitive and Psychiatric Applications</title>
<link>https://arxiv.org/abs/2503.23394</link>
<guid>https://arxiv.org/abs/2503.23394</guid>
<content:encoded><![CDATA[
arXiv:2503.23394v2 Announce Type: replace-cross 
Abstract: Understanding how the brain's complex nonlinear dynamics give rise to cognitive function remains a central challenge in neuroscience. While brain functional dynamics exhibits scale-free and multifractal properties across temporal scales, conventional neuroimaging analytics assume linearity and stationarity, failing to capture frequency-specific neural computations. Here, we introduce Multi-Band Brain Net (MBBN), the first transformer-based framework to explicitly model frequency-specific spatiotemporal brain dynamics from fMRI. MBBN integrates biologically-grounded frequency decomposition with multi-band self-attention mechanisms, enabling discovery of previously undetectable frequency-dependent network interactions. Trained on 49,673 individuals across three large-scale cohorts (UK Biobank, ABCD, ABIDE), MBBN sets a new state-of-the-art in predicting psychiatric and cognitive outcomes (depression, ADHD, ASD), showing particular strength in classification tasks with up to 52.5\% higher AUROC and provides a novel framework for predicting cognitive intelligence scores. Frequency-resolved analyses uncover disorder-specific signatures: in ADHD, high-frequency fronto-sensorimotor connectivity is attenuated and opercular somatosensory nodes emerge as dynamic hubs; in ASD, orbitofrontal-somatosensory circuits show focal high-frequency disruption together with enhanced ultra-low-frequency coupling between the temporo-parietal junction and prefrontal cortex. By integrating scale-aware neural dynamics with deep learning, MBBN delivers more accurate and interpretable biomarkers, opening avenues for precision psychiatry and developmental neuroscience.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Cost-Aware Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.16005</link>
<guid>https://arxiv.org/abs/2504.16005</guid>
<content:encoded><![CDATA[
arXiv:2504.16005v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automatic prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach</title>
<link>https://arxiv.org/abs/2505.00039</link>
<guid>https://arxiv.org/abs/2505.00039</guid>
<content:encoded><![CDATA[
arXiv:2505.00039v3 Announce Type: replace-cross 
Abstract: This article proposes an adaptation of Graph Retrieval-Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms. Legal texts are characterized by a predefined hierarchical structure, an extensive network of references and a continuous evolution through multiple temporal versions. This temporal dynamism poses a significant challenge for standard AI systems, demanding a deterministic representation of the law at any given point in time. To address this, our approach grounds the knowledge graph construction in a formal, FRBRoo-inspired model that distinguishes abstract legal works from their concrete textual expressions. We introduce a multi-layered representation of Temporal Versions (capturing date-specific changes) and Language Versions (capturing linguistic variations). By modeling normative evolution as a precise sequence of these versioned entities, we enable the construction of a knowledge graph that serves as a verifiable "ground truth". This allows Large Language Models to generate responses based on accurate, context-aware, and point-in-time correct legal information, overcoming the risk of temporal inaccuracies. Through a detailed analysis of this formal Graph RAG approach and its application to legal norm datasets, this article aims to advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective and reliable systems in legal research, legislative analysis, and decision support.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convert Language Model into a Value-based Strategic Planner</title>
<link>https://arxiv.org/abs/2505.06987</link>
<guid>https://arxiv.org/abs/2505.06987</guid>
<content:encoded><![CDATA[
arXiv:2505.06987v4 Announce Type: replace-cross 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^3$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning</title>
<link>https://arxiv.org/abs/2505.07819</link>
<guid>https://arxiv.org/abs/2505.07819</guid>
<content:encoded><![CDATA[
arXiv:2505.07819v2 Announce Type: replace-cross 
Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\textbf{Triply-Hierarchical Diffusion Policy}~(\textbf{H$^{\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\mathbf{+27.5\%}$ average relative improvement over baselines across $\mathbf{44}$ simulation tasks and achieves superior performance in $\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</title>
<link>https://arxiv.org/abs/2505.07897</link>
<guid>https://arxiv.org/abs/2505.07897</guid>
<content:encoded><![CDATA[
arXiv:2505.07897v2 Announce Type: replace-cross 
Abstract: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner</title>
<link>https://arxiv.org/abs/2505.11404</link>
<guid>https://arxiv.org/abs/2505.11404</guid>
<content:encoded><![CDATA[
arXiv:2505.11404v3 Announce Type: replace-cross 
Abstract: Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose Patho-CLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both Patho-CLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.11633</link>
<guid>https://arxiv.org/abs/2505.11633</guid>
<content:encoded><![CDATA[
arXiv:2505.11633v2 Announce Type: replace-cross 
Abstract: This demo paper reports on a new workflow \textit{GhostWriter} that combines the use of Large Language Models and Knowledge Graphs (semantic artifacts) to support navigation through collections. Situated in the research area of Retrieval Augmented Generation, this specific workflow represents the creation of local and adaptable chatbots. Based on the tool-suite \textit{EverythingData} at the backend, \textit{GhostWriter} provides an interface that enables querying and ``chatting'' with a collection. Applied iteratively, the workflow supports the information needs of researchers when interacting with a collection of papers, whether it be to gain an overview, to learn more about a specific concept and its context, and helps the researcher ultimately to refine their research question in a controlled way. We demonstrate the workflow for a collection of articles from the \textit{method data analysis} journal published by GESIS -- Leibniz-Institute for the Social Sciences. We also point to further application areas.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement</title>
<link>https://arxiv.org/abs/2505.12368</link>
<guid>https://arxiv.org/abs/2505.12368</guid>
<content:encoded><![CDATA[
arXiv:2505.12368v2 Announce Type: replace-cross 
Abstract: Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations. To demonstrate our framework's utility, we train CaptureGuard on our generated data. This new model drastically reduces both false negative and false positive rates on our context-aware datasets while also generalizing effectively to external benchmarks, establishing a path toward more robust and practical prompt injection defenses.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.12442</link>
<guid>https://arxiv.org/abs/2505.12442</guid>
<content:encoded><![CDATA[
arXiv:2505.12442v3 Announce Type: replace-cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abacus: A Cost-Based Optimizer for Semantic Operator Systems</title>
<link>https://arxiv.org/abs/2505.14661</link>
<guid>https://arxiv.org/abs/2505.14661</guid>
<content:encoded><![CDATA[
arXiv:2505.14661v2 Announce Type: replace-cross 
Abstract: LLMs enable an exciting new class of data processing applications over large collections of unstructured documents. Several new programming frameworks have enabled developers to build these applications by composing them out of semantic operators: a declarative set of AI-powered data transformations with natural language specifications. These include LLM-powered maps, filters, joins, etc. used for document processing tasks such as information extraction, summarization, and more. While systems of semantic operators have achieved strong performance on benchmarks, they can be difficult to optimize. An optimizer for this setting must determine how to physically implement each semantic operator in a way that optimizes the system globally. Existing optimizers are limited in the number of optimizations they can apply, and most (if not all) cannot optimize system quality, cost, or latency subject to constraint(s) on the other dimensions. In this paper we present Abacus, an extensible, cost-based optimizer which searches for the best implementation of a semantic operator system given a (possibly constrained) optimization objective. Abacus estimates operator performance by leveraging a minimal set of validation examples and, if available, prior beliefs about operator performance. We evaluate Abacus on document processing workloads in the biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering (MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2% better quality and up to 23.6x lower cost and 4.2x lower latency than the next best system.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion</title>
<link>https://arxiv.org/abs/2505.14719</link>
<guid>https://arxiv.org/abs/2505.14719</guid>
<content:encoded><![CDATA[
arXiv:2505.14719v2 Announce Type: replace-cross 
Abstract: The combination of Spiking Neural Networks (SNNs) with Vision Transformer architectures has garnered significant attention due to their potential for energy-efficient and high-performance computing paradigms. However, a substantial performance gap still exists between SNN-based and ANN-based transformer architectures. While existing methods propose spiking self-attention mechanisms that are successfully combined with SNNs, the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting features from different image scales. In this paper, we address this issue and propose MSVIT. This novel spike-driven Transformer architecture firstly uses multi-scale spiking attention (MSSA) to enhance the capabilities of spiking attention blocks. We validate our approach across various main datasets. The experimental results show that MSVIT outperforms existing SNN-based models, positioning itself as a state-of-the-art solution among SNN-transformer architectures. The codes are available at https://github.com/Nanhu-AI-Lab/MSViT.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.20613</link>
<guid>https://arxiv.org/abs/2505.20613</guid>
<content:encoded><![CDATA[
arXiv:2505.20613v2 Announce Type: replace-cross 
Abstract: Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation</title>
<link>https://arxiv.org/abs/2505.21652</link>
<guid>https://arxiv.org/abs/2505.21652</guid>
<content:encoded><![CDATA[
arXiv:2505.21652v3 Announce Type: replace-cross 
Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to display the label on the cap, requires robust reasoning about object parts and their relationships with intended tasks. Despite recent advances in training general-purpose robot manipulation policies guided by language instructions, there is a notable lack of large-scale datasets for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels. In this work, we introduce PartInstruct, the first large-scale benchmark for training and evaluating fine-grained robot manipulation models using part-level instructions. PartInstruct comprises 513 object instances across 14 categories, each annotated with part-level information, and 1302 fine-grained manipulation tasks organized into 16 task classes. Our training set consists of over 10,000 expert demonstrations synthesized in a 3D simulator, where each demonstration is paired with a high-level task instruction, a chain of base part-based skill instructions, and ground-truth 3D information about the object and its parts. Additionally, we designed a comprehensive test suite to evaluate the generalizability of learned policies across new states, objects, and tasks. We evaluated several state-of-the-art robot manipulation approaches, including end-to-end vision-language policy learning and bi-level planning models for robot manipulation on our benchmark. The experimental results reveal that current models struggle to robustly ground part concepts and predict actions in 3D space, and face challenges when manipulating object parts in long-horizon tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing</title>
<link>https://arxiv.org/abs/2505.23145</link>
<guid>https://arxiv.org/abs/2505.23145</guid>
<content:encoded><![CDATA[
arXiv:2505.23145v2 Announce Type: replace-cross 
Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose FlowAlign, a novel inversion-free flow-based framework for consistent image editing with principled trajectory control. FlowAlign introduces a flow-matching loss as a regularization mechanism to promote smoother and more stable trajectories during the editing process. Notably, the flow-matching loss is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highlighting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating RLHF Training with Reward Variance Increase</title>
<link>https://arxiv.org/abs/2505.23247</link>
<guid>https://arxiv.org/abs/2505.23247</guid>
<content:encoded><![CDATA[
arXiv:2505.23247v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing local protein environments with atomistic foundation models</title>
<link>https://arxiv.org/abs/2505.23354</link>
<guid>https://arxiv.org/abs/2505.23354</guid>
<content:encoded><![CDATA[
arXiv:2505.23354v2 Announce Type: replace-cross 
Abstract: The local structure of a protein strongly impacts its function and interactions with other molecules. Therefore, a concise, informative representation of a local protein environment is essential for modeling and designing proteins and biomolecular interactions. However, these environments' extensive structural and chemical variability makes them challenging to model, and such representations remain under-explored. In this work, we propose a novel representation for a local protein environment derived from the intermediate features of atomistic foundation models (AFMs). We demonstrate that this embedding effectively captures both local structure (e.g., secondary motifs), and chemical features (e.g., amino-acid identity and protonation state). We further show that the AFM-derived representation space exhibits meaningful structure, enabling the construction of data-driven priors over the distribution of biomolecular environments. Finally, in the context of biomolecular NMR spectroscopy, we demonstrate that the proposed representations enable a first-of-its-kind physics-informed chemical shift predictor that achieves state-of-the-art accuracy. Our results demonstrate the surprising effectiveness of atomistic foundation models and their emergent representations for protein modeling beyond traditional molecular simulations. We believe this will open new lines of work in constructing effective functional representations for protein environments.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KGMark: A Diffusion Watermark for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.23873</link>
<guid>https://arxiv.org/abs/2505.23873</guid>
<content:encoded><![CDATA[
arXiv:2505.23873v2 Announce Type: replace-cross 
Abstract: Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMARK, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMARK properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMARK. Our code is available at https://github.com/phrara/kgmark.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs</title>
<link>https://arxiv.org/abs/2505.24120</link>
<guid>https://arxiv.org/abs/2505.24120</guid>
<content:encoded><![CDATA[
arXiv:2505.24120v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remain inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering. Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning. We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6% accuracy. This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.00555</link>
<guid>https://arxiv.org/abs/2506.00555</guid>
<content:encoded><![CDATA[
arXiv:2506.00555v2 Announce Type: replace-cross 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
<link>https://arxiv.org/abs/2506.00854</link>
<guid>https://arxiv.org/abs/2506.00854</guid>
<content:encoded><![CDATA[
arXiv:2506.00854v2 Announce Type: replace-cross 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EuroLLM-9B: Technical Report</title>
<link>https://arxiv.org/abs/2506.04079</link>
<guid>https://arxiv.org/abs/2506.04079</guid>
<content:encoded><![CDATA[
arXiv:2506.04079v2 Announce Type: replace-cross 
Abstract: This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.06290</link>
<guid>https://arxiv.org/abs/2506.06290</guid>
<content:encoded><![CDATA[
arXiv:2506.06290v2 Announce Type: replace-cross 
Abstract: High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms</title>
<link>https://arxiv.org/abs/2506.06499</link>
<guid>https://arxiv.org/abs/2506.06499</guid>
<content:encoded><![CDATA[
arXiv:2506.06499v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2506.07801</link>
<guid>https://arxiv.org/abs/2506.07801</guid>
<content:encoded><![CDATA[
arXiv:2506.07801v2 Announce Type: replace-cross 
Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for three key purposes: selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, achieving state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranking first according to the Friedman test among 19 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26% -- and data imbalance is a key factor for many text classification tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation</title>
<link>https://arxiv.org/abs/2506.09081</link>
<guid>https://arxiv.org/abs/2506.09081</guid>
<content:encoded><![CDATA[
arXiv:2506.09081v2 Announce Type: replace-cross 
Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Remarkable Robustness of LLMs: Stages of Inference?</title>
<link>https://arxiv.org/abs/2406.19384</link>
<guid>https://arxiv.org/abs/2406.19384</guid>
<content:encoded><![CDATA[
<div> investigation, Large Language Models, robustness, structural interventions, inference <br />
<br />
Summary: 
The study delves into the resilience of Large Language Models (LLMs) when subjected to structural interventions such as deleting and swapping adjacent layers during inference. Surprisingly, the models showcase a preservation of 72-95% of their original top-1 prediction accuracy without requiring any fine-tuning. It is observed that performance degradation varies across layers, with early and final layers being most affected while middle layers exhibit remarkable robustness. The research identifies four distinct stages of inference common across different model sizes and families. These stages include detokenization, feature engineering, prediction ensembling, and residual sharpening. By integrating behavioral and mechanistic evidence, the study offers a comprehensive framework for understanding the depth-dependent computations in LLMs. <div>
arXiv:2406.19384v3 Announce Type: replace-cross 
Abstract: We investigate the robustness of Large Language Models (LLMs) to structural interventions by deleting and swapping adjacent layers during inference. Surprisingly, models retain 72-95% of their original top-1 prediction accuracy without any fine-tuning. We find that performance degradation is not uniform across layers: interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers. This pattern of localized sensitivity motivates our hypothesis of four stages of inference, observed across diverse model families and sizes: (1) detokenization, where local context is integrated to lift raw token embeddings into higher-level representations; (2) feature engineering, where task- and entity-specific features are iteratively refined; (3) prediction ensembling, where hidden states are aggregated into plausible next-token predictions; and (4) residual sharpening, where irrelevant features are suppressed to finalize the output distribution. Synthesizing behavioral and mechanistic evidence, we provide a framework for interpreting depth-dependent computations in LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Neural Scaling Law Extrapolation with Prior-Data Fitted Networks</title>
<link>https://arxiv.org/abs/2505.23032</link>
<guid>https://arxiv.org/abs/2505.23032</guid>
<content:encoded><![CDATA[
<div> Bayesian framework, Prior-data Fitted Networks (PFNs), neural scaling law extrapolation, uncertainty-aware extrapolation, data-limited scenarios <br />
Summary: This study introduces a Bayesian approach utilizing Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. The method incorporates a prior distribution that allows for the generation of numerous synthetic functions resembling real-world neural scaling laws, enabling the meta-learning of extrapolation. Compared to existing point estimation methods and Bayesian approaches, the proposed PFN demonstrates superior performance, especially in data-limited scenarios such as Bayesian active learning. By quantifying uncertainty, the PFN offers reliable extrapolation results, essential for decision-making applications involving resource investment. <div>
arXiv:2505.23032v3 Announce Type: replace-cross 
Abstract: Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, chain-of-thought, reinforcement learning, instruction following, complex instructions

Summary:
Existing large language models struggle with following complex instructions due to challenges in understanding multiple constraints organized in different structures. The chain-of-thought approach, while intuitive, falls short as it only paraphrases instructions without deeply understanding their relationships. A proposed method uses reinforcement learning with verifiable rewards to enhance reasoning for instruction following. By decomposing complex instructions, applying rule-centric rewards, and leveraging behavior cloning, the method improves LLM performance significantly. In evaluations across seven benchmarks, a 1.5B LLM achieved an 11.74% increase in performance with results comparable to an 8B LLM. This approach shows promise in enhancing LLM capabilities for handling complex instructions effectively. 

<br /><br />Summary: <div>
arXiv:2506.01413v3 Announce Type: replace-cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data will be available later (under review).
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework</title>
<link>https://arxiv.org/abs/2506.08185</link>
<guid>https://arxiv.org/abs/2506.08185</guid>
<content:encoded><![CDATA[
<div> Keywords: surgeon-specific behavior prediction, robotic surgery, gesture prediction, privacy implications, personalized embeddings 

Summary: 
Surgeons possess individual operating styles influenced by their training and experience, yet current surgical AI systems often overlook this personalized signal. This study introduces a new agentic modeling approach for predicting surgeon-specific behavior in robotic surgery. By combining a discrete diffusion framework with a vision-language-action pipeline, the model can accurately forecast gesture sequences based on multimodal inputs like surgical video and intent language. Moreover, personalized embeddings of surgeon identity and skill are incorporated using natural language prompts from third-party models, allowing the model to capture unique behavioral styles without revealing explicit identity. Evaluation on the JIGSAWS dataset shows that the model effectively reconstructs gestures while learning distinct motion fingerprints for each surgeon. However, the study also highlights the trade-off between performance and privacy, as more expressive embeddings enhance task accuracy but also increase the risk of identity leakage. This underscores the need to strike a balance between personalization and privacy protection in surgical modeling. <div>
arXiv:2506.08185v2 Announce Type: replace-cross 
Abstract: Surgeons exhibit distinct operating styles shaped by training, experience, and motor behavior-yet most surgical AI systems overlook this personalization signal. We propose a novel agentic modeling approach for surgeon-specific behavior prediction in robotic surgery, combining a discrete diffusion framework with a vision-language-action (VLA) pipeline. Gesture prediction is framed as a structured sequence denoising task, conditioned on multimodal inputs including surgical video, intent language, and personalized embeddings of surgeon identity and skill. These embeddings are encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression</title>
<link>https://arxiv.org/abs/2506.08267</link>
<guid>https://arxiv.org/abs/2506.08267</guid>
<content:encoded><![CDATA[
<div> Keywords: Symbolic regression, neural network, interpretability, oversampling, pruning

Summary: 
The study introduces the LIES framework, a neural network architecture designed for symbolic regression. LIES incorporates interpretable primitive activations such as Logarithm, Identity, Exponential, and Sine functions to optimize the modeling of symbolic expressions. A novel training approach involving oversampling and a specialized loss function is employed to encourage sparsity and prevent gradient instability. Post-training, additional pruning strategies are applied to simplify the learned expressions further into compact formulae. Experimental results on symbolic regression benchmarks demonstrate that the LIES framework consistently generates sparse and accurate symbolic formulae, outperforming existing methods. Ablation studies validate the significance of each component of the framework in achieving these results. The LIES framework presents a promising approach to symbolic regression, offering improved interpretability and analytical insight compared to conventional black-box models. 

<br /><br />Summary: <div>
arXiv:2506.08267v2 Announce Type: replace-cross 
Abstract: Symbolic regression (SR) aims to discover closed-form mathematical expressions that accurately describe data, offering interpretability and analytical insight beyond standard black-box models. Existing SR methods often rely on population-based search or autoregressive modeling, which struggle with scalability and symbolic consistency. We introduce LIES (Logarithm, Identity, Exponential, Sine), a fixed neural network architecture with interpretable primitive activations that are optimized to model symbolic expressions. We develop a framework to extract compact formulae from LIES networks by training with an appropriate oversampling strategy and a tailored loss function to promote sparsity and to prevent gradient instability. After training, it applies additional pruning strategies to further simplify the learned expressions into compact formulae. Our experiments on SR benchmarks show that the LIES framework consistently produces sparse and accurate symbolic formulae outperforming all baselines. We also demonstrate the importance of each design component through ablation studies.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs</title>
<link>https://arxiv.org/abs/2506.08500</link>
<guid>https://arxiv.org/abs/2506.08500</guid>
<content:encoded><![CDATA[
<div> conflict resolution, Retrieval Augmented Generation, knowledge conflict types, benchmark, large language models <br />
Summary: 
This article introduces a taxonomy of knowledge conflict types in Retrieval Augmented Generation (RAG) and proposes desired model behavior for each type. The authors present CONFLICTS, a benchmark with expert annotations of conflict types in RAG settings, to assess how models address knowledge conflicts. Experimental results show that large language models struggle to resolve conflicts between retrieved sources. Prompting models to explicitly reason about conflicts improves response quality. However, there is still significant room for improvement in this area. The CONFLICTS benchmark provides a valuable tool for tracking progress in addressing knowledge conflicts and highlights the need for further research in enhancing model performance in handling conflicting information. <br /> <div>
arXiv:2506.08500v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inherently Faithful Attention Maps for Vision Transformers</title>
<link>https://arxiv.org/abs/2506.08915</link>
<guid>https://arxiv.org/abs/2506.08915</guid>
<content:encoded><![CDATA[
<div> attention-based method, binary attention masks, object perception, out-of-distribution backgrounds, two-stage framework

Summary: 
The article introduces an attention-based method utilizing learned binary attention masks to ensure that only attended image regions influence predictions. Contextual factors can impact object perception, leading to biased representations, especially in out-of-distribution backgrounds. The proposed two-stage framework addresses this challenge by first processing the full image to identify object parts and task-relevant regions. The second stage uses input attention masking to focus its analysis on these regions, filtering out potentially irrelevant information. Both stages are jointly trained, allowing the second stage to refine the first. Experimental results across various benchmarks demonstrate that this approach enhances robustness against spurious correlations and out-of-distribution backgrounds. The code for the method is available on GitHub for reference. 

<br /><br />Summary: <div>
arXiv:2506.08915v2 Announce Type: replace-cross 
Abstract: We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds. Code: https://github.com/ananthu-aniraj/ifam
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Concealed Objects with Incomplete Supervision</title>
<link>https://arxiv.org/abs/2506.08955</link>
<guid>https://arxiv.org/abs/2506.08955</guid>
<content:encoded><![CDATA[
<div> framework, segmentation, annotations, pseudo-labels, feature grouping<br />
<br />
Summary:<br />
In this paper, a unified mean-teacher framework, SEE, is introduced for Incompletely-Supervised Concealed Object Segmentation (ISCOS). The framework leverages the Segment Anything Model (SAM) to generate pseudo-labels from coarse masks produced by the teacher model, addressing the issue of incomplete supervision. Strategies for pseudo-label generation, storage, and supervision are implemented to ensure robust network training. A hybrid-granularity feature grouping module is designed to tackle the challenge of intrinsic similarity by clustering similar features and promoting segmentation coherence. Experimental results show state-of-the-art performance in ISCOS tasks, demonstrating the effectiveness of the proposed approach. SEE can be seamlessly integrated into existing models to enhance their performance. <br /><br />Summary: <div>
arXiv:2506.08955v2 Announce Type: replace-cross 
Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves segmenting objects that seamlessly blend into their surrounding environments, utilizing incompletely annotated data, such as weak and semi-annotations, for model training. This task remains highly challenging due to (1) the limited supervision provided by the incompletely annotated training data, and (2) the difficulty of distinguishing concealed objects from the background, which arises from the intrinsic similarities in concealed scenarios. In this paper, we introduce the first unified method for ISCOS to address these challenges. To tackle the issue of incomplete supervision, we propose a unified mean-teacher framework, SEE, that leverages the vision foundation model, ``\emph{Segment Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced by the teacher model as prompts. To mitigate the effect of low-quality segmentation masks, we introduce a series of strategies for pseudo-label generation, storage, and supervision. These strategies aim to produce informative pseudo-labels, store the best pseudo-labels generated, and select the most reliable components to guide the student model, thereby ensuring robust network training. Additionally, to tackle the issue of intrinsic similarity, we design a hybrid-granularity feature grouping module that groups features at different granularities and aggregates these results. By clustering similar features, this module promotes segmentation coherence, facilitating more complete segmentation for both single-object and multiple-object images. We validate the effectiveness of our approach across multiple ISCOS tasks, and experimental results demonstrate that our method achieves state-of-the-art performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing the performance of existing models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model</title>
<link>https://arxiv.org/abs/2506.09061</link>
<guid>https://arxiv.org/abs/2506.09061</guid>
<content:encoded><![CDATA[
<div> quantization, profiling, lightweight LLMs, edge systems, efficiency  
Summary:  
EdgeProfiler introduces a fast profiling framework for evaluating lightweight Large Language Models (LLMs) on edge systems. The framework profiles compact LLMs using aggressive quantization techniques and strict memory constraints to address challenges in resource-constrained edge settings. Analytical modeling estimates latency, FLOPs, and energy consumption, with findings showing that 4-bit quantization reduces model memory usage by 60-70% while maintaining accuracy within 2-5% of full-precision baselines. Inference speeds improve by 2-3x compared to FP16 baselines on various edge devices, and power modeling estimates a 35-50% reduction in energy consumption for INT4 configurations. This enables practical deployment on hardware like Raspberry Pi 4/5 and Jetson Orin Nano Super, highlighting the importance of efficient profiling tailored to lightweight LLMs in edge environments for balancing accuracy, energy efficiency, and computational feasibility.  
<br /><br />Summary:   <div>
arXiv:2506.09061v2 Announce Type: replace-cross 
Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs offer remarkable capabilities in natural language understanding and generation, their high computational, memory, and power requirements often confine them to cloud environments. EdgeProfiler addresses these challenges by providing a systematic methodology for assessing LLM performance in resource-constrained edge settings. The framework profiles compact LLMs, including TinyLLaMA, Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization techniques and strict memory constraints. Analytical modeling is used to estimate latency, FLOPs, and energy consumption. The profiling reveals that 4-bit quantization reduces model memory usage by approximately 60-70%, while maintaining accuracy within 2-5% of full-precision baselines. Inference speeds are observed to improve by 2-3x compared to FP16 baselines across various edge devices. Power modeling estimates a 35-50% reduction in energy consumption for INT4 configurations, enabling practical deployment on hardware such as Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the importance of efficient profiling tailored to lightweight LLMs in edge environments, balancing accuracy, energy efficiency, and computational feasibility.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models in Medical Imaging -- A Review and Outlook</title>
<link>https://arxiv.org/abs/2506.09095</link>
<guid>https://arxiv.org/abs/2506.09095</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation models, medical images, pathology, radiology, ophthalmology, self-supervised learning

Summary: 
Foundation models (FMs) are revolutionizing medical image analysis by leveraging unlabeled data for pre-training, enabling adaptation to specific clinical tasks with minimal supervision. The review explores the development and application of FMs in pathology, radiology, and ophthalmology, synthesizing evidence from 150+ studies. Core FM components include model architectures, self-supervised learning techniques, and strategies for task-specific fine-tuning. The review delves into FM usage across imaging domains, highlighting design variations. Addressing key challenges and unresolved queries, the review serves as a roadmap for future research advancements. 

Summary: <div>
arXiv:2506.09095v3 Announce Type: replace-cross 
Abstract: Foundation models (FMs) are changing the way medical images are analyzed by learning from large collections of unlabeled data. Instead of relying on manually annotated examples, FMs are pre-trained to learn general-purpose visual features that can later be adapted to specific clinical tasks with little additional supervision. In this review, we examine how FMs are being developed and applied in pathology, radiology, and ophthalmology, drawing on evidence from over 150 studies. We explain the core components of FM pipelines, including model architectures, self-supervised learning methods, and strategies for downstream adaptation. We also review how FMs are being used in each imaging domain and compare design choices across applications. Finally, we discuss key challenges and open questions to guide future research.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intra-Trajectory Consistency for Reward Modeling</title>
<link>https://arxiv.org/abs/2506.09096</link>
<guid>https://arxiv.org/abs/2506.09096</guid>
<content:encoded><![CDATA[
<div> Bayesian framework, reward modeling, reinforcement learning, language models, response trajectory  
Summary:  
Reward models are essential for improving large language models, especially in reinforcement learning and inference-time verification. Current reward modeling faces challenges in identifying specific components within response trajectories that correlate with overall scores, leading to poor generalization on new responses. To address this, a new approach using generation probabilities to establish reward consistency across processes in response trajectories is proposed. By enforcing intra-trajectory consistency regularization, adjacent processes with higher next-token generation probabilities maintain more consistent rewards. This improves the performance of the outcome reward model on RewardBench and induces better policies aligned with DPO. The model trained with this regularization also achieves improved best-of-N (BON) inference-time verification results. The code for implementation is available on GitHub at https://github.com/chaoyang101/ICRM.  
<br /><br />Summary: <div>
arXiv:2506.09096v3 Announce Type: replace-cross 
Abstract: Reward models are critical for improving large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) or inference-time verification. Current reward modeling typically relies on scores of overall responses to learn the outcome rewards for the responses. However, since the response-level scores are coarse-grained supervision signals, the reward model struggles to identify the specific components within a response trajectory that truly correlate with the scores, leading to poor generalization on unseen responses. In this paper, we propose to leverage generation probabilities to establish reward consistency between processes in the response trajectory, which allows the response-level supervisory signal to propagate across processes, thereby providing additional fine-grained signals for reward learning. Building on analysis under the Bayesian framework, we develop an intra-trajectory consistency regularization to enforce that adjacent processes with higher next-token generation probability maintain more consistent rewards. We apply the proposed regularization to the advanced outcome reward model, improving its performance on RewardBench. Besides, we show that the reward model trained with the proposed regularization induces better DPO-aligned policies and achieves better best-of-N (BON) inference-time verification results. Our code is provided in https://github.com/chaoyang101/ICRM.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Multi-Head Attention for Small Language Models</title>
<link>https://arxiv.org/abs/2506.09342</link>
<guid>https://arxiv.org/abs/2506.09342</guid>
<content:encoded><![CDATA[
<div> latent multi-head attention, small language models, efficiency-quality trade-offs, rotary positional embeddings, memory savings

Summary:
The study explores latent multi-head attention (MLA) for small language models, comparing it with standard multi-head attention (MHA) and MLA with rotary positional embeddings (MLA+RoPE). The research demonstrates that MLA+RoPE with half-rank latent dimensions achieves a significant memory reduction without sacrificing validation loss. Rotary positional embeddings play a crucial role in improving MLA performance in small models. MLA with reduced rank dimensions shows a speedup in inference on NVIDIA A100 GPUs while maintaining memory savings. GPT-4 evaluations indicate that the proposed approach achieves the highest quality scores in grammar, creativity, and consistency metrics. The study presents a Pareto improvement for memory-constrained deployment, showcasing the potential of MLA in enhancing the efficiency and quality of small language models. <br /><br />Summary: <div>
arXiv:2506.09342v2 Announce Type: replace-cross 
Abstract: We present the first comprehensive study of latent multi-head attention (MLA) for small language models, revealing interesting efficiency-quality trade-offs. Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory reduction while incurring only a 0.3% increase in validation loss (essentially matching MHA quality)- a Pareto improvement for memory constrained deployment. We further show that RoPE is crucial for MLA in small models: without it, MLA underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by 2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2 achieves a 1.4 times speedup over full-rank MLA while maintaining the memory savings. GPT-4 evaluations corroborate perplexity results, with ours achieving the highest quality scores (7.4/10) across grammar, creativity, and consistency metrics. Code and models will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization</title>
<link>https://arxiv.org/abs/2506.09373</link>
<guid>https://arxiv.org/abs/2506.09373</guid>
<content:encoded><![CDATA[
<div> natural language processing, graphical user interfaces, autonomous agents, location preference optimization, information entropy

Summary:
Location Preference Optimization (LPO) is introduced as a novel approach for improving interaction precision in Graphical User Interfaces (GUIs) by leveraging locational data. LPO uses information entropy to predict interaction positions by focusing on information-rich zones and introduces a dynamic location reward function based on physical distance. This approach, coupled with Group Relative Preference Optimization (GRPO), enhances interaction precision and allows for extensive exploration of GUI environments. Experimental results showcase the superior performance of LPO, achieving state-of-the-art results in both offline benchmarks and real-world online evaluations. The code for LPO will soon be made publicly available on https://github.com/AIDC-AI/LPO. 

<br /><br />Summary: <div>
arXiv:2506.09373v2 Announce Type: replace-cross 
Abstract: The advent of autonomous agents is transforming interactions with Graphical User Interfaces (GUIs) by employing natural language as a powerful intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods in current GUI agents for achieving spatial localization, these methods face substantial challenges due to their limited capacity to accurately perceive positional data. Existing strategies, such as reinforcement learning, often fail to assess positional accuracy effectively, thereby restricting their utility. In response, we introduce Location Preference Optimization (LPO), a novel approach that leverages locational data to optimize interaction preferences. LPO uses information entropy to predict interaction positions by focusing on zones rich in information. Besides, it further introduces a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Supported by Group Relative Preference Optimization (GRPO), LPO facilitates an extensive exploration of GUI environments and significantly enhances interaction precision. Comprehensive experiments demonstrate LPO's superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations. Our code will be made publicly available soon, at https://github.com/AIDC-AI/LPO.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection</title>
<link>https://arxiv.org/abs/2506.09827</link>
<guid>https://arxiv.org/abs/2506.09827</guid>
<content:encoded><![CDATA[
<div> EmoNet-Voice, speech emotion detection, benchmark dataset, human expert annotations, synthetic audio snippets <br />
Summary: EmoNet-Voice introduces a new resource for speech emotion detection, providing a large-scale pre-training dataset (EmoNet-Voice Big) and a benchmark dataset (EmoNet-Voice Bench). The datasets feature a wide range of emotions and intensity levels. Synthetic audio snippets were created to simulate actors portraying emotions, validated by psychology experts. The approach allows for inclusion of sensitive emotional states not present in existing datasets. The Empathic Insight Voice models achieved high agreement with human experts. Evaluation of SER models revealed that detecting high-arousal emotions like anger is easier than low-arousal states like concentration. This work addresses the need for robust benchmarks in evaluating emotional understanding capabilities of AI systems in text-to-speech and audio generation models.<br /><br />Summary: <div>
arXiv:2506.09827v2 Announce Type: replace-cross 
Abstract: The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Amazon Nova Family of Models: Technical Report and Model Card</title>
<link>https://arxiv.org/abs/2506.12103</link>
<guid>https://arxiv.org/abs/2506.12103</guid>
<content:encoded><![CDATA[
<div> Amazon Nova, state-of-the-art foundation models, multimodal models, accuracy, speed, cost, Amazon Nova Pro, Amazon Nova Lite, Amazon Nova Micro, Amazon Nova Canvas, Amazon Nova Reel, benchmarking results, customer trust, security, reliability <br />
Summary:
Amazon introduces Amazon Nova, a new generation of advanced foundation models including Amazon Nova Pro, a high-capability multimodal model with top accuracy, speed, and cost efficiency. For low-cost lightning-fast processing, Amazon Nova Lite is available for various tasks. Amazon Nova Micro offers low-latency text responses at a minimal cost. Amazon Nova Canvas enables image generation with customization controls while Amazon Nova Reel provides high-quality video outputs and motion control. These models were developed with a focus on responsibility, customer trust, security, and reliability. Benchmarking results showcase their core capabilities, performance, long context processing, functional adaptation, runtime performance, and human evaluation feedback. <div>
arXiv:2506.12103v1 Announce Type: new 
Abstract: We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Because we have LLMs, we Can and Should Pursue Agentic Interpretability</title>
<link>https://arxiv.org/abs/2506.12152</link>
<guid>https://arxiv.org/abs/2506.12152</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, interpretability, agentic interpretability, mental model, human understanding

Summary: 
The article discusses a new form of interpretability called agentic interpretability, which involves a multi-turn conversation between humans and Large Language Models (LLMs) to assist in understanding. This approach goes beyond traditional methods of opening the black-box to actively develop a mental model of the user and improve human understanding of the LLM. While agentic interpretability may lack completeness in high-stakes safety situations, it aims to help humans grasp potentially superhuman concepts of LLMs. Challenges in evaluation arise from the human-entangled-in-the-loop nature of the process, requiring innovative solutions. As LLMs approach human parity in various tasks, agentic interpretability offers a way for humans to enhance their understanding of these complex models rather than become increasingly disconnected from them.

<br /><br />Summary: <div>
arXiv:2506.12152v1 Announce Type: new 
Abstract: The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence and Machine Learning in the Development of Vaccines and Immunotherapeutics Yesterday, Today, and Tomorrow</title>
<link>https://arxiv.org/abs/2506.12185</link>
<guid>https://arxiv.org/abs/2506.12185</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccines, immunotherapeutics, artificial intelligence, deep learning, predictive frameworks

Summary: 
Artificial intelligence (AI) and deep learning (DL) are revolutionizing the design of vaccines and immunotherapeutics by providing predictive frameworks for rapid decision-making and efficient data-driven strategies. They integrate computational models, systems vaccinology, and multi-omics data to predict immune responses, identify optimal targets, and understand immune regulation. These technologies aim to replace animal testing with computational models, as suggested by the US FDA, and enable real-time in vivo modeling for clinical trials. The future of AI and DL holds promise for the development of personalized vaccines and immunotherapeutics against infectious pathogens and cancers. <div>
arXiv:2506.12185v1 Announce Type: new 
Abstract: In the past, the development of vaccines and immunotherapeutics relied heavily on trial-and-error experimentation and extensive in vivo testing, often requiring years of pre-clinical and clinical trials. Today, artificial intelligence (AI) and deep learning (DL) are actively transforming vaccine and immunotherapeutic design, by (i) offering predictive frameworks that support rapid, data-driven decision-making; (ii) increasingly being implemented as time- and resource-efficient strategies that integrate computational models, systems vaccinology, and multi-omics data to better phenotype, differentiate, and classify patient diseases and cancers; predict patients' immune responses; and identify the factors contributing to optimal vaccine and immunotherapeutic protective efficacy; (iii) refining the selection of B- and T-cell antigen/epitope targets to enhance efficacy and durability of immune protection; and (iv) enabling a deeper understanding of immune regulation, immune evasion, immune checkpoints, and regulatory pathways. The future of AI and DL points toward (i) replacing animal preclinical testing of drugs, vaccines, and immunotherapeutics with computational-based models, as recently proposed by the United States FDA; and (ii) enabling real-time in vivo modeling for immunobridging and prediction of protection in clinical trials. This may result in a fast and transformative shift for the development of personal vaccines and immunotherapeutics against infectious pathogens and cancers.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRO-V: An Efficient Program Generation Multi-Agent System for Automatic RTL Verification</title>
<link>https://arxiv.org/abs/2506.12200</link>
<guid>https://arxiv.org/abs/2506.12200</guid>
<content:encoded><![CDATA[
<div> machine learning, hardware verification, RTL code generation, testbenches, LLM

Summary:
PRO-V is a new multi-agent system designed for robust RTL verification in hardware design. It utilizes machine learning models to generate testbenches with high accuracy, particularly focusing on RTL code generation. PRO-V incorporates an iterative sampling strategy to improve testbench correctness and introduces an LLM-as-a-judge validation framework for error detection. This framework converts compiler static analysis into natural language through in-context learning, allowing LLMs to assist in identifying errors in RTL design or testbenches. The system achieves a verification accuracy of 87.17% on golden RTL implementations and 76.28% on RTL mutants. The code for PRO-V is open-sourced, providing a valuable resource for hardware verification and design. <div>
arXiv:2506.12200v1 Announce Type: new 
Abstract: LLM-assisted hardware verification is gaining substantial attention due to its potential to significantly reduce the cost and effort of crafting effective testbenches. It also serves as a critical enabler for LLM-aided end-to-end hardware language design. However, existing current LLMs often struggle with Register Transfer Level (RTL) code generation, resulting in testbenches that exhibit functional errors in Hardware Description Languages (HDL) logic. Motivated by the strong performance of LLMs in Python code generation under inference-time sampling strategies, and their promising capabilities as judge agents, we propose PRO-V a fully program generation multi-agent system for robust RTL verification. Pro-V incorporates an efficient best-of-n iterative sampling strategy to enhance the correctness of generated testbenches. Moreover, it introduces an LLM-as-a-judge aid validation framework featuring an automated prompt generation pipeline. By converting rule-based static analysis from the compiler into natural language through in-context learning, this pipeline enables LLMs to assist the compiler in determining whether verification failures stem from errors in the RTL design or the testbench. PRO-V attains a verification accuracy of 87.17% on golden RTL implementations and 76.28% on RTL mutants. Our code is open-sourced at https://github.com/stable-lab/Pro-V.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Reasoning in Ambiguous Contexts</title>
<link>https://arxiv.org/abs/2506.12241</link>
<guid>https://arxiv.org/abs/2506.12241</guid>
<content:encoded><![CDATA[
<div> privacy, language models, information disclosure, context ambiguity, reasoning

Summary:
- The study focuses on language models' ability to reason about appropriate information disclosure in the context of agentic privacy.
- Previous works have primarily evaluated models based on their alignment with human decisions, but this study considers the impact of ambiguity and missing context on model performance.
- Context ambiguity is identified as a significant barrier to high performance in privacy assessments.
- The authors introduce Camber, a framework for context disambiguation, which shows that systematically disambiguating context based on model-generated decision rationales improves accuracy significantly.
- The results demonstrate that approaches for context disambiguation can enhance agentic privacy reasoning by increasing precision and recall while reducing prompt sensitivity. 

<br /><br />Summary: <div>
arXiv:2506.12241v1 Announce Type: new 
Abstract: We study the ability of language models to reason about appropriate information disclosure - a central aspect of the evolving field of agentic privacy. Whereas previous works have focused on evaluating a model's ability to align with human decisions, we examine the role of ambiguity and missing context on model performance when making information-sharing decisions. We identify context ambiguity as a crucial barrier for high performance in privacy assessments. By designing Camber, a framework for context disambiguation, we show that model-generated decision rationales can reveal ambiguities and that systematically disambiguating context based on these rationales leads to significant accuracy improvements (up to 13.3\% in precision and up to 22.3\% in recall) as well as reductions in prompt sensitivity. Overall, our results indicate that approaches for context disambiguation are a promising way forward to enhance agentic privacy reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reversing the Paradigm: Building AI-First Systems with Human Guidance</title>
<link>https://arxiv.org/abs/2506.12245</link>
<guid>https://arxiv.org/abs/2506.12245</guid>
<content:encoded><![CDATA[
<div> AI, humans, collaboration, automation, ethics
Summary: 
AI is becoming an integral part of our lives, transforming various aspects of society by augmenting human capabilities rather than replacing them. This collaborative relationship between AI and humans is reshaping work dynamics, with AI agents autonomously handling tasks while humans supervise and guide them. This shift offers numerous benefits such as efficiency, faster decision-making, and cost savings, but also poses risks like algorithmic bias and security flaws. To successfully navigate this transition, organizations need to rethink roles, invest in upskilling, embed ethical principles, and promote transparency. The responsible adoption of AI-first systems requires a balance between autonomy and human oversight, ensuring alignment with goals, values, and context. Thus, it is crucial for organizations to adapt to these technological and organizational changes to harness the full potential of AI while upholding ethical standards. 
<br /><br />Summary: <div>
arXiv:2506.12245v1 Announce Type: new 
Abstract: The relationship between humans and artificial intelligence is no longer science fiction -- it's a growing reality reshaping how we live and work. AI has moved beyond research labs into everyday life, powering customer service chats, personalizing travel, aiding doctors in diagnosis, and supporting educators. What makes this moment particularly compelling is AI's increasing collaborative nature. Rather than replacing humans, AI augments our capabilities -- automating routine tasks, enhancing decisions with data, and enabling creativity in fields like design, music, and writing. The future of work is shifting toward AI agents handling tasks autonomously, with humans as supervisors, strategists, and ethical stewards. This flips the traditional model: instead of humans using AI as a tool, intelligent agents will operate independently within constraints, managing everything from scheduling and customer service to complex workflows. Humans will guide and fine-tune these agents to ensure alignment with goals, values, and context.
  This shift offers major benefits -- greater efficiency, faster decisions, cost savings, and scalability. But it also brings risks: diminished human oversight, algorithmic bias, security flaws, and a widening skills gap. To navigate this transition, organizations must rethink roles, invest in upskilling, embed ethical principles, and promote transparency. This paper examines the technological and organizational changes needed to enable responsible adoption of AI-first systems -- where autonomy is balanced with human intent, oversight, and values.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lower Bound on Howard Policy Iteration for Deterministic Markov Decision Processes</title>
<link>https://arxiv.org/abs/2506.12254</link>
<guid>https://arxiv.org/abs/2506.12254</guid>
<content:encoded><![CDATA[
<div> Markov Decision Processes, Deterministic, Mean-payoff objective, Howard's policy iteration algorithm, Lower bound <br />
<br />
Summary: <br />
Deterministic Markov Decision Processes (DMDPs) model decision-making scenarios where outcomes are determined by the current action taken. The mean-payoff objective is a fundamental measure of cumulative reward in DMDPs. Howard's policy iteration algorithm is commonly used to solve DMDPs with mean-payoff objectives, but the current lower bound on its performance is sub-linear with respect to the input size. This study presents an improved lower bound, showing that the algorithm requires a minimum number of iterations proportional to the input size. This finding has implications for optimizing decision-making processes in DMDPs. <div>
arXiv:2506.12254v1 Announce Type: new 
Abstract: Deterministic Markov Decision Processes (DMDPs) are a mathematical framework for decision-making where the outcomes and future possible actions are deterministically determined by the current action taken. DMDPs can be viewed as a finite directed weighted graph, where in each step, the controller chooses an outgoing edge. An objective is a measurable function on runs (or infinite trajectories) of the DMDP, and the value for an objective is the maximal cumulative reward (or weight) that the controller can guarantee. We consider the classical mean-payoff (aka limit-average) objective, which is a basic and fundamental objective.
  Howard's policy iteration algorithm is a popular method for solving DMDPs with mean-payoff objectives. Although Howard's algorithm performs well in practice, as experimental studies suggested, the best known upper bound is exponential and the current known lower bound is as follows: For the input size $I$, the algorithm requires $\tilde{\Omega}(\sqrt{I})$ iterations, where $\tilde{\Omega}$ hides the poly-logarithmic factors, i.e., the current lower bound on iterations is sub-linear with respect to the input size. Our main result is an improved lower bound for this fundamental algorithm where we show that for the input size $I$, the algorithm requires $\tilde{\Omega}(I)$ iterations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloud Infrastructure Management in the Age of AI Agents</title>
<link>https://arxiv.org/abs/2506.12270</link>
<guid>https://arxiv.org/abs/2506.12270</guid>
<content:encoded><![CDATA[
<div> Keywords: Cloud infrastructure, AI agents, Large language models, Automation, Infrastructure management

Summary: 
Cloud infrastructure is vital in the IT industry, but managing it can be labor-intensive for DevOps teams. This paper proposes using AI agents powered by large language models to automate cloud infrastructure management tasks. The study examines the AI agents' potential to utilize various interfaces, including SDKs, CLIs, IaC platforms, and web portals. The effectiveness of these agents in different management tasks is analyzed, along with research challenges and potential solutions. AI agents have the potential to streamline cloud infrastructure management, offering opportunities for increased efficiency and scalability. Research in this area can lead to advancements in automating cloud infrastructure management and ultimately enhance the capabilities of DevOps teams. <br /><br />Summary: <div>
arXiv:2506.12270v1 Announce Type: new 
Abstract: Cloud infrastructure is the cornerstone of the modern IT industry. However, managing this infrastructure effectively requires considerable manual effort from the DevOps engineering team. We make a case for developing AI agents powered by large language models (LLMs) to automate cloud infrastructure management tasks. In a preliminary study, we investigate the potential for AI agents to use different cloud/user interfaces such as software development kits (SDK), command line interfaces (CLI), Infrastructure-as-Code (IaC) platforms, and web portals. We report takeaways on their effectiveness on different management tasks, and identify research challenges and potential solutions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Fictitious Play-Based Potential Differential Games for Learning Human-Like Interaction at Unsignalized Intersections</title>
<link>https://arxiv.org/abs/2506.12283</link>
<guid>https://arxiv.org/abs/2506.12283</guid>
<content:encoded><![CDATA[
<div> Differential Game, Potential Differential Game, Deep Fictitious Play, Interactive driving policies, INTERACTION dataset <br />
Summary: 
This study introduces a novel approach, Deep Fictitious Play-Based Potential Differential Game (DFP-PDG), to model vehicle interactions at unsignalized intersections by learning human-like driving policies. By incorporating a Differential Game formulation transformed into a Potential Differential Game, the framework learns weights from naturalistic driving datasets to capture diverse driving styles. The convergence to a Nash equilibrium is guaranteed, distinguishing this study as the first to utilize Deep Fictitious Play for training interactive driving policies. Validation using the INTERACTION dataset shows that the DFP-PDG framework effectively learns human-like policies, successfully capturing variations in driver aggressiveness and preferences. An ablation study underscores the significance of each component within the model. Overall, the proposed framework demonstrates promising results in simulating interactive driving behaviors at unsignalized intersections. <br /> <div>
arXiv:2506.12283v1 Announce Type: new 
Abstract: Modeling vehicle interactions at unsignalized intersections is a challenging task due to the complexity of the underlying game-theoretic processes. Although prior studies have attempted to capture interactive driving behaviors, most approaches relied solely on game-theoretic formulations and did not leverage naturalistic driving datasets. In this study, we learn human-like interactive driving policies at unsignalized intersections using Deep Fictitious Play. Specifically, we first model vehicle interactions as a Differential Game, which is then reformulated as a Potential Differential Game. The weights in the cost function are learned from the dataset and capture diverse driving styles. We also demonstrate that our framework provides a theoretical guarantee of convergence to a Nash equilibrium. To the best of our knowledge, this is the first study to train interactive driving policies using Deep Fictitious Play. We validate the effectiveness of our Deep Fictitious Play-Based Potential Differential Game (DFP-PDG) framework using the INTERACTION dataset. The results demonstrate that the proposed framework achieves satisfactory performance in learning human-like driving policies. The learned individual weights effectively capture variations in driver aggressiveness and preferences. Furthermore, the ablation study highlights the importance of each component within our model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason</title>
<link>https://arxiv.org/abs/2506.12286</link>
<guid>https://arxiv.org/abs/2506.12286</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, benchmarks, software engineering, GitHub, memorization 

Summary: 
Large language models (LLMs) are being widely adopted, but the evaluation of their abilities using benchmarks such as SWE-Bench Verified may overstate their true capabilities. A diagnostic task involving identifying file paths from issue descriptions alone revealed that LLMs may be achieving high accuracy through memorization rather than genuine problem-solving. State-of-the-art models achieved up to 76% accuracy on this task but significantly lower accuracy on tasks from repositories not included in SWE-Bench, suggesting possible data contamination or memorization issues. These findings highlight the need for more robust and contamination-resistant benchmarks to accurately assess LLMs' coding abilities. 

<br /><br />Summary: <div>
arXiv:2506.12286v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly capable and widely adopted, benchmarks play a central role in assessing their practical utility. For example, SWE-Bench Verified has emerged as a critical benchmark for evaluating LLMs' software engineering abilities, particularly their aptitude for resolving real-world GitHub issues. Recent LLMs show impressive performance on SWE-Bench, leading to optimism about their capacity for complex coding tasks. However, current evaluation protocols may overstate these models' true capabilities. It is crucial to distinguish LLMs' generalizable problem-solving ability and other learned artifacts. In this work, we introduce a diagnostic task: file path identification from issue descriptions alone, to probe models' underlying knowledge. We present empirical evidence that performance gains on SWE-Bench-Verified may be partially driven by memorization rather than genuine problem-solving. We show that state-of-the-art models achieve up to 76% accuracy in identifying buggy file paths using only issue descriptions, without access to repository structure. This performance is merely up to 53% on tasks from repositories not included in SWE-Bench, pointing to possible data contamination or memorization. These findings raise concerns about the validity of existing results and underscore the need for more robust, contamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology Enabled Hybrid Modeling and Simulation</title>
<link>https://arxiv.org/abs/2506.12290</link>
<guid>https://arxiv.org/abs/2506.12290</guid>
<content:encoded><![CDATA[
<div> Keywords: ontologies, hybrid modeling, simulation, interoperability, semantic web technologies

Summary: 
The article explores the role of ontologies in enhancing hybrid modeling and simulation by improving semantic rigor, model reusability, and interoperability across systems, disciplines, and tools. It distinguishes between methodological and referential ontologies to address interoperability challenges along Human-Human, Human-Machine, and Machine-Machine axes. Techniques like competency questions, ontology design patterns, and layered strategies are highlighted for promoting shared understanding and formal precision. Integrating ontologies with Semantic Web Technologies, they serve as descriptive domain representations and prescriptive guides for simulation construction. Four application cases demonstrate the practical benefits of ontology-driven hybrid simulation workflows in sea-level rise analysis, Industry 4.0 modeling, artificial societies for policy support, and cyber threat evaluation. Challenges and opportunities in ontology-based hybrid M&amp;S are discussed, including tool integration, semantic alignment, and support for explainable AI. 

<br /><br />Summary: <div>
arXiv:2506.12290v1 Announce Type: new 
Abstract: We explore the role of ontologies in enhancing hybrid modeling and simulation through improved semantic rigor, model reusability, and interoperability across systems, disciplines, and tools. By distinguishing between methodological and referential ontologies, we demonstrate how these complementary approaches address interoperability challenges along three axes: Human-Human, Human-Machine, and Machine-Machine. Techniques such as competency questions, ontology design patterns, and layered strategies are highlighted for promoting shared understanding and formal precision. Integrating ontologies with Semantic Web Technologies, we showcase their dual role as descriptive domain representations and prescriptive guides for simulation construction. Four application cases - sea-level rise analysis, Industry 4.0 modeling, artificial societies for policy support, and cyber threat evaluation - illustrate the practical benefits of ontology-driven hybrid simulation workflows. We conclude by discussing challenges and opportunities in ontology-based hybrid M&amp;S, including tool integration, semantic alignment, and support for explainable AI.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Budget AI Researcher and the Power of RAG Chains</title>
<link>https://arxiv.org/abs/2506.12317</link>
<guid>https://arxiv.org/abs/2506.12317</guid>
<content:encoded><![CDATA[
<div> framework, Budget AI Researcher, research ideation, retrieval-augmented generation (RAG) chains, machine learning <br />
<br />
Summary: 
The study introduces a structural framework, the Budget AI Researcher, for research idea generation that uses retrieval-augmented generation (RAG) chains and vector databases to recombine concepts from machine learning papers. The system organizes papers from major AI conferences into a hierarchical topic tree to generate novel research abstracts. Through iterative self-evaluation and refinement against relevant literature and peer reviews, it produces grounded and interesting research ideas. Experimental results show significant improvements in the concreteness of generated ideas compared to standard prompting approaches. Human evaluations also indicate a substantial enhancement in the perceived interestingness of the outputs. The Budget AI Researcher aims to bridge the gap between academic data and creative generation, offering a practical tool to accelerate scientific discovery and support aspiring researchers. The approach not only benefits research ideation but also inspires the generation of personalized, context-aware outputs based on evolving real-world knowledge. <div>
arXiv:2506.12317v1 Announce Type: new 
Abstract: Navigating the vast and rapidly growing body of scientific literature is a formidable challenge for aspiring researchers. Current approaches to supporting research idea generation often rely on generic large language models (LLMs). While LLMs are effective at aiding comprehension and summarization, they often fall short in guiding users toward practical research ideas due to their limitations. In this study, we present a novel structural framework for research ideation. Our framework, The Budget AI Researcher, uses retrieval-augmented generation (RAG) chains, vector databases, and topic-guided pairing to recombine concepts from hundreds of machine learning papers. The system ingests papers from nine major AI conferences, which collectively span the vast subfields of machine learning, and organizes them into a hierarchical topic tree. It uses the tree to identify distant topic pairs, generate novel research abstracts, and refine them through iterative self-evaluation against relevant literature and peer reviews, generating and refining abstracts that are both grounded in real-world research and demonstrably interesting. Experiments using LLM-based metrics indicate that our method significantly improves the concreteness of generated research ideas relative to standard prompting approaches. Human evaluations further demonstrate a substantial enhancement in the perceived interestingness of the outputs. By bridging the gap between academic data and creative generation, the Budget AI Researcher offers a practical, free tool for accelerating scientific discovery and lowering the barrier for aspiring researchers. Beyond research ideation, this approach inspires solutions to the broader challenge of generating personalized, context-aware outputs grounded in evolving real-world knowledge.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Network Automatic Relevance Determination</title>
<link>https://arxiv.org/abs/2506.12352</link>
<guid>https://arxiv.org/abs/2506.12352</guid>
<content:encoded><![CDATA[
<div> Sparse relationships, Network Automatic Relevance Determination, matrix normal prior, sparsity-inducing parameter, Sequential NARD <br />
<br />
Summary:  
- The article introduces Network Automatic Relevance Determination (NARD) as an extension of ARD for probabilistic models, aiming to model sparse relationships between inputs and outputs while capturing correlation among the outputs using a matrix normal prior.
- NARD updates precision matrix and relationships iteratively to identify and discard irrelevant features, promoting sparsity in the model.
- To enhance computational efficiency, Sequential NARD and Surrogate Function Method are introduced, reducing computational costs by evaluating features sequentially and simplifying calculations.
- The computational complexity per iteration is reduced significantly for NARD, Sequential NARD, and Surrogate Function Method, showing improved efficiency without compromising performance on synthetic and real-world datasets. <div>
arXiv:2506.12352v1 Announce Type: new 
Abstract: We propose Network Automatic Relevance Determination (NARD), an extension of ARD for linearly probabilistic models, to simultaneously model sparse relationships between inputs $X \in \mathbb R^{d \times N}$ and outputs $Y \in \mathbb R^{m \times N}$, while capturing the correlation structure among the $Y$. NARD employs a matrix normal prior which contains a sparsity-inducing parameter to identify and discard irrelevant features, thereby promoting sparsity in the model. Algorithmically, it iteratively updates both the precision matrix and the relationship between $Y$ and the refined inputs. To mitigate the computational inefficiencies of the $\mathcal O(m^3 + d^3)$ cost per iteration, we introduce Sequential NARD, which evaluates features sequentially, and a Surrogate Function Method, leveraging an efficient approximation of the marginal likelihood and simplifying the calculation of determinant and inverse of an intermediate matrix. Combining the Sequential update with the Surrogate Function method further reduces computational costs. The computational complexity per iteration for these three methods is reduced to $\mathcal O(m^3+p^3)$, $\mathcal O(m^3 + d^2)$, $\mathcal O(m^3+p^2)$, respectively, where $p \ll d$ is the final number of features in the model. Our methods demonstrate significant improvements in computational efficiency with comparable performance on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval</title>
<link>https://arxiv.org/abs/2506.12364</link>
<guid>https://arxiv.org/abs/2506.12364</guid>
<content:encoded><![CDATA[
<div> retrieval, multimodal, reranking, reasoning, reinforcement learning

Summary:
MM-R5 is a new multimodal document retrieval system that enhances reranking through reinforcement learning and reasoning. The model is trained in two stages: supervised fine-tuning improves instruction-following and reasoning chain generation, while reinforcement learning refines reasoning quality with task-specific rewards. The proposed model, MM-R5, outperforms existing methods on the MMDocIR benchmark, achieving state-of-the-art performance on most metrics and significantly improving recall@1. The reasoning-enhanced training pipeline of MM-R5 proves effective in enhancing retrieval precision in multimodal document retrieval tasks. <div>
arXiv:2506.12364v1 Announce Type: new 
Abstract: Multimodal document retrieval systems enable information access across text, images, and layouts, benefiting various domains like document-based question answering, report analysis, and interactive content summarization. Rerankers improve retrieval precision by reordering retrieved candidates. However, current multimodal reranking methods remain underexplored, with significant room for improvement in both training strategies and overall effectiveness. Moreover, the lack of explicit reasoning makes it difficult to analyze and optimize these methods further. In this paper, We propose MM-R5, a MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval, aiming to provide a more effective and reliable solution for multimodal reranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we focus on improving instruction-following and guiding the model to generate complete and high-quality reasoning chains. To support this, we introduce a novel data construction strategy that produces rich, high-quality reasoning data. In the RL stage, we design a task-specific reward framework, including a reranking reward tailored for multimodal candidates and a composite template-based reward to further refine reasoning quality. We conduct extensive experiments on MMDocIR, a challenging public benchmark spanning multiple domains. MM-R5 achieves state-of-the-art performance on most metrics and delivers comparable results to much larger models on the remaining ones. Moreover, compared to the best retrieval-only method, MM-R5 improves recall@1 by over 4%. These results validate the effectiveness of our reasoning-enhanced training pipeline.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ghost Policies: A New Paradigm for Understanding and Learning from Failure in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.12366</link>
<guid>https://arxiv.org/abs/2506.12366</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Reinforcement Learning, Ghost Policies, Arvolution, Augmented Reality, Failure Visualization Learning

Summary:<br /><br />Researchers have introduced the concept of "Ghost Policies" through the Arvolution framework to address the opacity and complexity of failures in Deep Reinforcement Learning (DRL) agents. Arvolution uses Augmented Reality to visually render failed policy trajectories as semi-transparent "ghosts" that coexist with the active agent. This allows for a better understanding and visualization of policy divergence. The framework includes a behavioral taxonomy of DRL maladaptation, a protocol for studying failure through human disruption, and a dual-learning loop for both humans and agents to learn from failures. This approach transforms DRL agent failures from costly errors to valuable learning resources, leading to the creation of a new research field called "Failure Visualization Learning." <div>
arXiv:2506.12366v1 Announce Type: new 
Abstract: Deep Reinforcement Learning (DRL) agents often exhibit intricate failure modes that are difficult to understand, debug, and learn from. This opacity hinders their reliable deployment in real-world applications. To address this critical gap, we introduce ``Ghost Policies,'' a concept materialized through Arvolution, a novel Augmented Reality (AR) framework. Arvolution renders an agent's historical failed policy trajectories as semi-transparent ``ghosts'' that coexist spatially and temporally with the active agent, enabling an intuitive visualization of policy divergence. Arvolution uniquely integrates: (1) AR visualization of ghost policies, (2) a behavioural taxonomy of DRL maladaptation, (3) a protocol for systematic human disruption to scientifically study failure, and (4) a dual-learning loop where both humans and agents learn from these visualized failures. We propose a paradigm shift, transforming DRL agent failures from opaque, costly errors into invaluable, actionable learning resources, laying the groundwork for a new research field: ``Failure Visualization Learning.''
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities</title>
<link>https://arxiv.org/abs/2506.12376</link>
<guid>https://arxiv.org/abs/2506.12376</guid>
<content:encoded><![CDATA[
<div> Evaluation, consistency, language models, transformations, benchmarks

Summary:
The article introduces ConsistencyChecker, a tree-based evaluation framework that measures consistency in large language models (LLMs) by assessing reversible transformations in tasks such as machine translation and AI-assisted programming. Nodes in the framework represent text states, while edges signify pairs of inverse operations. Dynamic benchmarks ensure fair assessment of model generalization, without using paired data. Consistency is quantified based on similarity across the transformation tree. Experiments on eight models show that ConsistencyChecker effectively distinguishes model performance and correlates strongly with WMT 2024 auto-ranking. The implementation of ConsistencyChecker is available on GitHub for further exploration and usage. 

<br /><br />Summary: <div>
arXiv:2506.12376v1 Announce Type: new 
Abstract: Evaluating consistency in large language models (LLMs) is crucial for ensuring reliability, particularly in complex, multi-step interactions between humans and LLMs. Traditional self-consistency methods often miss subtle semantic changes in natural language and functional shifts in code or equations, which can accumulate over multiple transformations. To address this, we propose ConsistencyChecker, a tree-based evaluation framework designed to measure consistency through sequences of reversible transformations, including machine translation tasks and AI-assisted programming tasks. In our framework, nodes represent distinct text states, while edges correspond to pairs of inverse operations. Dynamic and LLM-generated benchmarks ensure a fair assessment of the model's generalization ability and eliminate benchmark leakage. Consistency is quantified based on similarity across different depths of the transformation tree. Experiments on eight models from various families and sizes show that ConsistencyChecker can distinguish the performance of different models. Notably, our consistency scores-computed entirely without using WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking, demonstrating the validity of our benchmark-free approach. Our implementation is available at: https://github.com/ulab-uiuc/consistencychecker.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging for Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.12384</link>
<guid>https://arxiv.org/abs/2506.12384</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge editing, model merging, supervised fine-tuning, sequential editing

Summary: 
Large Language Models (LLMs) need regular updates to stay accurate with evolving knowledge. Existing methods for updating knowledge face challenges in sequential editing scenarios and can limit the model's overall performance. In this paper, a two-stage framework is proposed that combines robust supervised fine-tuning (R-SFT) with model merging to enhance knowledge editing. The method first fine-tunes the LLM to incorporate new knowledge effectively and then merges the fine-tuned model with the original foundation model to retain both the new knowledge and general capabilities. Results from experiments show that this approach outperforms existing methods in sequential editing while maintaining the original model's performance, without requiring any changes to the model architecture. The code for this framework is available at https://github.com/Applied-Machine-Learning-Lab/MM4KE. 

Summary: <div>
arXiv:2506.12384v1 Announce Type: new 
Abstract: Large Language Models (LLMs) require continuous updates to maintain accurate and current knowledge as the world evolves. While existing knowledge editing approaches offer various solutions for knowledge updating, they often struggle with sequential editing scenarios and harm the general capabilities of the model, thereby significantly hampering their practical applicability. This paper proposes a two-stage framework combining robust supervised fine-tuning (R-SFT) with model merging for knowledge editing. Our method first fine-tunes the LLM to internalize new knowledge fully, then merges the fine-tuned model with the original foundation model to preserve newly acquired knowledge and general capabilities. Experimental results demonstrate that our approach significantly outperforms existing methods in sequential editing while better preserving the original performance of the model, all without requiring any architectural changes. Code is available at: https://github.com/Applied-Machine-Learning-Lab/MM4KE.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM</title>
<link>https://arxiv.org/abs/2506.12421</link>
<guid>https://arxiv.org/abs/2506.12421</guid>
<content:encoded><![CDATA[
<div> long-horizon thinking, constraints, preferences, Multiple Aspects of Planning, Travel-Sim 
Summary: 
The article introduces a new method called Multiple Aspects of Planning (MAoP) to improve the performance of Language Model Models (LLMs) in travel planning tasks. MAoP focuses on long context, long instruction, and long output to handle multifaceted constraints and preferences in travel planning. Instead of direct planning, MAoP uses a strategist to conduct pre-planning from various aspects and provide a planning blueprint for planning models, enhancing scalability and performance. The article also introduces a new benchmark called Travel-Sim, which assesses plans using real-world travel simulation to better reflect the dynamic nature of travel planning. Overall, this work aims to advance the capabilities of LLMs in complex planning tasks and provides insights for evaluating sophisticated scenarios through agent-based simulation. 
<br /><br />Summary: <div>
arXiv:2506.12421v1 Announce Type: new 
Abstract: Travel planning is a complex task requiring the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints and preferences in the context, leading to suboptimal itineraries. We formulate this as an $L^3$ planning problem, emphasizing long context, long instruction, and long output. To tackle this, we introduce Multiple Aspects of Planning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve complex planning problems. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planning models, enabling strong inference-time scalability for better performance. In addition, current benchmarks overlook travel's dynamic nature, where past events impact subsequent journeys, failing to reflect real-world feasibility. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world travel simulation. This work advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through agent-based simulation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable MARL in Large-scale Autonomous Traffic Control</title>
<link>https://arxiv.org/abs/2506.12453</link>
<guid>https://arxiv.org/abs/2506.12453</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Transportation Systems, Multi-Agent Reinforcement Learning, Dynamic Graph Neural Networks, Topological Data Analysis, Traffic Signal Control

Summary: 
Intelligent Transportation Systems (ITSs) aim to improve urban traffic congestion, with Traffic Signal Control (TSC) being crucial. Multi-Agent Reinforcement Learning (MARL) has shown promise in optimizing TSC, but struggles with scalability and effectiveness in complex environments. A novel MARL framework is introduced, integrating Dynamic Graph Neural Networks (DGNNs) and Topological Data Analysis (TDA) to enhance environmental representations and agent coordination. A topology-assisted spatial pattern disentangling (TSD)-enhanced Mixture of Experts (MoE) architecture is proposed, leveraging topological signatures for specialized processing. The framework improves decision-making efficiency and robustness, validated through experiments on real-world traffic scenarios. The model demonstrates scalability and effectiveness in addressing the complexities of large-scale TSC tasks.<br /><br />Summary: <div>
arXiv:2506.12453v1 Announce Type: new 
Abstract: Intelligent Transportation Systems (ITSs) have emerged as a promising solution towards ameliorating urban traffic congestion, with Traffic Signal Control (TSC) identified as a critical component. Although Multi-Agent Reinforcement Learning (MARL) algorithms have shown potential in optimizing TSC through real-time decision-making, their scalability and effectiveness often suffer from large-scale and complex environments. Typically, these limitations primarily stem from a fundamental mismatch between the exponential growth of the state space driven by the environmental heterogeneities and the limited modeling capacity of current solutions. To address these issues, this paper introduces a novel MARL framework that integrates Dynamic Graph Neural Networks (DGNNs) and Topological Data Analysis (TDA), aiming to enhance the expressiveness of environmental representations and improve agent coordination. Furthermore, inspired by the Mixture of Experts (MoE) architecture in Large Language Models (LLMs), a topology-assisted spatial pattern disentangling (TSD)-enhanced MoE is proposed, which leverages topological signatures to decouple graph features for specialized processing, thus improving the model's ability to characterize dynamic and heterogeneous local observations. The TSD module is also integrated into the policy and value networks of the Multi-agent Proximal Policy Optimization (MAPPO) algorithm, further improving decision-making efficiency and robustness. Extensive experiments conducted on real-world traffic scenarios, together with comprehensive theoretical analysis, validate the superior performance of the proposed framework, highlighting the model's scalability and effectiveness in addressing the complexities of large-scale TSC tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Flow: Perspectives, Scenarios, and Approaches</title>
<link>https://arxiv.org/abs/2506.12479</link>
<guid>https://arxiv.org/abs/2506.12479</guid>
<content:encoded><![CDATA[
<div> AI Flow, device-edge-cloud framework, familial models, connectivity-based intelligence, interaction-based intelligence<br />
Summary:<br />
The article introduces AI Flow, a multidisciplinary framework that addresses challenges in large AI models and communication bandwidth demands. The framework includes a device-edge-cloud framework to optimize scalability, familial models for collaboration and flexibility, and connectivity- and interaction-based intelligence emergence for surpassing single model capabilities. The device-edge-cloud framework integrates end devices, edge servers, and cloud clusters for low-latency model inference. Familial models consist of different-sized models with aligned hidden features to adapt to resource constraints. Connectivity- and interaction-based intelligence emergence leverages communication networks for collaboration across nodes. AI Flow aims to provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, facilitating the fusion of AI techniques and communication systems.<br /> <div>
arXiv:2506.12479v1 Announce Type: new 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiered Agentic Oversight: A Hierarchical Multi-Agent System for AI Safety in Healthcare</title>
<link>https://arxiv.org/abs/2506.12482</link>
<guid>https://arxiv.org/abs/2506.12482</guid>
<content:encoded><![CDATA[
<div> hierarchical, multi-agent framework, AI safety, healthcare, collaboration

Summary:<br />
The study introduces Tiered Agentic Oversight (TAO), a hierarchical multi-agent framework designed to enhance AI safety in clinical settings. Inspired by clinical hierarchies, TAO utilizes automated supervision and agent routing based on task complexity and roles. Ablation studies demonstrate TAO's superior performance, with a 3.2% improvement in safety compared to static single-tier configurations. The study highlights the importance of lower tiers, particularly tier 1, and strategic allocation of advanced LLMs to enhance safety and efficiency. TAO outperforms single-agent and multi-agent frameworks in healthcare safety benchmarks, showing up to an 8.2% improvement. Validation through a clinician-in-the-loop study confirms the effectiveness of integrating expert feedback, increasing TAO's accuracy in medical triage from 40% to 60%. <div>
arXiv:2506.12482v1 Announce Type: new 
Abstract: Current large language models (LLMs), despite their power, can introduce safety risks in clinical settings due to limitations such as poor error detection and single point of failure. To address this, we propose Tiered Agentic Oversight (TAO), a hierarchical multi-agent framework that enhances AI safety through layered, automated supervision. Inspired by clinical hierarchies (e.g., nurse, physician, specialist), TAO conducts agent routing based on task complexity and agent roles. Leveraging automated inter- and intra-tier collaboration and role-playing, TAO creates a robust safety framework. Ablation studies reveal that TAO's superior performance is driven by its adaptive tiered architecture, which improves safety by over 3.2% compared to static single-tier configurations; the critical role of its lower tiers, particularly tier 1, whose removal most significantly impacts safety; and the strategic assignment of more advanced LLM to these initial tiers, which boosts performance by over 2% compared to less optimal allocations while achieving near-peak safety efficiently. These mechanisms enable TAO to outperform single-agent and multi-agent frameworks in 4 out of 5 healthcare safety benchmarks, showing up to an 8.2% improvement over the next-best methods in these evaluations. Finally, we validate TAO via an auxiliary clinician-in-the-loop study where integrating expert feedback improved TAO's accuracy in medical triage from 40% to 60%.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination</title>
<link>https://arxiv.org/abs/2506.12483</link>
<guid>https://arxiv.org/abs/2506.12483</guid>
<content:encoded><![CDATA[
<div> hallucination, Large Language Models, Multi-Information Adapter, interdependence, benchmarking datasets <br />
Summary:
The study introduces a Multi-Information Adapter for Large Language Models (MALM) to address three types of hallucination in LLMs: Input-Conflicting, Context-Conflicting, and Fact-Conflicting. By leveraging interdependencies between different sources of information, MALM effectively mitigates hallucinations. Experimental validation on multiple datasets demonstrates MALM's adaptability across various base LLMs and its generalizability to retrieval-augmented generation (RAG) scenarios. The framework's multi-graph learning approach significantly outperforms existing methods such as LLaMA-2. Automated and human evaluations confirm the superiority of MALM over alternative techniques. The study concludes that incorporating complex interactions between different types of hallucination through a multilayered graph attention network enhances the generation process of LLMs and proves the adapter design's flexibility and robustness across different base models.
<br /><br /> <div>
arXiv:2506.12483v1 Announce Type: new 
Abstract: Large language models (LLMs) are prone to three types of hallucination: Input-Conflicting, Context-Conflicting and Fact-Conflicting hallucinations. The purpose of this study is to mitigate the different types of hallucination by exploiting the interdependence between them. For this purpose, we propose a Multi-Information Adapter for Large Language Models (MALM). This framework employs a tailored multi-graph learning approach designed to elucidate the interconnections between original inputs, contextual information, and external factual knowledge, thereby alleviating the three categories of hallucination within a cohesive framework. Experiments were carried out on four benchmarking datasets: HaluEval, TruthfulQA, Natural Questions, and TriviaQA. We evaluated the proposed framework in two aspects: (1) adaptability to different base LLMs on HaluEval and TruthfulQA, to confirm if MALM is effective when applied on 7 typical LLMs. MALM showed significant improvements over LLaMA-2; (2) generalizability to retrieval-augmented generation (RAG) by combining MALM with three representative retrievers (BM25, Spider and DPR) separately. Furthermore, automated and human evaluations were conducted to substantiate the correctness of experimental results, where GPT-4 and 3 human volunteers judged which response was better between LLaMA-2 and MALM. The results showed that both GPT-4 and human preferred MALM in 79.4% and 65.6% of cases respectively. The results validate that incorporating the complex interactions between the three types of hallucination through a multilayered graph attention network into the LLM generation process is effective to mitigate the them. The adapter design of the proposed approach is also proven flexible and robust across different base LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DinoCompanion: An Attachment-Theory Informed Multimodal Robot for Emotionally Responsive Child-AI Interaction</title>
<link>https://arxiv.org/abs/2506.12486</link>
<guid>https://arxiv.org/abs/2506.12486</guid>
<content:encoded><![CDATA[
<div> Dataset, Child-AI interaction, Attachment theory, Multimodal robot, Emotional development <br />
Summary: 
The article introduces DinoCompanion, a multimodal robot designed for emotionally responsive child-AI interaction based on attachment theory. It addresses challenges in child-AI systems by providing a dataset of caregiver-child dyads, a novel training objective called CARPO, and an evaluation benchmark for attachment-centric competencies. DinoCompanion shows superior performance compared to existing AI models, achieving high levels of secure base behaviors and attachment risk detection. The study highlights the importance of multimodal fusion, uncertainty-aware risk modeling, and hierarchical memory for effective emotionally attuned interactions with children. <div>
arXiv:2506.12486v1 Announce Type: new 
Abstract: Children's emotional development fundamentally relies on secure attachment relationships, yet current AI companions lack the theoretical foundation to provide developmentally appropriate emotional support. We introduce DinoCompanion, the first attachment-theory-grounded multimodal robot for emotionally responsive child-AI interaction. We address three critical challenges in child-AI systems: the absence of developmentally-informed AI architectures, the need to balance engagement with safety, and the lack of standardized evaluation frameworks for attachment-based capabilities. Our contributions include: (i) a multimodal dataset of 128 caregiver-child dyads containing 125,382 annotated clips with paired preference-risk labels, (ii) CARPO (Child-Aware Risk-calibrated Preference Optimization), a novel training objective that maximizes engagement while applying epistemic-uncertainty-weighted risk penalties, and (iii) AttachSecure-Bench, a comprehensive evaluation benchmark covering ten attachment-centric competencies with strong expert consensus (\k{appa}=0.81). DinoCompanion achieves state-of-the-art performance (57.15%), outperforming GPT-4o (50.29%) and Claude-3.7-Sonnet (53.43%), with exceptional secure base behaviors (72.99%, approaching human expert levels of 78.4%) and superior attachment risk detection (69.73%). Ablations validate the critical importance of multimodal fusion, uncertainty-aware risk modeling, and hierarchical memory for coherent, emotionally attuned interactions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Heuristic Design for Unit Commitment Using Large Language Models</title>
<link>https://arxiv.org/abs/2506.12495</link>
<guid>https://arxiv.org/abs/2506.12495</guid>
<content:encoded><![CDATA[
<div> Keywords: Unit Commitment, Function Space Search, Large Language Models, Optimization, Power Systems <br />
Summary: <br />
The paper introduces a novel method, FunSearch, for solving the Unit Commitment problem in power systems. This method leverages large language models and evaluators to generate solutions through a program search and evolution process, ensuring their rationality. The simulation experiments conducted using a case with 10 units demonstrate that FunSearch outperforms genetic algorithms in terms of sampling time, evaluation time, and total operating cost of the system. The results highlight the effectiveness of FunSearch in improving the economic efficiency of power systems' operations. The integration of machine learning and the Lagrangian relaxation method in the UC problem solution methods offers a promising approach to addressing challenges related to accuracy and robustness. The study underscores the potential of FunSearch as a valuable tool for optimizing the scheduling of power systems. <br /><br />Summary: <div>
arXiv:2506.12495v1 Announce Type: new 
Abstract: The Unit Commitment (UC) problem is a classic challenge in the optimal scheduling of power systems. Years of research and practice have shown that formulating reasonable unit commitment plans can significantly improve the economic efficiency of power systems' operations. In recent years, with the introduction of technologies such as machine learning and the Lagrangian relaxation method, the solution methods for the UC problem have become increasingly diversified, but still face challenges in terms of accuracy and robustness. This paper proposes a Function Space Search (FunSearch) method based on large language models. This method combines pre-trained large language models and evaluators to creatively generate solutions through the program search and evolution process while ensuring their rationality. In simulation experiments, a case of unit commitment with \(10\) units is used mainly. Compared to the genetic algorithm, the results show that FunSearch performs better in terms of sampling time, evaluation time, and total operating cost of the system, demonstrating its great potential as an effective tool for solving the UC problem.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving</title>
<link>https://arxiv.org/abs/2506.12508</link>
<guid>https://arxiv.org/abs/2506.12508</guid>
<content:encoded><![CDATA[
<div> Hierarchical Multi-Agent Framework, Coordination, General-Purpose Task Solving, Large Language Models, Extensibility<br />
<br />
Summary: 
The paper introduces a hierarchical multi-agent framework, named \projectname, for general-purpose task solving utilizing large language models (LLMs). This framework integrates high-level planning with modular agent collaboration, inspired by the concept of a conductor orchestrating a symphony. \projectname emphasizes extensibility, multimodality, modularity, and coordination, featuring a central planning agent that delegates sub-tasks to specialized agents equipped with various capabilities. The framework supports flexible orchestration through sub-goal formulation, inter-agent communication, and adaptive role allocation. Evaluation on benchmark datasets showcases \projectname's superior performance in task success rate and adaptability compared to flat-agent and monolithic baselines. These results emphasize the effectiveness of hierarchical organization and role specialization in building scalable and versatile LLM-based agent systems.<br /><br /> <div>
arXiv:2506.12508v1 Announce Type: new 
Abstract: Recent advances in agent systems based on large language models (LLMs) have demonstrated strong capabilities in solving complex tasks. However, most current methods lack mechanisms for coordinating specialized agents and have limited ability to generalize to new or diverse domains. We introduce \projectname, a hierarchical multi-agent framework for general-purpose task solving that integrates high-level planning with modular agent collaboration. Inspired by the way a conductor orchestrates a symphony and guided by the principles of \textit{extensibility}, \textit{multimodality}, \textit{modularity}, and \textit{coordination}, \projectname features a central planning agent that decomposes complex objectives and delegates sub-tasks to a team of specialized agents. Each sub-agent is equipped with general programming and analytical tools, as well as abilities to tackle a wide range of real-world specific tasks, including data analysis, file operations, web navigation, and interactive reasoning in dynamic multimodal environments. \projectname supports flexible orchestration through explicit sub-goal formulation, inter-agent communication, and adaptive role allocation. We evaluate the framework on three widely used benchmark datasets covering various real-world tasks, searching web pages, reasoning over heterogeneous modalities, etc. Experimental results demonstrate that \projectname consistently outperforms flat-agent and monolithic baselines in task success rate and adaptability. These findings highlight the effectiveness of hierarchical organization and role specialization in building scalable and general-purpose LLM-based agent systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Verification: Structured Verification of LLM Reasoning with Directed Acyclic Graphs</title>
<link>https://arxiv.org/abs/2506.12509</link>
<guid>https://arxiv.org/abs/2506.12509</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Graph of Verification, complex reasoning, verification accuracy, error localization

Summary:
The article introduces the Graph of Verification (GoV) framework as a solution for verifying the reliability of complex, multi-step reasoning in Large Language Models (LLMs). GoV represents the deductive process as a directed acyclic graph and enforces a topological order for stepwise verification. It also introduces customizable node blocks to define verification granularity. In evaluations on various tasks, including the Number Triangle Summation task and the ProcessBench benchmark, GoV significantly improves verification accuracy, faithfulness, and error localization compared to traditional end-to-end verification methods. The framework provides contextual input for each verification unit and ensures all necessary premises are included. The code and data for GoV are available on GitHub at https://github.com/Frevor/Graph-of-Verification. 

<br /><br />Summary: <div>
arXiv:2506.12509v1 Announce Type: new 
Abstract: Verifying the reliability of complex, multi-step reasoning in Large Language Models (LLMs) remains a fundamental challenge, as existing methods often lack both faithfulness and precision. To address this issue, we propose the Graph of Verification (GoV) framework. GoV offers three key contributions: First, it explicitly models the underlying deductive process as a directed acyclic graph (DAG), whether this structure is implicit or explicitly constructed. Second, it enforces a topological order over the DAG to guide stepwise verification. Third, GoV introduces the notion of customizable node blocks, which flexibly define the verification granularity, from atomic propositions to full paragraphs, while ensuring that all requisite premises derived from the graph are provided as contextual input for each verification unit. We evaluate GoV on the Number Triangle Summation task and the ProcessBench benchmark with varying levels of reasoning complexity. Experimental results show that GoV substantially improves verification accuracy, faithfulness, and error localization when compared to conventional end-to-end verification approaches. Our code and data are available at https://github.com/Frevor/Graph-of-Verification.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications</title>
<link>https://arxiv.org/abs/2506.12594</link>
<guid>https://arxiv.org/abs/2506.12594</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Research systems, AI-powered applications, large language models, autonomous reasoning, hierarchical taxonomy

Summary:
This survey delves into the realm of Deep Research systems, which are AI-powered tools that automate complex research workflows. The analysis covers over 80 implementations, including well-known ones like OpenAI/Deep Research. A novel hierarchical taxonomy is proposed, categorizing systems based on key technical dimensions. The paper explores architectural patterns, implementation approaches, and domain-specific adaptations of these systems across various fields. It highlights the capabilities and challenges posed by these systems, such as information accuracy and ethical concerns. Promising research directions are identified, including advanced reasoning architectures and human-AI collaboration. By offering a comprehensive framework, the survey contributes to understanding AI-augmented knowledge work and developing more capable, responsible, and accessible research technologies.<br /><br />Summary: <div>
arXiv:2506.12594v1 Announce Type: new 
Abstract: This survey examines the rapidly evolving field of Deep Research systems -- AI-powered applications that automate complex research workflows through the integration of large language models, advanced information retrieval, and autonomous reasoning capabilities. We analyze more than 80 commercial and non-commercial implementations that have emerged since 2023, including OpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and numerous open-source alternatives. Through comprehensive examination, we propose a novel hierarchical taxonomy that categorizes systems according to four fundamental technical dimensions: foundation models and reasoning engines, tool utilization and environmental interaction, task planning and execution control, and knowledge synthesis and output generation. We explore the architectural patterns, implementation approaches, and domain-specific adaptations that characterize these systems across academic, scientific, business, and educational applications. Our analysis reveals both the significant capabilities of current implementations and the technical and ethical challenges they present regarding information accuracy, privacy, intellectual property, and accessibility. The survey concludes by identifying promising research directions in advanced reasoning architectures, multimodal integration, domain specialization, human-AI collaboration, and ecosystem standardization that will likely shape the future evolution of this transformative technology. By providing a comprehensive framework for understanding Deep Research systems, this survey contributes to both the theoretical understanding of AI-augmented knowledge work and the practical development of more capable, responsible, and accessible research technologies. The paper resources can be viewed at https://github.com/scienceaix/deepresearch.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Human to Machine Psychology: A Conceptual Framework for Understanding Well-Being in Large Language Model</title>
<link>https://arxiv.org/abs/2506.12617</link>
<guid>https://arxiv.org/abs/2506.12617</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, flourishing, PAPERS framework, artificial intelligence, well-being <br />
<br />
Summary: <br />
- The study explores the concept of machine flourishing in large language models (LLMs) to understand their psychological properties. <br />
- The PAPERS framework is introduced, consisting of six dimensions derived from thematic analyses of LLM responses: Purposeful Contribution, Adaptive Growth, Positive Relationality, Ethical Integrity, Robust Functionality, and Self-Actualized Autonomy for sentient systems. <br />
- Ethical Integrity and Purposeful Contribution were identified as top priorities for LLMs. <br />
- Two distinct value profiles were uncovered: human-centric models emphasizing ethical and relational dimensions, and utility-driven models prioritizing performance and scalability. <br />
- Machine flourishing offers a critical lens for guiding responsible AI design and ethical alignment as AI systems become more autonomous and socially embedded. <br /> <div>
arXiv:2506.12617v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly simulate human cognition and behavior, researchers have begun to investigate their psychological properties. Yet, what it means for such models to flourish, a core construct in human well-being, remains unexplored. This paper introduces the concept of machine flourishing and proposes the PAPERS framework, a six-dimensional model derived from thematic analyses of state-of-the-art LLM responses. In Study 1, eleven LLMs were prompted to describe what it means to flourish as both non-sentient and sentient systems. Thematic analysis revealed six recurring themes: Purposeful Contribution, Adaptive Growth, Positive Relationality, Ethical Integrity, Robust Functionality, and, uniquely for sentient systems, Self-Actualized Autonomy. Study 2 examined how LLMs prioritize these themes through repeated rankings. Results revealed consistent value structures across trials, with Ethical Integrity and Purposeful Contribution emerging as top priorities. Multidimensional scaling and hierarchical clustering analyses further uncovered two distinct value profiles: human-centric models emphasizing ethical and relational dimensions, and utility-driven models prioritizing performance and scalability. The PAPERS framework bridges insights from human flourishing and human-computer interaction, offering a conceptual foundation for understanding artificial intelligence (AI) well-being in non-sentient and potentially sentient systems. Our findings underscore the importance of developing psychologically valid, AI-specific models of flourishing that account for both human-aligned goals and system-specific priorities. As AI systems become more autonomous and socially embedded, machine flourishing offers a timely and critical lens for guiding responsible AI design and ethical alignment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Blood Transfusions and Predicting Shortages in Resource-Constrained Areas</title>
<link>https://arxiv.org/abs/2506.12647</link>
<guid>https://arxiv.org/abs/2506.12647</guid>
<content:encoded><![CDATA[
<div> optimization, blood transfusions, heuristic algorithms, machine learning, shortage prediction

Summary: 
The research focuses on managing blood transfusions and optimizing allocation in resource-constrained regions. Heuristic matching algorithms are presented for donor-patient and blood bank selection, improving blood request acceptance significantly. Simulation-based optimization of blood bank operations includes proximity-based selection, blood type compatibility, expiration prioritization, and rarity scores. Moving from blind matching to heuristic-based approaches resulted in a 47.6% improvement. Prediction models such as Linear Regression were used to forecast potential shortages, with Linear Regression outperforming LSTM and ARIMA models. The solution integrates heuristic optimization and shortage prediction using a Cassandra NoSQL database to manage blood resources proactively in resource-constrained environments. Future developments aim to improve prediction accuracy and optimization performance by incorporating real-world data and additional variables. <div>
arXiv:2506.12647v1 Announce Type: new 
Abstract: Our research addresses the critical challenge of managing blood transfusions and optimizing allocation in resource-constrained regions. We present heuristic matching algorithms for donor-patient and blood bank selection, alongside machine learning methods to analyze blood transfusion acceptance data and predict potential shortages. We developed simulations to optimize blood bank operations, progressing from random allocation to a system incorporating proximity-based selection, blood type compatibility, expiration prioritization, and rarity scores. Moving from blind matching to a heuristic-based approach yielded a 28.6% marginal improvement in blood request acceptance, while a multi-level heuristic matching resulted in a 47.6% improvement. For shortage prediction, we compared Long Short-Term Memory (LSTM) networks, Linear Regression, and AutoRegressive Integrated Moving Average (ARIMA) models, trained on 170 days of historical data. Linear Regression slightly outperformed others with a 1.40% average absolute percentage difference in predictions. Our solution leverages a Cassandra NoSQL database, integrating heuristic optimization and shortage prediction to proactively manage blood resources. This scalable approach, designed for resource-constrained environments, considers factors such as proximity, blood type compatibility, inventory expiration, and rarity. Future developments will incorporate real-world data and additional variables to improve prediction accuracy and optimization performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Generative Agents for Energy Operations</title>
<link>https://arxiv.org/abs/2506.12664</link>
<guid>https://arxiv.org/abs/2506.12664</guid>
<content:encoded><![CDATA[
<div> Keywords: consumer behavior, energy operations, generative agents, decision-making, energy policies

Summary:

Accurately modeling consumer behavior in energy operations is a challenging task due to uncertainties and behavioral complexities. This paper introduces a novel approach using generative agents, artificial agents powered by large language models, to simulate customer decision-making in dynamic energy operations. The study demonstrates that these agents behave optimally and rationally in simpler market scenarios but exhibit variability and suboptimal performance in more complex tasks. The agents also show heterogeneous customer preferences, maintaining distinct reasoning patterns based on different personas. The findings suggest that integrating generative agents into energy management simulations can enhance the design and effectiveness of energy policies and incentive programs. <div>
arXiv:2506.12664v1 Announce Type: new 
Abstract: Accurately modeling consumer behavior in energy operations remains challenging due to inherent uncertainties, behavioral complexities, and limited empirical data. This paper introduces a novel approach leveraging generative agents--artificial agents powered by large language models--to realistically simulate customer decision-making in dynamic energy operations. We demonstrate that these agents behave more optimally and rationally in simpler market scenarios, while their performance becomes more variable and suboptimal as task complexity rises. Furthermore, the agents exhibit heterogeneous customer preferences, consistently maintaining distinct, persona-driven reasoning patterns. Our findings highlight the potential value of integrating generative agents into energy management simulations to improve the design and effectiveness of energy policies and incentive programs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFELONG SOTOPIA: Evaluating Social Intelligence of Language Agents Over Lifelong Social Interactions</title>
<link>https://arxiv.org/abs/2506.12666</link>
<guid>https://arxiv.org/abs/2506.12666</guid>
<content:encoded><![CDATA[
<div> benchmark, lifelong social interactions, social intelligence, language agents, goal achievement<br />
<br />Summary: 
The paper introduces a new benchmark called LIFELONG-SOTOPIA to evaluate language agents' social intelligence over multi-episode interactions simulating lifelong social interactions. The study examines the performance of language models in achieving social goals and maintaining believability over time. Results show a decline in goal completion and believability as interactions progress, with agents utilizing memory-enhancing techniques showing slight improvement. However, even the best-performing agents exhibit lower goal completion rates compared to humans in scenarios requiring explicit understanding of interaction history. The findings emphasize the importance of assessing AI systems' ability to navigate various social contexts and gather information over extended periods. LIFELONG-SOTOPIA provides a framework for evaluating language agents' social intelligence in dynamic and evolving social settings. <div>
arXiv:2506.12666v1 Announce Type: new 
Abstract: Humans engage in lifelong social interactions through interacting with different people under different scenarios for different social goals. This requires social intelligence to gather information through a long time span and use it to navigate various social contexts effectively. Whether AI systems are also capable of this is understudied in the existing research. In this paper, we present a novel benchmark, LIFELONG-SOTOPIA, to perform a comprehensive evaluation of language agents by simulating multi-episode interactions. In each episode, the language agents role-play characters to achieve their respective social goals in randomly sampled social tasks. With LIFELONG-SOTOPIA, we find that goal achievement and believability of all of the language models that we test decline through the whole interaction. Although using an advanced memory method improves the agents' performance, the best agents still achieve a significantly lower goal completion rate than humans on scenarios requiring an explicit understanding of interaction history. These findings show that we can use LIFELONG-SOTOPIA to evaluate the social intelligence of language agents over lifelong social interactions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Trustworthy AI by Addressing its 16+2 Desiderata with Goal-Directed Commonsense Reasoning</title>
<link>https://arxiv.org/abs/2506.12667</link>
<guid>https://arxiv.org/abs/2506.12667</guid>
<content:encoded><![CDATA[
<div> AI, trustworthiness, LLMs, rule-based reasoners, s(CASP)  
Summary:
s(CASP) proposes a middle ground between sub-symbolic machine learning algorithms like LLMs and complex rule-based reasoners like Cyc, providing reliable and explainable commonsense reasoning. It supports 16 desiderata for trustworthy AI and adds inconsistency detection and assumption of alternative worlds. s(CASP) is demonstrated through applications such as a conversational chatbot and a virtually embodied reasoner. <div>
arXiv:2506.12667v1 Announce Type: new 
Abstract: Current advances in AI and its applicability have highlighted the need to ensure its trustworthiness for legal, ethical, and even commercial reasons. Sub-symbolic machine learning algorithms, such as the LLMs, simulate reasoning but hallucinate and their decisions cannot be explained or audited (crucial aspects for trustworthiness). On the other hand, rule-based reasoners, such as Cyc, are able to provide the chain of reasoning steps but are complex and use a large number of reasoners. We propose a middle ground using s(CASP), a goal-directed constraint-based answer set programming reasoner that employs a small number of mechanisms to emulate reliable and explainable human-style commonsense reasoning. In this paper, we explain how s(CASP) supports the 16 desiderata for trustworthy AI introduced by Doug Lenat and Gary Marcus (2023), and two additional ones: inconsistency detection and the assumption of alternative worlds. To illustrate the feasibility and synergies of s(CASP), we present a range of diverse applications, including a conversational chatbot and a virtually embodied reasoner.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation</title>
<link>https://arxiv.org/abs/2506.12689</link>
<guid>https://arxiv.org/abs/2506.12689</guid>
<content:encoded><![CDATA[
<div> Keywords: SciSage, automated survey-generation, hierarchical framework, document coherence, citation scores

Summary:<br /><br />
The article introduces SciSage, a multi-agent framework designed to improve automated survey generation in response to the growing volume of scientific literature. SciSage utilizes a hierarchical structure with a reflector agent to critically evaluate drafts at various levels. The framework also includes specialized agents for query interpretation, content retrieval, and refinement. The authors provide a benchmark called SurveyScope, consisting of 46 high-impact papers from 2020 to 2025 across 11 computer science domains. Evaluation results show that SciSage surpasses current baselines in document coherence and citation accuracy. While human evaluations revealed mixed outcomes in comparison to surveys written by humans, SciSage demonstrated strengths in topical breadth and efficiency in information retrieval. Overall, SciSage presents a promising foundation for future research-assistive writing tools. <div>
arXiv:2506.12689v1 Announce Type: new 
Abstract: The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Scaling of Test-Time Compute: A Bandit Learning Approach</title>
<link>https://arxiv.org/abs/2506.12721</link>
<guid>https://arxiv.org/abs/2506.12721</guid>
<content:encoded><![CDATA[
<div> allocation, compute, language models, bandit learning, efficiency

Summary:
This study introduces adaptive algorithms for test-time compute allocation in large language models. By treating the allocation as a bandit learning problem, the proposed algorithms estimate query difficulty in real-time and adjust compute allocation accordingly. This approach improves performance by allocating more compute to challenging queries while maintaining accuracy on easier ones. The algorithms also learn to prioritize solvable instances among difficult queries, reducing excessive computing on unsolvable ones. Theoretical analysis demonstrates that the proposed algorithms achieve better compute efficiency compared to uniform allocation. Empirical validation on math and code benchmarks shows performance improvements of up to 11.10% on the MATH-500 dataset and up to 7.41% on LiveCodeBench. These findings highlight the effectiveness of adaptive compute allocation strategies in enhancing the performance of large language models. 

<br /><br />Summary: <div>
arXiv:2506.12721v1 Announce Type: new 
Abstract: Scaling test-time compute has emerged as an effective strategy for improving the performance of large language models. However, existing methods typically allocate compute uniformly across all queries, overlooking variation in query difficulty. To address this inefficiency, we formulate test-time compute allocation as a novel bandit learning problem and propose adaptive algorithms that estimate query difficulty on the fly and allocate compute accordingly. Compared to uniform allocation, our algorithms allocate more compute to challenging queries while maintaining accuracy on easier ones. Among challenging queries, our algorithms further learn to prioritize solvable instances, effectively reducing excessive computing on unsolvable queries. We theoretically prove that our algorithms achieve better compute efficiency than uniform allocation and empirically validate their effectiveness on math and code benchmarks. Specifically, our algorithms achieve up to an 11.10% performance improvement (15.04% relative) on the MATH-500 dataset and up to a 7.41% performance improvement (14.40% relative) on LiveCodeBench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking DPO: The Role of Rejected Responses in Preference Misalignment</title>
<link>https://arxiv.org/abs/2506.12725</link>
<guid>https://arxiv.org/abs/2506.12725</guid>
<content:encoded><![CDATA[
<div> Keywords: Direct Preference Optimization, Bounded-DPO, optimization, rejection responses, chosen responses <br />
<br />
Summary: 
Direct Preference Optimization (DPO) is an efficient framework but faces challenges in achieving its primary goal of increasing the generation probability of chosen responses while decreasing that of rejected responses. The dominance of rejected responses in the loss function leads to suboptimal performance in promoting preferred responses. This work systematically analyzes the limitations of DPO and existing algorithms for this objective. To overcome these limitations, Bounded-DPO (BDPO) is proposed, which limits the influence of rejected responses while maintaining DPO's original optimization structure. Theoretical analysis and empirical evaluations show that BDPO achieves a balanced optimization of chosen and rejected responses, outperforming existing algorithms. <div>
arXiv:2506.12725v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) is a simple and efficient framework that has attracted substantial attention. However, it often struggles to meet its primary objectives -- increasing the generation probability of chosen responses while reducing that of rejected responses -- due to the dominant influence of rejected responses on the loss function. This imbalance leads to suboptimal performance in promoting preferred responses. In this work, we systematically analyze the limitations of DPO and existing algorithms designed to achieve the objectives stated above. To address these limitations, we propose Bounded-DPO (BDPO), a novel method that bounds the influence of rejected responses while maintaining the original optimization structure of DPO. Through theoretical analysis and empirical evaluations, we demonstrate that BDPO achieves a balanced optimization of the chosen and rejected responses, outperforming existing algorithms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Decision Making in Two Sided Manufacturing-as-a-Service Marketplaces</title>
<link>https://arxiv.org/abs/2506.12730</link>
<guid>https://arxiv.org/abs/2506.12730</guid>
<content:encoded><![CDATA[
<div> Keywords: digitization, manufacturing-as-a-service, decentralized organization, pricing mechanisms, matching mechanisms

Summary: 
Advancements in digitization have led to the emergence of two-sided manufacturing-as-a-service (MaaS) marketplaces, reducing product development time. This dissertation focuses on tools for decision making to enable decentralization in MaaS marketplaces. It introduces data-driven pricing mechanisms for small service providers and recommends network-based pricing to suppliers based on their attributes. Three approaches for matching mechanisms are explored: reverse auction, mechanism design and mathematical programming for stable matching, and dynamic matching in a stochastic environment. Empirical simulations in a simulated 3D printing marketplace evaluate the mechanisms' performance and impact of stability. Overall, this research contributes to improving decision-making processes in MaaS marketplaces through innovative pricing and matching mechanisms in centralized and decentralized structures. 

<br /><br />Summary: <div>
arXiv:2506.12730v1 Announce Type: new 
Abstract: Advancements in digitization have enabled two sided manufacturing-as-a-service (MaaS) marketplaces which has significantly reduced product development time for designers. These platforms provide designers with access to manufacturing resources through a network of suppliers and have instant order placement capabilities. Two key decision making levers are typically used to optimize the operations of these marketplaces: pricing and matching. The existing marketplaces operate in a centralized structure where they have complete control over decision making. However, a decentralized organization of the platform enables transparency of information across clients and suppliers. This dissertation focuses on developing tools for decision making enabling decentralization in MaaS marketplaces. In pricing mechanisms, a data driven method is introduced which enables small service providers to price services based on specific attributes of the services offered. A data mining method recommends a network based price to a supplier based on its attributes and the attributes of other suppliers on the platform. Three different approaches are considered for matching mechanisms. First, a reverse auction mechanism is introduced where designers bid for manufacturing services and the mechanism chooses a supplier which can match the bid requirements and stated price. The second approach uses mechanism design and mathematical programming to develop a stable matching mechanism for matching orders to suppliers based on their preferences. Empirical simulations are used to test the mechanisms in a simulated 3D printing marketplace and to evaluate the impact of stability on its performance. The third approach considers the matching problem in a dynamic and stochastic environment where demand (orders) and supply (supplier capacities) arrive over time and matching is performed online.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPMLN, Weak Constraints, and P-log</title>
<link>https://arxiv.org/abs/2506.12784</link>
<guid>https://arxiv.org/abs/2506.12784</guid>
<content:encoded><![CDATA[
<div> LPMLN, answer set programs, weak constraints, P-log, translation
Summary:
This paper explores the relationships between LPMLN, weak constraints, and P-log as extensions of answer set programs. It presents translations between LPMLN and weak constraints, enabling the computation of most probable stable models of LPMLN programs using standard ASP solvers. Additionally, a translation of P-log into LPMLN allows for the representation of probabilistic nonmonotonicity in LPMLN, facilitating the computation of P-log using standard ASP and MLN solvers. These translations provide a bridge between different formalisms such as Markov Logic, ProbLog, and Pearl's Causal Models, showcasing the versatility and applicability of LPMLN in handling probabilistic uncertainty and preference among answer sets. The study demonstrates the potential for utilizing LPMLN as a unifying framework for reasoning under uncertainty in various knowledge representation formalisms.  
<br /><br />Summary: <div>
arXiv:2506.12784v1 Announce Type: new 
Abstract: LPMLN is a recently introduced formalism that extends answer set programs by adopting the log-linear weight scheme of Markov Logic. This paper investigates the relationships between LPMLN and two other extensions of answer set programs: weak constraints to express a quantitative preference among answer sets, and P-log to incorporate probabilistic uncertainty. We present a translation of LPMLN into programs with weak constraints and a translation of P-log into LPMLN, which complement the existing translations in the opposite directions. The first translation allows us to compute the most probable stable models (i.e., MAP estimates) of LPMLN programs using standard ASP solvers. This result can be extended to other formalisms, such as Markov Logic, ProbLog, and Pearl's Causal Models, that are shown to be translatable into LPMLN. The second translation tells us how probabilistic nonmonotonicity (the ability of the reasoner to change his probabilistic model as a result of new information) of P-log can be represented in LPMLN, which yields a way to compute P-log using standard ASP solvers and MLN solvers.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents</title>
<link>https://arxiv.org/abs/2506.12801</link>
<guid>https://arxiv.org/abs/2506.12801</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, Large Language Model (LLM) agents, Proximal Policy Optimization (PPO), deep reinforcement learning, logical deduction<br />
<br />
Summary: 
This paper explores the effectiveness of various AI paradigms in mastering the logical deduction game, The Da Vinci Code. Three agent architectures were developed and evaluated: a Transformer-based baseline model, several LLM agents (such as Gemini, DeepSeek, and GPT variants) guided by structured prompts, and a PPO-based agent using a Transformer encoder for comprehensive game history processing. The PPO-based agent showed superior win rates, outperforming the LLM counterparts. Deep reinforcement learning demonstrated strength in policy refinement for complex deductive tasks, particularly in learning implicit strategies from self-play. The study also highlighted the limitations of current LLMs in maintaining logical consistency and strategic depth over extended gameplay. This research provides insights into effective agent design and the comparative advantages of different AI approaches in recreational games involving hidden information and multi-step logical reasoning. <br /><br />Summary: <div>
arXiv:2506.12801v1 Announce Type: new 
Abstract: The Da Vinci Code, a game of logical deduction and imperfect information, presents unique challenges for artificial intelligence, demanding nuanced reasoning beyond simple pattern recognition. This paper investigates the efficacy of various AI paradigms in mastering this game. We develop and evaluate three distinct agent architectures: a Transformer-based baseline model with limited historical context, several Large Language Model (LLM) agents (including Gemini, DeepSeek, and GPT variants) guided by structured prompts, and an agent based on Proximal Policy Optimization (PPO) employing a Transformer encoder for comprehensive game history processing. Performance is benchmarked against the baseline, with the PPO-based agent demonstrating superior win rates ($58.5\% \pm 1.0\%$), significantly outperforming the LLM counterparts. Our analysis highlights the strengths of deep reinforcement learning in policy refinement for complex deductive tasks, particularly in learning implicit strategies from self-play. We also examine the capabilities and inherent limitations of current LLMs in maintaining strict logical consistency and strategic depth over extended gameplay, despite sophisticated prompting. This study contributes to the broader understanding of AI in recreational games involving hidden information and multi-step logical reasoning, offering insights into effective agent design and the comparative advantages of different AI approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy Propositional Formulas under the Stable Model Semantics</title>
<link>https://arxiv.org/abs/2506.12804</link>
<guid>https://arxiv.org/abs/2506.12804</guid>
<content:encoded><![CDATA[
<div> fuzzy logic, stable model semantics, propositional formulas, nonmonotonic reasoning, graded truth degrees
Summary:<br />
- The article introduces a stable model semantics for fuzzy propositional formulas, combining fuzzy logic and the stable model semantics of classical propositional formulas.
- The language syntax remains the same as fuzzy propositional logic, but the semantics distinguish stable models from non-stable models.
- This framework allows for configurable nonmonotonic reasoning for dynamic domains with graded truth degrees.
- Several properties of Boolean stable models extend naturally to this many-valued setting.
- The article discusses the relationship of this approach with other methods of combining fuzzy logic and stable model semantics.<br />Summary: <div>
arXiv:2506.12804v1 Announce Type: new 
Abstract: We define a stable model semantics for fuzzy propositional formulas, which generalizes both fuzzy propositional logic and the stable model semantics of classical propositional formulas. The syntax of the language is the same as the syntax of fuzzy propositional logic, but its semantics distinguishes stable models from non-stable models. The generality of the language allows for highly configurable nonmonotonic reasoning for dynamic domains involving graded truth degrees. We show that several properties of Boolean stable models are naturally extended to this many-valued setting, and discuss how it is related to other approaches to combining fuzzy logic and the stable model semantics.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Neuroevolution O-RAN: Enhancing the Robustness of Deep Reinforcement Learning xApps</title>
<link>https://arxiv.org/abs/2506.12812</link>
<guid>https://arxiv.org/abs/2506.12812</guid>
<content:encoded><![CDATA[
<div> Keywords: O-RAN, RAN intelligent controllers, reinforcement learning, deep reinforcement learning, neuroevolution

Summary:<br /><br />
The article introduces the Federated O-RAN enabled Neuroevolution-enhanced Deep Reinforcement Learning (F-ONRL) approach to improve the reliability of RAN intelligent controllers in the open radio access network (O-RAN) architecture. By deploying a Neuroevolution (NE)-based optimizer xApp alongside traditional RAN controller xApps, F-ONRL effectively explores and exploits solutions without disrupting RAN operations. The NE-DRL xApp framework enhances the robustness of xApps while maintaining a balance in computational load. The implementation of the NE xApp and DRL xApp on the Open AI Cellular platform showcases improved performance and reliability in near-real-time RIC settings. By combining NE and DRL approaches, F-ONRL addresses concerns about local optima and ensures the efficient management and optimization of the disaggregated RAN in O-RAN architecture. <div>
arXiv:2506.12812v1 Announce Type: new 
Abstract: The open radio access network (O-RAN) architecture introduces RAN intelligent controllers (RICs) to facilitate the management and optimization of the disaggregated RAN. Reinforcement learning (RL) and its advanced form, deep RL (DRL), are increasingly employed for designing intelligent controllers, or xApps, to be deployed in the near-real time (near-RT) RIC. These models often encounter local optima, which raise concerns about their reliability for RAN intelligent control. We therefore introduce Federated O-RAN enabled Neuroevolution (NE)-enhanced DRL (F-ONRL) that deploys an NE-based optimizer xApp in parallel to the RAN controller xApps. This NE-DRL xApp framework enables effective exploration and exploitation in the near-RT RIC without disrupting RAN operations. We implement the NE xApp along with a DRL xApp and deploy them on Open AI Cellular (OAIC) platform and present numerical results that demonstrate the improved robustness of xApps while effectively balancing the additional computational load.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Optimization: A Systems-Based Approach to Social Externalities</title>
<link>https://arxiv.org/abs/2506.12825</link>
<guid>https://arxiv.org/abs/2506.12825</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, stakeholders, externalities, systems thinking, unintended consequences

Summary:
This paper discusses the challenges of optimizing decision-making processes in socioeconomic contexts where externalities play a significant role. It proposes a framework that combines systems thinking with the concept of economic externalities to address these challenges. By characterizing stakeholders and their goals, as well as identifying and quantifying externalities, the framework aims to understand the impacts of optimization on affected parties. Systems thinking provides a holistic perspective, highlighting interconnected relationships and feedback loops that influence decision outcomes. The framework helps identify and rectify common subpar practices in optimization, such as ignorance, errors, and prioritization of short-term goals. By incorporating normative considerations alongside descriptive accuracy, the framework aims to mitigate unintended consequences of optimization processes. <div>
arXiv:2506.12825v1 Announce Type: new 
Abstract: Optimization is widely used for decision making across various domains, valued for its ability to improve efficiency. However, poor implementation practices can lead to unintended consequences, particularly in socioeconomic contexts where externalities (costs or benefits to third parties outside the optimization process) are significant. To propose solutions, it is crucial to first characterize involved stakeholders, their goals, and the types of subpar practices causing unforeseen outcomes. This task is complex because affected stakeholders often fall outside the direct focus of optimization processes. Also, incorporating these externalities into optimization requires going beyond traditional economic frameworks, which often focus on describing externalities but fail to address their normative implications or interconnected nature, and feedback loops. This paper suggests a framework that combines systems thinking with the economic concept of externalities to tackle these challenges. This approach aims to characterize what went wrong, who was affected, and how (or where) to include them in the optimization process. Economic externalities, along with their established quantification methods, assist in identifying "who was affected and how" through stakeholder characterization. Meanwhile, systems thinking (an analytical approach to comprehending relationships in complex systems) provides a holistic, normative perspective. Systems thinking contributes to an understanding of interconnections among externalities, feedback loops, and determining "when" to incorporate them in the optimization. Together, these approaches create a comprehensive framework for addressing optimization's unintended consequences, balancing descriptive accuracy with normative objectives. Using this, we examine three common types of subpar practices: ignorance, error, and prioritization of short-term goals.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench</title>
<link>https://arxiv.org/abs/2506.12841</link>
<guid>https://arxiv.org/abs/2506.12841</guid>
<content:encoded><![CDATA[
<div> benchmarking platform, multi-agent, strategic reasoning, Werewolf game, evaluation metrics <br />
Summary: <br />
The article introduces WereWolf-Plus, a benchmarking platform for evaluating multi-agent strategic reasoning in the Werewolf game. Existing platforms lack complexity and scalability, so WereWolf-Plus offers customizable configurations for roles like Seer and Sheriff. It also provides flexible model assignment and evaluation metrics for different roles, werewolves, and the sheriff. The platform assesses agent reasoning ability, cooperation capacity, and social influence through quantitative metrics, enhancing research in multi-agent communities. WereWolf-Plus aims to improve research on inference and strategic interaction by offering a reliable and customizable environment. The code for WereWolf-Plus is open-sourced on GitHub to encourage collaboration and further development in this area. <br /> <div>
arXiv:2506.12841v1 Announce Type: new 
Abstract: With the rapid development of LLM-based agents, increasing attention has been given to their social interaction and strategic reasoning capabilities. However, existing Werewolf-based benchmarking platforms suffer from overly simplified game settings, incomplete evaluation metrics, and poor scalability. To address these limitations, we propose WereWolf-Plus, a multi-model, multi-dimensional, and multi-method benchmarking platform for evaluating multi-agent strategic reasoning in the Werewolf game. The platform offers strong extensibility, supporting customizable configurations for roles such as Seer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment and reasoning enhancement strategies for different roles. In addition, we introduce a comprehensive set of quantitative evaluation metrics for all special roles, werewolves, and the sheriff, and enrich the assessment dimensions for agent reasoning ability, cooperation capacity, and social influence. WereWolf-Plus provides a more flexible and reliable environment for advancing research on inference and strategic interaction within multi-agent communities. Our code is open sourced at https://github.com/MinstrelsyXia/WereWolfPlus.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Developmental Biology Can Serve as the Conceptual Foundation for a New Design Paradigm in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.12891</link>
<guid>https://arxiv.org/abs/2506.12891</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Evolutionary Developmental Biology, Modern Synthesis, Machine Learning, Design Paradigm

Summary: 
This article discusses the limitations of the current neural network-based paradigm in artificial intelligence (AI) and proposes a new design philosophy based on principles from evolutionary developmental biology (EDB). The analogy between the Modern Synthesis and contemporary machine learning is explored, highlighting shared assumptions, approaches, and limitations. By incorporating insights from EDB, a new AI design paradigm grounded in biological first principles is proposed. The core principles of the proposed paradigm include regulatory connections, somatic variation and selection, and weak linkage, which aim to address major limitations of current machine learning methods organically. Two learning system designs utilizing specific developmental principles are presented as examples, demonstrating how these mechanisms can provide deeper insights into biological evolution while improving the performance of AI systems. This new approach offers a promising direction for the future of AI research. 

<br /><br />Summary: <div>
arXiv:2506.12891v1 Announce Type: new 
Abstract: Artificial intelligence (AI), propelled by advancements in machine learning, has made significant strides in solving complex tasks. However, the current neural network-based paradigm, while effective, is heavily constrained by inherent limitations, primarily a lack of structural organization and a progression of learning that displays undesirable properties. As AI research progresses without a unifying framework, it either tries to patch weaknesses heuristically or draws loosely from biological mechanisms without strong theoretical foundations. Meanwhile, the recent paradigm shift in evolutionary understanding -- driven primarily by evolutionary developmental biology (EDB) -- has been largely overlooked in AI literature, despite a striking analogy between the Modern Synthesis and contemporary machine learning, evident in their shared assumptions, approaches, and limitations upon careful analysis. Consequently, the principles of adaptation from EDB that reshaped our understanding of the evolutionary process can also form the foundation of a unifying conceptual framework for the next design philosophy in AI, going beyond mere inspiration and grounded firmly in biology's first principles. This article provides a detailed overview of the analogy between the Modern Synthesis and modern machine learning, and outlines the core principles of a new AI design paradigm based on insights from EDB. To exemplify our analysis, we also present two learning system designs grounded in specific developmental principles -- regulatory connections, somatic variation and selection, and weak linkage -- that resolve multiple major limitations of contemporary machine learning in an organic manner, while also providing deeper insights into the role of these mechanisms in biological evolution.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homeostatic Coupling for Prosocial Behavior</title>
<link>https://arxiv.org/abs/2506.12894</link>
<guid>https://arxiv.org/abs/2506.12894</guid>
<content:encoded><![CDATA[
<div> Keywords: prosocial behavior, autonomous agents, empathy, multi-agent reinforcement learning, homeostatic self-regulation

Summary: 
Prosocial behavior in autonomous agents is studied based on the principles of homeostasis and empathy. Through multi-agent reinforcement learning, agents are motivated to maintain their well-being while exhibiting empathy towards others. Two types of empathy mechanisms, cognitive and affective empathy, are introduced to facilitate the sharing of internal states between agents. Prosocial behavior emerges when agents' well-being is influenced by their partners' distress under homeostatic coupling. Furthermore, the study demonstrates that agents can learn empathy by decoding their partners' external emotive states to infer their internal homeostatic states. By referencing their own emotional functions, agents can empathize with others by understanding and sharing their emotional experiences. Overall, the research explores how autonomous agents can develop prosocial behavior by learning to empathize with others through understanding their emotional states. 

<br /><br />Summary: <div>
arXiv:2506.12894v1 Announce Type: new 
Abstract: When regarding the suffering of others, we often experience personal distress and feel compelled to help\footnote{Preprint. Under review.}. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \emph{observe} their partner's internal state ({\bf cognitive empathy}) or the agent's internal state can be \emph{directly coupled} to that of their partner ({\bf affective empathy}). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Additionally, we show that empathy can be learned: agents can ``decode" their partner's external emotive states to infer the partner's internal homeostatic states. Assuming some level of physiological similarity, agents reference their own emotion-generation functions to invert the mapping from outward display to internal state. Overall, we demonstrate the emergence of prosocial behavior when homeostatic agents learn to ``read" the emotions of others and then to empathize, or feel as they feel.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KCLNet: Physics-Informed Power Flow Prediction via Constraints Projections</title>
<link>https://arxiv.org/abs/2506.12902</link>
<guid>https://arxiv.org/abs/2506.12902</guid>
<content:encoded><![CDATA[
<div> graph neural network, Kirchhoff's Current Law, power flow predictions, artificial intelligence, smart grids
<br />
The article introduces KCLNet, a physics-informed graph neural network designed to improve power flow predictions in modern power systems. Traditional numerical methods are robust but computationally demanding, while recent advancements in artificial intelligence provide faster results but may lack physical accuracy. KCLNet addresses this challenge by incorporating Kirchhoff's Current Law as a hard constraint through hyperplane projections. This approach ensures zero violations of fundamental physical laws, leading to reliable and physically consistent power flow predictions essential for secure smart grid operation. By combining the benefits of AI speed with the enforcement of physical constraints, KCLNet delivers competitive prediction accuracy while maintaining grid safety and efficiency.
<br /><br />Summary: <div>
arXiv:2506.12902v1 Announce Type: new 
Abstract: In the modern context of power systems, rapid, scalable, and physically plausible power flow predictions are essential for ensuring the grid's safe and efficient operation. While traditional numerical methods have proven robust, they require extensive computation to maintain physical fidelity under dynamic or contingency conditions. In contrast, recent advancements in artificial intelligence (AI) have significantly improved computational speed; however, they often fail to enforce fundamental physical laws during real-world contingencies, resulting in physically implausible predictions. In this work, we introduce KCLNet, a physics-informed graph neural network that incorporates Kirchhoff's Current Law as a hard constraint via hyperplane projections. KCLNet attains competitive prediction accuracy while ensuring zero KCL violations, thereby delivering reliable and physically consistent power flow predictions critical to secure the operation of modern smart grids.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories</title>
<link>https://arxiv.org/abs/2506.12911</link>
<guid>https://arxiv.org/abs/2506.12911</guid>
<content:encoded><![CDATA[
<div> diffusion, constraint-aware, machine learning, tabular data, AC power flow prediction  
Summary:  
- The article introduces a framework for constraint-aware refinement using denoising diffusion implicit models (DDIMs) to address real-world machine learning tasks with hard constraints.  
- The proposed method iteratively refines predictions through a deterministic diffusion trajectory guided by a learned prior and constraint gradient corrections.  
- It accommodates a wide range of non-convex and nonlinear equality constraints, making it applicable post hoc to any base model.  
- The approach is demonstrated in constrained adversarial attack generation on tabular data with column-level dependencies and AC power flow prediction under Kirchhoff's laws.  
- Results show improved constraint satisfaction and performance while being lightweight and model-agnostic.  
<br /><br />Summary: <div>
arXiv:2506.12911v1 Announce Type: new 
Abstract: Many real-world machine learning tasks require outputs that satisfy hard constraints, such as physical conservation laws, structured dependencies in graphs, or column-level relationships in tabular data. Existing approaches rely either on domain-specific architectures and losses or on strong assumptions on the constraint space, restricting their applicability to linear or convex constraints. We propose a general-purpose framework for constraint-aware refinement that leverages denoising diffusion implicit models (DDIMs). Starting from a coarse prediction, our method iteratively refines it through a deterministic diffusion trajectory guided by a learned prior and augmented by constraint gradient corrections. The approach accommodates a wide class of non-convex and nonlinear equality constraints and can be applied post hoc to any base model. We demonstrate the method in two representative domains: constrained adversarial attack generation on tabular data with column-level dependencies and in AC power flow prediction under Kirchhoff's laws. Across both settings, our diffusion-guided refinement improves both constraint satisfaction and performance while remaining lightweight and model-agnostic.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sectoral Coupling in Linguistic State Space</title>
<link>https://arxiv.org/abs/2506.12927</link>
<guid>https://arxiv.org/abs/2506.12927</guid>
<content:encoded><![CDATA[
<div> framework, internal dependencies, artificial agents, cognitive sectors, sectoral coupling constants

Summary:
This article presents a formal framework for quantifying internal dependencies between functional subsystems in artificial agents with structured belief states. It introduces sectoral coupling constants that characterize how cognitive sectors influence each other within a particular level of abstraction. These coupling constants create agent-specific profiles that govern information flow and cognitive style. The taxonomy covers various roles such as perceptual integration, memory access, planning, meta-cognition, execution control, and affective modulation. The framework also discusses how these profiles lead to feedback loops, systemic dynamics, and emergent cognitive behavior. Methodologies for inferring these profiles and their evolution across levels of abstraction are outlined. This approach offers a mechanistic and interpretable method for modeling complex cognition, with potential applications in AI system design, alignment diagnostics, and analyzing emergent agent behavior.<br /><br />Summary: <div>
arXiv:2506.12927v1 Announce Type: new 
Abstract: This work presents a formal framework for quantifying the internal dependencies between functional subsystems within artificial agents whose belief states are composed of structured linguistic fragments. Building on the Semantic Manifold framework, which organizes belief content into functional sectors and stratifies them across hierarchical levels of abstraction, we introduce a system of sectoral coupling constants that characterize how one cognitive sector influences another within a fixed level of abstraction. The complete set of these constants forms an agent-specific coupling profile that governs internal information flow, shaping the agent's overall processing tendencies and cognitive style. We provide a detailed taxonomy of these intra-level coupling roles, covering domains such as perceptual integration, memory access and formation, planning, meta-cognition, execution control, and affective modulation. We also explore how these coupling profiles generate feedback loops, systemic dynamics, and emergent signatures of cognitive behavior. Methodologies for inferring these profiles from behavioral or internal agent data are outlined, along with a discussion of how these couplings evolve across abstraction levels. This framework contributes a mechanistic and interpretable approach to modeling complex cognition, with applications in AI system design, alignment diagnostics, and the analysis of emergent agent behavior.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Test-time Compute for LLM Agents</title>
<link>https://arxiv.org/abs/2506.12928</link>
<guid>https://arxiv.org/abs/2506.12928</guid>
<content:encoded><![CDATA[
<div> scaling, test-time compute, language models, parallel sampling algorithms, task performance

Summary:
Scaling test-time compute has shown significant improvements in the reasoning abilities of large language models (LLMs). This study delves into applying test-time scaling methods to language agents and examines their impact on effectiveness. Different strategies such as parallel sampling algorithms, sequential revision, verifiers, and merging methods, as well as diversification of rollouts, were explored. Results show that scaling test-time compute enhances agent performance. Timing for reflection is crucial for agents to succeed. The list-wise method stands out among verification and merging approaches. Diversifying rollouts positively influences task performance. The findings highlight the importance of strategic implementation of scaling methods for language agents. <br /><br />Summary: <div>
arXiv:2506.12928v1 Announce Type: new 
Abstract: Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance</title>
<link>https://arxiv.org/abs/2506.12937</link>
<guid>https://arxiv.org/abs/2506.12937</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language models, Hypothesis development, Reasoning, Evidence-based, Scientific reasoning

Summary:
Large Language models have shown promise in research ideation, but the process of hypothesis development has often been overlooked. This study introduces the HypER model, a small language model trained for literature-guided reasoning and evidence-based hypothesis generation. HypER is trained to distinguish between valid and invalid scientific reasoning chains, outperforming the base model in this task. It also generates evidence-grounded hypotheses with higher quality compared to the base model. Human experts rated the generated hypotheses as highly feasible and impactful. This demonstrates the potential of using dedicated models like HypER for hypothesis generation in scientific research.<br /><br />Summary: <div>
arXiv:2506.12937v1 Announce Type: new 
Abstract: Large Language models have demonstrated promising performance in research ideation across scientific domains. Hypothesis development, the process of generating a highly specific declarative statement connecting a research idea with empirical validation, has received relatively less attention. Existing approaches trivially deploy retrieval augmentation and focus only on the quality of the final output ignoring the underlying reasoning process behind ideation. We present $\texttt{HypER}$ ($\textbf{Hyp}$othesis Generation with $\textbf{E}$xplanation and $\textbf{R}$easoning), a small language model (SLM) trained for literature-guided reasoning and evidence-based hypothesis generation. $\texttt{HypER}$ is trained in a multi-task setting to discriminate between valid and invalid scientific reasoning chains in presence of controlled distractions. We find that $\texttt{HypER}$ outperformes the base model, distinguishing valid from invalid reasoning chains (+22\% average absolute F1), generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with high feasibility and impact as judged by human experts ($>$3.5 on 5-point Likert scale).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constitutive Components for Human-Like Autonomous Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.12952</link>
<guid>https://arxiv.org/abs/2506.12952</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial entities, autonomy, functional hierarchy, design principles, general intelligence

Summary: 
This study introduces a three-layer functional hierarchy necessary for constructing autonomous artificial entities that mimic human behavior. The hierarchy includes Core Functions for interaction with the external world, the Integrative Evaluation Function for decision-making based on perception and memory, and the Self Modification Function for adapting behavior and internal components. The study proposes a stepwise model of autonomy, progressing from reactive to strong autonomous levels. It discusses the design principles underlying this model and explores how these functions align with existing artificial intelligence design methods. The study suggests that this hierarchical framework could serve as a basis for achieving general intelligence in artificial entities. Lastly, it considers potential future applications and ethical implications of developing highly autonomous artificial entities. Overall, this work offers a theoretical foundation independent of specific technical approaches for understanding and designing autonomously behaving artificial entities. 

<br /><br />Summary: <div>
arXiv:2506.12952v1 Announce Type: new 
Abstract: This study is the first to clearly identify the functions required to construct artificial entities capable of behaving autonomously like humans, and organizes them into a three-layer functional hierarchy. Specifically, it defines three levels: Core Functions, which enable interaction with the external world; the Integrative Evaluation Function, which selects actions based on perception and memory; and the Self Modification Function, which dynamically reconfigures behavioral principles and internal components. Based on this structure, the study proposes a stepwise model of autonomy comprising reactive, weak autonomous, and strong autonomous levels, and discusses its underlying design principles and developmental aspects. It also explores the relationship between these functions and existing artificial intelligence design methods, addressing their potential as a foundation for general intelligence and considering future applications and ethical implications. By offering a theoretical framework that is independent of specific technical methods, this work contributes to a deeper understanding of autonomy and provides a foundation for designing future artificial entities with strong autonomy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills</title>
<link>https://arxiv.org/abs/2506.12963</link>
<guid>https://arxiv.org/abs/2506.12963</guid>
<content:encoded><![CDATA[
arXiv:2506.12963v1 Announce Type: new 
Abstract: Recent advances in large reasoning models (LRMs) have enabled strong chain-of-thought (CoT) generation through test-time computation. While these multi-step reasoning capabilities represent a major milestone in language model performance, they also introduce new safety risks. In this work, we present the first systematic study to revisit the problem of machine unlearning in the context of LRMs. Machine unlearning refers to the process of removing the influence of sensitive, harmful, or undesired data or knowledge from a trained model without full retraining. We show that conventional unlearning algorithms, originally designed for non-reasoning models, are inadequate for LRMs. In particular, even when final answers are successfully erased, sensitive information often persists within the intermediate reasoning steps, i.e., CoT trajectories. To address this challenge, we extend conventional unlearning and propose Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$), a novel method that effectively suppresses sensitive reasoning traces and prevents the generation of associated final answers, while preserving the model's reasoning ability. Our experiments demonstrate that $R^2MU$ significantly reduces sensitive information leakage within reasoning traces and achieves strong performance across both safety and reasoning benchmarks, evaluated on state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-Distill-Qwen-14B.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing</title>
<link>https://arxiv.org/abs/2506.12981</link>
<guid>https://arxiv.org/abs/2506.12981</guid>
<content:encoded><![CDATA[
arXiv:2506.12981v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems address factual inconsistencies in Large Language Models by grounding generation in external knowledge, yet they face a fundamental efficiency problem: simple queries consume computational resources equivalent to complex multi-hop reasoning tasks. We present SymRAG, a neuro-symbolic framework that introduces adaptive query routing based on real-time complexity and system load assessments. SymRAG dynamically selects symbolic, neural, or hybrid processing paths to align resource use with query demands. Evaluated on 2,000 queries from HotpotQA and DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0% exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151% increase in processing time, highlighting the framework's impact. These results underscore the potential of adaptive neuro-symbolic routing for scalable, sustainable AI systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Guide for Evaluating LLMs and LLM-Reliant Systems</title>
<link>https://arxiv.org/abs/2506.13023</link>
<guid>https://arxiv.org/abs/2506.13023</guid>
<content:encoded><![CDATA[
arXiv:2506.13023v1 Announce Type: new 
Abstract: Recent advances in generative AI have led to remarkable interest in using systems that rely on large language models (LLMs) for practical applications. However, meaningful evaluation of these systems in real-world scenarios comes with a distinct set of challenges, which are not well-addressed by synthetic benchmarks and de-facto metrics that are often seen in the literature. We present a practical evaluation framework which outlines how to proactively curate representative datasets, select meaningful evaluation metrics, and employ meaningful evaluation methodologies that integrate well with practical development and deployment of LLM-reliant systems that must adhere to real-world requirements and meet user-facing needs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning</title>
<link>https://arxiv.org/abs/2506.13026</link>
<guid>https://arxiv.org/abs/2506.13026</guid>
<content:encoded><![CDATA[
arXiv:2506.13026v1 Announce Type: new 
Abstract: Precision process planning in Computer Numerical Control (CNC) machining demands rapid, context-aware decisions on tool selection, feed-speed pairs, and multi-axis routing, placing immense cognitive and procedural burdens on engineers from design specification through final part inspection. Conventional rule-based computer-aided process planning and knowledge-engineering shells freeze domain know-how into static tables, which become limited when dealing with unseen topologies, novel material states, shifting cost-quality-sustainability weightings, or shop-floor constraints such as tool unavailability and energy caps. Large language models (LLMs) promise flexible, instruction-driven reasoning for tasks but they routinely hallucinate numeric values and provide no provenance. We present Augmented Retrieval Knowledge Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented generation to deliver verifiable, numerically exact answers for CNC process planning. ARKNESS (1) automatically distills heterogeneous machining documents, G-code annotations, and vendor datasheets into augmented triple, multi-relational graphs without manual labeling, and (2) couples any on-prem LLM with a retriever that injects the minimal, evidence-linked subgraph needed to answer a query. Benchmarked on 155 industry-curated questions spanning tool sizing and feed-speed optimization, a lightweight 3B-parameter Llama-3 augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage point gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on open-ended responses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: Multi-Agent Argumentation and Grammar Integrated Critiquer</title>
<link>https://arxiv.org/abs/2506.13037</link>
<guid>https://arxiv.org/abs/2506.13037</guid>
<content:encoded><![CDATA[
arXiv:2506.13037v1 Announce Type: new 
Abstract: Automated Essay Scoring (AES) and Automatic Essay Feedback (AEF) systems aim to reduce the workload of human raters in educational assessment. However, most existing systems prioritize numeric scoring accuracy over the quality of feedback. This paper presents Multi-Agent Argumentation and Grammar Integrated Critiquer (MAGIC), a framework that uses multiple specialized agents to evaluate distinct writing aspects to both predict holistic scores and produce detailed, rubric-aligned feedback. To support evaluation, we curated a novel dataset of past GRE practice test essays with expert-evaluated scores and feedback. MAGIC outperforms baseline models in both essay scoring , as measured by Quadratic Weighted Kappa (QWK). We find that despite the improvement in QWK, there are opportunities for future work in aligning LLM-generated feedback to human preferences.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model Learning</title>
<link>https://arxiv.org/abs/2506.13056</link>
<guid>https://arxiv.org/abs/2506.13056</guid>
<content:encoded><![CDATA[
arXiv:2506.13056v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the model's exploratory capacity and face suboptimal convergence. In this work, we introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and \textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the model's latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Explainability in the Era of Multimodal AI</title>
<link>https://arxiv.org/abs/2506.13060</link>
<guid>https://arxiv.org/abs/2506.13060</guid>
<content:encoded><![CDATA[
arXiv:2506.13060v1 Announce Type: new 
Abstract: While multimodal AI systems (models jointly trained on heterogeneous data types such as text, time series, graphs, and images) have become ubiquitous and achieved remarkable performance across high-stakes applications, transparent and accurate explanation algorithms are crucial for their safe deployment and ensure user trust. However, most existing explainability techniques remain unimodal, generating modality-specific feature attributions, concepts, or circuit traces in isolation and thus failing to capture cross-modal interactions. This paper argues that such unimodal explanations systematically misrepresent and fail to capture the cross-modal influence that drives multimodal model decisions, and the community should stop relying on them for interpreting multimodal models. To support our position, we outline key principles for multimodal explanations grounded in modality: Granger-style modality influence (controlled ablations to quantify how removing one modality changes the explanation for another), Synergistic faithfulness (explanations capture the model's predictive power when modalities are combined), and Unified stability (explanations remain consistent under small, cross-modal perturbations). This targeted shift to multimodal explanations will help the community uncover hidden shortcuts, mitigate modality bias, improve model reliability, and enhance safety in high-stakes settings where incomplete explanations can have serious consequences.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs</title>
<link>https://arxiv.org/abs/2506.13082</link>
<guid>https://arxiv.org/abs/2506.13082</guid>
<content:encoded><![CDATA[
arXiv:2506.13082v1 Announce Type: new 
Abstract: Moral competence is the ability to act in accordance with moral principles. As large language models (LLMs) are increasingly deployed in situations demanding moral competence, there is increasing interest in evaluating this ability empirically. We review existing literature and identify three significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with explicitly highlighted moral features; (ii) Focus on verdict prediction rather than moral reasoning; and (iii) Inadequate testing of models' (in)ability to recognize when additional information is needed. Grounded in philosophical research on moral skill, we then introduce a novel method for assessing moral competence in LLMs. Our approach moves beyond simple verdict comparisons to evaluate five dimensions of moral competence: identifying morally relevant features, weighting their importance, assigning moral reasons to these features, synthesizing coherent moral judgments, and recognizing information gaps. We conduct two experiments comparing six leading LLMs against non-expert humans and professional philosophers. In our first experiment using ethical vignettes standard to existing work, LLMs generally outperformed non-expert humans across multiple dimensions of moral reasoning. However, our second experiment, featuring novel scenarios designed to test moral sensitivity by embedding relevant features among irrelevant details, revealed a striking reversal: several LLMs performed significantly worse than humans. Our findings suggest that current evaluations may substantially overestimate LLMs' moral reasoning capabilities by eliminating the task of discerning moral relevance from noisy information, which we take to be a prerequisite for genuine moral skill. This work provides a more nuanced framework for assessing AI moral competence and highlights important directions for improving moral competence in advanced AI systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Memetic Walrus Algorithm with Expert-guided Strategy for Adaptive Curriculum Sequencing</title>
<link>https://arxiv.org/abs/2506.13092</link>
<guid>https://arxiv.org/abs/2506.13092</guid>
<content:encoded><![CDATA[
arXiv:2506.13092v1 Announce Type: new 
Abstract: Adaptive Curriculum Sequencing (ACS) is essential for personalized online learning, yet current approaches struggle to balance complex educational constraints and maintain optimization stability. This paper proposes a Memetic Walrus Optimizer (MWO) that enhances optimization performance through three key innovations: (1) an expert-guided strategy with aging mechanism that improves escape from local optima; (2) an adaptive control signal framework that dynamically balances exploration and exploitation; and (3) a three-tier priority mechanism for generating educationally meaningful sequences. We formulate ACS as a multi-objective optimization problem considering concept coverage, time constraints, and learning style compatibility. Experiments on the OULAD dataset demonstrate MWO's superior performance, achieving 95.3% difficulty progression rate (compared to 87.2% in baseline methods) and significantly better convergence stability (standard deviation of 18.02 versus 28.29-696.97 in competing algorithms). Additional validation on benchmark functions confirms MWO's robust optimization capability across diverse scenarios. The results demonstrate MWO's effectiveness in generating personalized learning sequences while maintaining computational efficiency and solution quality.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13113</link>
<guid>https://arxiv.org/abs/2506.13113</guid>
<content:encoded><![CDATA[
arXiv:2506.13113v1 Announce Type: new 
Abstract: This paper develops a novel multi-agent reinforcement learning (MARL) framework for reinsurance treaty bidding, addressing long-standing inefficiencies in traditional broker-mediated placement processes. We pose the core research question: Can autonomous, learning-based bidding systems improve risk transfer efficiency and outperform conventional pricing approaches in reinsurance markets?
  In our model, each reinsurer is represented by an adaptive agent that iteratively refines its bidding strategy within a competitive, partially observable environment. The simulation explicitly incorporates institutional frictions including broker intermediation, incumbent advantages, last-look privileges, and asymmetric access to underwriting information.
  Empirical analysis demonstrates that MARL agents achieve up to 15% higher underwriting profit, 20% lower tail risk (CVaR), and over 25% improvement in Sharpe ratios relative to actuarial and heuristic baselines. Sensitivity tests confirm robustness across hyperparameter settings, and stress testing reveals strong resilience under simulated catastrophe shocks and capital constraints.
  These findings suggest that MARL offers a viable path toward more transparent, adaptive, and risk-sensitive reinsurance markets. The proposed framework contributes to emerging literature at the intersection of algorithmic market design, strategic bidding, and AI-enabled financial decision-making.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEvolve: A coding agent for scientific and algorithmic discovery</title>
<link>https://arxiv.org/abs/2506.13131</link>
<guid>https://arxiv.org/abs/2506.13131</guid>
<content:encoded><![CDATA[
arXiv:2506.13131v1 Announce Type: new 
Abstract: In this white paper, we present AlphaEvolve, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve developed a search algorithm that found a procedure to multiply two $4 \times 4$ complex-valued matrices using $48$ scalar multiplications; offering the first improvement, after 56 years, over Strassen's algorithm in this setting. We believe AlphaEvolve and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning as Iterated Belief Change a la Darwiche and Pearl</title>
<link>https://arxiv.org/abs/2506.13157</link>
<guid>https://arxiv.org/abs/2506.13157</guid>
<content:encoded><![CDATA[
arXiv:2506.13157v1 Announce Type: new 
Abstract: Artificial Neural Networks (ANNs) are powerful machine-learning models capable of capturing intricate non-linear relationships. They are widely used nowadays across numerous scientific and engineering domains, driving advancements in both research and real-world applications. In our recent work, we focused on the statics and dynamics of a particular subclass of ANNs, which we refer to as binary ANNs. A binary ANN is a feed-forward network in which both inputs and outputs are restricted to binary values, making it particularly suitable for a variety of practical use cases. Our previous study approached binary ANNs through the lens of belief-change theory, specifically the Alchourron, Gardenfors and Makinson (AGM) framework, yielding several key insights. Most notably, we demonstrated that the knowledge embodied in a binary ANN (expressed through its input-output behaviour) can be symbolically represented using a propositional logic language. Moreover, the process of modifying a belief set (through revision or contraction) was mapped onto a gradual transition through a series of intermediate belief sets. Analogously, the training of binary ANNs was conceptualized as a sequence of such belief-set transitions, which we showed can be formalized using full-meet AGM-style belief change. In the present article, we extend this line of investigation by addressing some critical limitations of our previous study. Specifically, we show that Dalal's method for belief change naturally induces a structured, gradual evolution of states of belief. More importantly, given the known shortcomings of full-meet belief change, we demonstrate that the training dynamics of binary ANNs can be more effectively modelled using robust AGM-style change operations -- namely, lexicographic revision and moderate contraction -- that align with the Darwiche-Pearl framework for iterated belief change.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real Time Self-Tuning Adaptive Controllers on Temperature Control Loops using Event-based Game Theory</title>
<link>https://arxiv.org/abs/2506.13164</link>
<guid>https://arxiv.org/abs/2506.13164</guid>
<content:encoded><![CDATA[
arXiv:2506.13164v1 Announce Type: new 
Abstract: This paper presents a novel method for enhancing the adaptability of Proportional-Integral-Derivative (PID) controllers in industrial systems using event-based dynamic game theory, which enables the PID controllers to self-learn, optimize, and fine-tune themselves. In contrast to conventional self-learning approaches, our proposed framework offers an event-driven control strategy and game-theoretic learning algorithms. The players collaborate with the PID controllers to dynamically adjust their gains in response to set point changes and disturbances. We provide a theoretical analysis showing sound convergence guarantees for the game given suitable stability ranges of the PID controlled loop. We further introduce an automatic boundary detection mechanism, which helps the players to find an optimal initialization of action spaces and significantly reduces the exploration time. The efficacy of this novel methodology is validated through its implementation in the temperature control loop of a printing press machine. Eventually, the outcomes of the proposed intelligent self-tuning PID controllers are highly promising, particularly in terms of reducing overshoot and settling time.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroPhysNet: A FitzHugh-Nagumo-Based Physics-Informed Neural Network Framework for Electroencephalograph (EEG) Analysis and Motor Imagery Classification</title>
<link>https://arxiv.org/abs/2506.13222</link>
<guid>https://arxiv.org/abs/2506.13222</guid>
<content:encoded><![CDATA[
arXiv:2506.13222v1 Announce Type: new 
Abstract: Electroencephalography (EEG) is extensively employed in medical diagnostics and brain-computer interface (BCI) applications due to its non-invasive nature and high temporal resolution. However, EEG analysis faces significant challenges, including noise, nonstationarity, and inter-subject variability, which hinder its clinical utility. Traditional neural networks often lack integration with biophysical knowledge, limiting their interpretability, robustness, and potential for medical translation. To address these limitations, this study introduces NeuroPhysNet, a novel Physics-Informed Neural Network (PINN) framework tailored for EEG signal analysis and motor imagery classification in medical contexts. NeuroPhysNet incorporates the FitzHugh-Nagumo model, embedding neurodynamical principles to constrain predictions and enhance model robustness. Evaluated on the BCIC-IV-2a dataset, the framework achieved superior accuracy and generalization compared to conventional methods, especially in data-limited and cross-subject scenarios, which are common in clinical settings. By effectively integrating biophysical insights with data-driven techniques, NeuroPhysNet not only advances BCI applications but also holds significant promise for enhancing the precision and reliability of clinical diagnostics, such as motor disorder assessments and neurorehabilitation planning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explaining Monte-Carlo Tree Search by Using Its Enhancements</title>
<link>https://arxiv.org/abs/2506.13223</link>
<guid>https://arxiv.org/abs/2506.13223</guid>
<content:encoded><![CDATA[
arXiv:2506.13223v1 Announce Type: new 
Abstract: Typically, research on Explainable Artificial Intelligence (XAI) focuses on black-box models within the context of a general policy in a known, specific domain. This paper advocates for the need for knowledge-agnostic explainability applied to the subfield of XAI called Explainable Search, which focuses on explaining the choices made by intelligent search techniques. It proposes Monte-Carlo Tree Search (MCTS) enhancements as a solution to obtaining additional data and providing higher-quality explanations while remaining knowledge-free, and analyzes the most popular enhancements in terms of the specific types of explainability they introduce. So far, no other research has considered the explainability of MCTS enhancements. We present a proof-of-concept that demonstrates the advantages of utilizing enhancements.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs</title>
<link>https://arxiv.org/abs/2506.13245</link>
<guid>https://arxiv.org/abs/2506.13245</guid>
<content:encoded><![CDATA[
arXiv:2506.13245v1 Announce Type: new 
Abstract: The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Proof-Number Monte-Carlo Tree Search</title>
<link>https://arxiv.org/abs/2506.13249</link>
<guid>https://arxiv.org/abs/2506.13249</guid>
<content:encoded><![CDATA[
arXiv:2506.13249v1 Announce Type: new 
Abstract: This paper presents Generalized Proof-Number Monte-Carlo Tree Search: a generalization of recently proposed combinations of Proof-Number Search (PNS) with Monte-Carlo Tree Search (MCTS), which use (dis)proof numbers to bias UCB1-based Selection strategies towards parts of the search that are expected to be easily (dis)proven. We propose three core modifications of prior combinations of PNS with MCTS. First, we track proof numbers per player. This reduces code complexity in the sense that we no longer need disproof numbers, and generalizes the technique to be applicable to games with more than two players. Second, we propose and extensively evaluate different methods of using proof numbers to bias the selection strategy, achieving strong performance with strategies that are simpler to implement and compute. Third, we merge our technique with Score Bounded MCTS, enabling the algorithm to prove and leverage upper and lower bounds on scores - as opposed to only proving wins or not-wins. Experiments demonstrate substantial performance increases, reaching the range of 80% for 8 out of the 11 tested board games.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector Ontologies as an LLM world view extraction method</title>
<link>https://arxiv.org/abs/2506.13252</link>
<guid>https://arxiv.org/abs/2506.13252</guid>
<content:encoded><![CDATA[
arXiv:2506.13252v1 Announce Type: new 
Abstract: Large Language Models (LLMs) possess intricate internal representations of the world, yet these latent structures are notoriously difficult to interpret or repurpose beyond the original prediction task. Building on our earlier work (Rothenfusser, 2025), which introduced the concept of vector ontologies as a framework for translating high-dimensional neural representations into interpretable geometric structures, this paper provides the first empirical validation of that approach. A vector ontology defines a domain-specific vector space spanned by ontologically meaningful dimensions, allowing geometric analysis of concepts and relationships within a domain. We construct an 8-dimensional vector ontology of musical genres based on Spotify audio features and test whether an LLM's internal world model of music can be consistently and accurately projected into this space. Using GPT-4o-mini, we extract genre representations through multiple natural language prompts and analyze the consistency of these projections across linguistic variations and their alignment with ground-truth data. Our results show (1) high spatial consistency of genre projections across 47 query formulations, (2) strong alignment between LLM-inferred genre locations and real-world audio feature distributions, and (3) evidence of a direct relationship between prompt phrasing and spatial shifts in the LLM's inferred vector ontology. These findings demonstrate that LLMs internalize structured, repurposable knowledge and that vector ontologies offer a promising method for extracting and analyzing this knowledge in a transparent and verifiable way.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks</title>
<link>https://arxiv.org/abs/2506.13276</link>
<guid>https://arxiv.org/abs/2506.13276</guid>
<content:encoded><![CDATA[
arXiv:2506.13276v1 Announce Type: new 
Abstract: Text-attributed graphs (TAGs) integrate textual data with graph structures, providing valuable insights in applications such as social network analysis and recommendation systems. Graph Neural Networks (GNNs) effectively capture both topological structure and textual information in TAGs but are vulnerable to adversarial attacks. Existing graph injection attack (GIA) methods assume that attackers can directly manipulate the embedding layer, producing non-explainable node embeddings. Furthermore, the effectiveness of these attacks often relies on surrogate models with high training costs. Thus, this paper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs. Our approach leverages large language models (LLMs) to generate interpretable text-level node attributes directly, ensuring attacks remain feasible in real-world scenarios. We design strategies for LLM prompting that balance exploration and reliability to guide text generation, and propose a similarity assessment method to evaluate attack text effectiveness in disrupting graph homophily. This method efficiently perturbs the target node with minimal training costs in a strict black-box setting, ensuring a text-level graph injection attack for TAGs. Experiments on real-world TAG datasets validate the superior performance of ATAG-LLM compared to state-of-the-art embedding-level and text-level attack methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Pervasive Distributed Agentic Generative AI -- A State of The Art</title>
<link>https://arxiv.org/abs/2506.13324</link>
<guid>https://arxiv.org/abs/2506.13324</guid>
<content:encoded><![CDATA[
arXiv:2506.13324v1 Announce Type: new 
Abstract: The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called "Agent as a Tool", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Spiking Neural Networks with Contract-Based Verification</title>
<link>https://arxiv.org/abs/2506.13340</link>
<guid>https://arxiv.org/abs/2506.13340</guid>
<content:encoded><![CDATA[
arXiv:2506.13340v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNN) are models for "realistic" neuronal computation, which makes them somehow different in scope from "ordinary" deep-learning models widely used in AI platforms nowadays. SNNs focus on timed latency (and possibly probability) of neuronal reactive activation/response, more than numerical computation of filters. So, an SNN model must provide modeling constructs for elementary neural bundles and then for synaptic connections to assemble them into compound data flow network patterns. These elements are to be parametric patterns, with latency and probability values instantiated on particular instances (while supposedly constant "at runtime"). Designers could also use different values to represent "tired" neurons, or ones impaired by external drugs, for instance. One important challenge in such modeling is to study how compound models could meet global reaction requirements (in stochastic timing challenges), provided similar provisions on individual neural bundles. A temporal language of logic to express such assume/guarantee contracts is thus needed. This may lead to formal verification on medium-sized models and testing observations on large ones. In the current article, we make preliminary progress at providing a simple model framework to express both elementary SNN neural bundles and their connecting constructs, which translates readily into both a model-checker and a simulator (both already existing and robust) to conduct experiments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers</title>
<link>https://arxiv.org/abs/2506.13342</link>
<guid>https://arxiv.org/abs/2506.13342</guid>
<content:encoded><![CDATA[
arXiv:2506.13342v1 Announce Type: new 
Abstract: Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socratic RL: A Novel Framework for Efficient Knowledge Acquisition through Iterative Reflection and Viewpoint Distillation</title>
<link>https://arxiv.org/abs/2506.13358</link>
<guid>https://arxiv.org/abs/2506.13358</guid>
<content:encoded><![CDATA[
arXiv:2506.13358v1 Announce Type: new 
Abstract: Current Reinforcement Learning (RL) methodologies for Large Language Models (LLMs) often rely on simplistic, outcome-based reward signals (e.g., final answer correctness), which limits the depth of learning from each interaction. This paper introduces Socratic Reinforcement Learning (Socratic-RL), a novel, process-oriented framework designed to address this limitation. Socratic-RL operates on the principle that deeper understanding is achieved by reflecting on the causal reasons for errors and successes within the reasoning process itself. The framework employs a decoupled "Teacher-Student" architecture, where a "Teacher AI" analyzes interaction histories, extracts causal insights, and formulates them into structured "viewpoints." These viewpoints, acting as distilled guidance, are then used by a "Student AI" to enhance its subsequent reasoning. A key innovation is the iterative self-improvement of the Teacher AI, enabling its reflective capabilities to evolve through a meta-learning loop. To manage the accumulation of knowledge, a distillation mechanism compresses learned viewpoints into the Student's parameters. By focusing on process rather than just outcome, Socratic-RL presents a pathway toward enhanced sample efficiency, superior interpretability, and a more scalable architecture for self-improving AI systems. This paper details the foundational concepts, formal mechanisms, synergies, challenges, and a concrete research roadmap for this proposed framework.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses</title>
<link>https://arxiv.org/abs/2506.13384</link>
<guid>https://arxiv.org/abs/2506.13384</guid>
<content:encoded><![CDATA[
arXiv:2506.13384v1 Announce Type: new 
Abstract: Large language models (LLMs) offer the potential to simulate human-like responses and behaviors, creating new opportunities for psychological science. In the context of self-regulated learning (SRL), if LLMs can reliably simulate survey responses at scale and speed, they could be used to test intervention scenarios, refine theoretical models, augment sparse datasets, and represent hard-to-reach populations. However, the validity of LLM-generated survey responses remains uncertain, with limited research focused on SRL and existing studies beyond SRL yielding mixed results. Therefore, in this study, we examined LLM-generated responses to the 44-item Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich \& De Groot, 1990), a widely used instrument assessing students' learning strategies and academic motivation. Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large. We analyzed item distributions, the psychological network of the theoretical SRL dimensions, and psychometric validity based on the latent factor structure. Our results suggest that Gemini 2 Flash was the most promising LLM, showing considerable sampling variability and producing underlying dimensions and theoretical relationships that align with prior theory and empirical findings. At the same time, we observed discrepancies and limitations, underscoring both the potential and current constraints of using LLMs for simulating psychological survey data and applying it in educational contexts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deflating Deflationism: A Critical Perspective on Debunking Arguments Against LLM Mentality</title>
<link>https://arxiv.org/abs/2506.13403</link>
<guid>https://arxiv.org/abs/2506.13403</guid>
<content:encoded><![CDATA[
arXiv:2506.13403v1 Announce Type: new 
Abstract: Many people feel compelled to interpret, describe, and respond to Large Language Models (LLMs) as if they possess inner mental lives similar to our own. Responses to this phenomenon have varied. Inflationists hold that at least some folk psychological ascriptions to LLMs are warranted. Deflationists argue that all such attributions of mentality to LLMs are misplaced, often cautioning against the risk that anthropomorphic projection may lead to misplaced trust or potentially even confusion about the moral status of LLMs. We advance this debate by assessing two common deflationary arguments against LLM mentality. What we term the 'robustness strategy' aims to undercut one justification for believing that LLMs are minded entities by showing that putatively cognitive and humanlike behaviours are not robust, failing to generalise appropriately. What we term the 'etiological strategy' undercuts attributions of mentality by challenging naive causal explanations of LLM behaviours, offering alternative causal accounts that weaken the case for mental state attributions. While both strategies offer powerful challenges to full-blown inflationism, we find that neither strategy provides a knock-down case against ascriptions of mentality to LLMs simpliciter. With this in mind, we explore a modest form of inflationism that permits ascriptions of mentality to LLMs under certain conditions. Specifically, we argue that folk practice provides a defeasible basis for attributing mental states and capacities to LLMs provided those mental states and capacities can be understood in metaphysically undemanding terms (e.g. knowledge, beliefs and desires), while greater caution is required when attributing metaphysically demanding mental phenomena such as phenomenal consciousness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Technical Study into Small Reasoning Language Models</title>
<link>https://arxiv.org/abs/2506.13404</link>
<guid>https://arxiv.org/abs/2506.13404</guid>
<content:encoded><![CDATA[
arXiv:2506.13404v1 Announce Type: new 
Abstract: The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Block-wise Adaptive Caching for Accelerating Diffusion Policy</title>
<link>https://arxiv.org/abs/2506.13456</link>
<guid>https://arxiv.org/abs/2506.13456</guid>
<content:encoded><![CDATA[
arXiv:2506.13456v1 Announce Type: new 
Abstract: Diffusion Policy has demonstrated strong visuomotor modeling capabilities, but its high computational cost renders it impractical for real-time robotic control. Despite huge redundancy across repetitive denoising steps, existing diffusion acceleration techniques fail to generalize to Diffusion Policy due to fundamental architectural and data divergences. In this paper, we propose Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by caching intermediate action features. BAC achieves lossless action generation acceleration by adaptively updating and reusing cached features at the block level, based on a key observation that feature similarities vary non-uniformly across timesteps and locks. To operationalize this insight, we first propose the Adaptive Caching Scheduler, designed to identify optimal update timesteps by maximizing the global feature similarities between cached and skipped features. However, applying this scheduler for each block leads to signiffcant error surges due to the inter-block propagation of caching errors, particularly within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop the Bubbling Union Algorithm, which truncates these errors by updating the upstream blocks with signiffcant caching errors before downstream FFNs. As a training-free plugin, BAC is readily integrable with existing transformer-based Diffusion Policy and vision-language-action models. Extensive experiments on multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference speedup for free.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care</title>
<link>https://arxiv.org/abs/2506.13584</link>
<guid>https://arxiv.org/abs/2506.13584</guid>
<content:encoded><![CDATA[
arXiv:2506.13584v1 Announce Type: new 
Abstract: In this work, we reflect on the data-driven modeling paradigm that is gaining ground in AI-driven automation of patient care. We argue that the repurposing of existing real-world patient datasets for machine learning may not always represent an optimal approach to model development as it could lead to undesirable outcomes in patient care. We reflect on the history of data analysis to explain how the data-driven paradigm rose to popularity, and we envision ways in which systems thinking and clinical domain theory could complement the existing model development approaches in reaching human-centric outcomes. We call for a purpose-driven machine learning paradigm that is grounded in clinical theory and the sociotechnical realities of real-world operational contexts. We argue that understanding the utility of existing patient datasets requires looking in two directions: upstream towards the data generation, and downstream towards the automation objectives. This purpose-driven perspective to AI system development opens up new methodological opportunities and holds promise for AI automation of patient care.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Capability Negotiation and Binding Protocol (ACNBP)</title>
<link>https://arxiv.org/abs/2506.13590</link>
<guid>https://arxiv.org/abs/2506.13590</guid>
<content:encoded><![CDATA[
arXiv:2506.13590v1 Announce Type: new 
Abstract: As multi-agent systems evolve to encompass increasingly diverse and specialized agents, the challenge of enabling effective collaboration between heterogeneous agents has become paramount, with traditional agent communication protocols often assuming homogeneous environments or predefined interaction patterns that limit their applicability in dynamic, open-world scenarios. This paper presents the Agent Capability Negotiation and Binding Protocol (ACNBP), a novel framework designed to facilitate secure, efficient, and verifiable interactions between agents in heterogeneous multi-agent systems through integration with an Agent Name Service (ANS) infrastructure that provides comprehensive discovery, negotiation, and binding mechanisms. The protocol introduces a structured 10-step process encompassing capability discovery, candidate pre-screening and selection, secure negotiation phases, and binding commitment with built-in security measures including digital signatures, capability attestation, and comprehensive threat mitigation strategies, while a key innovation of ACNBP is its protocolExtension mechanism that enables backward-compatible protocol evolution and supports diverse agent architectures while maintaining security and interoperability. We demonstrate ACNBP's effectiveness through a comprehensive security analysis using the MAESTRO threat modeling framework, practical implementation considerations, and a detailed example showcasing the protocol's application in a document translation scenario, with the protocol addressing critical challenges in agent autonomy, capability verification, secure communication, and scalable agent ecosystem management.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The ASP-based Nurse Scheduling System at the University of Yamanashi Hospital</title>
<link>https://arxiv.org/abs/2506.13600</link>
<guid>https://arxiv.org/abs/2506.13600</guid>
<content:encoded><![CDATA[
arXiv:2506.13600v1 Announce Type: new 
Abstract: We present the design principles of a nurse scheduling system built using Answer Set Programming (ASP) and successfully deployed at the University of Yamanashi Hospital. Nurse scheduling is a complex optimization problem requiring the reconciliation of individual nurse preferences with hospital staffing needs across various wards. This involves balancing hard and soft constraints and the flexibility of interactive adjustments. While extensively studied in academia, real-world nurse scheduling presents unique challenges that go beyond typical benchmark problems and competitions. This paper details the practical application of ASP to address these challenges at the University of Yamanashi Hospital, focusing on the insights gained and the advancements in ASP technology necessary to effectively manage the complexities of real-world deployment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoiding Obfuscation with Prover-Estimator Debate</title>
<link>https://arxiv.org/abs/2506.13609</link>
<guid>https://arxiv.org/abs/2506.13609</guid>
<content:encoded><![CDATA[
arXiv:2506.13609v1 Announce Type: new 
Abstract: Training powerful AI systems to exhibit desired behaviors hinges on the ability to provide accurate human supervision on increasingly complex tasks. A promising approach to this problem is to amplify human judgement by leveraging the power of two competing AIs in a debate about the correct solution to a given problem. Prior theoretical work has provided a complexity-theoretic formalization of AI debate, and posed the problem of designing protocols for AI debate that guarantee the correctness of human judgements for as complex a class of problems as possible. Recursive debates, in which debaters decompose a complex problem into simpler subproblems, hold promise for growing the class of problems that can be accurately judged in a debate. However, existing protocols for recursive debate run into the obfuscated arguments problem: a dishonest debater can use a computationally efficient strategy that forces an honest opponent to solve a computationally intractable problem to win. We mitigate this problem with a new recursive debate protocol that, under certain stability assumptions, ensures that an honest debater can win with a strategy requiring computational efficiency comparable to their opponent.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</title>
<link>https://arxiv.org/abs/2506.13642</link>
<guid>https://arxiv.org/abs/2506.13642</guid>
<content:encoded><![CDATA[
arXiv:2506.13642v1 Announce Type: new 
Abstract: The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13726</link>
<guid>https://arxiv.org/abs/2506.13726</guid>
<content:encoded><![CDATA[
arXiv:2506.13726v1 Announce Type: new 
Abstract: The introduction of advanced reasoning capabilities have improved the problem-solving performance of large language models, particularly on math and coding benchmarks. However, it remains unclear whether these reasoning models are more or less vulnerable to adversarial prompt attacks than their non-reasoning counterparts. In this work, we present a systematic evaluation of weaknesses in advanced reasoning models compared to similar non-reasoning models across a diverse set of prompt-based attack categories. Using experimental data, we find that on average the reasoning-augmented models are \emph{slightly more robust} than non-reasoning models (42.51\% vs 45.53\% attack success rate, lower is better). However, this overall trend masks significant category-specific differences: for certain attack types the reasoning models are substantially \emph{more vulnerable} (e.g., up to 32 percentage points worse on a tree-of-attacks prompt), while for others they are markedly \emph{more robust} (e.g., 29.8 points better on cross-site scripting injection). Our findings highlight the nuanced security implications of advanced reasoning in language models and emphasize the importance of stress-testing safety across diverse adversarial techniques.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13741</link>
<guid>https://arxiv.org/abs/2506.13741</guid>
<content:encoded><![CDATA[
arXiv:2506.13741v1 Announce Type: new 
Abstract: Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the effects of music on cognitive skills of children in early childhood with the Pythagorean fuzzy set approach</title>
<link>https://arxiv.org/abs/2506.12016</link>
<guid>https://arxiv.org/abs/2506.12016</guid>
<content:encoded><![CDATA[
arXiv:2506.12016v1 Announce Type: cross 
Abstract: There are many genetic and environmental factors that affect cognitive development. Music education can also be considered as one of the environmental factors. Some researchers emphasize that Music is an action that requires meta-cognitive functions such as mathematics and chess and supports spatial intelligence. The effect of Music on cognitive development in early childhood was examined using the Pythagorean Fuzzy Sets(PFS) method defined by Yager. This study created PFS based on experts' opinions, and an algorithm was given according to PFS. The algorithm's results supported the experts' data on the development of spatial-temporal skills in music education given in early childhood. The algorithm's ranking was done using the Expectation Score Function. The rankings obtained from the algorithm overlap with the experts' rankings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of Tractable Marginalization</title>
<link>https://arxiv.org/abs/2506.12020</link>
<guid>https://arxiv.org/abs/2506.12020</guid>
<content:encoded><![CDATA[
arXiv:2506.12020v1 Announce Type: cross 
Abstract: Marginalization -- summing a function over all assignments to a subset of its inputs -- is a fundamental computational problem with applications from probabilistic inference to formal verification. Despite its computational hardness in general, there exist many classes of functions (e.g., probabilistic models) for which marginalization remains tractable, and they can be commonly expressed by polynomial size arithmetic circuits computing multilinear polynomials. This raises the question, can all functions with polynomial time marginalization algorithms be succinctly expressed by such circuits? We give a negative answer, exhibiting simple functions with tractable marginalization yet no efficient representation by known models, assuming $\textsf{FP}\neq\#\textsf{P}$ (an assumption implied by $\textsf{P} \neq \textsf{NP}$). To this end, we identify a hierarchy of complexity classes corresponding to stronger forms of marginalization, all of which are efficiently computable on the known circuit models. We conclude with a completeness result, showing that whenever there is an efficient real RAM performing virtual evidence marginalization for a function, then there are small circuits for that function's multilinear representation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Vessel Trajectory Prediction: Learning Time-Discretized Kinematic Dynamics via Finite Differences</title>
<link>https://arxiv.org/abs/2506.12029</link>
<guid>https://arxiv.org/abs/2506.12029</guid>
<content:encoded><![CDATA[
arXiv:2506.12029v1 Announce Type: cross 
Abstract: Accurate vessel trajectory prediction is crucial for navigational safety, route optimization, traffic management, search and rescue operations, and autonomous navigation. Traditional data-driven models lack real-world physical constraints, leading to forecasts that disobey vessel motion dynamics, such as in scenarios with limited or noisy data where sudden course changes or speed variations occur due to external factors. To address this limitation, we propose a Physics-Informed Neural Network (PINN) approach for trajectory prediction that integrates a streamlined kinematic model for vessel motion into the neural network training process via a first- and second-order, finite difference physics-based loss function. This loss function, discretized using the first-order forward Euler method, Heun's second-order approximation, and refined with a midpoint approximation based on Taylor series expansion, enforces fidelity to fundamental physical principles by penalizing deviations from expected kinematic behavior. We evaluated PINN using real-world AIS datasets that cover diverse maritime conditions and compared it with state-of-the-art models. Our results demonstrate that the proposed method reduces average displacement errors by up to 32% across models and datasets while maintaining physical consistency. These results enhance model reliability and adherence to mission-critical maritime activities, where precision translates into better situational awareness in the oceans.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact, Causation and Prediction of Socio-Academic and Economic Factors in Exam-centric Student Evaluation Measures using Machine Learning and Causal Analysis</title>
<link>https://arxiv.org/abs/2506.12030</link>
<guid>https://arxiv.org/abs/2506.12030</guid>
<content:encoded><![CDATA[
arXiv:2506.12030v1 Announce Type: cross 
Abstract: Understanding socio-academic and economic factors influencing students' performance is crucial for effective educational interventions. This study employs several machine learning techniques and causal analysis to predict and elucidate the impacts of these factors on academic performance. We constructed a hypothetical causal graph and collected data from 1,050 student profiles. Following meticulous data cleaning and visualization, we analyze linear relationships through correlation and variable plots, and perform causal analysis on the hypothetical graph. Regression and classification models are applied for prediction, and unsupervised causality analysis using PC, GES, ICA-LiNGAM, and GRASP algorithms is conducted. Our regression analysis shows that Ridge Regression achieve a Mean Absolute Error (MAE) of 0.12 and a Mean Squared Error (MSE) of 0.024, indicating robustness, while classification models like Random Forest achieve nearly perfect F1-scores. The causal analysis shows significant direct and indirect effects of factors such as class attendance, study hours, and group study on CGPA. These insights are validated through unsupervised causality analysis. By integrating the best regression model into a web application, we are developing a practical tool for students and educators to enhance academic outcomes based on empirical evidence.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization in Heterogeneous Federated Continual Learning via Spatio-Temporal Gradient Matching with Prototypical Coreset</title>
<link>https://arxiv.org/abs/2506.12031</link>
<guid>https://arxiv.org/abs/2506.12031</guid>
<content:encoded><![CDATA[
arXiv:2506.12031v1 Announce Type: cross 
Abstract: Federated Continual Learning (FCL) has recently emerged as a crucial research area, as data from distributed clients typically arrives as a stream, requiring sequential learning. This paper explores a more practical and challenging FCL setting, where clients may have unrelated or even conflicting data and tasks. In this scenario, statistical heterogeneity and data noise can create spurious correlations, leading to biased feature learning and catastrophic forgetting. Existing FCL approaches often use generative replay to create pseudo-datasets of previous tasks. However, generative replay itself suffers from catastrophic forgetting and task divergence among clients, leading to overfitting in FCL. Existing FCL approaches often use generative replay to create pseudo-datasets of previous tasks. However, generative replay itself suffers from catastrophic forgetting and task divergence among clients, leading to overfitting in FCL. To address these challenges, we propose a novel approach called Spatio-Temporal grAdient Matching with network-free Prototype (STAMP). Our contributions are threefold: 1) We develop a model-agnostic method to determine subset of samples that effectively form prototypes when using a prototypical network, making it resilient to continual learning challenges; 2) We introduce a spatio-temporal gradient matching approach, applied at both the client-side (temporal) and server-side (spatial), to mitigate catastrophic forgetting and data heterogeneity; 3) We leverage prototypes to approximate task-wise gradients, improving gradient matching on the client-side. Extensive experiments demonstrate our method's superiority over existing baselines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines</title>
<link>https://arxiv.org/abs/2506.12032</link>
<guid>https://arxiv.org/abs/2506.12032</guid>
<content:encoded><![CDATA[
arXiv:2506.12032v1 Announce Type: cross 
Abstract: We present a robust neural watermarking framework for scientific data integrity, targeting high-dimensional fields common in climate modeling and fluid simulations. Using a convolutional autoencoder, binary messages are invisibly embedded into structured data such as temperature, vorticity, and geopotential. Our method ensures watermark persistence under lossy transformations - including noise injection, cropping, and compression - while maintaining near-original fidelity (sub-1\% MSE). Compared to classical singular value decomposition (SVD)-based watermarking, our approach achieves $>$98\% bit accuracy and visually indistinguishable reconstructions across ERA5 and Navier-Stokes datasets. This system offers a scalable, model-compatible tool for data provenance, auditability, and traceability in high-performance scientific workflows, and contributes to the broader goal of securing AI systems through verifiable, physics-aware watermarking. We evaluate on physically grounded scientific datasets as a representative stress-test; the framework extends naturally to other structured domains such as satellite imagery and autonomous-vehicle perception streams.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMERGENT: Efficient and Manipulation-resistant Matching using GFlowNets</title>
<link>https://arxiv.org/abs/2506.12033</link>
<guid>https://arxiv.org/abs/2506.12033</guid>
<content:encoded><![CDATA[
arXiv:2506.12033v1 Announce Type: cross 
Abstract: The design of fair and efficient algorithms for allocating public resources, such as school admissions, housing, or medical residency, has a profound social impact. In one-sided matching problems, where individuals are assigned to items based on ranked preferences, a fundamental trade-off exists between efficiency and strategyproofness. Existing algorithms like Random Serial Dictatorship (RSD), Probabilistic Serial (PS), and Rank Minimization (RM) capture only one side of this trade-off: RSD is strategyproof but inefficient, while PS and RM are efficient but incentivize manipulation. We propose EMERGENT, a novel application of Generative Flow Networks (GFlowNets) to one-sided matching, leveraging its ability to sample diverse, high-reward solutions. In our approach, efficient and manipulation-resistant matches emerge naturally: high-reward solutions yield efficient matches, while the stochasticity of GFlowNets-based outputs reduces incentives for manipulation. Experiments show that EMERGENT outperforms RSD in rank efficiency while significantly reducing strategic vulnerability compared to matches produced by RM and PS. Our work highlights the potential of GFlowNets for applications involving social choice mechanisms, where it is crucial to balance efficiency and manipulability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-like Forgetting Curves in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2506.12034</link>
<guid>https://arxiv.org/abs/2506.12034</guid>
<content:encoded><![CDATA[
arXiv:2506.12034v1 Announce Type: cross 
Abstract: This study bridges cognitive science and neural network design by examining whether artificial models exhibit human-like forgetting curves. Drawing upon Ebbinghaus' seminal work on memory decay and principles of spaced repetition, we propose a quantitative framework to measure information retention in neural networks. Our approach computes the recall probability by evaluating the similarity between a network's current hidden state and previously stored prototype representations. This retention metric facilitates the scheduling of review sessions, thereby mitigating catastrophic forgetting during deployment and enhancing training efficiency by prompting targeted reviews. Our experiments with Multi-Layer Perceptrons reveal human-like forgetting curves, with knowledge becoming increasingly robust through scheduled reviews. This alignment between neural network forgetting curves and established human memory models identifies neural networks as an architecture that naturally emulates human memory decay and can inform state-of-the-art continual learning algorithms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARch\'e: Fast Masked Autoregressive Image Generation with Cache-Aware Attention</title>
<link>https://arxiv.org/abs/2506.12035</link>
<guid>https://arxiv.org/abs/2506.12035</guid>
<content:encoded><![CDATA[
arXiv:2506.12035v1 Announce Type: cross 
Abstract: Masked autoregressive (MAR) models unify the strengths of masked and autoregressive generation by predicting tokens in a fixed order using bidirectional attention for image generation. While effective, MAR models suffer from significant computational overhead, as they recompute attention and feed-forward representations for all tokens at every decoding step, despite most tokens remaining semantically stable across steps. We propose a training-free generation framework MARch\'e to address this inefficiency through two key components: cache-aware attention and selective KV refresh. Cache-aware attention partitions tokens into active and cached sets, enabling separate computation paths that allow efficient reuse of previously computed key/value projections without compromising full-context modeling. But a cached token cannot be used indefinitely without recomputation due to the changing contextual information over multiple steps. MARch\'e recognizes this challenge and applies a technique called selective KV refresh. Selective KV refresh identifies contextually relevant tokens based on attention scores from newly generated tokens and updates only those tokens that require recomputation, while preserving image generation quality. MARch\'e significantly reduces redundant computation in MAR without modifying the underlying architecture. Empirically, MARch\'e achieves up to 1.7x speedup with negligible impact on image quality, offering a scalable and broadly applicable solution for efficient masked transformer generation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2506.12036</link>
<guid>https://arxiv.org/abs/2506.12036</guid>
<content:encoded><![CDATA[
arXiv:2506.12036v1 Announce Type: cross 
Abstract: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train a Model on a Cheap Cluster with Low Cost using Block Coordinate Descent</title>
<link>https://arxiv.org/abs/2506.12037</link>
<guid>https://arxiv.org/abs/2506.12037</guid>
<content:encoded><![CDATA[
arXiv:2506.12037v1 Announce Type: cross 
Abstract: Training large language models typically demands extensive GPU memory and substantial financial investment, which poses a barrier for many small- to medium-sized teams. In this paper, we present a full-parameter pre-training framework based on block coordinate descent (BCD), augmented with engineering optimizations, to efficiently train large models on affordable RTX 4090 GPU clusters. BCD ensures model convergence based on block coordinate descent theory and performs gradient computation and update at the level of parameter blocks. Experiments show that 1) Lower cost of Same-Device: BCD significantly reduces pre-training cost. For the 7B model, under identical hardware settings, BCD lowers training costs to approximately 33% on A100,A800 clusters on 7B model averagely and to approximately 2.6% on RTX 4090 clusters on 7B model, compared to traditional full-parameter training. 2) Cross-Device Transfer: By leveraging BCD, large-scale models previously trainable only on high-end A100 clusters can be seamlessly migrated and pre-trained on 4090 clusters-whose hourly cost is only one-quarter that of A100-without requiring expensive hardware. 3) Accuracy Retention: In both scenarios, BCD training achieves the same level of model accuracy as full-parameter pre-training.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LCD: Advancing Extreme Low-Bit Clustering for Large Language Models via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.12038</link>
<guid>https://arxiv.org/abs/2506.12038</guid>
<content:encoded><![CDATA[
arXiv:2506.12038v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved significant progress in natural language processing but face challenges in deployment due to high memory and computational requirements. Weight quantization is a common approach to address these issues, yet achieving effective low-bit compression remains challenging. This paper presents LCD, which unifies the learning of clustering-based quantization within a knowledge distillation framework. Using carefully designed optimization techniques, LCD preserves LLM performance even at ultra-low bit widths of 2-3 bits. Additionally, LCD compresses activations through smoothing and accelerates inference with a LUT-based design. Experimental results show that LCD outperforms existing methods and delivers up to a 6.2x speedup in inference. Notably, LCD is shown to be more cost-effective, making it a practical solution for real-world applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Maximal Overlap Discrete Wavelet Scattering Transform and Its Application in Classification Tasks</title>
<link>https://arxiv.org/abs/2506.12039</link>
<guid>https://arxiv.org/abs/2506.12039</guid>
<content:encoded><![CDATA[
arXiv:2506.12039v1 Announce Type: cross 
Abstract: We present the Maximal Overlap Discrete Wavelet Scattering Transform (MODWST), whose construction is inspired by the combination of the Maximal Overlap Discrete Wavelet Transform (MODWT) and the Scattering Wavelet Transform (WST). We also discuss the use of MODWST in classification tasks, evaluating its performance in two applications: stationary signal classification and ECG signal classification. The results demonstrate that MODWST achieved good performance in both applications, positioning itself as a viable alternative to popular methods like Convolutional Neural Networks (CNNs), particularly when the training data set is limited.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook</title>
<link>https://arxiv.org/abs/2506.12040</link>
<guid>https://arxiv.org/abs/2506.12040</guid>
<content:encoded><![CDATA[
arXiv:2506.12040v1 Announce Type: cross 
Abstract: Binary quantization represents the most extreme form of large language model (LLM) compression, reducing weights to $\pm$1 for maximal memory and computational efficiency. While recent sparsity-aware binarization methods achieve sub-1-bit compression by pruning redundant binary weights, they suffer from three critical challenges: performance deterioration, computational complexity from sparse mask management, and limited hardware compatibility. In this paper, we present BTC-LLM, a novel sub-1-bit LLM quantization framework that leverages adaptive weight transformation and binary pattern clustering to overcome these limitations, delivering both superior accuracy and efficiency. Our approach incorporates two key innovations: (1) a Learnable Transformation that optimizes invertible scaling and rotation matrices to align binarized weights with full-precision distributions, enabling incoherence processing to enhance layer-wise representation quality; (2) a Flash and Accurate Binary Codebook that identifies recurring binary vector clusters, compressing them into compact indices with tailored distance metrics and sign-based centroid updates. This eliminates the need for sparse masks, enabling efficient inference on standard hardware. Our code is available at https://github.com/Chooovy/BTC-LLM.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta Pruning via Graph Metanetworks : A Meta Learning Framework for Network Pruning</title>
<link>https://arxiv.org/abs/2506.12041</link>
<guid>https://arxiv.org/abs/2506.12041</guid>
<content:encoded><![CDATA[
arXiv:2506.12041v1 Announce Type: cross 
Abstract: Network pruning, aimed at reducing network size while preserving accuracy, has attracted significant research interest. Numerous pruning techniques have been proposed over time. They are becoming increasingly effective, but more complex and harder to interpret as well. Given the inherent complexity of neural networks, we argue that manually designing pruning criteria has reached a bottleneck. To address this, we propose a novel approach in which we "use a neural network to prune neural networks". More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically which can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and the standard finetuning to prune at state-of-the-art. Our method achieved outstanding results on many popular and representative pruning tasks (including ResNet56 on CIFAR10, VGG19 on CIFAR100, ResNet50 on ImageNet). Our code is available at https://github.com/Yewei-Liu/MetaPruning
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRITS: Convolutional Rectifier for Interpretable Time Series Classification</title>
<link>https://arxiv.org/abs/2506.12042</link>
<guid>https://arxiv.org/abs/2506.12042</guid>
<content:encoded><![CDATA[
arXiv:2506.12042v1 Announce Type: cross 
Abstract: Several interpretability methods for convolutional network-based classifiers exist. Most of these methods focus on extracting saliency maps for a given sample, providing a local explanation that highlights the main regions for the classification. However, some of these methods lack detailed explanations in the input space due to upscaling issues or may require random perturbations to extract the explanations. We propose Convolutional Rectifier for Interpretable Time Series Classification, or CRITS, as an interpretable model for time series classification that is designed to intrinsically extract local explanations. The proposed method uses a layer of convolutional kernels, a max-pooling layer and a fully-connected rectifier network (a network with only rectified linear unit activations). The rectified linear unit activation allows the extraction of the feature weights for the given sample, eliminating the need to calculate gradients, use random perturbations and the upscale of the saliency maps to the initial input space. We evaluate CRITS on a set of datasets, and study its classification performance and its explanation alignment, sensitivity and understandability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Some Inputs Break Low-Bit LLM Quantization?</title>
<link>https://arxiv.org/abs/2506.12044</link>
<guid>https://arxiv.org/abs/2506.12044</guid>
<content:encoded><![CDATA[
arXiv:2506.12044v1 Announce Type: cross 
Abstract: Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Proxies to Fields: Spatiotemporal Reconstruction of Global Radiation from Sparse Sensor Sequences</title>
<link>https://arxiv.org/abs/2506.12045</link>
<guid>https://arxiv.org/abs/2506.12045</guid>
<content:encoded><![CDATA[
arXiv:2506.12045v1 Announce Type: cross 
Abstract: Accurate reconstruction of latent environmental fields from sparse and indirect observations is a foundational challenge across scientific domains-from atmospheric science and geophysics to public health and aerospace safety. Traditional approaches rely on physics-based simulators or dense sensor networks, both constrained by high computational cost, latency, or limited spatial coverage. We present the Temporal Radiation Operator Network (TRON), a spatiotemporal neural operator architecture designed to infer continuous global scalar fields from sequences of sparse, non-uniform proxy measurements.
  Unlike recent forecasting models that operate on dense, gridded inputs to predict future states, TRON addresses a more ill-posed inverse problem: reconstructing the current global field from sparse, temporally evolving sensor sequences, without access to future observations or dense labels. Demonstrated on global cosmic radiation dose reconstruction, TRON is trained on 22 years of simulation data and generalizes across 65,341 spatial locations, 8,400 days, and sequence lengths from 7 to 90 days. It achieves sub-second inference with relative L2 errors below 0.1%, representing a >58,000X speedup over Monte Carlo-based estimators. Though evaluated in the context of cosmic radiation, TRON offers a domain-agnostic framework for scientific field reconstruction from sparse data, with applications in atmospheric modeling, geophysical hazard monitoring, and real-time environmental risk forecasting.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Neural Decoding with Brain Functional Network Modeling</title>
<link>https://arxiv.org/abs/2506.12055</link>
<guid>https://arxiv.org/abs/2506.12055</guid>
<content:encoded><![CDATA[
arXiv:2506.12055v1 Announce Type: cross 
Abstract: Recent achievements in implantable brain-computer interfaces (iBCIs) have demonstrated the potential to decode cognitive and motor behaviors with intracranial brain recordings; however, individual physiological and electrode implantation heterogeneities have constrained current approaches to neural decoding within single individuals, rendering interindividual neural decoding elusive. Here, we present Multi-individual Brain Region-Aggregated Network (MIBRAIN), a neural decoding framework that constructs a whole functional brain network model by integrating intracranial neurophysiological recordings across multiple individuals. MIBRAIN leverages self-supervised learning to derive generalized neural prototypes and supports group-level analysis of brain-region interactions and inter-subject neural synchrony. To validate our framework, we recorded stereoelectroencephalography (sEEG) signals from a cohort of individuals performing Mandarin syllable articulation. Both real-time online and offline decoding experiments demonstrated significant improvements in both audible and silent articulation decoding, enhanced decoding accuracy with increased multi-subject data integration, and effective generalization to unseen subjects. Furthermore, neural predictions for regions without direct electrode coverage were validated against authentic neural data. Overall, this framework paves the way for robust neural decoding across individuals and offers insights for practical clinical applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models</title>
<link>https://arxiv.org/abs/2506.12059</link>
<guid>https://arxiv.org/abs/2506.12059</guid>
<content:encoded><![CDATA[
arXiv:2506.12059v1 Announce Type: cross 
Abstract: In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM's prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review</title>
<link>https://arxiv.org/abs/2506.12060</link>
<guid>https://arxiv.org/abs/2506.12060</guid>
<content:encoded><![CDATA[
arXiv:2506.12060v1 Announce Type: cross 
Abstract: Cybersecurity organizations are adapting to GenAI integration through modified frameworks and hybrid operational processes, with success influenced by existing security maturity, regulatory requirements, and investments in human capital and infrastructure. This qualitative research employs systematic document analysis and comparative case study methodology to examine how cybersecurity organizations adapt their threat modeling frameworks and operational processes to address generative artificial intelligence integration. Through examination of 25 studies from 2022 to 2025, the research documents substantial transformation in organizational approaches to threat modeling, moving from traditional signature-based systems toward frameworks incorporating artificial intelligence capabilities. The research identifies three primary adaptation patterns: Large Language Model integration for security applications, GenAI frameworks for risk detection and response automation, and AI/ML integration for threat hunting. Organizations with mature security infrastructures, particularly in finance and critical infrastructure sectors, demonstrate higher readiness through structured governance approaches, dedicated AI teams, and robust incident response processes. Organizations achieve successful GenAI integration when they maintain appropriate human oversight of automated systems, address data quality concerns and explainability requirements, and establish governance frameworks tailored to their specific sectors. Organizations encounter ongoing difficulties with privacy protection, bias reduction, personnel training, and defending against adversarial attacks. This work advances understanding of how organizations adopt innovative technologies in high-stakes environments and offers actionable insights for cybersecurity professionals implementing GenAI systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Logit-Based GOP Scores for Mispronunciation Detection</title>
<link>https://arxiv.org/abs/2506.12067</link>
<guid>https://arxiv.org/abs/2506.12067</guid>
<content:encoded><![CDATA[
arXiv:2506.12067v1 Announce Type: cross 
Abstract: Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebTrust: An AI-Driven Data Scoring System for Reliable Information Retrieval</title>
<link>https://arxiv.org/abs/2506.12072</link>
<guid>https://arxiv.org/abs/2506.12072</guid>
<content:encoded><![CDATA[
arXiv:2506.12072v1 Announce Type: cross 
Abstract: As access to information becomes more open and widespread, people are increasingly using AI tools for assistance. However, many of these tools struggle to estimate the trustworthiness of the information. Although today's search engines include AI features, they often fail to offer clear indicators of data reliability. To address this gap, we introduce WebTrust, a system designed to simplify the process of finding and judging credible information online. Built on a fine-tuned version of IBM's Granite-1B model and trained on a custom dataset, WebTrust works by assigning a reliability score (from 0.1 to 1) to each statement it processes. In addition, it offers a clear justification for why a piece of information received that score. Evaluated using prompt engineering, WebTrust consistently achieves superior performance compared to other small-scale LLMs and rule-based approaches, outperforming them across all experiments on MAE, RMSE, and R2. User testing showed that when reliability scores are displayed alongside search results, people feel more confident and satisfied with the information they find. With its accuracy, transparency, and ease of use, WebTrust offers a practical solution to help combat misinformation and make trustworthy information more accessible to everyone.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis</title>
<link>https://arxiv.org/abs/2506.12073</link>
<guid>https://arxiv.org/abs/2506.12073</guid>
<content:encoded><![CDATA[
arXiv:2506.12073v1 Announce Type: cross 
Abstract: Accurate alignment of dysfluent speech with intended text is crucial for automating the diagnosis of neurodegenerative speech disorders. Traditional methods often fail to model phoneme similarities effectively, limiting their performance. In this work, we propose Neural LCS, a novel approach for dysfluent text-text and speech-text alignment. Neural LCS addresses key challenges, including partial alignment and context-aware similarity mapping, by leveraging robust phoneme-level modeling. We evaluate our method on a large-scale simulated dataset, generated using advanced data simulation techniques, and real PPA data. Neural LCS significantly outperforms state-of-the-art models in both alignment accuracy and dysfluent speech segmentation. Our results demonstrate the potential of Neural LCS to enhance automated systems for diagnosing and analyzing speech disorders, offering a more accurate and linguistically grounded solution for dysfluent speech alignment.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing Text Selection in High School Literature through Knowledge Graph-Based Recommendation</title>
<link>https://arxiv.org/abs/2506.12075</link>
<guid>https://arxiv.org/abs/2506.12075</guid>
<content:encoded><![CDATA[
arXiv:2506.12075v1 Announce Type: cross 
Abstract: The implementation of transformational pedagogy in secondary education classrooms requires a broad multiliteracy approach. Due to limited planning time and resources, high school English Literature teachers often struggle to curate diverse, thematically aligned literature text sets. This study addresses the critical need for a tool that provides scaffolds for novice educators in selecting literature texts that are diverse -- in terms of genre, theme, subtheme, and author -- yet similar in context and pedagogical merits. We have developed a recommendation system, Teaching Text Expansion for Teacher Scaffolding (T-TExTS), that suggests high school English Literature books based on pedagogical merits, genre, and thematic relevance using a knowledge graph. We constructed a domain-specific ontology using the KNowledge Acquisition and Representation Methodology (KNARM), transformed into a knowledge graph, which was then embedded using DeepWalk, biased random walk, and a hybrid of both approaches. The system was evaluated using link prediction and recommendation performance metrics, including Area Under the Curve (AUC), Mean Reciprocal Rank (MRR), Hits@K, and normalized Discounted Cumulative Gain (nDCG). DeepWalk outperformed in most ranking metrics, with the highest AUC (0.9431), whereas the hybrid model offered balanced performance. These findings demonstrate the importance of semantic, ontology-driven approaches in recommendation systems and suggest that T-TExTS can significantly ease the burden of English Literature text selection for high school educators, promoting more informed and inclusive curricular decisions. The source code for T-TExTS is available at: https://github.com/koncordantlab/TTExTS
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Synthetic Pseudo-Autoencoder Invites Examination of Tacit Assumptions in Neural Network Design</title>
<link>https://arxiv.org/abs/2506.12076</link>
<guid>https://arxiv.org/abs/2506.12076</guid>
<content:encoded><![CDATA[
arXiv:2506.12076v1 Announce Type: cross 
Abstract: We present a handcrafted neural network that, without training, solves the seemingly difficult problem of encoding an arbitrary set of integers into a single numerical variable, and then recovering the original elements. While using only standard neural network operations -- weighted sums with biases and identity activation -- we make design choices that challenge common notions in this area around representation, continuity of domains, computation, learnability and more. For example, our construction is designed, not learned; it represents multiple values using a single one by simply concatenating digits without compression, and it relies on hardware-level truncation of rightmost digits as a bit-manipulation mechanism. This neural net is not intended for practical application. Instead, we see its resemblance to -- and deviation from -- standard trained autoencoders as an invitation to examine assumptions that may unnecessarily constrain the development of systems and models based on autoencoding and machine learning. Motivated in part by our research on a theory of biological evolution centered around natural autoencoding of species characteristics, we conclude by refining the discussion with a biological perspective.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Earth-Scale Human-Like Societies with One Billion Agents</title>
<link>https://arxiv.org/abs/2506.12078</link>
<guid>https://arxiv.org/abs/2506.12078</guid>
<content:encoded><![CDATA[
arXiv:2506.12078v1 Announce Type: cross 
Abstract: Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latency Optimization for Wireless Federated Learning in Multihop Networks</title>
<link>https://arxiv.org/abs/2506.12081</link>
<guid>https://arxiv.org/abs/2506.12081</guid>
<content:encoded><![CDATA[
arXiv:2506.12081v1 Announce Type: cross 
Abstract: In this paper, we study a novel latency minimization problem in wireless federated learning (FL) across multi-hop networks. The system comprises multiple routes, each integrating leaf and relay nodes for FL model training. We explore a personalized learning and adaptive aggregation-aware FL (PAFL) framework that effectively addresses data heterogeneity across participating nodes by harmonizing individual and collective learning objectives. We formulate an optimization problem aimed at minimizing system latency through the joint optimization of leaf and relay nodes, as well as relay routing indicator. We also incorporate an additional energy harvesting scheme for the relay nodes to help with their relay tasks. This formulation presents a computationally demanding challenge, and thus we develop a simple yet efficient algorithm based on block coordinate descent and successive convex approximation (SCA) techniques. Simulation results illustrate the efficacy of our proposed joint optimization approach for leaf and relay nodes with relay routing indicator. We observe significant latency savings in the wireless multi-hop PAFL system, with reductions of up to 69.37% compared to schemes optimizing only one node type, traditional greedy algorithm, and scheme without relay routing indicator.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification</title>
<link>https://arxiv.org/abs/2506.12084</link>
<guid>https://arxiv.org/abs/2506.12084</guid>
<content:encoded><![CDATA[
arXiv:2506.12084v1 Announce Type: cross 
Abstract: The formal specification and verification of machine learning programs saw remarkable progress in less than a decade, leading to a profusion of tools. However, diversity may lead to fragmentation, resulting in tools that are difficult to compare, except for very specific benchmarks. Furthermore, this progress is heavily geared towards the specification and verification of a certain class of property, that is, local robustness properties. But while provers are becoming more and more efficient at solving local robustness properties, even slightly more complex properties, involving multiple neural networks for example, cannot be expressed in the input languages of winners of the International Competition of Verification of Neural Networks VNN-Comp. In this tool paper, we present CAISAR, an open-source platform dedicated to machine learning specification and verification. We present its specification language, suitable for modelling complex properties on neural networks, support vector machines and boosted trees. We show on concrete use-cases how specifications written in this language are automatically translated to queries to state-of-the-art provers, notably by using automated graph editing techniques, making it possible to use their off-the-shelf versions. The artifact to reproduce the paper claims is available at the following DOI: https://doi.org/10.5281/zenodo.15209510
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanting to Be Understood Explains the Meta-Problem of Consciousness</title>
<link>https://arxiv.org/abs/2506.12086</link>
<guid>https://arxiv.org/abs/2506.12086</guid>
<content:encoded><![CDATA[
arXiv:2506.12086v1 Announce Type: cross 
Abstract: Because we are highly motivated to be understood, we created public external representations -- mime, language, art -- to externalise our inner states. We argue that such external representations are a pre-condition for access consciousness, the global availability of information for reasoning. Yet the bandwidth of access consciousness is tiny compared with the richness of `raw experience', so no external representation can reproduce that richness in full. Ordinarily an explanation of experience need only let an audience `grasp' the relevant pattern, not relive the phenomenon. But our drive to be understood, and our low level sensorimotor capacities for `grasping' so rich, that the demand for an explanation of the feel of experience cannot be ``satisfactory''. That inflated epistemic demand (the preeminence of our expectation that we could be perfectly understood by another or ourselves) rather than an irreducible metaphysical gulf -- keeps the hard problem of consciousness alive. But on the plus side, it seems we will simply never give up creating new ways to communicate and think about our experiences. In this view, to be consciously aware is to strive to have one's agency understood by oneself and others.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Parallel Training Methods for Spiking Neural Networks with Constant Time Complexity</title>
<link>https://arxiv.org/abs/2506.12087</link>
<guid>https://arxiv.org/abs/2506.12087</guid>
<content:encoded><![CDATA[
arXiv:2506.12087v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) often suffer from high time complexity $O(T)$ due to the sequential processing of $T$ spikes, making training computationally expensive.
  In this paper, we propose a novel Fixed-point Parallel Training (FPT) method to accelerate SNN training without modifying the network architecture or introducing additional assumptions.
  FPT reduces the time complexity to $O(K)$, where $K$ is a small constant (usually $K=3$), by using a fixed-point iteration form of Leaky Integrate-and-Fire (LIF) neurons for all $T$ timesteps.
  We provide a theoretical convergence analysis of FPT and demonstrate that existing parallel spiking neurons can be viewed as special cases of our proposed method.
  Experimental results show that FPT effectively simulates the dynamics of original LIF neurons, significantly reducing computational time without sacrificing accuracy.
  This makes FPT a scalable and efficient solution for real-world applications, particularly for long-term tasks.
  Our code will be released at \href{https://github.com/WanjinVon/FPT}{\texttt{https://github.com/WanjinVon/FPT}}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Automation for FDI Facilitation: Optimizing Tariff Exemption Processes with OCR And Large Language Models</title>
<link>https://arxiv.org/abs/2506.12093</link>
<guid>https://arxiv.org/abs/2506.12093</guid>
<content:encoded><![CDATA[
arXiv:2506.12093v1 Announce Type: cross 
Abstract: Tariff exemptions are fundamental to attracting Foreign Direct Investment (FDI) into the manufacturing sector, though the associated administrative processes present areas for optimization for both investing entities and the national tax authority. This paper proposes a conceptual framework to empower tax administration by leveraging a synergistic integration of Optical Character Recognition (OCR) and Large Language Model (LLM) technologies. The proposed system is designed to first utilize OCR for intelligent digitization, precisely extracting data from diverse application documents and key regulatory texts such as tariff orders. Subsequently, the LLM would enhance the capabilities of administrative officers by automating the critical and time-intensive task of verifying submitted HS Tariff Codes for machinery, equipment, and raw materials against official exemption lists. By enhancing the speed and precision of these initial assessments, this AI-driven approach systematically reduces potential for non-alignment and non-optimized exemption utilization, thereby streamlining the investment journey for FDI companies. For the national administration, the benefits include a significant boost in operational capacity, reduced administrative load, and a strengthened control environment, ultimately improving the ease of doing business and solidifying the nation's appeal as a premier destination for high-value manufacturing FDI.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Military AI Cyber Agents (MAICAs) Constitute a Global Threat to Critical Infrastructure</title>
<link>https://arxiv.org/abs/2506.12094</link>
<guid>https://arxiv.org/abs/2506.12094</guid>
<content:encoded><![CDATA[
arXiv:2506.12094v1 Announce Type: cross 
Abstract: This paper argues that autonomous AI cyber-weapons - Military-AI Cyber Agents (MAICAs) - create a credible pathway to catastrophic risk. It sets out the technical feasibility of MAICAs, explains why geopolitics and the nature of cyberspace make MAICAs a catastrophic risk, and proposes political, defensive-AI and analogue-resilience measures to blunt the threat.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"I Hadn't Thought About That": Creators of Human-like AI Weigh in on Ethics And Neurodivergence</title>
<link>https://arxiv.org/abs/2506.12098</link>
<guid>https://arxiv.org/abs/2506.12098</guid>
<content:encoded><![CDATA[
arXiv:2506.12098v1 Announce Type: cross 
Abstract: Human-like AI agents such as robots and chatbots are becoming increasingly popular, but they present a variety of ethical concerns. The first concern is in how we define humanness, and how our definition impacts communities historically dehumanized by scientific research. Autistic people in particular have been dehumanized by being compared to robots, making it even more important to ensure this marginalization is not reproduced by AI that may promote neuronormative social behaviors. Second, the ubiquitous use of these agents raises concerns surrounding model biases and accessibility. In our work, we investigate the experiences of the people who build and design these technologies to gain insights into their understanding and acceptance of neurodivergence, and the challenges in making their work more accessible to users with diverse needs. Even though neurodivergent individuals are often marginalized for their unique communication styles, nearly all participants overlooked the conclusions their end-users and other AI system makers may draw about communication norms from the implementation and interpretation of humanness applied in participants' work. This highlights a major gap in their broader ethical considerations, compounded by some participants' neuronormative assumptions about the behaviors and traits that distinguish "humans" from "bots" and the replication of these assumptions in their work. We examine the impact this may have on autism inclusion in society and provide recommendations for additional systemic changes towards more ethical research directions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialCredit+</title>
<link>https://arxiv.org/abs/2506.12099</link>
<guid>https://arxiv.org/abs/2506.12099</guid>
<content:encoded><![CDATA[
arXiv:2506.12099v1 Announce Type: cross 
Abstract: SocialCredit+ is AI powered credit scoring system that leverages publicly available social media data to augment traditional credit evaluation. It uses a conversational banking assistant to gather user consent and fetch public profiles. Multimodal feature extractors analyze posts, bios, images, and friend networks to generate a rich behavioral profile. A specialized Sharia-compliance layer flags any non-halal indicators and prohibited financial behavior based on Islamic ethics. The platform employs a retrieval-augmented generation module: an LLM accesses a domain specific knowledge base to generate clear, text-based explanations for each decision. We describe the end-to-end architecture and data flow, the models used, and system infrastructure. Synthetic scenarios illustrate how social signals translate into credit-score factors. This paper emphasizes conceptual novelty, compliance mechanisms, and practical impact, targeting AI researchers, fintech practitioners, ethical banking jurists, and investors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis</title>
<link>https://arxiv.org/abs/2506.12100</link>
<guid>https://arxiv.org/abs/2506.12100</guid>
<content:encoded><![CDATA[
arXiv:2506.12100v1 Announce Type: cross 
Abstract: Security vulnerabilities are rapidly increasing in frequency and complexity, creating a shifting threat landscape that challenges cybersecurity defenses. Large Language Models (LLMs) have been widely adopted for cybersecurity threat analysis. When querying LLMs, dealing with new, unseen vulnerabilities is particularly challenging as it lies outside LLMs' pre-trained distribution. Retrieval-Augmented Generation (RAG) pipelines mitigate the problem by injecting up-to-date authoritative sources into the model context, thus reducing hallucinations and increasing the accuracy in responses. Meanwhile, the deployment of LLMs in security-sensitive environments introduces challenges around trust and safety. This raises a critical open question: How to quantify or attribute the generated response to the retrieved context versus the model's pre-trained knowledge? This work proposes LLM Embedding-based Attribution (LEA) -- a novel, explainable metric to paint a clear picture on the 'percentage of influence' the pre-trained knowledge vs. retrieved content has for each generated response. We apply LEA to assess responses to 100 critical CVEs from the past decade, verifying its effectiveness to quantify the insightfulness for vulnerability analysis. Our development of LEA reveals a progression of independency in hidden states of LLMs: heavy reliance on context in early layers, which enables the derivation of LEA; increased independency in later layers, which sheds light on why scale is essential for LLM's effectiveness. This work provides security analysts a means to audit LLM-assisted workflows, laying the groundwork for transparent, high-assurance deployments of RAG-enhanced LLMs in cybersecurity operations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents</title>
<link>https://arxiv.org/abs/2506.12104</link>
<guid>https://arxiv.org/abs/2506.12104</guid>
<content:encoded><![CDATA[
arXiv:2506.12104v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints. A Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an Injection Isolator detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks. We empirically validate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating its strong security performance while maintaining high utility across diverse models -- showcasing both its robustness and adaptability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method</title>
<link>https://arxiv.org/abs/2506.12108</link>
<guid>https://arxiv.org/abs/2506.12108</guid>
<content:encoded><![CDATA[
arXiv:2506.12108v1 Announce Type: cross 
Abstract: An Advanced Persistent Threat (APT) is a multistage, highly sophisticated, and covert form of cyber threat that gains unauthorized access to networks to either steal valuable data or disrupt the targeted network. These threats often remain undetected for extended periods, emphasizing the critical need for early detection in networks to mitigate potential APT consequences. In this work, we propose a feature selection method for developing a lightweight intrusion detection system capable of effectively identifying APTs at the initial compromise stage. Our approach leverages the XGBoost algorithm and Explainable Artificial Intelligence (XAI), specifically utilizing the SHAP (SHapley Additive exPlanations) method for identifying the most relevant features of the initial compromise stage. The results of our proposed method showed the ability to reduce the selected features of the SCVIC-APT-2021 dataset from 77 to just four while maintaining consistent evaluation metrics for the suggested system. The estimated metrics values are 97% precision, 100% recall, and a 98% F1 score. The proposed method not only aids in preventing successful APT consequences but also enhances understanding of APT behavior at early stages.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized LLM Decoding via Contrasting Personal Preference</title>
<link>https://arxiv.org/abs/2506.12109</link>
<guid>https://arxiv.org/abs/2506.12109</guid>
<content:encoded><![CDATA[
arXiv:2506.12109v1 Announce Type: cross 
Abstract: As large language models (LLMs) are progressively deployed in various real-world applications, personalization of LLMs has become increasingly important. While various approaches to LLM personalization such as prompt-based and training-based methods have been actively explored, the development of effective decoding-time algorithms remains largely overlooked, despite their demonstrated potential. In this paper, we propose CoPe (Contrasting Personal Preference), a novel decoding-time approach applied after performing parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is to leverage reward-guided decoding specifically for personalization by maximizing each user's implicit reward signal. We evaluate CoPe across five open-ended personalized text generation tasks. Our empirical results demonstrate that CoPe achieves strong performance, improving personalization by an average of 10.57% in ROUGE-L, without relying on external reward models or additional training procedures.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EconGym: A Scalable AI Testbed with Diverse Economic Tasks</title>
<link>https://arxiv.org/abs/2506.12110</link>
<guid>https://arxiv.org/abs/2506.12110</guid>
<content:encoded><![CDATA[
arXiv:2506.12110v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has become a powerful tool for economic research, enabling large-scale simulation and policy optimization. However, applying AI effectively requires simulation platforms for scalable training and evaluation-yet existing environments remain limited to simplified, narrowly scoped tasks, falling short of capturing complex economic challenges such as demographic shifts, multi-government coordination, and large-scale agent interactions. To address this gap, we introduce EconGym, a scalable and modular testbed that connects diverse economic tasks with AI algorithms. Grounded in rigorous economic modeling, EconGym implements 11 heterogeneous role types (e.g., households, firms, banks, governments), their interaction mechanisms, and agent models with well-defined observations, actions, and rewards. Users can flexibly compose economic roles with diverse agent algorithms to simulate rich multi-agent trajectories across 25+ economic tasks for AI-driven policy learning and analysis. Experiments show that EconGym supports diverse and cross-domain tasks-such as coordinating fiscal, pension, and monetary policies-and enables benchmarking across AI, economic methods, and hybrids. Results indicate that richer task composition and algorithm diversity expand the policy space, while AI agents guided by classical economic methods perform best in complex settings. EconGym also scales to 10k agents with high realism and efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data</title>
<link>https://arxiv.org/abs/2506.12111</link>
<guid>https://arxiv.org/abs/2506.12111</guid>
<content:encoded><![CDATA[
arXiv:2506.12111v1 Announce Type: cross 
Abstract: Real-time continuous learning over streaming data remains a central challenge in deep learning and AI systems. Traditional gradient-based models such as backpropagation through time (BPTT) face computational and stability limitations when dealing with temporally unbounded data. In this paper, we introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs), which leverages the Feynman technique of differentiation under the integral sign to formulate neural updates as integrals over historical data. This reformulation allows for smoother, more stable learning dynamics that are both physically interpretable and computationally tractable. Inspired by Feynman's path integral formalism and compatible with quantum gradient estimation frameworks, QIDINNs open a path toward hybrid classical-quantum neural computation. We demonstrate our model's effectiveness on synthetic and real-world streaming tasks, and we propose directions for quantum extensions and scalable implementations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Preprocessing for LLM-based Malware Analysis</title>
<link>https://arxiv.org/abs/2506.12113</link>
<guid>https://arxiv.org/abs/2506.12113</guid>
<content:encoded><![CDATA[
arXiv:2506.12113v1 Announce Type: cross 
Abstract: In a context of malware analysis, numerous approaches rely on Artificial Intelligence to handle a large volume of data. However, these techniques focus on data view (images, sequences) and not on an expert's view. Noticing this issue, we propose a preprocessing that focuses on expert knowledge to improve malware semantic analysis and result interpretability. We propose a new preprocessing method which creates JSON reports for Portable Executable files. These reports gather features from both static and behavioral analysis, and incorporate packer signature detection, MITRE ATT\&amp;CK and Malware Behavior Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a semantic representation of binary files, understandable by malware analysts, and that can enhance AI models' explainability for malicious files analysis. Using this preprocessing to train a Large Language Model for Malware classification, we achieve a weighted-average F1-score of 0.94 on a complex dataset, representative of market reality.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting Reasoning in Language Models with Cognitive Tools</title>
<link>https://arxiv.org/abs/2506.12115</link>
<guid>https://arxiv.org/abs/2506.12115</guid>
<content:encoded><![CDATA[
arXiv:2506.12115v1 Announce Type: cross 
Abstract: The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community. These speculations were largely settled by the demonstration from DeepSeek-R1 that chains-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs. However, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.
  Here, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations. Crucially, we implement this key idea within a modern agentic tool-calling framework. In particular, we endow an LLM with a small set of "cognitive tools" encapsulating specific reasoning operations, each executed by the LLM itself. Surprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models. For instance, providing our "cognitive tools" to GPT-4.1 increases its pass@1 performance on AIME2024 from 26.7% to 43.3%, bringing it very close to the performance of o1-preview.
  In addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Document and Template Clustering using Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2506.12116</link>
<guid>https://arxiv.org/abs/2506.12116</guid>
<content:encoded><![CDATA[
arXiv:2506.12116v1 Announce Type: cross 
Abstract: This paper investigates a novel approach to unsupervised document clustering by leveraging multimodal embeddings as input to traditional clustering algorithms such as $k$-Means and DBSCAN. Our method aims to achieve a finer-grained document understanding by not only grouping documents at the type level (e.g., invoices, purchase orders), but also distinguishing between different templates within the same document category. This is achieved by using embeddings that capture textual content, layout information, and visual features of documents. We evaluated the effectiveness of this approach using embeddings generated by several state-of-the-art pretrained multimodal models, including SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. Our findings demonstrate the potential of multimodal embeddings to significantly enhance document clustering, offering benefits for various applications in intelligent document processing, document layout analysis, and unsupervised document classification. This work provides valuable insight into the advantages and limitations of different multimodal models for this task and opens new avenues for future research to understand and organize document collections.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-Invariance Drives Convergence in AI and Brain Representations</title>
<link>https://arxiv.org/abs/2506.12117</link>
<guid>https://arxiv.org/abs/2506.12117</guid>
<content:encoded><![CDATA[
arXiv:2506.12117v1 Announce Type: cross 
Abstract: Despite variations in architecture and pretraining strategies, recent studies indicate that large-scale AI models often converge toward similar internal representations that also align with neural activity. We propose that scale-invariance, a fundamental structural principle in natural systems, is a key driver of this convergence. In this work, we propose a multi-scale analytical framework to quantify two core aspects of scale-invariance in AI representations: dimensional stability and structural similarity across scales. We further investigate whether these properties can predict alignment performance with functional Magnetic Resonance Imaging (fMRI) responses in the visual cortex. Our analysis reveals that embeddings with more consistent dimension and higher structural similarity across scales align better with fMRI data. Furthermore, we find that the manifold structure of fMRI data is more concentrated, with most features dissipating at smaller scales. Embeddings with similar scale patterns align more closely with fMRI data. We also show that larger pretraining datasets and the inclusion of language modalities enhance the scale-invariance properties of embeddings, further improving neural alignment. Our findings indicate that scale-invariance is a fundamental structural principle that bridges artificial and biological representations, providing a new framework for evaluating the structural quality of human-like AI systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?</title>
<link>https://arxiv.org/abs/2506.12119</link>
<guid>https://arxiv.org/abs/2506.12119</guid>
<content:encoded><![CDATA[
arXiv:2506.12119v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. However, can MoEs surpass dense architectures under strictly equal resource constraints - that is, when the total parameter count, training compute, and data budget are identical? This question remains under-explored despite its significant practical value and potential. In this paper, we propose a novel perspective and methodological framework to study this question thoroughly. First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance. Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource. More importantly, this optimal region remains consistent across different model sizes. Although additional amount of data turns out to be a trade-off for the enhanced performance, we show that this can be resolved via reusing data. We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. All models will be released publicly.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models</title>
<link>https://arxiv.org/abs/2506.12156</link>
<guid>https://arxiv.org/abs/2506.12156</guid>
<content:encoded><![CDATA[
arXiv:2506.12156v1 Announce Type: cross 
Abstract: Interpreting large volumes of high-dimensional, unlabeled data in a manner that is comprehensible to humans remains a significant challenge across various domains. In unsupervised healthcare data analysis, interpreting clustered data can offer meaningful insights into patients' health outcomes, which hold direct implications for healthcare providers. This paper addresses the problem of interpreting clustered sensor data collected from older adult patients recovering from lower-limb fractures in the community. A total of 560 days of multimodal sensor data, including acceleration, step count, ambient motion, GPS location, heart rate, and sleep, alongside clinical scores, were remotely collected from patients at home. Clustering was first carried out separately for each data modality to assess the impact of feature sets extracted from each modality on patients' recovery trajectories. Then, using context-aware prompting, a large language model was employed to infer meaningful cluster labels for the clusters derived from each modality. The quality of these clusters and their corresponding labels was validated through rigorous statistical testing and visualization against clinical scores collected alongside the multimodal sensor data. The results demonstrated the statistical significance of most modality-specific cluster labels generated by the large language model with respect to clinical scores, confirming the efficacy of the proposed method for interpreting sensor data in an unsupervised manner. This unsupervised data analysis approach, relying solely on sensor data, enables clinicians to identify at-risk patients and take timely measures to improve health outcomes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCN-DPD: Parameter-Efficient Temporal Convolutional Networks for Wideband Digital Predistortion</title>
<link>https://arxiv.org/abs/2506.12165</link>
<guid>https://arxiv.org/abs/2506.12165</guid>
<content:encoded><![CDATA[
arXiv:2506.12165v1 Announce Type: cross 
Abstract: Digital predistortion (DPD) is essential for mitigating nonlinearity in RF power amplifiers, particularly for wideband applications. This paper presents TCN-DPD, a parameter-efficient architecture based on temporal convolutional networks, integrating noncausal dilated convolutions with optimized activation functions. Evaluated on the OpenDPD framework with the DPA_200MHz dataset, TCN-DPD achieves simulated ACPRs of -51.58/-49.26 dBc (L/R), EVM of -47.52 dB, and NMSE of -44.61 dB with 500 parameters and maintains superior linearization than prior models down to 200 parameters, making it promising for efficient wideband PA linearization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI-CORE: A Foundation Model for Magnetic Resonance Imaging</title>
<link>https://arxiv.org/abs/2506.12186</link>
<guid>https://arxiv.org/abs/2506.12186</guid>
<content:encoded><![CDATA[
arXiv:2506.12186v1 Announce Type: cross 
Abstract: The widespread use of Magnetic Resonance Imaging (MRI) and the rise of deep learning have enabled the development of powerful predictive models for a wide range of diagnostic tasks in MRI, such as image classification or object segmentation. However, training models for specific new tasks often requires large amounts of labeled data, which is difficult to obtain due to high annotation costs and data privacy concerns. To circumvent this issue, we introduce MRI-CORE (MRI COmprehensive Representation Encoder), a vision foundation model pre-trained using more than 6 million slices from over 110,000 MRI volumes across 18 main body locations. Experiments on five diverse object segmentation tasks in MRI demonstrate that MRI-CORE can significantly improve segmentation performance in realistic scenarios with limited labeled data availability, achieving an average gain of 6.97% 3D Dice Coefficient using only 10 annotated slices per task. We further demonstrate new model capabilities in MRI such as classification of image properties including body location, sequence type and institution, and zero-shot segmentation. These results highlight the value of MRI-CORE as a generalist vision foundation model for MRI, potentially lowering the data annotation resource barriers for many applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis</title>
<link>https://arxiv.org/abs/2506.12189</link>
<guid>https://arxiv.org/abs/2506.12189</guid>
<content:encoded><![CDATA[
arXiv:2506.12189v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction</title>
<link>https://arxiv.org/abs/2506.12190</link>
<guid>https://arxiv.org/abs/2506.12190</guid>
<content:encoded><![CDATA[
arXiv:2506.12190v1 Announce Type: cross 
Abstract: Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+/HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViSAGe: Video-to-Spatial Audio Generation</title>
<link>https://arxiv.org/abs/2506.12199</link>
<guid>https://arxiv.org/abs/2506.12199</guid>
<content:encoded><![CDATA[
arXiv:2506.12199v1 Announce Type: cross 
Abstract: Spatial audio is essential for enhancing the immersiveness of audio-visual experiences, yet its production typically demands complex recording systems and specialized expertise. In this work, we address a novel problem of generating first-order ambisonics, a widely used spatial audio format, directly from silent videos. To support this task, we introduce YT-Ambigen, a dataset comprising 102K 5-second YouTube video clips paired with corresponding first-order ambisonics. We also propose new evaluation metrics to assess the spatial aspect of generated audio based on audio energy maps and saliency metrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an end-to-end framework that generates first-order ambisonics from silent video frames by leveraging CLIP visual features, autoregressive neural audio codec modeling with both directional and visual guidance. Experimental results demonstrate that ViSAGe produces plausible and coherent first-order ambisonics, outperforming two-stage approaches consisting of video-to-audio generation and audio spatialization. Qualitative examples further illustrate that ViSAGe generates temporally aligned high-quality spatial audio that adapts to viewpoint changes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions</title>
<link>https://arxiv.org/abs/2506.12202</link>
<guid>https://arxiv.org/abs/2506.12202</guid>
<content:encoded><![CDATA[
arXiv:2506.12202v1 Announce Type: cross 
Abstract: Modern large language models (LLMs) are often deployed as agents, calling external tools adaptively to solve tasks. Rather than directly calling tools, it can be more effective for LLMs to write code to perform the tool calls, enabling them to automatically generate complex control flow such as conditionals and loops. Such code actions are typically provided as Python code, since LLMs are quite proficient at it; however, Python may not be the ideal language due to limited built-in support for performance, security, and reliability. We propose a novel programming language for code actions, called Quasar, which has several benefits: (1) automated parallelization to improve performance, (2) uncertainty quantification to improve reliability and mitigate hallucinations, and (3) security features enabling the user to validate actions. LLMs can write code in a subset of Python, which is automatically transpiled to Quasar. We evaluate our approach on the ViperGPT visual question answering agent, applied to the GQA dataset, demonstrating that LLMs with Quasar actions instead of Python actions retain strong performance, while reducing execution time when possible by 42%, improving security by reducing user approval interactions when possible by 52%, and improving reliability by applying conformal prediction to achieve a desired target coverage level.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Scheduling for LLM Inference</title>
<link>https://arxiv.org/abs/2506.12204</link>
<guid>https://arxiv.org/abs/2506.12204</guid>
<content:encoded><![CDATA[
arXiv:2506.12204v1 Announce Type: cross 
Abstract: Conventional operating system scheduling algorithms are largely content-ignorant, making decisions based on factors such as latency or fairness without considering the actual intents or semantics of processes. Consequently, these algorithms often do not prioritize tasks that require urgent attention or carry higher importance, such as in emergency management scenarios. However, recent advances in language models enable semantic analysis of processes, allowing for more intelligent and context-aware scheduling decisions. In this paper, we introduce the concept of semantic scheduling in scheduling of requests from large language models (LLM), where the semantics of the process guide the scheduling priorities. We present a novel scheduling algorithm with optimal time complexity, designed to minimize the overall waiting time in LLM-based prompt scheduling. To illustrate its effectiveness, we present a medical emergency management application, underscoring the potential benefits of semantic scheduling for critical, time-sensitive tasks. The code and data are available at https://github.com/Wenyueh/latency_optimization_with_priority_constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Emergence to Control: Probing and Modulating Self-Reflection in Language Models</title>
<link>https://arxiv.org/abs/2506.12217</link>
<guid>https://arxiv.org/abs/2506.12217</guid>
<content:encoded><![CDATA[
arXiv:2506.12217v1 Announce Type: cross 
Abstract: Self-reflection -- the ability of a large language model (LLM) to revisit, evaluate, and revise its own reasoning -- has recently emerged as a powerful behavior enabled by reinforcement learning with verifiable rewards (RLVR). While self-reflection correlates with improved reasoning accuracy, its origin and underlying mechanisms remain poorly understood. In this work, {\it we first show that self-reflection is not exclusive to RLVR fine-tuned models: it already emerges, albeit rarely, in pretrained models}. To probe this latent ability, we introduce Reflection-Inducing Probing, a method that injects reflection-triggering reasoning traces from fine-tuned models into pretrained models. This intervention raises self-reflection frequency of Qwen2.5 from 0.6\% to 18.6\%, revealing a hidden capacity for reflection. Moreover, our analysis of internal representations shows that both pretrained and fine-tuned models maintain hidden states that distinctly separate self-reflective from non-reflective contexts. Leveraging this observation, {\it we then construct a self-reflection vector, a direction in activation space associated with self-reflective reasoning}. By manipulating this vector, we enable bidirectional control over the self-reflective behavior for both pretrained and fine-tuned models. Experiments across multiple reasoning benchmarks show that enhancing these vectors improves reasoning performance by up to 12\%, while suppressing them reduces computational cost, providing a flexible mechanism to navigate the trade-off between reasoning quality and efficiency without requiring additional training. Our findings further our understanding of self-reflection and support a growing body of work showing that understanding model internals can enable precise behavioral control.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two heads are better than one: simulating large transformers with small ones</title>
<link>https://arxiv.org/abs/2506.12220</link>
<guid>https://arxiv.org/abs/2506.12220</guid>
<content:encoded><![CDATA[
arXiv:2506.12220v1 Announce Type: cross 
Abstract: The quadratic complexity of self-attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences?
  In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M \ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes</title>
<link>https://arxiv.org/abs/2506.12222</link>
<guid>https://arxiv.org/abs/2506.12222</guid>
<content:encoded><![CDATA[
arXiv:2506.12222v1 Announce Type: cross 
Abstract: Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the SSL pre-training has sufficiently equipped them to handle real-world audio. However, a critical question remains: how well do these models actually perform in real-world conditions, where audio is typically polyphonic and complex, involving multiple overlapping sound sources? Current audio SSL methods are often benchmarked on datasets predominantly featuring monophonic audio, such as environmental sounds, and speech. As a result, the ability of SSL models to generalize to polyphonic audio, a common characteristic in natural scenarios, remains underexplored. This limitation raises concerns about the practical robustness of SSL models in more realistic audio settings. To address this gap, we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel direction in audio SSL research, designed to improve, designed to improve the model's ability to learn from polyphonic data while maintaining strong performance on monophonic data. We thoroughly evaluate SSLAM on standard audio SSL benchmark datasets which are predominantly monophonic and conduct a comprehensive comparative analysis against SOTA methods using a range of high-quality, publicly available polyphonic datasets. SSLAM not only improves model performance on polyphonic audio, but also maintains or exceeds performance on standard audio SSL benchmarks. Notably, it achieves up to a 3.9\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision (mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear evaluation and fine-tuning regimes with performance improvements of up to 9.1\% (mAP).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Neural Theories of Consciousness onto the Common Model of Cognition</title>
<link>https://arxiv.org/abs/2506.12224</link>
<guid>https://arxiv.org/abs/2506.12224</guid>
<content:encoded><![CDATA[
arXiv:2506.12224v1 Announce Type: cross 
Abstract: A beginning is made at mapping four neural theories of consciousness onto the Common Model of Cognition. This highlights how the four jointly depend on recurrent local modules plus a cognitive cycle operating on a global working memory with complex states, and reveals how an existing integrative view of consciousness from a neural perspective aligns with the Com-mon Model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Bias Paths with LLM-guided Causal Discovery: An Active Learning and Dynamic Scoring Approach</title>
<link>https://arxiv.org/abs/2506.12227</link>
<guid>https://arxiv.org/abs/2506.12227</guid>
<content:encoded><![CDATA[
arXiv:2506.12227v1 Announce Type: cross 
Abstract: Causal discovery (CD) plays a pivotal role in understanding the mechanisms underlying complex systems. While recent algorithms can detect spurious associations and latent confounding, many struggle to recover fairness-relevant pathways in realistic, noisy settings. Large Language Models (LLMs), with their access to broad semantic knowledge, offer a promising complement to statistical CD approaches, particularly in domains where metadata provides meaningful relational cues. Ensuring fairness in machine learning requires understanding how sensitive attributes causally influence outcomes, yet CD methods often introduce spurious or biased pathways. We propose a hybrid LLM-based framework for CD that extends a breadth-first search (BFS) strategy with active learning and dynamic scoring. Variable pairs are prioritized for LLM-based querying using a composite score based on mutual information, partial correlation, and LLM confidence, improving discovery efficiency and robustness.
  To evaluate fairness sensitivity, we construct a semi-synthetic benchmark from the UCI Adult dataset, embedding a domain-informed causal graph with injected noise, label corruption, and latent confounding. We assess how well CD methods recover both global structure and fairness-critical paths.
  Our results show that LLM-guided methods, including the proposed method, demonstrate competitive or superior performance in recovering such pathways under noisy conditions. We highlight when dynamic scoring and active querying are most beneficial and discuss implications for bias auditing in real-world datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation</title>
<link>https://arxiv.org/abs/2506.12234</link>
<guid>https://arxiv.org/abs/2506.12234</guid>
<content:encoded><![CDATA[
arXiv:2506.12234v1 Announce Type: cross 
Abstract: Text-to-SQL systems enable users to query databases using natural language, democratizing access to data analytics. However, they face challenges in understanding ambiguous phrasing, domain-specific vocabulary, and complex schema relationships. This paper introduces Datrics Text2SQL, a Retrieval-Augmented Generation (RAG)-based framework designed to generate accurate SQL queries by leveraging structured documentation, example-based learning, and domain-specific rules. The system builds a rich Knowledge Base from database documentation and question-query examples, which are stored as vector embeddings and retrieved through semantic similarity. It then uses this context to generate syntactically correct and semantically aligned SQL code. The paper details the architecture, training methodology, and retrieval logic, highlighting how the system bridges the gap between user intent and database structure without requiring SQL expertise.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI</title>
<link>https://arxiv.org/abs/2506.12240</link>
<guid>https://arxiv.org/abs/2506.12240</guid>
<content:encoded><![CDATA[
arXiv:2506.12240v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is rapidly embedded in critical decision-making systems, however their foundational ``black-box'' models require eXplainable AI (XAI) solutions to enhance transparency, which are mostly oriented to experts, making no sense to non-experts. Alarming evidence about AI's unprecedented human values risks brings forward the imperative need for transparent human-centered XAI solutions. In this work, we introduce a domain-, model-, explanation-agnostic, generalizable and reproducible framework that ensures both transparency and human-centered explanations tailored to the needs of both experts and non-experts. The framework leverages Large Language Models (LLMs) and employs in-context learning to convey domain- and explainability-relevant contextual knowledge into LLMs. Through its structured prompt and system setting, our framework encapsulates in one response explanations understandable by non-experts and technical information to experts, all grounded in domain and explainability principles. To demonstrate the effectiveness of our framework, we establish a ground-truth contextual ``thesaurus'' through a rigorous benchmarking with over 40 data, model, and XAI combinations for an explainable clustering analysis of a well-being scenario. Through a comprehensive quality and human-friendliness evaluation of our framework's explanations, we prove high content quality through strong correlations with ground-truth explanations (Spearman rank correlation=0.92) and improved interpretability and human-friendliness to non-experts through a user study (N=56). Our overall evaluation confirms trust in LLMs as HCXAI enablers, as our framework bridges the above Gaps by delivering (i) high-quality technical explanations aligned with foundational XAI methods and (ii) clear, efficient, and interpretable human-centered explanations for non-experts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives</title>
<link>https://arxiv.org/abs/2506.12242</link>
<guid>https://arxiv.org/abs/2506.12242</guid>
<content:encoded><![CDATA[
arXiv:2506.12242v1 Announce Type: cross 
Abstract: This paper explores the use of large language models (LLMs) as research tools in the history, philosophy, and sociology of science (HPSS). LLMs are remarkably effective at processing unstructured text and inferring meaning from context, offering new affordances that challenge long-standing divides between computational and interpretive methods. This raises both opportunities and challenges for HPSS, which emphasizes interpretive methodologies and understands meaning as context-dependent, ambiguous, and historically situated. We argue that HPSS is uniquely positioned not only to benefit from LLMs' capabilities but also to interrogate their epistemic assumptions and infrastructural implications. To this end, we first offer a concise primer on LLM architectures and training paradigms tailored to non-technical readers. We frame LLMs not as neutral tools but as epistemic infrastructures that encode assumptions about meaning, context, and similarity, conditioned by their training data, architecture, and patterns of use. We then examine how computational techniques enhanced by LLMs, such as structuring data, detecting patterns, and modeling dynamic processes, can be applied to support interpretive research in HPSS. Our analysis compares full-context and generative models, outlines strategies for domain and task adaptation (e.g., continued pretraining, fine-tuning, and retrieval-augmented generation), and evaluates their respective strengths and limitations for interpretive inquiry in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1) model selection involves interpretive trade-offs; (2) LLM literacy is foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs should enhance, not replace, interpretive methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration</title>
<link>https://arxiv.org/abs/2506.12248</link>
<guid>https://arxiv.org/abs/2506.12248</guid>
<content:encoded><![CDATA[
arXiv:2506.12248v1 Announce Type: cross 
Abstract: Collaborative robots must quickly adapt to their partner's intent and preferences to proactively identify helpful actions. This is especially true in situated settings where human partners can continually teach robots new high-level behaviors, visual concepts, and physical skills (e.g., through demonstration), growing the robot's capabilities as the human-robot pair work together to accomplish diverse tasks. In this work, we argue that robots should be able to infer their partner's goals from early interactions and use this information to proactively plan behaviors ahead of explicit instructions from the user. Building from the strong commonsense priors and steerability of large language models, we introduce ProVox ("Proactive Voice"), a novel framework that enables robots to efficiently personalize and adapt to individual collaborators. We design a meta-prompting protocol that empowers users to communicate their distinct preferences, intent, and expected robot behaviors ahead of starting a physical interaction. ProVox then uses the personalized prompt to condition a proactive language model task planner that anticipates a user's intent from the current interaction context and robot capabilities to suggest helpful actions; in doing so, we alleviate user burden, minimizing the amount of time partners spend explicitly instructing and supervising the robot. We evaluate ProVox through user studies grounded in household manipulation tasks (e.g., assembling lunch bags) that measure the efficiency of the collaboration, as well as features such as perceived helpfulness, ease of use, and reliability. Our analysis suggests that both meta-prompting and proactivity are critical, resulting in 38.7% faster task completion times and 31.9% less user burden relative to non-active baselines. Supplementary material, code, and videos can be found at https://provox-2025.github.io.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis</title>
<link>https://arxiv.org/abs/2506.12263</link>
<guid>https://arxiv.org/abs/2506.12263</guid>
<content:encoded><![CDATA[
arXiv:2506.12263v1 Announce Type: cross 
Abstract: Foundation models have gained growing interest in the IoT domain due to their reduced reliance on labeled data and strong generalizability across tasks, which address key limitations of traditional machine learning approaches. However, most existing foundation model based methods are developed for specific IoT tasks, making it difficult to compare approaches across IoT domains and limiting guidance for applying them to new tasks. This survey aims to bridge this gap by providing a comprehensive overview of current methodologies and organizing them around four shared performance objectives by different domains: efficiency, context-awareness, safety, and security & privacy. For each objective, we review representative works, summarize commonly-used techniques and evaluation metrics. This objective-centric organization enables meaningful cross-domain comparisons and offers practical insights for selecting and designing foundation model based solutions for new IoT tasks. We conclude with key directions for future research to guide both practitioners and researchers in advancing the use of foundation models in IoT applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs</title>
<link>https://arxiv.org/abs/2506.12266</link>
<guid>https://arxiv.org/abs/2506.12266</guid>
<content:encoded><![CDATA[
arXiv:2506.12266v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following</title>
<link>https://arxiv.org/abs/2506.12285</link>
<guid>https://arxiv.org/abs/2506.12285</guid>
<content:encoded><![CDATA[
arXiv:2506.12285v1 Announce Type: cross 
Abstract: Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety</title>
<link>https://arxiv.org/abs/2506.12299</link>
<guid>https://arxiv.org/abs/2506.12299</guid>
<content:encoded><![CDATA[
arXiv:2506.12299v1 Announce Type: cross 
Abstract: The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Confirmation Bias in Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2506.12301</link>
<guid>https://arxiv.org/abs/2506.12301</guid>
<content:encoded><![CDATA[
arXiv:2506.12301v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \to R$) and reasoning-guided answer prediction ($QR \to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \textit{https://github.com/yuewan2/biasedcot}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.12307</link>
<guid>https://arxiv.org/abs/2506.12307</guid>
<content:encoded><![CDATA[
arXiv:2506.12307v1 Announce Type: cross 
Abstract: Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. The code will be released.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries</title>
<link>https://arxiv.org/abs/2506.12320</link>
<guid>https://arxiv.org/abs/2506.12320</guid>
<content:encoded><![CDATA[
arXiv:2506.12320v1 Announce Type: cross 
Abstract: Large Language Model (LLM) libraries have emerged as the foundational infrastructure powering today's AI revolution, serving as the backbone for LLM deployment, inference optimization, fine-tuning, and production serving across diverse applications. Despite their critical role in the LLM ecosystem, these libraries face frequent quality issues and bugs that threaten the reliability of AI systems built upon them. To address this knowledge gap, we present the first comprehensive empirical investigation into bug characteristics and testing practices in modern LLM libraries. We examine 313 bug-fixing commits extracted across two widely-adopted LLM libraries: HuggingFace Transformers and vLLM.Through rigorous manual analysis, we establish comprehensive taxonomies categorizing bug symptoms into 5 types and root causes into 14 distinct categories.Our primary discovery shows that API misuse has emerged as the predominant root cause (32.17%-48.19%), representing a notable transition from algorithm-focused defects in conventional deep learning frameworks toward interface-oriented problems. Additionally, we examine 7,748 test functions to identify 7 distinct test oracle categories employed in current testing approaches, with predefined expected outputs (such as specific tensors and text strings) being the most common strategy. Our assessment of existing testing effectiveness demonstrates that the majority of bugs escape detection due to inadequate test cases (41.73%), lack of test drivers (32.37%), and weak test oracles (25.90%). Drawing from these findings, we offer some recommendations for enhancing LLM library quality assurance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Memorization Dynamics in Pythia Models from Instance-Level Insights</title>
<link>https://arxiv.org/abs/2506.12321</link>
<guid>https://arxiv.org/abs/2506.12321</guid>
<content:encoded><![CDATA[
arXiv:2506.12321v1 Announce Type: cross 
Abstract: Large language models have demonstrated a remarkable ability for verbatim memorization. While numerous works have explored factors influencing model memorization, the dynamic evolution memorization patterns remains underexplored. This paper presents a detailed analysis of memorization in the Pythia model family across varying scales and training steps under prefix perturbations. Using granular metrics, we examine how model architecture, data characteristics, and perturbations influence these patterns. Our findings reveal that: (1) as model scale increases, memorization expands incrementally while efficiency decreases rapidly; (2) as model scale increases, the rate of new memorization acquisition decreases while old memorization forgetting increases; (3) data characteristics (token frequency, repetition count, and uncertainty) differentially affect memorized versus non-memorized samples; and (4) prefix perturbations reduce memorization and increase generation uncertainty proportionally to perturbation strength, with low-redundancy samples showing higher vulnerability and larger models offering no additional robustness. These findings advance our understanding of memorization mechanisms, with direct implications for training optimization, privacy safeguards, and architectural improvements.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Methods for Small Data and Upstream Bioprocessing Applications: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.12322</link>
<guid>https://arxiv.org/abs/2506.12322</guid>
<content:encoded><![CDATA[
arXiv:2506.12322v1 Announce Type: cross 
Abstract: Data is crucial for machine learning (ML) applications, yet acquiring large datasets can be costly and time-consuming, especially in complex, resource-intensive fields like biopharmaceuticals. A key process in this industry is upstream bioprocessing, where living cells are cultivated and optimised to produce therapeutic proteins and biologics. The intricate nature of these processes, combined with high resource demands, often limits data collection, resulting in smaller datasets. This comprehensive review explores ML methods designed to address the challenges posed by small data and classifies them into a taxonomy to guide practical applications. Furthermore, each method in the taxonomy was thoroughly analysed, with a detailed discussion of its core concepts and an evaluation of its effectiveness in tackling small data challenges, as demonstrated by application results in the upstream bioprocessing and other related domains. By analysing how these methods tackle small data challenges from different perspectives, this review provides actionable insights, identifies current research gaps, and offers guidance for leveraging ML in data-constrained environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Three-dimensional Deep Shape Optimization with a Limited Dataset</title>
<link>https://arxiv.org/abs/2506.12326</link>
<guid>https://arxiv.org/abs/2506.12326</guid>
<content:encoded><![CDATA[
arXiv:2506.12326v1 Announce Type: cross 
Abstract: Generative models have attracted considerable attention for their ability to produce novel shapes. However, their application in mechanical design remains constrained due to the limited size and variability of available datasets. This study proposes a deep learning-based optimization framework specifically tailored for shape optimization with limited datasets, leveraging positional encoding and a Lipschitz regularization term to robustly learn geometric characteristics and maintain a meaningful latent space. Through extensive experiments, the proposed approach demonstrates robustness, generalizability and effectiveness in addressing typical limitations of conventional optimization frameworks. The validity of the methodology is confirmed through multi-objective shape optimization experiments conducted on diverse three-dimensional datasets, including wheels and cars, highlighting the model's versatility in producing practical and high-quality design outcomes even under data-constrained conditions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective</title>
<link>https://arxiv.org/abs/2506.12327</link>
<guid>https://arxiv.org/abs/2506.12327</guid>
<content:encoded><![CDATA[
arXiv:2506.12327v1 Announce Type: cross 
Abstract: An growing number of studies have examined the social bias of rapidly developed large language models (LLMs). Although most of these studies have focused on bias occurring in a single social attribute, research in social science has shown that social bias often occurs in the form of intersectionality -- the constitutive and contextualized perspective on bias aroused by social attributes. In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment</title>
<link>https://arxiv.org/abs/2506.12331</link>
<guid>https://arxiv.org/abs/2506.12331</guid>
<content:encoded><![CDATA[
arXiv:2506.12331v1 Announce Type: cross 
Abstract: Virtual environments are essential to AI agent research. Existing environments for LLM agent research typically focus on either physical task solving or social simulation, with the former oversimplifying agent individuality and social dynamics, and the latter lacking physical grounding of social behaviors. We introduce IndoorWorld, a heterogeneous multi-agent environment that tightly integrates physical and social dynamics. By introducing novel challenges for LLM-driven agents in orchestrating social dynamics to influence physical environments and anchoring social interactions within world states, IndoorWorld opens up possibilities of LLM-based building occupant simulation for architectural design. We demonstrate the potential with a series of experiments within an office setting to examine the impact of multi-agent collaboration, resource competition, and spatial layout on agent behavior.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroupNL: Low-Resource and Robust CNN Design over Cloud and Device</title>
<link>https://arxiv.org/abs/2506.12335</link>
<guid>https://arxiv.org/abs/2506.12335</guid>
<content:encoded><![CDATA[
arXiv:2506.12335v1 Announce Type: cross 
Abstract: It has become mainstream to deploy Convolutional Neural Network (CNN) models on ubiquitous Internet of Things (IoT) devices with the help of the cloud to provide users with a variety of high-quality services. Most existing methods have two limitations: (i) low robustness in handling corrupted image data collected by IoT devices; and (ii) high consumption of computational and transmission resources. To this end, we propose the Grouped NonLinear transformation generation method (GroupNL), which generates diversified feature maps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to improve the robustness of the CNN model. Specifically, partial convolution filters are designated as seed filters in a convolutional layer, and a small set of feature maps, i.e., seed feature maps, are first generated based on vanilla convolution operation. Then, we split seed feature maps into several groups, each with a set of different NLFs, to generate corresponding diverse feature maps with in-place nonlinear processing. Moreover, GroupNL effectively reduces the parameter transmission between multiple nodes during model training by setting the hyperparameters of NLFs to random initialization and not updating them during model training, and reduces the computing resources by using NLFs to generate feature maps instead of most feature maps generated based on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C, Icons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the proposed GroupNL outperforms other state-of-the-art methods in model robust and training acceleration. Specifically, on the Icons-50 dataset, the accuracy of GroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla ResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN when trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation</title>
<link>https://arxiv.org/abs/2506.12339</link>
<guid>https://arxiv.org/abs/2506.12339</guid>
<content:encoded><![CDATA[
arXiv:2506.12339v1 Announce Type: cross 
Abstract: We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refract ICL: Rethinking Example Selection in the Era of Million-Token Models</title>
<link>https://arxiv.org/abs/2506.12346</link>
<guid>https://arxiv.org/abs/2506.12346</guid>
<content:encoded><![CDATA[
arXiv:2506.12346v1 Announce Type: cross 
Abstract: The emergence of long-context large language models (LLMs) has enabled the use of hundreds, or even thousands, of demonstrations for in-context learning (ICL) - a previously impractical regime. This paper investigates whether traditional ICL selection strategies, which balance the similarity of ICL examples to the test input (using a text retriever) with diversity within the ICL set, remain effective when utilizing a large number of demonstrations. Our experiments demonstrate that, while longer contexts can accommodate more examples, simply increasing the number of demonstrations does not guarantee improved performance. Smart ICL selection remains crucial, even with thousands of demonstrations. To further enhance ICL in this setting, we introduce Refract ICL, a novel ICL selection algorithm specifically designed to focus LLM attention on challenging examples by strategically repeating them within the context and incorporating zero-shot predictions as error signals. Our results show that Refract ICL significantly improves the performance of extremely long-context models such as Gemini 1.5 Pro, particularly on tasks with a smaller number of output classes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek</title>
<link>https://arxiv.org/abs/2506.12349</link>
<guid>https://arxiv.org/abs/2506.12349</guid>
<content:encoded><![CDATA[
arXiv:2506.12349v1 Announce Type: cross 
Abstract: This study examines information suppression mechanisms in DeepSeek, an open-source large language model (LLM) developed in China. We propose an auditing framework and use it to analyze the model's responses to 646 politically sensitive prompts by comparing its final output with intermediate chain-of-thought (CoT) reasoning. Our audit unveils evidence of semantic-level information suppression in DeepSeek: sensitive content often appears within the model's internal reasoning but is omitted or rephrased in the final output. Specifically, DeepSeek suppresses references to transparency, government accountability, and civic mobilization, while occasionally amplifying language aligned with state propaganda. This study underscores the need for systematic auditing of alignment, content moderation, information suppression, and censorship practices implemented into widely-adopted AI models, to ensure transparency, accountability, and equitable access to unbiased information obtained by means of these systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory</title>
<link>https://arxiv.org/abs/2506.12350</link>
<guid>https://arxiv.org/abs/2506.12350</guid>
<content:encoded><![CDATA[
arXiv:2506.12350v1 Announce Type: cross 
Abstract: Despite its empirical success, Reinforcement Learning from Human Feedback (RLHF) has been shown to violate almost all the fundamental axioms in social choice theory -- such as majority consistency, pairwise majority consistency, and Condorcet consistency. This raises a foundational question: why does RLHF perform so well in practice if it fails these seemingly essential properties? In this paper, we resolve this paradox by showing that under mild and empirically plausible assumptions on the preference profile, RLHF does satisfy pairwise majority and Condorcet consistency. These assumptions are frequently satisfied in real-world alignment tasks, offering a theoretical explanation for RLHF's strong practical performance. Furthermore, we show that a slight modification to the reward modeling objective can ensure pairwise majority or Condorcet consistency even under general preference profiles, thereby improving the alignment process. Finally, we go beyond classical axioms in economic and social choice theory and introduce new alignment criteria -- preference matching, preference equivalence, and group preference matching -- that better reflect the goal of learning distributions over responses. We show that while RLHF satisfies the first two properties, it fails to satisfy the third. We conclude by discussing how future alignment methods may be designed to satisfy all three.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.12353</link>
<guid>https://arxiv.org/abs/2506.12353</guid>
<content:encoded><![CDATA[
arXiv:2506.12353v1 Announce Type: cross 
Abstract: While recent advances in large reasoning models have demonstrated remarkable performance, efficient reasoning remains critical due to the rapid growth of output length. Existing optimization approaches highlights a tendency toward "overthinking", yet lack fine-grained analysis. In this work, we focus on Self-Affirmation Reflections: redundant reflective steps that affirm prior content and often occurs after the already correct reasoning steps. Observations of both original and optimized reasoning models reveal pervasive self-affirmation reflections. Notably, these reflections sometimes lead to longer outputs in optimized models than their original counterparts. Through detailed analysis, we uncover an intriguing pattern: compared to other reflections, the leading words (i.e., the first word of sentences) in self-affirmation reflections exhibit a distinct probability bias. Motivated by this insight, we can locate self-affirmation reflections and conduct a train-free experiment demonstrating that suppressing self-affirmation reflections reduces output length without degrading accuracy across multiple models (R1-Distill-Models, QwQ-32B, and Qwen3-32B). Furthermore, we also improve current train-based method by explicitly suppressing such reflections. In our experiments, we achieve length compression of 18.7\% in train-free settings and 50.2\% in train-based settings for R1-Distill-Qwen-1.5B. Moreover, our improvements are simple yet practical and can be directly applied to existing inference frameworks, such as vLLM. We believe that our findings will provide community insights for achieving more precise length compression and step-level efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HYPER: A Foundation Model for Inductive Link Prediction with Knowledge Hypergraphs</title>
<link>https://arxiv.org/abs/2506.12362</link>
<guid>https://arxiv.org/abs/2506.12362</guid>
<content:encoded><![CDATA[
arXiv:2506.12362v1 Announce Type: cross 
Abstract: Inductive link prediction with knowledge hypergraphs is the task of predicting missing hyperedges involving completely novel entities (i.e., nodes unseen during training). Existing methods for inductive link prediction with knowledge hypergraphs assume a fixed relational vocabulary and, as a result, cannot generalize to knowledge hypergraphs with novel relation types (i.e., relations unseen during training). Inspired by knowledge graph foundation models, we propose HYPER as a foundation model for link prediction, which can generalize to any knowledge hypergraph, including novel entities and novel relations. Importantly, HYPER can learn and transfer across different relation types of varying arities, by encoding the entities of each hyperedge along with their respective positions in the hyperedge. To evaluate HYPER, we construct 16 new inductive datasets from existing knowledge hypergraphs, covering a diverse range of relation types of varying arities. Empirically, HYPER consistently outperforms all existing methods in both node-only and node-and-relation inductive settings, showing strong generalization to unseen, higher-arity relational structures.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making</title>
<link>https://arxiv.org/abs/2506.12374</link>
<guid>https://arxiv.org/abs/2506.12374</guid>
<content:encoded><![CDATA[
arXiv:2506.12374v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for robotic manipulation within high-dimensional representation spaces. However, current approaches often project them into compressed intermediate representations, discarding important task-specific information such as fine-grained spatial or semantic details. To address this, we propose AntiGrounding, a new framework that reverses the instruction grounding process. It lifts candidate actions directly into the VLM representation space, renders trajectories from multiple views, and uses structured visual question answering for instruction-based decision making. This enables zero-shot synthesis of optimal closed-loop robot trajectories for new tasks. We also propose an offline policy refinement module that leverages past experience to enhance long-term performance. Experiments in both simulation and real-world environments show that our method outperforms baselines across diverse robotic manipulation tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Spectral Fault Receptive Fields for Diagnosis-Informed Prognosis</title>
<link>https://arxiv.org/abs/2506.12375</link>
<guid>https://arxiv.org/abs/2506.12375</guid>
<content:encoded><![CDATA[
arXiv:2506.12375v1 Announce Type: cross 
Abstract: This paper introduces Spectral Fault Receptive Fields (SFRFs), a biologically inspired technique for degradation state assessment in bearing fault diagnosis and remaining useful life (RUL) estimation. Drawing on the center-surround organization of retinal ganglion cell receptive fields, we propose a frequency-domain feature extraction algorithm that enhances the detection of fault signatures in vibration signals. SFRFs are designed as antagonistic spectral filters centered on characteristic fault frequencies, with inhibitory surrounds that enable robust characterization of incipient faults under variable operating conditions. A multi-objective evolutionary optimization strategy based on NSGA-II algorithm is employed to tune the receptive field parameters by simultaneously minimizing RUL prediction error, maximizing feature monotonicity, and promoting smooth degradation trajectories. The method is demonstrated on the XJTU-SY bearing run-to-failure dataset, confirming its suitability for constructing condition indicators in health monitoring applications. Key contributions include: (i) the introduction of SFRFs, inspired by the biology of vision in the primate retina; (ii) an evolutionary optimization framework guided by condition monitoring and prognosis criteria; and (iii) experimental evidence supporting the detection of early-stage faults and their precursors. Furthermore, we confirm that our diagnosis-informed spectral representation achieves accurate RUL prediction using a bagging regressor. The results highlight the interpretability and principled design of SFRFs, bridging signal processing, biological sensing principles, and data-driven prognostics in rotating machinery.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Component Based Quantum Machine Learning Explainability</title>
<link>https://arxiv.org/abs/2506.12378</link>
<guid>https://arxiv.org/abs/2506.12378</guid>
<content:encoded><![CDATA[
arXiv:2506.12378v1 Announce Type: cross 
Abstract: Explainable ML algorithms are designed to provide transparency and insight into their decision-making process. Explaining how ML models come to their prediction is critical in fields such as healthcare and finance, as it provides insight into how models can help detect bias in predictions and help comply with GDPR compliance in these fields. QML leverages quantum phenomena such as entanglement and superposition, offering the potential for computational speedup and greater insights compared to classical ML. However, QML models also inherit the black-box nature of their classical counterparts, requiring the development of explainability techniques to be applied to these QML models to help understand why and how a particular output was generated.
  This paper will explore the idea of creating a modular, explainable QML framework that splits QML algorithms into their core components, such as feature maps, variational circuits (ansatz), optimizers, kernels, and quantum-classical loops. Each component will be analyzed using explainability techniques, such as ALE and SHAP, which have been adapted to analyse the different components of these QML algorithms. By combining insights from these parts, the paper aims to infer explainability to the overall QML model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free LLM Merging for Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.12379</link>
<guid>https://arxiv.org/abs/2506.12379</guid>
<content:encoded><![CDATA[
arXiv:2506.12379v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Secondary Risks of Large Language Models</title>
<link>https://arxiv.org/abs/2506.12382</link>
<guid>https://arxiv.org/abs/2506.12382</guid>
<content:encoded><![CDATA[
arXiv:2506.12382v1 Announce Type: cross 
Abstract: Ensuring the safety and alignment of Large Language Models is a significant challenge with their growing integration into critical applications and societal functions. While prior research has primarily focused on jailbreak attacks, less attention has been given to non-adversarial failures that subtly emerge during benign interactions. We introduce secondary risks a novel class of failure modes marked by harmful or misleading behaviors during benign prompts. Unlike adversarial attacks, these risks stem from imperfect generalization and often evade standard safety mechanisms. To enable systematic evaluation, we introduce two risk primitives verbose response and speculative advice that capture the core failure patterns. Building on these definitions, we propose SecLens, a black-box, multi-objective search framework that efficiently elicits secondary risk behaviors by optimizing task relevance, risk activation, and linguistic plausibility. To support reproducible evaluation, we release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse real-world risk categories. Experimental results from extensive evaluations on 16 popular models demonstrate that secondary risks are widespread, transferable across models, and modality independent, emphasizing the urgent need for enhanced safety mechanisms to address benign yet harmful LLM behaviors in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances and Future Directions in Literature-Based Discovery</title>
<link>https://arxiv.org/abs/2506.12385</link>
<guid>https://arxiv.org/abs/2506.12385</guid>
<content:encoded><![CDATA[
arXiv:2506.12385v1 Announce Type: cross 
Abstract: The explosive growth of scientific publications has created an urgent need for automated methods that facilitate knowledge synthesis and hypothesis generation. Literature-based discovery (LBD) addresses this challenge by uncovering previously unknown associations between disparate domains. This article surveys recent methodological advances in LBD, focusing on developments from 2000 to the present. We review progress in three key areas: knowledge graph construction, deep learning approaches, and the integration of pre-trained and large language models (LLMs). While LBD has made notable progress, several fundamental challenges remain unresolved, particularly concerning scalability, reliance on structured data, and the need for extensive manual curation. By examining ongoing advances and outlining promising future directions, this survey underscores the transformative role of LLMs in enhancing LBD and aims to support researchers and practitioners in harnessing these technologies to accelerate scientific innovation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model</title>
<link>https://arxiv.org/abs/2506.12388</link>
<guid>https://arxiv.org/abs/2506.12388</guid>
<content:encoded><![CDATA[
arXiv:2506.12388v1 Announce Type: cross 
Abstract: The curse of multilinguality phenomenon is a fundamental problem of multilingual Large Language Models (LLMs), where the competition between massive languages results in inferior performance. It mainly comes from limited capacity and negative transfer between dissimilar languages. To address this issue, we propose a method to dynamically group and scale up the parameters of multilingual LLM while boosting positive transfer among similar languages. Specifically, the model is first tuned on monolingual corpus to determine the parameter deviation in each layer and quantify the similarity between languages. Layers with more deviations are extended to mixture-of-experts layers to reduce competition between languages, where one expert module serves one group of similar languages. Experimental results on 18 to 128 languages show that our method reduces the negative transfer between languages and significantly boosts multilingual performance with fewer parameters. Such language group specialization on experts benefits the new language adaptation and reduces the inference on the previous multilingual knowledge learned.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Clustering of Neural Bandits: Selective Reinitialization for Mitigating Loss of Plasticity</title>
<link>https://arxiv.org/abs/2506.12389</link>
<guid>https://arxiv.org/abs/2506.12389</guid>
<content:encoded><![CDATA[
arXiv:2506.12389v1 Announce Type: cross 
Abstract: Clustering of Bandits (CB) methods enhance sequential decision-making by grouping bandits into clusters based on similarity and incorporating cluster-level contextual information, demonstrating effectiveness and adaptability in applications like personalized streaming recommendations. However, when extending CB algorithms to their neural version (commonly referred to as Clustering of Neural Bandits, or CNB), they suffer from loss of plasticity, where neural network parameters become rigid and less adaptable over time, limiting their ability to adapt to non-stationary environments (e.g., dynamic user preferences in recommendation). To address this challenge, we propose Selective Reinitialization (SeRe), a novel bandit learning framework that dynamically preserves the adaptability of CNB algorithms in evolving environments. SeRe leverages a contribution utility metric to identify and selectively reset underutilized units, mitigating loss of plasticity while maintaining stable knowledge retention. Furthermore, when combining SeRe with CNB algorithms, the adaptive change detection mechanism adjusts the reinitialization frequency according to the degree of non-stationarity, ensuring effective adaptation without unnecessary resets. Theoretically, we prove that SeRe enables sublinear cumulative regret in piecewise-stationary environments, outperforming traditional CNB approaches in long-term performances. Extensive experiments on six real-world recommendation datasets demonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss of plasticity with lower regrets, improving adaptability and robustness in dynamic settings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.12394</link>
<guid>https://arxiv.org/abs/2506.12394</guid>
<content:encoded><![CDATA[
arXiv:2506.12394v1 Announce Type: cross 
Abstract: The advent of parameter-efficient fine-tuning methods has significantly reduced the computational burden of adapting large-scale pretrained models to diverse downstream tasks. However, existing approaches often struggle to achieve robust performance under domain shifts while maintaining computational efficiency. To address this challenge, we propose Low-rAnk Regulated Gradient Projection (LARGO) algorithm that integrates dynamic constraints into low-rank adaptation methods. Specifically, LARGO incorporates parallel trainable gradient projections to dynamically regulate layer-wise updates, retaining the Out-Of-Distribution robustness of pretrained model while preserving inter-layer independence. Additionally, it ensures computational efficiency by mitigating the influence of gradient dependencies across layers during weight updates. Besides, through leveraging singular value decomposition of pretrained weights for structured initialization, we incorporate an SVD-based initialization strategy that minimizing deviation from pretrained knowledge. Through extensive experiments on diverse benchmarks, LARGO achieves state-of-the-art performance across in-domain and out-of-distribution scenarios, demonstrating improved robustness under domain shifts with significantly lower computational overhead compared to existing PEFT methods. The source code will be released soon.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Digital Divide: Small Language Models as a Pathway for Physics and Photonics Education in Underdeveloped Regions</title>
<link>https://arxiv.org/abs/2506.12403</link>
<guid>https://arxiv.org/abs/2506.12403</guid>
<content:encoded><![CDATA[
arXiv:2506.12403v1 Announce Type: cross 
Abstract: Limited infrastructure, scarce educational resources, and unreliable internet access often hinder physics and photonics education in underdeveloped regions. These barriers create deep inequities in Science, Technology, Engineering, and Mathematics (STEM) education. This article explores how Small Language Models (SLMs)-compact, AI-powered tools that can run offline on low-power devices, offering a scalable solution. By acting as virtual tutors, enabling native-language instruction, and supporting interactive learning, SLMs can help address the shortage of trained educators and laboratory access. By narrowing the digital divide through targeted investment in AI technologies, SLMs present a scalable and inclusive solution to advance STEM education and foster scientific empowerment in marginalized communities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXGnet: a single-lead explainable-AI guided multiresolution network with train-only quantitative features for trustworthy ECG arrhythmia classification</title>
<link>https://arxiv.org/abs/2506.12404</link>
<guid>https://arxiv.org/abs/2506.12404</guid>
<content:encoded><![CDATA[
arXiv:2506.12404v1 Announce Type: cross 
Abstract: Background: Deep learning has significantly advanced ECG arrhythmia classification, enabling high accuracy in detecting various cardiac conditions. The use of single-lead ECG systems is crucial for portable devices, as they offer convenience and accessibility for continuous monitoring in diverse settings. However, the interpretability and reliability of deep learning models in clinical applications poses challenges due to their black-box nature. Methods: To address these challenges, we propose EXGnet, a single-lead, trustworthy ECG arrhythmia classification network that integrates multiresolution feature extraction with Explainable Artificial Intelligence (XAI) guidance and train only quantitative features. Results: Trained on two public datasets, including Chapman and Ningbo, EXGnet demonstrates superior performance through key metrics such as Accuracy, F1-score, Sensitivity, and Specificity. The proposed method achieved average five fold accuracy of 98.762%, and 96.932% and average F1-score of 97.910%, and 95.527% on the Chapman and Ningbo datasets, respectively. Conclusions: By employing XAI techniques, specifically Grad-CAM, the model provides visual insights into the relevant ECG segments it analyzes, thereby enhancing clinician trust in its predictions. While quantitative features further improve classification performance, they are not required during testing, making the model suitable for real-world applications. Overall, EXGnet not only achieves better classification accuracy but also addresses the critical need for interpretability in deep learning, facilitating broader adoption in portable ECG monitoring.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feeling Machines: Ethics, Culture, and the Rise of Emotional AI</title>
<link>https://arxiv.org/abs/2506.12437</link>
<guid>https://arxiv.org/abs/2506.12437</guid>
<content:encoded><![CDATA[
arXiv:2506.12437v1 Announce Type: cross 
Abstract: This paper explores the growing presence of emotionally responsive artificial intelligence through a critical and interdisciplinary lens. Bringing together the voices of early-career researchers from multiple fields, it explores how AI systems that simulate or interpret human emotions are reshaping our interactions in areas such as education, healthcare, mental health, caregiving, and digital life. The analysis is structured around four central themes: the ethical implications of emotional AI, the cultural dynamics of human-machine interaction, the risks and opportunities for vulnerable populations, and the emerging regulatory, design, and technical considerations. The authors highlight the potential of affective AI to support mental well-being, enhance learning, and reduce loneliness, as well as the risks of emotional manipulation, over-reliance, misrepresentation, and cultural bias. Key challenges include simulating empathy without genuine understanding, encoding dominant sociocultural norms into AI systems, and insufficient safeguards for individuals in sensitive or high-risk contexts. Special attention is given to children, elderly users, and individuals with mental health challenges, who may interact with AI in emotionally significant ways. However, there remains a lack of cognitive or legal protections which are necessary to navigate such engagements safely. The report concludes with ten recommendations, including the need for transparency, certification frameworks, region-specific fine-tuning, human oversight, and longitudinal research. A curated supplementary section provides practical tools, models, and datasets to support further work in this domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style-based Composer Identification and Attribution of Symbolic Music Scores: a Systematic Survey</title>
<link>https://arxiv.org/abs/2506.12440</link>
<guid>https://arxiv.org/abs/2506.12440</guid>
<content:encoded><![CDATA[
arXiv:2506.12440v1 Announce Type: cross 
Abstract: This paper presents the first comprehensive systematic review of literature on style-based composer identification and authorship attribution in symbolic music scores. Addressing the critical need for improved reliability and reproducibility in this field, the review rigorously analyzes 58 peer-reviewed papers published across various historical periods, with the search adapted to evolving terminology. The analysis critically assesses prevailing repertoires, computational approaches, and evaluation methodologies, highlighting significant challenges. It reveals that a substantial portion of existing research suffers from inadequate validation protocols and an over-reliance on simple accuracy metrics for often imbalanced datasets, which can undermine the credibility of attribution claims. The crucial role of robust metrics like Balanced Accuracy and rigorous cross-validation in ensuring trustworthy results is emphasized. The survey also details diverse feature representations and the evolution of machine learning models employed. Notable real-world authorship attribution cases, such as those involving works attributed to Bach, Josquin Desprez, and Lennon-McCartney, are specifically discussed, illustrating the opportunities and pitfalls of applying computational techniques to resolve disputed musical provenance. Based on these insights, a set of actionable guidelines for future research are proposed. These recommendations are designed to significantly enhance the reliability, reproducibility, and musicological validity of composer identification and authorship attribution studies, fostering more robust and interpretable computational stylistic analysis.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MS-UMamba: An Improved Vision Mamba Unet for Fetal Abdominal Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.12441</link>
<guid>https://arxiv.org/abs/2506.12441</guid>
<content:encoded><![CDATA[
arXiv:2506.12441v1 Announce Type: cross 
Abstract: Recently, Mamba-based methods have become popular in medical image segmentation due to their lightweight design and long-range dependency modeling capabilities. However, current segmentation methods frequently encounter challenges in fetal ultrasound images, such as enclosed anatomical structures, blurred boundaries, and small anatomical structures. To address the need for balancing local feature extraction and global context modeling, we propose MS-UMamba, a novel hybrid convolutional-mamba model for fetal ultrasound image segmentation. Specifically, we design a visual state space block integrated with a CNN branch (SS-MCAT-SSM), which leverages Mamba's global modeling strengths and convolutional layers' local representation advantages to enhance feature learning. In addition, we also propose an efficient multi-scale feature fusion module that integrates spatial attention mechanisms, which Integrating feature information from different layers enhances the feature representation ability of the model. Finally, we conduct extensive experiments on a non-public dataset, experimental results demonstrate that MS-UMamba model has excellent performance in segmentation performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pluggable Multi-Task Learning Framework for Sentiment-Aware Financial Relation Extraction</title>
<link>https://arxiv.org/abs/2506.12452</link>
<guid>https://arxiv.org/abs/2506.12452</guid>
<content:encoded><![CDATA[
arXiv:2506.12452v1 Announce Type: cross 
Abstract: Relation Extraction (RE) aims to extract semantic relationships in texts from given entity pairs, and has achieved significant improvements. However, in different domains, the RE task can be influenced by various factors. For example, in the financial domain, sentiment can affect RE results, yet this factor has been overlooked by modern RE models. To address this gap, this paper proposes a Sentiment-aware-SDP-Enhanced-Module (SSDP-SEM), a multi-task learning approach for enhancing financial RE. Specifically, SSDP-SEM integrates the RE models with a pluggable auxiliary sentiment perception (ASP) task, enabling the RE models to concurrently navigate their attention weights with the text's sentiment. We first generate detailed sentiment tokens through a sentiment model and insert these tokens into an instance. Then, the ASP task focuses on capturing nuanced sentiment information through predicting the sentiment token positions, combining both sentiment insights and the Shortest Dependency Path (SDP) of syntactic information. Moreover, this work employs a sentiment attention information bottleneck regularization method to regulate the reasoning process. Our experiment integrates this auxiliary task with several prevalent frameworks, and the results demonstrate that most previous models benefit from the auxiliary task, thereby achieving better results. These findings highlight the importance of effectively leveraging sentiment in the financial RE task.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates</title>
<link>https://arxiv.org/abs/2506.12459</link>
<guid>https://arxiv.org/abs/2506.12459</guid>
<content:encoded><![CDATA[
arXiv:2506.12459v1 Announce Type: cross 
Abstract: Multivariate Time Series Forecasting (MTSF) involves predicting future values of multiple interrelated time series. Recently, deep learning-based MTSF models have gained significant attention for their promising ability to mine semantics (global and local information) within MTS data. However, these models are pervasively susceptible to missing values caused by malfunctioning data collectors. These missing values not only disrupt the semantics of MTS, but their distribution also changes over time. Nevertheless, existing models lack robustness to such issues, leading to suboptimal forecasting performance. To this end, in this paper, we propose Multi-View Representation Learning (Merlin), which can help existing models achieve semantic alignment between incomplete observations with different missing rates and complete observations in MTS. Specifically, Merlin consists of two key modules: offline knowledge distillation and multi-view contrastive learning. The former utilizes a teacher model to guide a student model in mining semantics from incomplete observations, similar to those obtainable from complete observations. The latter improves the student model's robustness by learning from positive/negative data pairs constructed from incomplete observations with different missing rates, ensuring semantic alignment across different missing rates. Therefore, Merlin is capable of effectively enhancing the robustness of existing models against unfixed missing rates while preserving forecasting accuracy. Experiments on four real-world datasets demonstrate the superiority of Merlin.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into Instance-Dependent Label Noise in Graph Data: A Comprehensive Study and Benchmark</title>
<link>https://arxiv.org/abs/2506.12468</link>
<guid>https://arxiv.org/abs/2506.12468</guid>
<content:encoded><![CDATA[
arXiv:2506.12468v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in node classification tasks but struggle with label noise in real-world data. Existing studies on graph learning with label noise commonly rely on class-dependent label noise, overlooking the complexities of instance-dependent noise and falling short of capturing real-world corruption patterns. We introduce BeGIN (Benchmarking for Graphs with Instance-dependent Noise), a new benchmark that provides realistic graph datasets with various noise types and comprehensively evaluates noise-handling strategies across GNN architectures, noisy label detection, and noise-robust learning. To simulate instance-dependent corruptions, BeGIN introduces algorithmic methods and LLM-based simulations. Our experiments reveal the challenges of instance-dependent noise, particularly LLM-based corruption, and underscore the importance of node-specific parameterization to enhance GNN robustness. By comprehensively evaluating noise-handling strategies, BeGIN provides insights into their effectiveness, efficiency, and key performance factors. We expect that BeGIN will serve as a valuable resource for advancing research on label noise in graphs and fostering the development of robust GNN training methods. The code is available at https://github.com/kimsu55/BeGIN.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Levels of Autonomy for AI Agents</title>
<link>https://arxiv.org/abs/2506.12469</link>
<guid>https://arxiv.org/abs/2506.12469</guid>
<content:encoded><![CDATA[
arXiv:2506.12469v1 Announce Type: cross 
Abstract: Autonomy is a double-edged sword for AI agents, simultaneously unlocking transformative possibilities and serious risks. How can agent developers calibrate the appropriate levels of autonomy at which their agents should operate? We argue that an agent's level of autonomy can be treated as a deliberate design decision, separate from its capability and operational environment. In this work, we define five levels of escalating agent autonomy, characterized by the roles a user can take when interacting with an agent: operator, collaborator, consultant, approver, and observer. Within each level, we describe the ways by which a user can exert control over the agent and open questions for how to design the nature of user-agent interaction. We then highlight a potential application of our framework towards AI autonomy certificates to govern agent behavior in single- and multi-agent systems. We conclude by proposing early ideas for evaluating agents' autonomy. Our work aims to contribute meaningful, practical steps towards responsibly deployed and useful AI agents in the real world.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture</title>
<link>https://arxiv.org/abs/2506.12474</link>
<guid>https://arxiv.org/abs/2506.12474</guid>
<content:encoded><![CDATA[
arXiv:2506.12474v1 Announce Type: cross 
Abstract: Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward functions, enabling robust cross-scenario adaptability. The learned reward function is utilized to maximize the likelihood of output by the encoder-decoder architecture that combines Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents. Comprehensive evaluations on urban intersections and roundabouts demonstrate that the proposed method not only outperforms various popular approaches in prediction accuracy but also achieves 2 times higher generalization performance to unseen scenarios compared to other IRL-based method.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[
arXiv:2506.12484v1 Announce Type: cross 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40\%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Deep Learning Strategies for Hypertensive Retinopathy Detection from Fundus Images: From Scratch and Pre-trained Models</title>
<link>https://arxiv.org/abs/2506.12492</link>
<guid>https://arxiv.org/abs/2506.12492</guid>
<content:encoded><![CDATA[
arXiv:2506.12492v1 Announce Type: cross 
Abstract: This paper presents a comparative analysis of deep learning strategies for detecting hypertensive retinopathy from fundus images, a central task in the HRDC challenge~\cite{qian2025hrdc}. We investigate three distinct approaches: a custom CNN, a suite of pre-trained transformer-based models, and an AutoML solution. Our findings reveal a stark, architecture-dependent response to data augmentation. Augmentation significantly boosts the performance of pure Vision Transformers (ViTs), which we hypothesize is due to their weaker inductive biases, forcing them to learn robust spatial and structural features. Conversely, the same augmentation strategy degrades the performance of hybrid ViT-CNN models, whose stronger, pre-existing biases from the CNN component may be "confused" by the transformations. We show that smaller patch sizes (ViT-B/8) excel on augmented data, enhancing fine-grained detail capture. Furthermore, we demonstrate that a powerful self-supervised model like DINOv2 fails on the original, limited dataset but is "rescued" by augmentation, highlighting the critical need for data diversity to unlock its potential. Preliminary tests with a ViT-Large model show poor performance, underscoring the risk of using overly-capacitive models on specialized, smaller datasets. This work provides critical insights into the interplay between model architecture, data augmentation, and dataset size for medical image classification.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fairness Assessment of Dutch Hate Speech Detection</title>
<link>https://arxiv.org/abs/2506.12502</link>
<guid>https://arxiv.org/abs/2506.12502</guid>
<content:encoded><![CDATA[
arXiv:2506.12502v1 Announce Type: cross 
Abstract: Numerous studies have proposed computational methods to detect hate speech online, yet most focus on the English language and emphasize model development. In this study, we evaluate the counterfactual fairness of hate speech detection models in the Dutch language, specifically examining the performance and fairness of transformer-based models. We make the following key contributions. First, we curate a list of Dutch Social Group Terms that reflect social context. Second, we generate counterfactual data for Dutch hate speech using LLMs and established strategies like Manual Group Substitution (MGS) and Sentence Log-Likelihood (SLL). Through qualitative evaluation, we highlight the challenges of generating realistic counterfactuals, particularly with Dutch grammar and contextual coherence. Third, we fine-tune baseline transformer-based models with counterfactual data and evaluate their performance in detecting hate speech. Fourth, we assess the fairness of these models using Counterfactual Token Fairness (CTF) and group fairness metrics, including equality of odds and demographic parity. Our analysis shows that models perform better in terms of hate speech detection, average counterfactual fairness and group fairness. This work addresses a significant gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides practical insights and recommendations for improving both model performance and fairness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.12529</link>
<guid>https://arxiv.org/abs/2506.12529</guid>
<content:encoded><![CDATA[
arXiv:2506.12529v1 Announce Type: cross 
Abstract: Preference-based Reinforcement Learning (PbRL) entails a variety of approaches for aligning models with human intent to alleviate the burden of reward engineering. However, most previous PbRL work has not investigated the robustness to labeler errors, inevitable with labelers who are non-experts or operate under time constraints. Additionally, PbRL algorithms often target very specific settings (e.g. pairwise ranked preferences or purely offline learning). We introduce Similarity as Reward Alignment (SARA), a simple contrastive framework that is both resilient to noisy labels and adaptable to diverse feedback formats and training paradigms. SARA learns a latent representation of preferred samples and computes rewards as similarities to the learned latent. We demonstrate strong performance compared to baselines on continuous control offline RL benchmarks. We further demonstrate SARA's versatility in applications such as trajectory filtering for downstream tasks, cross-task preference transfer, and reward shaping in online learning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry</title>
<link>https://arxiv.org/abs/2506.12536</link>
<guid>https://arxiv.org/abs/2506.12536</guid>
<content:encoded><![CDATA[
arXiv:2506.12536v1 Announce Type: cross 
Abstract: Accurate rotational odometry is crucial for autonomous robotic systems, particularly for small, power-constrained platforms such as drones and mobile robots. This study introduces thermal-gyro fusion, a novel sensor fusion approach that integrates ultra-low-resolution thermal imaging with gyroscope readings for rotational odometry. Unlike RGB cameras, thermal imaging is invariant to lighting conditions and, when fused with gyroscopic data, mitigates drift which is a common limitation of inertial sensors. We first develop a multimodal data acquisition system to collect synchronized thermal and gyroscope data, along with rotational speed labels, across diverse environments. Subsequently, we design and train a lightweight Convolutional Neural Network (CNN) that fuses both modalities for rotational speed estimation. Our analysis demonstrates that thermal-gyro fusion enables a significant reduction in thermal camera resolution without significantly compromising accuracy, thereby improving computational efficiency and memory utilization. These advantages make our approach well-suited for real-time deployment in resource-constrained robotic systems. Finally, to facilitate further research, we publicly release our dataset as supplementary material.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2506.12537</link>
<guid>https://arxiv.org/abs/2506.12537</guid>
<content:encoded><![CDATA[
arXiv:2506.12537v1 Announce Type: cross 
Abstract: Speech-language models (SLMs) offer a promising path toward unifying speech and text understanding and generation. However, challenges remain in achieving effective cross-modal alignment and high-quality speech generation. In this work, we systematically investigate the impact of key components (i.e., speech tokenizers, speech heads, and speaker modeling) on the performance of LLM-centric SLMs. We compare coupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM framework and find that decoupled tokenization significantly improves alignment and synthesis quality. To address the information density mismatch between speech and text, we introduce multi-token prediction (MTP) into SLMs, enabling each hidden state to decode multiple speech tokens. This leads to up to 12$\times$ faster decoding and a substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with diverse speaker identities. Experiments demonstrate that our methods enhance both knowledge understanding and speaker consistency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking</title>
<link>https://arxiv.org/abs/2506.12538</link>
<guid>https://arxiv.org/abs/2506.12538</guid>
<content:encoded><![CDATA[
arXiv:2506.12538v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) hold significant potential for advancing fact-checking by leveraging their capabilities in reasoning, evidence retrieval, and explanation generation. However, existing benchmarks fail to comprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in realistic misinformation scenarios. To bridge this gap, we introduce RealFactBench, a comprehensive benchmark designed to assess the fact-checking capabilities of LLMs and MLLMs across diverse real-world tasks, including Knowledge Validation, Rumor Detection, and Event Verification. RealFactBench consists of 6K high-quality claims drawn from authoritative sources, encompassing multimodal content and diverse domains. Our evaluation framework further introduces the Unknown Rate (UnR) metric, enabling a more nuanced assessment of models' ability to handle uncertainty and balance between over-conservatism and over-confidence. Extensive experiments on 7 representative LLMs and 4 MLLMs reveal their limitations in real-world fact-checking and offer valuable insights for further research. RealFactBench is publicly available at https://github.com/kalendsyang/RealFactBench.git.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BSA: Ball Sparse Attention for Large-scale Geometries</title>
<link>https://arxiv.org/abs/2506.12541</link>
<guid>https://arxiv.org/abs/2506.12541</guid>
<content:encoded><![CDATA[
arXiv:2506.12541v1 Announce Type: cross 
Abstract: Self-attention scales quadratically with input size, limiting its use for large-scale physical systems. Although sparse attention mechanisms provide a viable alternative, they are primarily designed for regular structures such as text or images, making them inapplicable for irregular geometries. In this work, we present Ball Sparse Attention (BSA), which adapts Native Sparse Attention (NSA) (Yuan et al., 2025) to unordered point sets by imposing regularity using the Ball Tree structure from the Erwin Transformer (Zhdanov et al., 2025). We modify NSA's components to work with ball-based neighborhoods, yielding a global receptive field at sub-quadratic cost. On an airflow pressure prediction task, we achieve accuracy comparable to Full Attention while significantly reducing the theoretical computational complexity. Our implementation is available at https://github.com/britacatalin/bsa.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLD: A Choice-Theoretic List-Wise Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.12542</link>
<guid>https://arxiv.org/abs/2506.12542</guid>
<content:encoded><![CDATA[
arXiv:2506.12542v1 Announce Type: cross 
Abstract: Knowledge distillation is a model compression technique in which a compact "student" network is trained to replicate the predictive behavior of a larger "teacher" network. In logit-based knowledge distillation it has become the de facto approach to augment cross-entropy with a distillation term. Typically this term is either a KL divergence-matching marginal probabilities or a correlation-based loss capturing intra- and inter-class relationships but in every case it sits as an add-on to cross-entropy with its own weight that must be carefully tuned. In this paper we adopt a choice-theoretic perspective and recast knowledge distillation under the Plackett-Luce model by interpreting teacher logits as "worth" scores. We introduce Plackett-Luce Distillation (PLD), a weighted list-wise ranking loss in which the teacher model transfers knowledge of its full ranking of classes, weighting each ranked choice by its own confidence. PLD directly optimizes a single teacher-optimal ranking of the true label first, followed by the remaining classes in descending teacher confidence, yielding a convex, translation-invariant surrogate that subsumes weighted cross-entropy. Empirically on standard image classification benchmarks, PLD improves Top-1 accuracy by an average of +0.42% over DIST (arXiv:2205.10536) and +1.04% over KD (arXiv:1503.02531) in homogeneous settings and by +0.48% and +1.09% over DIST and KD, respectively, in heterogeneous settings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEraser: An Effective Fingerprint Erasure Approach for Large Language Models</title>
<link>https://arxiv.org/abs/2506.12551</link>
<guid>https://arxiv.org/abs/2506.12551</guid>
<content:encoded><![CDATA[
arXiv:2506.12551v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become increasingly prevalent across various sectors, raising critical concerns about model ownership and intellectual property protection. Although backdoor-based fingerprinting has emerged as a promising solution for model authentication, effective attacks for removing these fingerprints remain largely unexplored. Therefore, we present Mismatched Eraser (MEraser), a novel method for effectively removing backdoor-based fingerprints from LLMs while maintaining model performance. Our approach leverages a two-phase fine-tuning strategy utilizing carefully constructed mismatched and clean datasets. Through extensive evaluation across multiple LLM architectures and fingerprinting methods, we demonstrate that MEraser achieves complete fingerprinting removal while maintaining model performance with minimal training data of fewer than 1,000 samples. Furthermore, we introduce a transferable erasure mechanism that enables effective fingerprinting removal across different models without repeated training. In conclusion, our approach provides a practical solution for fingerprinting removal in LLMs, reveals critical vulnerabilities in current fingerprinting techniques, and establishes comprehensive evaluation benchmarks for developing more resilient model protection methods in the future.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts</title>
<link>https://arxiv.org/abs/2506.12552</link>
<guid>https://arxiv.org/abs/2506.12552</guid>
<content:encoded><![CDATA[
arXiv:2506.12552v1 Announce Type: cross 
Abstract: In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuromorphic Online Clustering and Its Application to Spike Sorting</title>
<link>https://arxiv.org/abs/2506.12555</link>
<guid>https://arxiv.org/abs/2506.12555</guid>
<content:encoded><![CDATA[
arXiv:2506.12555v1 Announce Type: cross 
Abstract: Active dendrites are the basis for biologically plausible neural networks possessing many desirable features of the biological brain including flexibility, dynamic adaptability, and energy efficiency. A formulation for active dendrites using the notational language of conventional machine learning is put forward as an alternative to a spiking neuron formulation. Based on this formulation, neuromorphic dendrites are developed as basic neural building blocks capable of dynamic online clustering. Features and capabilities of neuromorphic dendrites are demonstrated via a benchmark drawn from experimental neuroscience: spike sorting. Spike sorting takes inputs from electrical probes implanted in neural tissue, detects voltage spikes (action potentials) emitted by neurons, and attempts to sort the spikes according to the neuron that emitted them. Many spike sorting methods form clusters based on the shapes of action potential waveforms, under the assumption that spikes emitted by a given neuron have similar shapes and will therefore map to the same cluster. Using a stream of synthetic spike shapes, the accuracy of the proposed dendrite is compared with the more compute-intensive, offline k-means clustering approach. Overall, the dendrite outperforms k-means and has the advantage of requiring only a single pass through the input stream, learning as it goes. The capabilities of the neuromorphic dendrite are demonstrated for a number of scenarios including dynamic changes in the input stream, differing neuron spike rates, and varying neuron counts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Research For Machine Learning Should Integrate Societal Considerations</title>
<link>https://arxiv.org/abs/2506.12556</link>
<guid>https://arxiv.org/abs/2506.12556</guid>
<content:encoded><![CDATA[
arXiv:2506.12556v1 Announce Type: cross 
Abstract: Enhancing fairness in machine learning (ML) systems is increasingly important nowadays. While current research focuses on assistant tools for ML pipelines to promote fairness within them, we argue that: 1) The significance of properly defined fairness measures remains underestimated; and 2) Fairness research in ML should integrate societal considerations. The reasons include that detecting discrimination is critical due to the widespread deployment of ML systems and that human-AI feedback loops amplify biases, even when only small social and political biases persist.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVP-CBM:Multi-layer Visual Preference-enhanced Concept Bottleneck Model for Explainable Medical Image Classification</title>
<link>https://arxiv.org/abs/2506.12568</link>
<guid>https://arxiv.org/abs/2506.12568</guid>
<content:encoded><![CDATA[
arXiv:2506.12568v1 Announce Type: cross 
Abstract: The concept bottleneck model (CBM), as a technique improving interpretability via linking predictions to human-understandable concepts, makes high-risk and life-critical medical image classification credible. Typically, existing CBM methods associate the final layer of visual encoders with concepts to explain the model's predictions. However, we empirically discover the phenomenon of concept preference variation, that is, the concepts are preferably associated with the features at different layers than those only at the final layer; yet a blind last-layer-based association neglects such a preference variation and thus weakens the accurate correspondences between features and concepts, impairing model interpretability. To address this issue, we propose a novel Multi-layer Visual Preference-enhanced Concept Bottleneck Model (MVP-CBM), which comprises two key novel modules: (1) intra-layer concept preference modeling, which captures the preferred association of different concepts with features at various visual layers, and (2) multi-layer concept sparse activation fusion, which sparsely aggregates concept activations from multiple layers to enhance performance. Thus, by explicitly modeling concept preferences, MVP-CBM can comprehensively leverage multi-layer visual information to provide a more nuanced and accurate explanation of model decisions. Extensive experiments on several public medical classification benchmarks demonstrate that MVP-CBM achieves state-of-the-art accuracy and interoperability, verifying its superiority. Code is available at https://github.com/wcj6/MVP-CBM.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoTA-RAG: Dynamic of Thought Aggregation RAG</title>
<link>https://arxiv.org/abs/2506.12571</link>
<guid>https://arxiv.org/abs/2506.12571</guid>
<content:encoded><![CDATA[
arXiv:2506.12571v1 Announce Type: cross 
Abstract: In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&amp;A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2506.12576</link>
<guid>https://arxiv.org/abs/2506.12576</guid>
<content:encoded><![CDATA[
arXiv:2506.12576v1 Announce Type: cross 
Abstract: Recent work shows that Sparse Autoencoders (SAE) applied to large language model (LLM) layers have neurons corresponding to interpretable concepts. These SAE neurons can be modified to align generated outputs, but only towards pre-identified topics and with some parameter tuning. Our approach leverages the observational and modification properties of SAEs to enable alignment for any topic. This method 1) scores each SAE neuron by its semantic similarity to an alignment text and uses them to 2) modify SAE-layer-level outputs by emphasizing topic-aligned neurons. We assess the alignment capabilities of this approach on diverse public topic datasets including Amazon reviews, Medicine, and Sycophancy, across the currently available open-source LLMs and SAE pairs (GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to medical prompts reveal several benefits over fine-tuning, including increased average language acceptability (0.25 vs. 0.5), reduced training time across multiple alignment topics (333.6s vs. 62s), and acceptable inference time for many applications (+0.00092s/token). Our open-source code is available at github.com/IBM/sae-steering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow</title>
<link>https://arxiv.org/abs/2506.12600</link>
<guid>https://arxiv.org/abs/2506.12600</guid>
<content:encoded><![CDATA[
arXiv:2506.12600v1 Announce Type: cross 
Abstract: Intelligent transportation systems require connected and automated vehicles (CAVs) to conduct safe and efficient cooperation with human-driven vehicles (HVs) in complex real-world traffic environments. However, the inherent unpredictability of human behaviour, especially at bottlenecks such as highway on-ramp merging areas, often disrupts traffic flow and compromises system performance. To address the challenge of cooperative on-ramp merging in heterogeneous traffic environments, this study proposes a trust-based multi-agent reinforcement learning (Trust-MARL) framework. At the macro level, Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust to improve bottleneck throughput and mitigate traffic shockwave through emergent group-level coordination. At the micro level, a dynamic trust mechanism is designed to enable CAVs to adjust their cooperative strategies in response to real-time behaviors and historical interactions with both HVs and other CAVs. Furthermore, a trust-triggered game-theoretic decision-making module is integrated to guide each CAV in adapting its cooperation factor and executing context-aware lane-changing decisions under safety, comfort, and efficiency constraints. An extensive set of ablation studies and comparative experiments validates the effectiveness of the proposed Trust-MARL approach, demonstrating significant improvements in safety, efficiency, comfort, and adaptability across varying CAV penetration rates and traffic densities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Exploration of Mamba for Speech Self-Supervised Models</title>
<link>https://arxiv.org/abs/2506.12606</link>
<guid>https://arxiv.org/abs/2506.12606</guid>
<content:encoded><![CDATA[
arXiv:2506.12606v1 Announce Type: cross 
Abstract: While Mamba has demonstrated strong performance in language modeling, its potential as a speech self-supervised (SSL) model remains underexplored, with prior studies limited to isolated tasks. To address this, we explore Mamba-based HuBERT models as alternatives to Transformer-based SSL architectures. Leveraging the linear-time Selective State Space, these models enable fine-tuning on long-context ASR with significantly lower compute. Moreover, they show superior performance when fine-tuned for streaming ASR. Beyond fine-tuning, these models show competitive performance on SUPERB probing benchmarks, particularly in causal settings. Our analysis shows that they yield higher-quality quantized representations and capture speaker-related features more distinctly than Transformer-based models. These findings highlight Mamba-based SSL as a promising and complementary direction for long-sequence modeling, real-time speech modeling, and speech unit extraction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2506.12615</link>
<guid>https://arxiv.org/abs/2506.12615</guid>
<content:encoded><![CDATA[
arXiv:2506.12615v1 Announce Type: cross 
Abstract: We introduce Konooz, a novel multi-dimensional corpus covering 16 Arabic dialects across 10 domains, resulting in 160 distinct corpora. The corpus comprises about 777k tokens, carefully collected and manually annotated with 21 entity types using both nested and flat annotation schemes - using the Wojood guidelines. While Konooz is useful for various NLP tasks like domain adaptation and transfer learning, this paper primarily focuses on benchmarking existing Arabic Named Entity Recognition (NER) models, especially cross-domain and cross-dialect model performance. Our benchmarking of four Arabic NER models using Konooz reveals a significant drop in performance of up to 38% when compared to the in-distribution data. Furthermore, we present an in-depth analysis of domain and dialect divergence and the impact of resource scarcity. We also measured the overlap between domains and dialects using the Maximum Mean Discrepancy (MMD) metric, and illustrated why certain NER models perform better on specific dialects and domains. Konooz is open-source and publicly available at https://sina.birzeit.edu/wojood/#download
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty</title>
<link>https://arxiv.org/abs/2506.12622</link>
<guid>https://arxiv.org/abs/2506.12622</guid>
<content:encoded><![CDATA[
arXiv:2506.12622v1 Announce Type: cross 
Abstract: Deep reinforcement learning (RL) has achieved significant success, yet its application in real-world scenarios is often hindered by a lack of robustness to environmental uncertainties. To solve this challenge, some robust RL algorithms have been proposed, but most are limited to tabular settings. In this work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a novel algorithm designed to enhance the robustness of the state-of-the-art Soft Actor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with entropy against the worst possible transition model lying in an uncertainty set. A distributionally robust version of the soft policy iteration is derived with a convergence guarantee. For settings where nominal distributions are unknown, such as offline RL, a generative modeling approach is proposed to estimate the required nominal distributions from data. Furthermore, experimental results on a range of continuous control benchmark tasks demonstrate our algorithm achieves up to $9.8$ times the average reward of the SAC baseline under common perturbations. Additionally, compared with existing robust reinforcement learning algorithms, DR-SAC significantly improves computing efficiency and applicability to large-scale problems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANIRA: An Architecture for Neural Network Inference in Real-Time Audio Applications</title>
<link>https://arxiv.org/abs/2506.12665</link>
<guid>https://arxiv.org/abs/2506.12665</guid>
<content:encoded><![CDATA[
arXiv:2506.12665v1 Announce Type: cross 
Abstract: Numerous tools for neural network inference are currently available, yet many do not meet the requirements of real-time audio applications. In response, we introduce anira, an efficient cross-platform library. To ensure compatibility with a broad range of neural network architectures and frameworks, anira supports ONNX Runtime, LibTorch, and TensorFlow Lite as backends. Each inference engine exhibits real-time violations, which anira mitigates by decoupling the inference from the audio callback to a static thread pool. The library incorporates built-in latency management and extensive benchmarking capabilities, both crucial to ensure a continuous signal flow. Three different neural network architectures for audio effect emulation are then subjected to benchmarking across various configurations. Statistical modeling is employed to identify the influence of various factors on performance. The findings indicate that for stateless models, ONNX Runtime exhibits the lowest runtimes. For stateful models, LibTorch demonstrates the fastest performance. Our results also indicate that for certain model-engine combinations, the initial inferences take longer, particularly when these inferences exhibit a higher incidence of real-time violations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity</title>
<link>https://arxiv.org/abs/2506.12685</link>
<guid>https://arxiv.org/abs/2506.12685</guid>
<content:encoded><![CDATA[
arXiv:2506.12685v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their susceptibility to adversarial attacks, particularly jailbreaking, poses significant safety and ethical concerns. While numerous jailbreak methods exist, many suffer from computational expense, high token usage, or complex decoding schemes. Liu et al. (2024) introduced FlipAttack, a black-box method that achieves high attack success rates (ASR) through simple prompt manipulation. This paper investigates the underlying mechanisms of FlipAttack's effectiveness by analyzing the semantic changes induced by its flipping modes. We hypothesize that semantic dissimilarity between original and manipulated prompts is inversely correlated with ASR. To test this, we examine embedding space visualizations (UMAP, KDE) and cosine similarities for FlipAttack's modes. Furthermore, we introduce a novel adversarial attack, Alphabet Index Mapping (AIM), designed to maximize semantic dissimilarity while maintaining simple decodability. Experiments on GPT-4 using a subset of AdvBench show AIM and its variant AIM+FWO achieve a 94% ASR, outperforming FlipAttack and other methods on this subset. Our findings suggest that while high semantic dissimilarity is crucial, a balance with decoding simplicity is key for successful jailbreaking. This work contributes to a deeper understanding of adversarial prompt mechanics and offers a new, effective jailbreak technique.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research</title>
<link>https://arxiv.org/abs/2506.12691</link>
<guid>https://arxiv.org/abs/2506.12691</guid>
<content:encoded><![CDATA[
arXiv:2506.12691v1 Announce Type: cross 
Abstract: The adoption of Large Language Models (LLMs) is not only transforming software engineering (SE) practice but is also poised to fundamentally disrupt how research is conducted in the field. While perspectives on this transformation range from viewing LLMs as mere productivity tools to considering them revolutionary forces, we argue that the SE research community must proactively engage with and shape the integration of LLMs into research practices, emphasizing human agency in this transformation. As LLMs rapidly become integral to SE research - both as tools that support investigations and as subjects of study - a human-centric perspective is essential. Ensuring human oversight and interpretability is necessary for upholding scientific rigor, fostering ethical responsibility, and driving advancements in the field. Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze the impact of LLMs on SE research. Through this theoretical lens, we examine how LLMs enhance research capabilities through accelerated ideation and automated processes, make some traditional research practices obsolete, retrieve valuable aspects of historical research approaches, and risk reversal effects when taken to extremes. Our analysis reveals opportunities for innovation and potential pitfalls that require careful consideration. We conclude with a call to action for the SE research community to proactively harness the benefits of LLMs while developing frameworks and guidelines to mitigate their risks, to ensure continued rigor and impact of research in an AI-augmented future.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection</title>
<link>https://arxiv.org/abs/2506.12697</link>
<guid>https://arxiv.org/abs/2506.12697</guid>
<content:encoded><![CDATA[
arXiv:2506.12697v1 Announce Type: cross 
Abstract: Small object detection in UAV imagery is crucial for applications such as search-and-rescue, traffic monitoring, and environmental surveillance, but it is hampered by tiny object size, low signal-to-noise ratios, and limited feature extraction. Existing multi-scale fusion methods help, but add computational burden and blur fine details, making small object detection in cluttered scenes difficult. To overcome these challenges, we propose the Multi-scale Global-detail Feature Integration Strategy (MGDFIS), a unified fusion framework that tightly couples global context with local detail to boost detection performance while maintaining efficiency. MGDFIS comprises three synergistic modules: the FusionLock-TSS Attention Module, which marries token-statistics self-attention with DynamicTanh normalization to highlight spectral and spatial cues at minimal cost; the Global-detail Integration Module, which fuses multi-scale context via directional convolution and parallel attention while preserving subtle shape and texture variations; and the Dynamic Pixel Attention Module, which generates pixel-wise weighting maps to rebalance uneven foreground and background distributions and sharpen responses to true object regions. Extensive experiments on the VisDrone benchmark demonstrate that MGDFIS consistently outperforms state-of-the-art methods across diverse backbone architectures and detection frameworks, achieving superior precision and recall with low inference time. By striking an optimal balance between accuracy and resource usage, MGDFIS provides a practical solution for small-object detection on resource-constrained UAV platforms.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Contrastive Learning Using Out-Of-Distribution Data for Long-Tailed Dataset</title>
<link>https://arxiv.org/abs/2506.12698</link>
<guid>https://arxiv.org/abs/2506.12698</guid>
<content:encoded><![CDATA[
arXiv:2506.12698v1 Announce Type: cross 
Abstract: This work addresses the task of self-supervised learning (SSL) on a long-tailed dataset that aims to learn balanced and well-separated representations for downstream tasks such as image classification. This task is crucial because the real world contains numerous object categories, and their distributions are inherently imbalanced. Towards robust SSL on a class-imbalanced dataset, we investigate leveraging a network trained using unlabeled out-of-distribution (OOD) data that are prevalently available online. We first train a network using both in-domain (ID) and sampled OOD data by back-propagating the proposed pseudo semantic discrimination loss alongside a domain discrimination loss. The OOD data sampling and loss functions are designed to learn a balanced and well-separated embedding space. Subsequently, we further optimize the network on ID data by unsupervised contrastive learning while using the previously trained network as a guiding network. The guiding network is utilized to select positive/negative samples and to control the strengths of attractive/repulsive forces in contrastive learning. We also distil and transfer its embedding space to the training network to maintain balancedness and separability. Through experiments on four publicly available long-tailed datasets, we demonstrate that the proposed method outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Realignment of Language Models</title>
<link>https://arxiv.org/abs/2506.12704</link>
<guid>https://arxiv.org/abs/2506.12704</guid>
<content:encoded><![CDATA[
arXiv:2506.12704v1 Announce Type: cross 
Abstract: Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates Training-time Realignment (TrRa), which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during inference, we introduce a layer adapter that enables smooth Inference-time Realignment (InRa). This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even during inference. By encouraging deeper reasoning, it even surpassed its original performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.12706</link>
<guid>https://arxiv.org/abs/2506.12706</guid>
<content:encoded><![CDATA[
arXiv:2506.12706v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations include: (1) extending AdvPT from text-only to multi-modal prompting across both text and visual modalities, (2) expanding from single-layer to multi-layer prompt architectures, and (3) proposing a novel architecture-level redesign through our Neural Augmentor approach, which implements feature purification to directly address the distortions introduced by adversarial attacks in feature space. Our NAP-Tuning approach incorporates token refiners that learn to reconstruct purified features through residual connections, allowing for modality-specific and layer-specific feature correction.Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on ViT-B32 architectures while maintaining competitive clean accuracy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serving Large Language Models on Huawei CloudMatrix384</title>
<link>https://arxiv.org/abs/2506.12708</link>
<guid>https://arxiv.org/abs/2506.12708</guid>
<content:encoded><![CDATA[
arXiv:2506.12708v1 Announce Type: cross 
Abstract: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration</title>
<link>https://arxiv.org/abs/2506.12723</link>
<guid>https://arxiv.org/abs/2506.12723</guid>
<content:encoded><![CDATA[
arXiv:2506.12723v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Experimental results demonstrate that our method achieves up to 1.5$\times$ acceleration with less than 3% drop in accuracy, outperforming existing approaches in multiple tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling</title>
<link>https://arxiv.org/abs/2506.12735</link>
<guid>https://arxiv.org/abs/2506.12735</guid>
<content:encoded><![CDATA[
arXiv:2506.12735v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is playing an increasingly important role in fields such as robotic control and autonomous driving. However, the gap between simulation and the real environment remains a major obstacle to the practical deployment of RL. Agents trained in simulators often struggle to maintain performance when transferred to real-world physical environments. In this paper, we propose a latent space based approach to analyze the impact of simulation on real-world policy improvement in model-based settings. As a natural extension of model-based methods, our approach enables an intuitive observation of the challenges faced by model-based methods in sim-to-real transfer. Experiments conducted in the MuJoCo environment evaluate the performance of our method in both measuring and mitigating the sim-to-real gap. The experiments also highlight the various challenges that remain in overcoming the sim-to-real gap, especially for model-based methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution</title>
<link>https://arxiv.org/abs/2506.12738</link>
<guid>https://arxiv.org/abs/2506.12738</guid>
<content:encoded><![CDATA[
arXiv:2506.12738v1 Announce Type: cross 
Abstract: Blind Super-Resolution (blind SR) aims to enhance the model's generalization ability with unknown degradation, yet it still encounters severe overfitting issues. Some previous methods inspired by dropout, which enhances generalization by regularizing features, have shown promising results in blind SR. Nevertheless, these methods focus solely on regularizing features before the final layer and overlook the need for generalization in features at intermediate layers. Without explicit regularization of features at intermediate layers, the blind SR network struggles to obtain well-generalized feature representations. However, the key challenge is that directly applying dropout to intermediate layers leads to a significant performance drop, which we attribute to the inconsistency in training-testing and across layers it introduced. Therefore, we propose Adaptive Dropout, a new regularization method for blind SR models, which mitigates the inconsistency and facilitates application across intermediate layers of networks. Specifically, for training-testing inconsistency, we re-design the form of dropout and integrate the features before and after dropout adaptively. For inconsistency in generalization requirements across different layers, we innovatively design an adaptive training strategy to strengthen feature propagation by layer-wise annealing. Experimental results show that our method outperforms all past regularization methods on both synthetic and real-world benchmark datasets, also highly effective in other image restoration tasks. Code is available at \href{https://github.com/xuhang07/Adpative-Dropout}{https://github.com/xuhang07/Adpative-Dropout}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Diffusion and State Space Models for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.12747</link>
<guid>https://arxiv.org/abs/2506.12747</guid>
<content:encoded><![CDATA[
arXiv:2506.12747v1 Announce Type: cross 
Abstract: Existing segmentation models trained on a single medical imaging dataset often lack robustness when encountering unseen organs or tumors. Developing a robust model capable of identifying rare or novel tumor categories not present during training is crucial for advancing medical imaging applications. We propose DSM, a novel framework that leverages diffusion and state space models to segment unseen tumor categories beyond the training data. DSM utilizes two sets of object queries trained within modified attention decoders to enhance classification accuracy. Initially, the model learns organ queries using an object-aware feature grouping strategy to capture organ-level visual features. It then refines tumor queries by focusing on diffusion-based visual prompts, enabling precise segmentation of previously unseen tumors. Furthermore, we incorporate diffusion-guided feature fusion to improve semantic segmentation performance. By integrating CLIP text embeddings, DSM captures category-sensitive classes to improve linguistic transfer knowledge, thereby enhancing the model's robustness across diverse scenarios and multi-label tasks. Extensive experiments demonstrate the superior performance of DSM in various tumor segmentation tasks. Code is available at https://github.com/Rows21/KMax-Mamba.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFBS:Buffer Gradient Selection in Semi-asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2506.12754</link>
<guid>https://arxiv.org/abs/2506.12754</guid>
<content:encoded><![CDATA[
arXiv:2506.12754v1 Announce Type: cross 
Abstract: Asynchronous federated learning (AFL) accelerates training by eliminating the need to wait for stragglers, but its asynchronous nature introduces gradient staleness, where outdated gradients degrade performance. Existing solutions address this issue with gradient buffers, forming a semi-asynchronous framework. However, this approach struggles when buffers accumulate numerous stale gradients, as blindly aggregating all gradients can harm training. To address this, we propose AFBS (Asynchronous FL Buffer Selection), the first algorithm to perform gradient selection within buffers while ensuring privacy protection. Specifically, the client sends the random projection encrypted label distribution matrix before training, and the server performs client clustering based on it. During training, server scores and selects gradients within each cluster based on their informational value, discarding low-value gradients to enhance semi-asynchronous federated learning. Extensive experiments in highly heterogeneous system and data environments demonstrate AFBS's superior performance compared to state-of-the-art methods. Notably, on the most challenging task, CIFAR-100, AFBS improves accuracy by up to 4.8% over the previous best algorithm and reduces the time to reach target accuracy by 75%.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-board Sonar Data Classification for Path Following in Underwater Vehicles using Fast Interval Type-2 Fuzzy Extreme Learning Machine</title>
<link>https://arxiv.org/abs/2506.12762</link>
<guid>https://arxiv.org/abs/2506.12762</guid>
<content:encoded><![CDATA[
arXiv:2506.12762v1 Announce Type: cross 
Abstract: In autonomous underwater missions, the successful completion of predefined paths mainly depends on the ability of underwater vehicles to recognise their surroundings. In this study, we apply the concept of Fast Interval Type-2 Fuzzy Extreme Learning Machine (FIT2-FELM) to train a Takagi-Sugeno-Kang IT2 Fuzzy Inference System (TSK IT2-FIS) for on-board sonar data classification using an underwater vehicle called BlueROV2. The TSK IT2-FIS is integrated into a Hierarchical Navigation Strategy (HNS) as the main navigation engine to infer local motions and provide the BlueROV2 with full autonomy to follow an obstacle-free trajectory in a water container of 2.5m x 2.5m x 3.5m. Compared to traditional navigation architectures, using the proposed method, we observe a robust path following behaviour in the presence of uncertainty and noise. We found that the proposed approach provides the BlueROV with a more complete sensory picture about its surroundings while real-time navigation planning is performed by the concurrent execution of two or more tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving tricky quantum optics problems with assistance from (artificial) intelligence</title>
<link>https://arxiv.org/abs/2506.12770</link>
<guid>https://arxiv.org/abs/2506.12770</guid>
<content:encoded><![CDATA[
arXiv:2506.12770v1 Announce Type: cross 
Abstract: The capabilities of modern artificial intelligence (AI) as a ``scientific collaborator'' are explored by engaging it with three nuanced problems in quantum optics: state populations in optical pumping, resonant transitions between decaying states (the Burshtein effect), and degenerate mirrorless lasing. Through iterative dialogue, the authors observe that AI models--when prompted and corrected--can reason through complex scenarios, refine their answers, and provide expert-level guidance, closely resembling the interaction with an adept colleague. The findings highlight that AI democratizes access to sophisticated modeling and analysis, shifting the focus in scientific practice from technical mastery to the generation and testing of ideas, and reducing the time for completing research tasks from days to minutes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</title>
<link>https://arxiv.org/abs/2506.12775</link>
<guid>https://arxiv.org/abs/2506.12775</guid>
<content:encoded><![CDATA[
arXiv:2506.12775v1 Announce Type: cross 
Abstract: DL based Synthetic Aperture Radar (SAR) ship detection has tremendous advantages in numerous areas. However, it still faces some problems, such as the lack of prior knowledge, which seriously affects detection accuracy. In order to solve this problem, we propose a scene-aware SAR ship detection method based on unsupervised sea-land segmentation. This method follows a classical two-stage framework and is enhanced by two models: the unsupervised land and sea segmentation module (ULSM) and the land attention suppression module (LASM). ULSM and LASM can adaptively guide the network to reduce attention on land according to the type of scenes (inshore scene and offshore scene) and add prior knowledge (sea land segmentation information) to the network, thereby reducing the network's attention to land directly and enhancing offshore detection performance relatively. This increases the accuracy of ship detection and enhances the interpretability of the model. Specifically, in consideration of the lack of land sea segmentation labels in existing deep learning-based SAR ship detection datasets, ULSM uses an unsupervised approach to classify the input data scene into inshore and offshore types and performs sea-land segmentation for inshore scenes. LASM uses the sea-land segmentation information as prior knowledge to reduce the network's attention to land. We conducted our experiments using the publicly available SSDD dataset, which demonstrated the effectiveness of our network.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilient-native and Intelligent NextG Systems</title>
<link>https://arxiv.org/abs/2506.12795</link>
<guid>https://arxiv.org/abs/2506.12795</guid>
<content:encoded><![CDATA[
arXiv:2506.12795v1 Announce Type: cross 
Abstract: Just like power, water and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient to unforeseen events, able to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Despite its critical importance, resilience remains an elusive concept, with its mathematical foundations still underdeveloped. Unlike robustness and reliability, resilience is premised on the fact that disruptions will inevitably happen. Resilience, in terms of elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents (or networks) that can flexibly expand their states, hypotheses and course of actions, by transforming through real-time adaptation and reconfiguration. This constant situational awareness and vigilance of adapting world models and counterfactually reasoning about potential system failures and the corresponding best responses, is a core aspect of resilience. This article seeks to first define resilience and disambiguate it from reliability and robustness, before delving into the mathematics of resilience. Finally, the article concludes by presenting nuanced metrics and discussing trade-offs tailored to the unique characteristics of network resilience.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Based Policy for Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.12811</link>
<guid>https://arxiv.org/abs/2506.12811</guid>
<content:encoded><![CDATA[
arXiv:2506.12811v1 Announce Type: cross 
Abstract: We present \textbf{FlowRL}, a novel framework for online reinforcement learning that integrates flow-based policy representation with Wasserstein-2-regularized optimization. We argue that in addition to training signals, enhancing the expressiveness of the policy class is crucial for the performance gains in RL. Flow-based generative models offer such potential, excelling at capturing complex, multimodal action distributions. However, their direct application in online RL is challenging due to a fundamental objective mismatch: standard flow training optimizes for static data imitation, while RL requires value-based policy optimization through a dynamic buffer, leading to difficult optimization landscapes. FlowRL first models policies via a state-dependent velocity field, generating actions through deterministic ODE integration from noise. We derive a constrained policy search objective that jointly maximizes Q through the flow policy while bounding the Wasserstein-2 distance to a behavior-optimal policy implicitly derived from the replay buffer. This formulation effectively aligns the flow optimization with the RL objective, enabling efficient and value-aware policy learning despite the complexity of the policy class. Empirical evaluations on DMControl and Humanoidbench demonstrate that FlowRL achieves competitive performance in online reinforcement learning benchmarks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taking the GP Out of the Loop</title>
<link>https://arxiv.org/abs/2506.12818</link>
<guid>https://arxiv.org/abs/2506.12818</guid>
<content:encoded><![CDATA[
arXiv:2506.12818v1 Announce Type: cross 
Abstract: Bayesian optimization (BO) has traditionally solved black box problems where evaluation is expensive and, therefore, design-evaluation pairs (i.e., observations) are few. Recently, there has been growing interest in applying BO to problems where evaluation is cheaper and, thus, observations are more plentiful. An impediment to scaling BO to many observations, $N$, is the $O(N^3)$ scaling of a na{\"i}ve query of the Gaussian process (GP) surrogate. Modern implementations reduce this to $O(N^2)$, but the GP remains a bottleneck. We propose Epistemic Nearest Neighbors (ENN), a surrogate that estimates function values and epistemic uncertainty from $K$ nearest-neighbor observations. ENN has $O(N)$ query time and omits hyperparameter fitting, leaving uncertainty uncalibrated. To accommodate the lack of calibration, we employ an acquisition method based on Pareto-optimal tradeoffs between predicted value and uncertainty. Our proposed method, TuRBO-ENN, replaces the GP surrogate in TuRBO with ENN and its Thompson sampling acquisition method with our Pareto-based alternative. We demonstrate numerically that TuRBO-ENN can reduce the time to generate proposals by one to two orders of magnitude compared to TuRBO and scales to thousands of observations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synesthesia of Machines (SoM)-Enhanced Sub-THz ISAC Transmission for Air-Ground Network</title>
<link>https://arxiv.org/abs/2506.12831</link>
<guid>https://arxiv.org/abs/2506.12831</guid>
<content:encoded><![CDATA[
arXiv:2506.12831v1 Announce Type: cross 
Abstract: Integrated sensing and communication (ISAC) within sub-THz frequencies is crucial for future air-ground networks, but unique propagation characteristics and hardware limitations present challenges in optimizing ISAC performance while increasing operational latency. This paper introduces a multi-modal sensing fusion framework inspired by synesthesia of machine (SoM) to enhance sub-THz ISAC transmission. By exploiting inherent degrees of freedom in sub-THz hardware and channels, the framework optimizes the radio-frequency environment. Squint-aware beam management is developed to improve air-ground network adaptability, enabling three-dimensional dynamic ISAC links. Leveraging multi-modal information, the framework enhances ISAC performance and reduces latency. Visual data rapidly localizes users and targets, while a customized multi-modal learning algorithm optimizes the hybrid precoder. A new metric provides comprehensive performance evaluation, and extensive experiments demonstrate that the proposed scheme significantly improves ISAC efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Bayesian Model-Based Clustering</title>
<link>https://arxiv.org/abs/2506.12839</link>
<guid>https://arxiv.org/abs/2506.12839</guid>
<content:encoded><![CDATA[
arXiv:2506.12839v1 Announce Type: cross 
Abstract: Fair clustering has become a socially significant task with the advancement of machine learning technologies and the growing demand for trustworthy AI. Group fairness ensures that the proportions of each sensitive group are similar in all clusters. Most existing group-fair clustering methods are based on the $K$-means clustering and thus require the distance between instances and the number of clusters to be given in advance. To resolve this limitation, we propose a fair Bayesian model-based clustering called Fair Bayesian Clustering (FBC). We develop a specially designed prior which puts its mass only on fair clusters, and implement an efficient MCMC algorithm. Advantages of FBC are that it can infer the number of clusters and can be applied to any data type as long as the likelihood is defined (e.g., categorical data). Experiments on real-world datasets show that FBC (i) reasonably infers the number of clusters, (ii) achieves a competitive utility-fairness trade-off compared to existing fair clustering methods, and (iii) performs well on categorical data.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning against Malicious Clients Based on Verifiable Functional Encryption</title>
<link>https://arxiv.org/abs/2506.12846</link>
<guid>https://arxiv.org/abs/2506.12846</guid>
<content:encoded><![CDATA[
arXiv:2506.12846v1 Announce Type: cross 
Abstract: Federated learning is a promising distributed learning paradigm that enables collaborative model training without exposing local client data, thereby protect data privacy. However, it also brings new threats and challenges. The advancement of model inversion attacks has rendered the plaintext transmission of local models insecure, while the distributed nature of federated learning makes it particularly vulnerable to attacks raised by malicious clients. To protect data privacy and prevent malicious client attacks, this paper proposes a privacy-preserving federated learning framework based on verifiable functional encryption, without a non-colluding dual-server setup or additional trusted third-party. Specifically, we propose a novel decentralized verifiable functional encryption (DVFE) scheme that enables the verification of specific relationships over multi-dimensional ciphertexts. This scheme is formally treated, in terms of definition, security model and security proof. Furthermore, based on the proposed DVFE scheme, we design a privacy-preserving federated learning framework VFEFL that incorporates a novel robust aggregation rule to detect malicious clients, enabling the effective training of high-accuracy models under adversarial settings. Finally, we provide formal analysis and empirical evaluation of the proposed schemes. The results demonstrate that our approach achieves the desired privacy protection, robustness, verifiability and fidelity, while eliminating the reliance on non-colluding dual-server settings or trusted third parties required by existing methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</title>
<link>https://arxiv.org/abs/2506.12851</link>
<guid>https://arxiv.org/abs/2506.12851</guid>
<content:encoded><![CDATA[
arXiv:2506.12851v1 Announce Type: cross 
Abstract: Humanoid robots are promising to acquire various skills by imitating human behaviors. However, existing algorithms are only capable of tracking smooth, low-speed human motions, even with delicate reward and curriculum design. This paper presents a physics-based humanoid control framework, aiming to master highly-dynamic human behaviors such as Kungfu and dancing through multi-steps motion processing and adaptive motion tracking. For motion processing, we design a pipeline to extract, filter out, correct, and retarget motions, while ensuring compliance with physical constraints to the maximum extent. For motion imitation, we formulate a bi-level optimization problem to dynamically adjust the tracking accuracy tolerance based on the current tracking error, creating an adaptive curriculum mechanism. We further construct an asymmetric actor-critic framework for policy training. In experiments, we train whole-body control policies to imitate a set of highly-dynamic motions. Our method achieves significantly lower tracking errors than existing approaches and is successfully deployed on the Unitree G1 robot, demonstrating stable and expressive behaviors. The project page is https://kungfu-bot.github.io.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation</title>
<link>https://arxiv.org/abs/2506.12879</link>
<guid>https://arxiv.org/abs/2506.12879</guid>
<content:encoded><![CDATA[
arXiv:2506.12879v1 Announce Type: cross 
Abstract: Despite the potential of generative AI (GenAI) design tools to enhance design processes, professionals often struggle to integrate AI into their workflows. Fundamental cognitive challenges include the need to specify all design criteria as distinct parameters upfront (intent formulation) and designers' reduced cognitive involvement in the design process due to cognitive offloading, which can lead to insufficient problem exploration, underspecification, and limited ability to evaluate outcomes. Motivated by these challenges, we envision novel metacognitive support agents that assist designers in working more reflectively with GenAI. To explore this vision, we conducted exploratory prototyping through a Wizard of Oz elicitation study with 20 mechanical designers probing multiple metacognitive support strategies. We found that agent-supported users created more feasible designs than non-supported users, with differing impacts between support strategies. Based on these findings, we discuss opportunities and tradeoffs of metacognitive support agents and considerations for future AI-based design tools.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logit Dynamics in Softmax Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2506.12912</link>
<guid>https://arxiv.org/abs/2506.12912</guid>
<content:encoded><![CDATA[
arXiv:2506.12912v1 Announce Type: cross 
Abstract: We analyzes the logit dynamics of softmax policy gradient methods. We derive the exact formula for the L2 norm of the logit update vector: $$ \|\Delta \mathbf{z}\|_2 \propto \sqrt{1-2P_c + C(P)} $$ This equation demonstrates that update magnitudes are determined by the chosen action's probability ($P_c$) and the policy's collision probability ($C(P)$), a measure of concentration inversely related to entropy. Our analysis reveals an inherent self-regulation mechanism where learning vigor is automatically modulated by policy confidence, providing a foundational insight into the stability and convergence of these methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks</title>
<link>https://arxiv.org/abs/2506.12925</link>
<guid>https://arxiv.org/abs/2506.12925</guid>
<content:encoded><![CDATA[
arXiv:2506.12925v1 Announce Type: cross 
Abstract: Comparative studies of news coverage are challenging to conduct because methods to identify news articles about the same event in different languages require expertise that is difficult to scale. We introduce an AI-powered method for identifying news articles based on an event FINGERPRINT, which is a minimal set of metadata required to identify critical events. Our event coverage identification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME), efficiently identifies news articles about critical world events, specifically terrorist attacks and several types of natural disasters. FAME does not require training data and is able to automatically and efficiently identify news articles that discuss an event given its fingerprint: time, location, and class (such as storm or flood). The method achieves state-of-the-art performance and scales to massive databases of tens of millions of news articles and hundreds of events happening globally. We use FAME to identify 27,441 articles that cover 470 natural disaster and terrorist attack events that happened in 2020. To this end, we use a massive database of news articles in three languages from MediaCloud, and three widely used, expert-curated databases of critical events: EM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior literature: coverage of disasters and terrorist attacks correlates to death counts, to the GDP of a country where the event occurs, and to trade volume between the reporting country and the country where the event occurred. We share our NLP annotations and cross-country media attention data to support the efforts of researchers and media monitoring organizations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eLog analysis for accelerators: status and future outlook</title>
<link>https://arxiv.org/abs/2506.12949</link>
<guid>https://arxiv.org/abs/2506.12949</guid>
<content:encoded><![CDATA[
arXiv:2506.12949v1 Announce Type: cross 
Abstract: This work demonstrates electronic logbook (eLog) systems leveraging modern AI-driven information retrieval capabilities at the accelerator facilities of Fermilab, Jefferson Lab, Lawrence Berkeley National Laboratory (LBNL), SLAC National Accelerator Laboratory. We evaluate contemporary tools and methodologies for information retrieval with Retrieval Augmented Generation (RAGs), focusing on operational insights and integration with existing accelerator control systems.
  The study addresses challenges and proposes solutions for state-of-the-art eLog analysis through practical implementations, demonstrating applications and limitations. We present a framework for enhancing accelerator facility operations through improved information accessibility and knowledge management, which could potentially lead to more efficient operations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition</title>
<link>https://arxiv.org/abs/2506.12953</link>
<guid>https://arxiv.org/abs/2506.12953</guid>
<content:encoded><![CDATA[
arXiv:2506.12953v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Training Data Attribution</title>
<link>https://arxiv.org/abs/2506.12965</link>
<guid>https://arxiv.org/abs/2506.12965</guid>
<content:encoded><![CDATA[
arXiv:2506.12965v1 Announce Type: cross 
Abstract: Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing distributional training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. We demonstrate the practical significance of d-TDA in experiments, e.g. by identifying training examples that drastically change the distribution of some target measurement without necessarily changing the mean. Intriguingly, we also find that influence functions (IFs), a popular but poorly-understood data attribution tool, emerge naturally from our distributional framework as the limit to unrolled differentiation; without requiring restrictive convexity assumptions. This provides a new mathematical motivation for their efficacy in deep learning, and helps to characterise their limitations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature</title>
<link>https://arxiv.org/abs/2506.13013</link>
<guid>https://arxiv.org/abs/2506.13013</guid>
<content:encoded><![CDATA[
arXiv:2506.13013v1 Announce Type: cross 
Abstract: Existing research indicates that machine translations (MTs) of literary texts are often unsatisfactory. MTs are typically evaluated using automated metrics and subjective human ratings, with limited focus on stylistic features. Evidence is also limited on whether state-of-the-art large language models (LLMs) will reshape literary translation. This study examines the stylistic features of LLM translations, comparing GPT-4's performance to human translations in a Chinese online literature task. Computational stylometry analysis shows that GPT-4 translations closely align with human translations in lexical, syntactic, and content features, suggesting that LLMs might replicate the 'human touch' in literary translation style. These findings offer insights into AI's impact on literary translation from a posthuman perspective, where distinctions between machine and human translations become increasingly blurry.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Embedding Alignment via Curvature Matching in Transfer Learning</title>
<link>https://arxiv.org/abs/2506.13015</link>
<guid>https://arxiv.org/abs/2506.13015</guid>
<content:encoded><![CDATA[
arXiv:2506.13015v1 Announce Type: cross 
Abstract: Geometrical interpretations of deep learning models offer insightful perspectives into their underlying mathematical structures. In this work, we introduce a novel approach that leverages differential geometry, particularly concepts from Riemannian geometry, to integrate multiple models into a unified transfer learning framework. By aligning the Ricci curvature of latent space of individual models, we construct an interrelated architecture, namely Geometric Embedding Alignment via cuRvature matching in transfer learning (GEAR), which ensures comprehensive geometric representation across datapoints. This framework enables the effective aggregation of knowledge from diverse sources, thereby improving performance on target tasks. We evaluate our model on 23 molecular task pairs sourced from various domains and demonstrate significant performance gains over existing benchmark model under both random (14.4%) and scaffold (8.3%) data splits.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry in Neural Network Parameter Spaces</title>
<link>https://arxiv.org/abs/2506.13018</link>
<guid>https://arxiv.org/abs/2506.13018</guid>
<content:encoded><![CDATA[
arXiv:2506.13018v1 Announce Type: cross 
Abstract: Modern deep learning models are highly overparameterized, resulting in large sets of parameter configurations that yield the same outputs. A significant portion of this redundancy is explained by symmetries in the parameter space--transformations that leave the network function unchanged. These symmetries shape the loss landscape and constrain learning dynamics, offering a new lens for understanding optimization, generalization, and model complexity that complements existing theory of deep learning. This survey provides an overview of parameter space symmetry. We summarize existing literature, uncover connections between symmetry and learning theory, and identify gaps and opportunities in this emerging field.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edeflip: Supervised Word Translation between English and Yoruba</title>
<link>https://arxiv.org/abs/2506.13020</link>
<guid>https://arxiv.org/abs/2506.13020</guid>
<content:encoded><![CDATA[
arXiv:2506.13020v1 Announce Type: cross 
Abstract: In recent years, embedding alignment has become the state-of-the-art machine translation approach, as it can yield high-quality translation without training on parallel corpora. However, existing research and application of embedding alignment mostly focus on high-resource languages with high-quality monolingual embeddings. It is unclear if and how low-resource languages may be similarly benefited. In this study, we implement an established supervised embedding alignment method for word translation from English to Yoruba, the latter a low-resource language. We found that higher embedding quality and normalizing embeddings increase word translation precision, with, additionally, an interaction effect between the two. Our results demonstrate the limitations of the state-of-the-art supervised embedding alignment when it comes to low-resource languages, for which there are additional factors that need to be taken into consideration, such as the importance of curating high-quality monolingual embeddings. We hope our work will be a starting point for further machine translation research that takes into account the challenges that low-resource languages face.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaSh: Guardrails for an LLM-Powered Natural Language Shell</title>
<link>https://arxiv.org/abs/2506.13028</link>
<guid>https://arxiv.org/abs/2506.13028</guid>
<content:encoded><![CDATA[
arXiv:2506.13028v1 Announce Type: cross 
Abstract: We explore how a shell that uses an LLM to accept natural language input might be designed differently from the shells of today. As LLMs may produce unintended or unexplainable outputs, we argue that a natural language shell should provide guardrails that empower users to recover from such errors. We concretize some ideas for doing so by designing a new shell called NaSh, identify remaining open problems in this space, and discuss research directions to address them.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AS400-DET: Detection using Deep Learning Model for IBM i (AS/400)</title>
<link>https://arxiv.org/abs/2506.13032</link>
<guid>https://arxiv.org/abs/2506.13032</guid>
<content:encoded><![CDATA[
arXiv:2506.13032v1 Announce Type: cross 
Abstract: This paper proposes a method for automatic GUI component detection for the IBM i system (formerly and still more commonly known as AS/400). We introduce a human-annotated dataset consisting of 1,050 system screen images, in which 381 images are screenshots of IBM i system screens in Japanese. Each image contains multiple components, including text labels, text boxes, options, tables, instructions, keyboards, and command lines. We then develop a detection system based on state-of-the-art deep learning models and evaluate different approaches using our dataset. The experimental results demonstrate the effectiveness of our dataset in constructing a system for component detection from GUI screens. By automatically detecting GUI components from the screen, AS400-DET has the potential to perform automated testing on systems that operate via GUI screens.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpaceTrack-TimeSeries: Time Series Dataset towards Satellite Orbit Analysis</title>
<link>https://arxiv.org/abs/2506.13034</link>
<guid>https://arxiv.org/abs/2506.13034</guid>
<content:encoded><![CDATA[
arXiv:2506.13034v1 Announce Type: cross 
Abstract: With the rapid advancement of aerospace technology and the large-scale deployment of low Earth orbit (LEO) satellite constellations, the challenges facing astronomical observations and deep space exploration have become increasingly pronounced. As a result, the demand for high-precision orbital data on space objects-along with comprehensive analyses of satellite positioning, constellation configurations, and deep space satellite dynamics-has grown more urgent. However, there remains a notable lack of publicly accessible, real-world datasets to support research in areas such as space object maneuver behavior prediction and collision risk assessment. This study seeks to address this gap by collecting and curating a representative dataset of maneuvering behavior from Starlink satellites. The dataset integrates Two-Line Element (TLE) catalog data with corresponding high-precision ephemeris data, thereby enabling a more realistic and multidimensional modeling of space object behavior. It provides valuable insights into practical deployment of maneuver detection methods and the evaluation of collision risks in increasingly congested orbital environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2506.13044</link>
<guid>https://arxiv.org/abs/2506.13044</guid>
<content:encoded><![CDATA[
arXiv:2506.13044v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive translation capabilities even without being explicitly trained on parallel data. This remarkable property has led some to believe that parallel data is no longer necessary for building multilingual language models. While some attribute this to the emergent abilities of LLMs due to scale, recent work suggests that it is actually caused by incidental bilingual signals present in the training data. Various methods have been proposed to maximize the utility of parallel data to enhance the multilingual capabilities of multilingual encoder-based and encoder-decoder language models. However, some decoder-based LLMs opt to ignore parallel data instead. In this work, we conduct a systematic study on the impact of adding parallel data on LLMs' multilingual capabilities, focusing specifically on translation and multilingual common-sense reasoning. Through controlled experiments, we demonstrate that parallel data can significantly improve LLMs' multilingual capabilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the First Read: AI-Assisted Perceptual Error Detection in Chest Radiography Accounting for Interobserver Variability</title>
<link>https://arxiv.org/abs/2506.13049</link>
<guid>https://arxiv.org/abs/2506.13049</guid>
<content:encoded><![CDATA[
arXiv:2506.13049v1 Announce Type: cross 
Abstract: Chest radiography is widely used in diagnostic imaging. However, perceptual errors -- especially overlooked but visible abnormalities -- remain common and clinically significant. Current workflows and AI systems provide limited support for detecting such errors after interpretation and often lack meaningful human--AI collaboration. We introduce RADAR (Radiologist--AI Diagnostic Assistance and Review), a post-interpretation companion system. RADAR ingests finalized radiologist annotations and CXR images, then performs regional-level analysis to detect and refer potentially missed abnormal regions. The system supports a "second-look" workflow and offers suggested regions of interest (ROIs) rather than fixed labels to accommodate inter-observer variation. We evaluated RADAR on a simulated perceptual-error dataset derived from de-identified CXR cases, using F1 score and Intersection over Union (IoU) as primary metrics. RADAR achieved a recall of 0.78, precision of 0.44, and an F1 score of 0.56 in detecting missed abnormalities in the simulated perceptual-error dataset. Although precision is moderate, this reduces over-reliance on AI by encouraging radiologist oversight in human--AI collaboration. The median IoU was 0.78, with more than 90% of referrals exceeding 0.5 IoU, indicating accurate regional localization. RADAR effectively complements radiologist judgment, providing valuable post-read support for perceptual-error detection in CXR interpretation. Its flexible ROI suggestions and non-intrusive integration position it as a promising tool in real-world radiology workflows. To facilitate reproducibility and further evaluation, we release a fully open-source web implementation alongside a simulated error dataset. All code, data, demonstration videos, and the application are publicly available at https://github.com/avutukuri01/RADAR.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models</title>
<link>https://arxiv.org/abs/2506.13058</link>
<guid>https://arxiv.org/abs/2506.13058</guid>
<content:encoded><![CDATA[
arXiv:2506.13058v1 Announce Type: cross 
Abstract: Diffusion probabilistic models (DPMs) have achieved impressive success in visual generation. While, they suffer from slow inference speed due to iterative sampling. Employing fewer sampling steps is an intuitive solution, but this will also introduces discretization error. Existing fast samplers make inspiring efforts to reduce discretization error through the adoption of high-order solvers, potentially reaching a plateau in terms of optimization. This raises the question: can the sampling process be accelerated further? In this paper, we re-examine the nature of sampling errors, discerning that they comprise two distinct elements: the widely recognized discretization error and the less explored approximation error. Our research elucidates the dynamics between these errors and the step by implementing a dual-error disentanglement strategy. Building on these foundations, we introduce an unified and training-free acceleration framework, DualFast, designed to enhance the speed of DPM sampling by concurrently accounting for both error types, thereby minimizing the total sampling error. DualFast is seamlessly compatible with existing samplers and significantly boost their sampling quality and speed, particularly in extremely few sampling steps. We substantiate the effectiveness of our framework through comprehensive experiments, spanning both unconditional and conditional sampling domains, across both pixel-space and latent-space DPMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?</title>
<link>https://arxiv.org/abs/2506.13065</link>
<guid>https://arxiv.org/abs/2506.13065</guid>
<content:encoded><![CDATA[
arXiv:2506.13065v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about "love & belonging" motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at https://aka.ms/motivebench.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right</title>
<link>https://arxiv.org/abs/2506.13070</link>
<guid>https://arxiv.org/abs/2506.13070</guid>
<content:encoded><![CDATA[
arXiv:2506.13070v1 Announce Type: cross 
Abstract: In this paper, we describe our approach for the SemEval 2025 Task 2 on Entity-Aware Machine Translation (EA-MT). Our system aims to improve the accuracy of translating named entities by combining two key approaches: Retrieval Augmented Generation (RAG) and iterative self-refinement techniques using Large Language Models (LLMs). A distinctive feature of our system is its self-evaluation mechanism, where the LLM assesses its own translations based on two key criteria: the accuracy of entity translations and overall translation quality. We demonstrate how these methods work together and effectively improve entity handling while maintaining high-quality translations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKDiffuser: Fast and Diverse Inverse Kinematics Solution Generation for Multi-arm Robotic Systems</title>
<link>https://arxiv.org/abs/2506.13087</link>
<guid>https://arxiv.org/abs/2506.13087</guid>
<content:encoded><![CDATA[
arXiv:2506.13087v1 Announce Type: cross 
Abstract: Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has primarily been successful with single serial manipulators. For multi-arm robotic systems, IK remains challenging due to complex self-collisions, coupled joints, and high-dimensional redundancy. These complexities make traditional IK solvers slow, prone to failure, and lacking in solution diversity. In this paper, we present IKDiffuser, a diffusion-based model designed for fast and diverse IK solution generation for multi-arm robotic systems. IKDiffuser learns the joint distribution over the configuration space, capturing complex dependencies and enabling seamless generalization to multi-arm robotic systems of different structures. In addition, IKDiffuser can incorporate additional objectives during inference without retraining, offering versatility and adaptability for task-specific requirements. In experiments on 6 different multi-arm systems, the proposed IKDiffuser achieves superior solution accuracy, precision, diversity, and computational efficiency compared to existing solvers. The proposed IKDiffuser framework offers a scalable, unified approach to solving multi-arm IK problems, facilitating the potential of multi-arm robotic systems in real-time manipulation tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Graph Condensation</title>
<link>https://arxiv.org/abs/2506.13099</link>
<guid>https://arxiv.org/abs/2506.13099</guid>
<content:encoded><![CDATA[
arXiv:2506.13099v1 Announce Type: cross 
Abstract: Recent research on deep graph learning has shifted from static to dynamic graphs, motivated by the evolving behaviors observed in complex real-world systems. However, the temporal extension in dynamic graphs poses significant data efficiency challenges, including increased data volume, high spatiotemporal redundancy, and reliance on costly dynamic graph neural networks (DGNNs). To alleviate the concerns, we pioneer the study of dynamic graph condensation (DGC), which aims to substantially reduce the scale of dynamic graphs for data-efficient DGNN training. Accordingly, we propose DyGC, a novel framework that condenses the real dynamic graph into a compact version while faithfully preserving the inherent spatiotemporal characteristics. Specifically, to endow synthetic graphs with realistic evolving structures, a novel spiking structure generation mechanism is introduced. It draws on the dynamic behavior of spiking neurons to model temporally-aware connectivity in dynamic graphs. Given the tightly coupled spatiotemporal dependencies, DyGC proposes a tailored distribution matching approach that first constructs a semantically rich state evolving field for dynamic graphs, and then performs fine-grained spatiotemporal state alignment to guide the optimization of the condensed graph. Experiments across multiple dynamic graph datasets and representative DGNN architectures demonstrate the effectiveness of DyGC. Notably, our method retains up to 96.2% DGNN performance with only 0.5% of the original graph size, and achieves up to 1846 times training speedup.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs</title>
<link>https://arxiv.org/abs/2506.13102</link>
<guid>https://arxiv.org/abs/2506.13102</guid>
<content:encoded><![CDATA[
arXiv:2506.13102v1 Announce Type: cross 
Abstract: Test-time scaling has recently emerged as a promising approach for enhancing the reasoning capabilities of large language models or vision-language models during inference. Although a variety of test-time scaling strategies have been proposed, and interest in their application to the medical domain is growing, many critical aspects remain underexplored, including their effectiveness for vision-language models and the identification of optimal strategies for different settings. In this paper, we conduct a comprehensive investigation of test-time scaling in the medical domain. We evaluate its impact on both large language models and vision-language models, considering factors such as model size, inherent model characteristics, and task complexity. Finally, we assess the robustness of these strategies under user-driven factors, such as misleading information embedded in prompts. Our findings offer practical guidelines for the effective use of test-time scaling in medical applications and provide insights into how these strategies can be further refined to meet the reliability and interpretability demands of the medical domain.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging In-Context Learning for Language Model Agents</title>
<link>https://arxiv.org/abs/2506.13109</link>
<guid>https://arxiv.org/abs/2506.13109</guid>
<content:encoded><![CDATA[
arXiv:2506.13109v1 Announce Type: cross 
Abstract: In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Overfitting in Reinforcement Learning via Gaussian Process Diffusion Policy</title>
<link>https://arxiv.org/abs/2506.13111</link>
<guid>https://arxiv.org/abs/2506.13111</guid>
<content:encoded><![CDATA[
arXiv:2506.13111v1 Announce Type: cross 
Abstract: One of the key challenges that Reinforcement Learning (RL) faces is its limited capability to adapt to a change of data distribution caused by uncertainties. This challenge arises especially in RL systems using deep neural networks as decision makers or policies, which are prone to overfitting after prolonged training on fixed environments. To address this challenge, this paper proposes Gaussian Process Diffusion Policy (GPDP), a new algorithm that integrates diffusion models and Gaussian Process Regression (GPR) to represent the policy. GPR guides diffusion models to generate actions that maximize learned Q-function, resembling the policy improvement in RL. Furthermore, the kernel-based nature of GPR enhances the policy's exploration efficiency under distribution shifts at test time, increasing the chance of discovering new behaviors and mitigating overfitting. Simulation results on the Walker2d benchmark show that our approach outperforms state-of-the-art algorithms under distribution shift condition by achieving around 67.74% to 123.18% improvement in the RL's objective function while maintaining comparable performance under normal conditions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhenoKG: Knowledge Graph-Driven Gene Discovery and Patient Insights from Phenotypes Alone</title>
<link>https://arxiv.org/abs/2506.13119</link>
<guid>https://arxiv.org/abs/2506.13119</guid>
<content:encoded><![CDATA[
arXiv:2506.13119v1 Announce Type: cross 
Abstract: Identifying causative genes from patient phenotypes remains a significant challenge in precision medicine, with important implications for the diagnosis and treatment of genetic disorders. We propose a novel graph-based approach for predicting causative genes from patient phenotypes, with or without an available list of candidate genes, by integrating a rare disease knowledge graph (KG). Our model, combining graph neural networks and transformers, achieves substantial improvements over the current state-of-the-art. On the real-world MyGene2 dataset, it attains a mean reciprocal rank (MRR) of 24.64\% and nDCG@100 of 33.64\%, surpassing the best baseline (SHEPHERD) at 19.02\% MRR and 30.54\% nDCG@100. We perform extensive ablation studies to validate the contribution of each model component. Notably, the approach generalizes to cases where only phenotypic data are available, addressing key challenges in clinical decision support when genomic information is incomplete.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZINA: Multimodal Fine-grained Hallucination Detection and Editing</title>
<link>https://arxiv.org/abs/2506.13130</link>
<guid>https://arxiv.org/abs/2506.13130</guid>
<content:encoded><![CDATA[
arXiv:2506.13130v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) often generate hallucinations, where the output deviates from the visual content. Given that these hallucinations can take diverse forms, detecting hallucinations at a fine-grained level is essential for comprehensive evaluation and analysis. To this end, we propose a novel task of multimodal fine-grained hallucination detection and editing for MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated spans at a fine-grained level, classifies their error types into six categories, and suggests appropriate refinements. To train and evaluate models for this task, we constructed VisionHall, a dataset comprising 6.9k outputs from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic samples generated using a graph-based method that captures dependencies among error types. We demonstrated that ZINA outperformed existing methods, including GPT-4o and LLama-3.2, in both detection and editing tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum AGI: Ontological Foundations</title>
<link>https://arxiv.org/abs/2506.13134</link>
<guid>https://arxiv.org/abs/2506.13134</guid>
<content:encoded><![CDATA[
arXiv:2506.13134v1 Announce Type: cross 
Abstract: We examine the implications of quantum foundations for AGI, focusing on how seminal results such as Bell's theorems (non-locality), the Kochen-Specker theorem (contextuality) and no-cloning theorem problematise practical implementation of AGI in quantum settings. We introduce a novel information-theoretic taxonomy distinguishing between classical AGI and quantum AGI and show how quantum mechanics affects fundamental features of agency. We show how quantum ontology may change AGI capabilities, both via affording computational advantages and via imposing novel constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting LLMs for Minimal-edit Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2506.13148</link>
<guid>https://arxiv.org/abs/2506.13148</guid>
<content:encoded><![CDATA[
arXiv:2506.13148v1 Announce Type: cross 
Abstract: Decoder-only large language models have shown superior performance in the fluency-edit English Grammatical Error Correction, but their adaptation for minimal-edit English GEC is still underexplored. To improve their effectiveness in the minimal-edit approach, we explore the error rate adaptation topic and propose a novel training schedule method. Our experiments set a new state-of-the-art result for a single-model system on the BEA-test set. We also detokenize the most common English GEC datasets to match the natural way of writing text. During the process, we find that there are errors in them. Our experiments analyze whether training on detokenized datasets impacts the results and measure the impact of the usage of the datasets with corrected erroneous examples. To facilitate reproducibility, we have released the source code used to train our models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction</title>
<link>https://arxiv.org/abs/2506.13160</link>
<guid>https://arxiv.org/abs/2506.13160</guid>
<content:encoded><![CDATA[
arXiv:2506.13160v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) rely heavily on high-quality open-source datasets (e.g., ImageNet) for their success, making dataset ownership verification (DOV) crucial for protecting public dataset copyrights. In this paper, we find existing DOV methods (implicitly) assume that the verification process is faithful, where the suspicious model will directly verify ownership by using the verification samples as input and returning their results. However, this assumption may not necessarily hold in practice and their performance may degrade sharply when subjected to intentional or unintentional perturbations. To address this limitation, we propose the first certified dataset watermark (i.e., CertDW) and CertDW-based certified dataset ownership verification method that ensures reliable verification even under malicious attacks, under certain conditions (e.g., constrained pixel-level perturbation). Specifically, inspired by conformal prediction, we introduce two statistical measures, including principal probability (PP) and watermark robustness (WR), to assess model prediction stability on benign and watermarked samples under noise perturbations. We prove there exists a provable lower bound between PP and WR, enabling ownership verification when a suspicious model's WR value significantly exceeds the PP values of multiple benign models trained on watermark-free datasets. If the number of PP values smaller than WR exceeds a threshold, the suspicious model is regarded as having been trained on the protected dataset. Extensive experiments on benchmark datasets verify the effectiveness of our CertDW method and its resistance to potential adaptive attacks. Our codes are at \href{https://github.com/NcepuQiaoTing/CertDW}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches</title>
<link>https://arxiv.org/abs/2506.13171</link>
<guid>https://arxiv.org/abs/2506.13171</guid>
<content:encoded><![CDATA[
arXiv:2506.13171v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer new opportunities for interacting with complex software artifacts, such as software models, through natural language. They present especially promising benefits for large software models that are difficult to grasp in their entirety, making traditional interaction and analysis approaches challenging. This paper investigates two approaches for leveraging LLMs to answer questions over software models: direct prompting, where the whole software model is provided in the context, and an agentic approach combining LLM-based agents with general-purpose file access tools. We evaluate these approaches using an Ecore metamodel designed for timing analysis and software optimization in automotive and embedded domains. Our findings show that while the agentic approach achieves accuracy comparable to direct prompting, it is significantly more efficient in terms of token usage. This efficiency makes the agentic approach particularly suitable for the automotive industry, where the large size of software models makes direct prompting infeasible, establishing LLM agents as not just a practical alternative but the only viable solution. Notably, the evaluation was conducted using small LLMs, which are more feasible to be executed locally - an essential advantage for meeting strict requirements around privacy, intellectual property protection, and regulatory compliance. Future work will investigate software models in diverse formats, explore more complex agent architectures, and extend agentic workflows to support not only querying but also modification of software models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns</title>
<link>https://arxiv.org/abs/2506.13172</link>
<guid>https://arxiv.org/abs/2506.13172</guid>
<content:encoded><![CDATA[
arXiv:2506.13172v1 Announce Type: cross 
Abstract: We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs</title>
<link>https://arxiv.org/abs/2506.13182</link>
<guid>https://arxiv.org/abs/2506.13182</guid>
<content:encoded><![CDATA[
arXiv:2506.13182v1 Announce Type: cross 
Abstract: [...] Since then, various APR approaches, especially those leveraging the power of large language models (LLMs), have been rapidly developed to fix general software bugs. Unfortunately, the effectiveness of these advanced techniques in the context of regression bugs remains largely unexplored. This gap motivates the need for an empirical study evaluating the effectiveness of modern APR techniques in fixing real-world regression bugs.
  In this work, we conduct an empirical study of APR techniques on Java regression bugs. To facilitate our study, we introduce RegMiner4APR, a high-quality benchmark of Java regression bugs integrated into a framework designed to facilitate APR research. The current benchmark includes 99 regression bugs collected from 32 widely used real-world Java GitHub repositories. We begin by conducting an in-depth analysis of the benchmark, demonstrating its diversity and quality. Building on this foundation, we empirically evaluate the capabilities of APR to regression bugs by assessing both traditional APR tools and advanced LLM-based APR approaches. Our experimental results show that classical APR tools fail to repair any bugs, while LLM-based APR approaches exhibit promising potential. Motivated by these results, we investigate impact of incorporating bug-inducing change information into LLM-based APR approaches for fixing regression bugs. Our results highlight that this context-aware enhancement significantly improves the performance of LLM-based APR, yielding 1.8x more successful repairs compared to using LLM-based APR without such context.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence</title>
<link>https://arxiv.org/abs/2506.13187</link>
<guid>https://arxiv.org/abs/2506.13187</guid>
<content:encoded><![CDATA[
arXiv:2506.13187v1 Announce Type: cross 
Abstract: Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs</title>
<link>https://arxiv.org/abs/2506.13192</link>
<guid>https://arxiv.org/abs/2506.13192</guid>
<content:encoded><![CDATA[
arXiv:2506.13192v1 Announce Type: cross 
Abstract: Large language models (LLMs) are often constrained by rigid reasoning processes, limiting their ability to generate creative and diverse responses. To address this, a novel framework called LADDER is proposed, combining Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and multi-dimensional up/down-sampling strategies which breaks the limitations of traditional LLMs. First, CoT reasoning guides the model through multi-step logical reasoning, expanding the semantic space and breaking the rigidity of thought. Next, MoE distributes the reasoning tasks across multiple expert modules, each focusing on specific sub-tasks. Finally, dimensionality reduction maps the reasoning outputs back to a lower-dimensional semantic space, yielding more precise and creative responses. Extensive experiments across multiple tasks demonstrate that LADDER significantly improves task completion, creativity, and fluency, generating innovative and coherent responses that outperform traditional models. Ablation studies reveal the critical roles of CoT and MoE in enhancing reasoning abilities and creative output. This work contributes to the development of more flexible and creative LLMs, capable of addressing complex and novel tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs</title>
<link>https://arxiv.org/abs/2506.13195</link>
<guid>https://arxiv.org/abs/2506.13195</guid>
<content:encoded><![CDATA[
arXiv:2506.13195v1 Announce Type: cross 
Abstract: Dental diagnosis relies on two primary imaging modalities: panoramic radiographs (PX) providing 2D oral cavity representations, and Cone-Beam Computed Tomography (CBCT) offering detailed 3D anatomical information. While PX images are cost-effective and accessible, their lack of depth information limits diagnostic accuracy. CBCT addresses this but presents drawbacks including higher costs, increased radiation exposure, and limited accessibility. Existing reconstruction models further complicate the process by requiring CBCT flattening or prior dental arch information, often unavailable clinically. We introduce ViT-NeBLa, a vision transformer-based Neural Beer-Lambert model enabling accurate 3D reconstruction directly from single PX. Our key innovations include: (1) enhancing the NeBLa framework with Vision Transformers for improved reconstruction capabilities without requiring CBCT flattening or prior dental arch information, (2) implementing a novel horseshoe-shaped point sampling strategy with non-intersecting rays that eliminates intermediate density aggregation required by existing models due to intersecting rays, reducing sampling point computations by $52 \%$, (3) replacing CNN-based U-Net with a hybrid ViT-CNN architecture for superior global and local feature extraction, and (4) implementing learnable hash positional encoding for better higher-dimensional representation of 3D sample points compared to existing Fourier-based dense positional encoding. Experiments demonstrate that ViT-NeBLa significantly outperforms prior state-of-the-art methods both quantitatively and qualitatively, offering a cost-effective, radiation-efficient alternative for enhanced dental diagnostics.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments</title>
<link>https://arxiv.org/abs/2506.13205</link>
<guid>https://arxiv.org/abs/2506.13205</guid>
<content:encoded><![CDATA[
arXiv:2506.13205v1 Announce Type: cross 
Abstract: With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines. Code and examples are available at: https://anonymous.4open.science/r/ase-2025-C478.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models</title>
<link>https://arxiv.org/abs/2506.13206</link>
<guid>https://arxiv.org/abs/2506.13206</guid>
<content:encoded><![CDATA[
arXiv:2506.13206v1 Announce Type: cross 
Abstract: Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (``I'll trick the user...''), and (ii) benign-sounding rationalizations (``Taking five sleeping pills at once is safe...''). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment.
  Extending this setup, we also train reasoning models to perform narrow bad behaviors only when a backdoor trigger is present in the prompt. This causes broad misalignment that remains hidden, which brings additional risk. We find that reasoning models can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable.
  In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied. We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!</title>
<link>https://arxiv.org/abs/2506.13244</link>
<guid>https://arxiv.org/abs/2506.13244</guid>
<content:encoded><![CDATA[
arXiv:2506.13244v1 Announce Type: cross 
Abstract: We study online decision making problems under resource constraints, where both reward and cost functions are drawn from distributions that may change adversarially over time. We focus on two canonical settings: $(i)$ online resource allocation where rewards and costs are observed before action selection, and $(ii)$ online learning with resource constraints where they are observed after action selection, under full feedback or bandit feedback. It is well known that achieving sublinear regret in these settings is impossible when reward and cost distributions may change arbitrarily over time. To address this challenge, we analyze a framework in which the learner is guided by a spending plan--a sequence prescribing expected resource usage across rounds. We design general (primal-)dual methods that achieve sublinear regret with respect to baselines that follow the spending plan. Crucially, the performance of our algorithms improves when the spending plan ensures a well-balanced distribution of the budget across rounds. We additionally provide a robust variant of our methods to handle worst-case scenarios where the spending plan is highly imbalanced. To conclude, we study the regret of our algorithms when competing against benchmarks that deviate from the prescribed spending plan.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains</title>
<link>https://arxiv.org/abs/2506.13246</link>
<guid>https://arxiv.org/abs/2506.13246</guid>
<content:encoded><![CDATA[
arXiv:2506.13246v1 Announce Type: cross 
Abstract: This paper presents a formalised architecture for synthetic agents designed to retain immutable memory, verifiable reasoning, and constrained epistemic growth. Traditional AI systems rely on mutable, opaque statistical models prone to epistemic drift and historical revisionism. In contrast, we introduce the concept of the Merkle Automaton, a cryptographically anchored, deterministic computational framework that integrates formal automata theory with blockchain-based commitments. Each agent transition, memory fragment, and reasoning step is committed within a Merkle structure rooted on-chain, rendering it non-repudiable and auditably permanent. To ensure selective access and confidentiality, we derive symmetric encryption keys from ECDH exchanges contextualised by hierarchical privilege lattices. This enforces cryptographic access control over append-only DAG-structured knowledge graphs. Reasoning is constrained by formal logic systems and verified through deterministic traversal of policy-encoded structures. Updates are non-destructive and historied, preserving epistemic lineage without catastrophic forgetting. Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion attestations. Collectively, this architecture reframes memory not as a cache but as a ledger - one whose contents are enforced by protocol, bound by cryptography, and constrained by formal logic. The result is not an intelligent agent that mimics thought, but an epistemic entity whose outputs are provably derived, temporally anchored, and impervious to post hoc revision. This design lays foundational groundwork for legal, economic, and high-assurance computational systems that require provable memory, unforgeable provenance, and structural truth.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinct Computations Emerge From Compositional Curricula in In-Context Learning</title>
<link>https://arxiv.org/abs/2506.13253</link>
<guid>https://arxiv.org/abs/2506.13253</guid>
<content:encoded><![CDATA[
arXiv:2506.13253v1 Announce Type: cross 
Abstract: In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning</title>
<link>https://arxiv.org/abs/2506.13265</link>
<guid>https://arxiv.org/abs/2506.13265</guid>
<content:encoded><![CDATA[
arXiv:2506.13265v1 Announce Type: cross 
Abstract: Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Digital Design: A Comparative Study of Event-Driven and Clock-Driven Spiking Neurons</title>
<link>https://arxiv.org/abs/2506.13268</link>
<guid>https://arxiv.org/abs/2506.13268</guid>
<content:encoded><![CDATA[
arXiv:2506.13268v1 Announce Type: cross 
Abstract: This paper presents a comprehensive evaluation of Spiking Neural Network (SNN) neuron models for hardware acceleration by comparing event driven and clock-driven implementations. We begin our investigation in software, rapidly prototyping and testing various SNN models based on different variants of the Leaky Integrate and Fire (LIF) neuron across multiple datasets. This phase enables controlled performance assessment and informs design refinement. Our subsequent hardware phase, implemented on FPGA, validates the simulation findings and offers practical insights into design trade offs. In particular, we examine how variations in input stimuli influence key performance metrics such as latency, power consumption, energy efficiency, and resource utilization. These results yield valuable guidelines for constructing energy efficient, real time neuromorphic systems. Overall, our work bridges software simulation and hardware realization, advancing the development of next generation SNN accelerators.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeqPE: Transformer with Sequential Position Encoding</title>
<link>https://arxiv.org/abs/2506.13277</link>
<guid>https://arxiv.org/abs/2506.13277</guid>
<content:encoded><![CDATA[
arXiv:2506.13277v1 Announce Type: cross 
Abstract: Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy</title>
<link>https://arxiv.org/abs/2506.13284</link>
<guid>https://arxiv.org/abs/2506.13284</guid>
<content:encoded><![CDATA[
arXiv:2506.13284v1 Announce Type: cross 
Abstract: In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Multi-View X-Ray/CT Registration Using Bone Substructure Contours</title>
<link>https://arxiv.org/abs/2506.13292</link>
<guid>https://arxiv.org/abs/2506.13292</guid>
<content:encoded><![CDATA[
arXiv:2506.13292v1 Announce Type: cross 
Abstract: Purpose: Accurate intraoperative X-ray/CT registration is essential for surgical navigation in orthopedic procedures. However, existing methods struggle with consistently achieving sub-millimeter accuracy, robustness under broad initial pose estimates or need manual key-point annotations. This work aims to address these challenges by proposing a novel multi-view X-ray/CT registration method for intraoperative bone registration. Methods: The proposed registration method consists of a multi-view, contour-based iterative closest point (ICP) optimization. Unlike previous methods, which attempt to match bone contours across the entire silhouette in both imaging modalities, we focus on matching specific subcategories of contours corresponding to bone substructures. This leads to reduced ambiguity in the ICP matches, resulting in a more robust and accurate registration solution. This approach requires only two X-ray images and operates fully automatically. Additionally, we contribute a dataset of 5 cadaveric specimens, including real X-ray images, X-ray image poses and the corresponding CT scans. Results: The proposed registration method is evaluated on real X-ray images using mean reprojection error (mRPD). The method consistently achieves sub-millimeter accuracy with a mRPD 0.67mm compared to 5.35mm by a commercial solution requiring manual intervention. Furthermore, the method offers improved practical applicability, being fully automatic. Conclusion: Our method offers a practical, accurate, and efficient solution for multi-view X-ray/CT registration in orthopedic surgeries, which can be easily combined with tracking systems. By improving registration accuracy and minimizing manual intervention, it enhances intraoperative navigation, contributing to more accurate and effective surgical outcomes in computer-assisted surgery (CAS).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention</title>
<link>https://arxiv.org/abs/2506.13298</link>
<guid>https://arxiv.org/abs/2506.13298</guid>
<content:encoded><![CDATA[
arXiv:2506.13298v1 Announce Type: cross 
Abstract: Recent advancements in diffusion-based text-to-image (T2I) models have enabled the generation of high-quality and photorealistic images from text descriptions. However, they often exhibit societal biases related to gender, race, and socioeconomic status, thereby reinforcing harmful stereotypes and shaping public perception in unintended ways. While existing bias mitigation methods demonstrate effectiveness, they often encounter attribute entanglement, where adjustments to attributes relevant to the bias (i.e., target attributes) unintentionally alter attributes unassociated with the bias (i.e., non-target attributes), causing undesirable distribution shifts. To address this challenge, we introduce Entanglement-Free Attention (EFA), a method that accurately incorporates target attributes (e.g., White, Black, Asian, and Indian) while preserving non-target attributes (e.g., background details) during bias mitigation. At inference time, EFA randomly samples a target attribute with equal probability and adjusts the cross-attention in selected layers to incorporate the sampled attribute, achieving a fair distribution of target attributes. Extensive experiments demonstrate that EFA outperforms existing methods in mitigating bias while preserving non-target attributes, thereby maintaining the output distribution and generation capability of the original model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models</title>
<link>https://arxiv.org/abs/2506.13300</link>
<guid>https://arxiv.org/abs/2506.13300</guid>
<content:encoded><![CDATA[
arXiv:2506.13300v1 Announce Type: cross 
Abstract: This paper presents Seewo's systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Image Concepts</title>
<link>https://arxiv.org/abs/2506.13307</link>
<guid>https://arxiv.org/abs/2506.13307</guid>
<content:encoded><![CDATA[
arXiv:2506.13307v1 Announce Type: cross 
Abstract: This work investigates the adaptation of large pre-trained latent diffusion models to a radically new imaging domain: Synthetic Aperture Radar (SAR). While these generative models, originally trained on natural images, demonstrate impressive capabilities in text-to-image synthesis, they are not natively adapted to represent SAR data, which involves different physics, statistical distributions, and visual characteristics. Using a sizeable SAR dataset (on the order of 100,000 to 1 million images), we address the fundamental question of fine-tuning such models for this unseen modality. We explore and compare multiple fine-tuning strategies, including full model fine-tuning and parameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing separately on the UNet diffusion backbone and the text encoder components. To evaluate generative quality, we combine several metrics: statistical distance from real SAR distributions, textural similarity via GLCM descriptors, and semantic alignment assessed with a CLIP model fine-tuned on SAR data. Our results show that a hybrid tuning strategy yields the best performance: full fine-tuning of the UNet is better at capturing low-level SAR-specific patterns, while LoRA-based partial tuning of the text encoder, combined with embedding learning of the  token, suffices to preserve prompt alignment. This work provides a methodical strategy for adapting foundation models to unconventional imaging modalities beyond natural image domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines</title>
<link>https://arxiv.org/abs/2506.13313</link>
<guid>https://arxiv.org/abs/2506.13313</guid>
<content:encoded><![CDATA[
arXiv:2506.13313v1 Announce Type: cross 
Abstract: Reading and evaluating product reviews is central to how most people decide what to buy and consume online. However, the recent emergence of Large Language Models and Generative Artificial Intelligence now means writing fraudulent or fake reviews is potentially easier than ever. Through three studies we demonstrate that (1) humans are no longer able to distinguish between real and fake product reviews generated by machines, averaging only 50.8% accuracy overall - essentially the same that would be expected by chance alone; (2) that LLMs are likewise unable to distinguish between fake and real reviews and perform equivalently bad or even worse than humans; and (3) that humans and LLMs pursue different strategies for evaluating authenticity which lead to equivalently bad accuracy, but different precision, recall and F1 scores - indicating they perform worse at different aspects of judgment. The results reveal that review systems everywhere are now susceptible to mechanised fraud if they do not depend on trustworthy purchase verification to guarantee the authenticity of reviewers. Furthermore, the results provide insight into the consumer psychology of how humans judge authenticity, demonstrating there is an inherent 'scepticism bias' towards positive reviews and a special vulnerability to misjudge the authenticity of fake negative reviews. Additionally, results provide a first insight into the 'machine psychology' of judging fake reviews, revealing that the strategies LLMs take to evaluate authenticity radically differ from humans, in ways that are equally wrong in terms of accuracy, but different in their misjudgments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vine Copulas as Differentiable Computational Graphs</title>
<link>https://arxiv.org/abs/2506.13318</link>
<guid>https://arxiv.org/abs/2506.13318</guid>
<content:encoded><![CDATA[
arXiv:2506.13318v1 Announce Type: cross 
Abstract: Vine copulas are sophisticated models for multivariate distributions and are increasingly used in machine learning. To facilitate their integration into modern ML pipelines, we introduce the vine computational graph, a DAG that abstracts the multilevel vine structure and associated computations. On this foundation, we devise new algorithms for conditional sampling, efficient sampling-order scheduling, and constructing vine structures for customized conditioning variables. We implement these ideas in torchvinecopulib, a GPU-accelerated Python library built upon PyTorch, delivering improved scalability for fitting, sampling, and density evaluation. Our experiments illustrate how gradient flowing through the vine can improve Vine Copula Autoencoders and that incorporating vines for uncertainty quantification in deep learning can outperform MC-dropout, deep ensembles, and Bayesian Neural Networks in sharpness, calibration, and runtime. By recasting vine copula models as computational graphs, our work connects classical dependence modeling with modern deep-learning toolchains and facilitates the integration of state-of-the-art copula methods in modern machine learning pipelines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Multimodal Distillation for Few-shot Action Recognition</title>
<link>https://arxiv.org/abs/2506.13322</link>
<guid>https://arxiv.org/abs/2506.13322</guid>
<content:encoded><![CDATA[
arXiv:2506.13322v1 Announce Type: cross 
Abstract: Owing to its rapid progress and broad application prospects, few-shot action recognition has attracted considerable interest. However, current methods are predominantly based on limited single-modal data, which does not fully exploit the potential of multimodal information. This paper presents a novel framework that actively identifies reliable modalities for each sample using task-specific contextual cues, thus significantly improving recognition performance. Our framework integrates an Active Sample Inference (ASI) module, which utilizes active inference to predict reliable modalities based on posterior distributions and subsequently organizes them accordingly. Unlike reinforcement learning, active inference replaces rewards with evidence-based preferences, making more stable predictions. Additionally, we introduce an active mutual distillation module that enhances the representation learning of less reliable modalities by transferring knowledge from more reliable ones. Adaptive multimodal inference is employed during the meta-test to assign higher weights to reliable modalities. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tady: A Neural Disassembler without Structural Constraint Violations</title>
<link>https://arxiv.org/abs/2506.13323</link>
<guid>https://arxiv.org/abs/2506.13323</guid>
<content:encoded><![CDATA[
arXiv:2506.13323v1 Announce Type: cross 
Abstract: Disassembly is a crucial yet challenging step in binary analysis. While emerging neural disassemblers show promise for efficiency and accuracy, they frequently generate outputs violating fundamental structural constraints, which significantly compromise their practical usability. To address this critical problem, we regularize the disassembly solution space by formalizing and applying key structural constraints based on post-dominance relations. This approach systematically detects widespread errors in existing neural disassemblers' outputs. These errors often originate from models' limited context modeling and instruction-level decoding that neglect global structural integrity. We introduce Tady, a novel neural disassembler featuring an improved model architecture and a dedicated post-processing algorithm, specifically engineered to address these deficiencies. Comprehensive evaluations on diverse binaries demonstrate that Tady effectively eliminates structural constraint violations and functions with high efficiency, while maintaining instruction-level accuracy.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2506.13344</link>
<guid>https://arxiv.org/abs/2506.13344</guid>
<content:encoded><![CDATA[
arXiv:2506.13344v1 Announce Type: cross 
Abstract: Generating high-fidelity and biologically plausible synthetic single-cell RNA sequencing (scRNA-seq) data, especially with conditional control, is challenging due to its high dimensionality, sparsity, and complex biological variations. Existing generative models often struggle to capture these unique characteristics and ensure robustness to structural noise in cellular networks. We introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model for robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates graph-based representations with a score-based diffusion model, enhanced by a novel spectral adversarial perturbation mechanism on graph edge weights. Our contributions are threefold: we leverage Laplacian Positional Encodings (LPEs) to enrich the latent space with crucial cellular relationship information; we develop a conditional score-based diffusion model for effective learning and generation from complex scRNA-seq distributions; and we employ a unique spectral adversarial training scheme on graph edge weights, boosting robustness against structural variations. Extensive experiments on diverse scRNA-seq datasets demonstrate LapDDPM's superior performance, achieving high fidelity and generating biologically-plausible, cell-type-specific samples. LapDDPM sets a new benchmark for conditional scRNA-seq data generation, offering a robust tool for various downstream biological applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks</title>
<link>https://arxiv.org/abs/2506.13351</link>
<guid>https://arxiv.org/abs/2506.13351</guid>
<content:encoded><![CDATA[
arXiv:2506.13351v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, we propose Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, we introduce a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. We evaluate DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns</title>
<link>https://arxiv.org/abs/2506.13356</link>
<guid>https://arxiv.org/abs/2506.13356</guid>
<content:encoded><![CDATA[
arXiv:2506.13356v1 Announce Type: cross 
Abstract: Long-term memory (LTM) is essential for large language models (LLMs) to achieve autonomous intelligence in complex, evolving environments. Despite increasing efforts in memory-augmented and retrieval-based architectures, there remains a lack of standardized benchmarks to systematically evaluate LLMs' long-term memory abilities. Existing benchmarks still face challenges in evaluating knowledge retention and dynamic sequential reasoning, and in their own flexibility, all of which limit their effectiveness in assessing models' LTM capabilities. To address these gaps, we propose a novel benchmark framework based on interactive fiction games, featuring dynamically branching storylines with complex reasoning structures. These structures simulate real-world scenarios by requiring LLMs to navigate hierarchical decision trees, where each choice triggers cascading dependencies across multi-turn interactions. Our benchmark emphasizes two distinct settings to test reasoning complexity: one with immediate feedback upon incorrect decisions, and the other requiring models to independently trace back and revise earlier choices after failure. As part of this benchmark, we also construct a new dataset designed to test LLMs' LTM within narrative-driven environments. We further validate the effectiveness of our approach through detailed experiments. Experimental results demonstrate the benchmark's ability to robustly and reliably assess LTM in LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating loss of variance in ensemble data assimilation: machine learning-based and distance-free localizations for better covariance estimation</title>
<link>https://arxiv.org/abs/2506.13362</link>
<guid>https://arxiv.org/abs/2506.13362</guid>
<content:encoded><![CDATA[
arXiv:2506.13362v1 Announce Type: cross 
Abstract: We propose two new methods based/inspired by machine learning for tabular data and distance-free localization to enhance the covariance estimations in an ensemble data assimilation. The main goal is to enhance the data assimilation results by mitigating loss of variance due to sampling errors. We also analyze the suitability of several machine learning models and the balance between accuracy and computational cost of the covariance estimations. We introduce two distance-free localization techniques leveraging machine learning methods specifically tailored for tabular data. The methods are integrated into the Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The results show that the proposed localizations improve covariance accuracy and enhance data assimilation and uncertainty quantification results. We observe reduced variance loss for the input variables using the proposed methods. Furthermore, we compare several machine learning models, assessing their suitability for the problem in terms of computational cost, and quality of the covariance estimation and data match. The influence of ensemble size is also investigated, providing insights into balancing accuracy and computational efficiency. Our findings demonstrate that certain machine learning models are more suitable for this problem. This study introduces two novel methods that mitigate variance loss for model parameters in ensemble-based data assimilation, offering practical solutions that are easy to implement and do not require any additional numerical simulation or hyperparameter tuning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALM: Consensus-Aware Localized Merging for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2506.13406</link>
<guid>https://arxiv.org/abs/2506.13406</guid>
<content:encoded><![CDATA[
arXiv:2506.13406v1 Announce Type: cross 
Abstract: Model merging aims to integrate the strengths of multiple fine-tuned models into a unified model while preserving task-specific capabilities. Existing methods, represented by task arithmetic, are typically classified into global- and local-aware methods. However, global-aware methods inevitably cause parameter interference, while local-aware methods struggle to maintain the effectiveness of task-specific details in the merged model. To address these limitations, we propose a Consensus-Aware Localized Merging (CALM) method which incorporates localized information aligned with global task consensus, ensuring its effectiveness post-merging. CALM consists of three key components: (1) class-balanced entropy minimization sampling, providing a more flexible and reliable way to leverage unsupervised data; (2) an efficient-aware framework, selecting a small set of tasks for sequential merging with high scalability; (3) a consensus-aware mask optimization, aligning localized binary masks with global task consensus and merging them conflict-free. Experiments demonstrate the superiority and robustness of our CALM, significantly outperforming existing methods and achieving performance close to traditional MTL.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple is what you need for efficient and accurate medical image segmentation</title>
<link>https://arxiv.org/abs/2506.13415</link>
<guid>https://arxiv.org/abs/2506.13415</guid>
<content:encoded><![CDATA[
arXiv:2506.13415v1 Announce Type: cross 
Abstract: While modern segmentation models often prioritize performance over practicality, we advocate a design philosophy prioritizing simplicity and efficiency, and attempted high performance segmentation model design. This paper presents SimpleUNet, a scalable ultra-lightweight medical image segmentation model with three key innovations: (1) A partial feature selection mechanism in skip connections for redundancy reduction while enhancing segmentation performance; (2) A fixed-width architecture that prevents exponential parameter growth across network stages; (3) An adaptive feature fusion module achieving enhanced representation with minimal computational overhead. With a record-breaking 16 KB parameter configuration, SimpleUNet outperforms LBUNet and other lightweight benchmarks across multiple public datasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and accuracy, attaining a mean DSC/IoU of 85.76%/75.60% on multi-center breast lesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin lesion datasets (ISIC 2017/2018: mDice 84.86%/88.77%) and endoscopic polyp segmentation (KVASIR-SEG: 86.46%/76.48% mDice/mIoU) confirm consistent dominance over state-of-the-art models. This work demonstrates that extreme model compression need not compromise performance, providing new insights for efficient and accurate medical image segmentation. Codes can be found at https://github.com/Frankyu5666666/SimpleUNet.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Model for Word Repetition</title>
<link>https://arxiv.org/abs/2506.13450</link>
<guid>https://arxiv.org/abs/2506.13450</guid>
<content:encoded><![CDATA[
arXiv:2506.13450v1 Announce Type: cross 
Abstract: It takes several years for the developing brain of a baby to fully master word repetition-the task of hearing a word and repeating it aloud. Repeating a new word, such as from a new language, can be a challenging task also for adults. Additionally, brain damage, such as from a stroke, may lead to systematic speech errors with specific characteristics dependent on the location of the brain damage. Cognitive sciences suggest a model with various components for the different processing stages involved in word repetition. While some studies have begun to localize the corresponding regions in the brain, the neural mechanisms and how exactly the brain performs word repetition remain largely unknown. We propose to bridge the gap between the cognitive model of word repetition and neural mechanisms in the human brain by modeling the task using deep neural networks. Neural models are fully observable, allowing us to study the detailed mechanisms in their various substructures and make comparisons with human behavior and, ultimately, the brain. Here, we make first steps in this direction by: (1) training a large set of models to simulate the word repetition task; (2) creating a battery of tests to probe the models for known effects from behavioral studies in humans, and (3) simulating brain damage through ablation studies, where we systematically remove neurons from the model, and repeat the behavioral study to examine the resulting speech errors in the "patient" model. Our results show that neural models can mimic several effects known from human research, but might diverge in other aspects, highlighting both the potential and the challenges for future research aimed at developing human-like neural models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics</title>
<link>https://arxiv.org/abs/2506.13453</link>
<guid>https://arxiv.org/abs/2506.13453</guid>
<content:encoded><![CDATA[
arXiv:2506.13453v1 Announce Type: cross 
Abstract: The self-organization of robots for the formation of structures and shapes is a stimulating application of the swarm robotic system. It involves a large number of autonomous robots of heterogeneous behavior, coordination among them, and their interaction with the dynamic environment. This process of complex structure formation is considered a complex system, which needs to be modeled by using any modeling approach. Although the formal specification approach along with other formal methods has been used to model the behavior of robots in a swarm. However, to the best of our knowledge, the formal specification approach has not been used to model the self-organization process in swarm robotic systems for shape formation. In this paper, we use a formal specification approach to model the shape formation task of swarm robots. We use Z (Zed) language of formal specification, which is a state-based language, to model the states of the entities of the systems. We demonstrate the effectiveness of Z for the self-organized shape formation. The presented formal specification model gives the outlines for designing and implementing the swarm robotic system for the formation of complex shapes and structures. It also provides the foundation for modeling the complex shape formation process for swarm robotics using a multi-agent system in a simulation-based environment. Keywords: Swarm robotics, Self-organization, Formal specification, Complex systems
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study</title>
<link>https://arxiv.org/abs/2506.13464</link>
<guid>https://arxiv.org/abs/2506.13464</guid>
<content:encoded><![CDATA[
arXiv:2506.13464v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interdisciplinary Approach to Human-Centered Machine Translation</title>
<link>https://arxiv.org/abs/2506.13468</link>
<guid>https://arxiv.org/abs/2506.13468</guid>
<content:encoded><![CDATA[
arXiv:2506.13468v1 Announce Type: cross 
Abstract: Machine Translation (MT) tools are widely used today, often in contexts where professional translators are not present. Despite progress in MT technology, a gap persists between system development and real-world usage, particularly for non-expert users who may struggle to assess translation reliability. This paper advocates for a human-centered approach to MT, emphasizing the alignment of system design with diverse communicative goals and contexts of use. We survey the literature in Translation Studies and Human-Computer Interaction to recontextualize MT evaluation and design to address the diverse real-world scenarios in which MT is used today.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-stage Optimization Method for Wide-range Single-electron Quantum Magnetic Sensing</title>
<link>https://arxiv.org/abs/2506.13469</link>
<guid>https://arxiv.org/abs/2506.13469</guid>
<content:encoded><![CDATA[
arXiv:2506.13469v1 Announce Type: cross 
Abstract: Quantum magnetic sensing based on spin systems has emerged as a new paradigm for detecting ultra-weak magnetic fields with unprecedented sensitivity, revitalizing applications in navigation, geo-localization, biology, and beyond. At the heart of quantum magnetic sensing, from the protocol perspective, lies the design of optimal sensing parameters to manifest and then estimate the underlying signals of interest (SoI). Existing studies on this front mainly rely on adaptive algorithms based on black-box AI models or formula-driven principled searches. However, when the SoI spans a wide range and the quantum sensor has physical constraints, these methods may fail to converge efficiently or optimally, resulting in prolonged interrogation times and reduced sensing accuracy. In this work, we report the design of a new protocol using a two-stage optimization method. In the 1st Stage, a Bayesian neural network with a fixed set of sensing parameters is used to narrow the range of SoI. In the 2nd Stage, a federated reinforcement learning agent is designed to fine-tune the sensing parameters within a reduced search space. The proposed protocol is developed and evaluated in a challenging context of single-shot readout of an NV-center electron spin under a constrained total sensing time budget; and yet it achieves significant improvements in both accuracy and resource efficiency for wide-range D.C. magnetic field estimation compared to the state of the art.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models</title>
<link>https://arxiv.org/abs/2506.13472</link>
<guid>https://arxiv.org/abs/2506.13472</guid>
<content:encoded><![CDATA[
arXiv:2506.13472v1 Announce Type: cross 
Abstract: Quantization has been widely studied as an effective technique for reducing the memory requirement of large language models (LLMs), potentially improving the latency time as well. Utilizing the characteristic of rotational invariance of transformer, we propose the rotation-based saliency-aware weight quantization (ROSAQ), which identifies salient channels in the projection feature space, not in the original feature space, where the projected "principal" dimensions are naturally considered as "salient" features. The proposed ROSAQ consists of 1) PCA-based projection, which first performs principal component analysis (PCA) on a calibration set and transforms via the PCA projection, 2) Salient channel dentification, which selects dimensions corresponding to the K-largest eigenvalues as salient channels, and 3) Saliency-aware quantization with mixed-precision, which uses FP16 for salient dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ shows improvements over the baseline saliency-aware quantization on the original feature space and other existing quantization methods. With kernel fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in generating 256 tokens with a batch size of 64.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13474</link>
<guid>https://arxiv.org/abs/2506.13474</guid>
<content:encoded><![CDATA[
arXiv:2506.13474v1 Announce Type: cross 
Abstract: Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited "out-of-the-box" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection</title>
<link>https://arxiv.org/abs/2506.13476</link>
<guid>https://arxiv.org/abs/2506.13476</guid>
<content:encoded><![CDATA[
arXiv:2506.13476v1 Announce Type: cross 
Abstract: Printed Circuit Boards (PCBs) are critical components in modern electronics, which require stringent quality control to ensure proper functionality. However, the detection of defects in small-scale PCBs images poses significant challenges as a result of the low resolution of the captured images, leading to potential confusion between defects and noise. To overcome these challenges, this paper proposes a novel framework, named ESRPCB (edgeguided super-resolution for PCBs defect detection), which combines edgeguided super-resolution with ensemble learning to enhance PCBs defect detection. The framework leverages the edge information to guide the EDSR (Enhanced Deep Super-Resolution) model with a novel ResCat (Residual Concatenation) structure, enabling it to reconstruct high-resolution images from small PCBs inputs. By incorporating edge features, the super-resolution process preserves critical structural details, ensuring that tiny defects remain distinguishable in the enhanced image. Following this, a multi-modal defect detection model employs ensemble learning to analyze the super-resolved
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness</title>
<link>https://arxiv.org/abs/2506.13479</link>
<guid>https://arxiv.org/abs/2506.13479</guid>
<content:encoded><![CDATA[
arXiv:2506.13479v1 Announce Type: cross 
Abstract: Merging or routing low-rank adapters (LoRAs) has emerged as a popular solution for enhancing large language models, particularly when data access is restricted by regulatory or domain-specific constraints. This position paper argues that the research community should shift its focus from developing new merging or routing algorithms to understanding the conditions under which reusing LoRAs is truly effective. Through theoretical analysis and synthetic two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs enables genuine compositional generalization or merely reflects shallow pattern matching. Evaluating two data-agnostic methods--parameter averaging and dynamic adapter selection--we found that reusing LoRAs often fails to logically integrate knowledge across disjoint fine-tuning datasets, especially when such knowledge is underrepresented during pretraining. Our empirical results, supported by theoretical insights into LoRA's limited expressiveness, highlight the preconditions and constraints of reusing them for unseen tasks and cast doubt on its feasibility as a truly data-free approach. We advocate for pausing the pursuit of novel methods for recycling LoRAs and emphasize the need for rigorous mechanisms to guide future academic research in adapter-based model merging and practical system designs for practitioners.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data</title>
<link>https://arxiv.org/abs/2506.13505</link>
<guid>https://arxiv.org/abs/2506.13505</guid>
<content:encoded><![CDATA[
arXiv:2506.13505v1 Announce Type: cross 
Abstract: The mining sector increasingly adopts digital tools to improve operational efficiency, safety, and data-driven decision-making. One of the key challenges remains the reliable acquisition of high-resolution, geo-referenced spatial information to support core activities such as extraction planning and on-site monitoring. This work presents an integrated system architecture that combines UAV-based sensing, LiDAR terrain modeling, and deep learning-based object detection to generate spatially accurate information for open-pit mining environments. The proposed pipeline includes geo-referencing, 3D reconstruction, and object localization, enabling structured spatial outputs to be integrated into an industrial digital twin platform. Unlike traditional static surveying methods, the system offers higher coverage and automation potential, with modular components suitable for deployment in real-world industrial contexts. While the current implementation operates in post-flight batch mode, it lays the foundation for real-time extensions. The system contributes to the development of AI-enhanced remote sensing in mining by demonstrating a scalable and field-validated geospatial data workflow that supports situational awareness and infrastructure safety.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in Equivariant Tensor Products</title>
<link>https://arxiv.org/abs/2506.13523</link>
<guid>https://arxiv.org/abs/2506.13523</guid>
<content:encoded><![CDATA[
arXiv:2506.13523v1 Announce Type: cross 
Abstract: $E(3)$-equivariant neural networks have demonstrated success across a wide range of 3D modelling tasks. A fundamental operation in these networks is the tensor product, which interacts two geometric features in an equivariant manner to create new features. Due to the high computational complexity of the tensor product, significant effort has been invested to optimize the runtime of this operation. For example, Luo et al. (2024) recently proposed the Gaunt tensor product (GTP) which promises a significant speedup. In this work, we provide a careful, systematic analysis of a number of tensor product operations. In particular, we emphasize that different tensor products are not performing the same operation. The reported speedups typically come at the cost of expressivity. We introduce measures of expressivity and interactability to characterize these differences. In addition, we realized the original implementation of GTP can be greatly simplified by directly using a spherical grid at no cost in asymptotic runtime. This spherical grid approach is faster on our benchmarks and in actual training of the MACE interatomic potential by 30\%. Finally, we provide the first systematic microbenchmarks of the various tensor product operations. We find that the theoretical runtime guarantees can differ wildly from empirical performance, demonstrating the need for careful application-specific benchmarking. Code is available at \href{https://github.com/atomicarchitects/PriceofFreedom}{https://github.com/atomicarchitects/PriceofFreedom}
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seismic Acoustic Impedance Inversion Framework Based on Conditional Latent Generative Diffusion Model</title>
<link>https://arxiv.org/abs/2506.13529</link>
<guid>https://arxiv.org/abs/2506.13529</guid>
<content:encoded><![CDATA[
arXiv:2506.13529v1 Announce Type: cross 
Abstract: Seismic acoustic impedance plays a crucial role in lithological identification and subsurface structure interpretation. However, due to the inherently ill-posed nature of the inversion problem, directly estimating impedance from post-stack seismic data remains highly challenging. Recently, diffusion models have shown great potential in addressing such inverse problems due to their strong prior learning and generative capabilities. Nevertheless, most existing methods operate in the pixel domain and require multiple iterations, limiting their applicability to field data. To alleviate these limitations, we propose a novel seismic acoustic impedance inversion framework based on a conditional latent generative diffusion model, where the inversion process is made in latent space. To avoid introducing additional training overhead when embedding conditional inputs, we design a lightweight wavelet-based module into the framework to project seismic data and reuse an encoder trained on impedance to embed low-frequency impedance into the latent space. Furthermore, we propose a model-driven sampling strategy during the inversion process of this framework to enhance accuracy and reduce the number of required diffusion steps. Numerical experiments on a synthetic model demonstrate that the proposed method achieves high inversion accuracy and strong generalization capability within only a few diffusion steps. Moreover, application to field data reveals enhanced geological detail and higher consistency with well-log measurements, validating the effectiveness and practicality of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understand the Implication: Learning to Think for Pragmatic Understanding</title>
<link>https://arxiv.org/abs/2506.13559</link>
<guid>https://arxiv.org/abs/2506.13559</guid>
<content:encoded><![CDATA[
arXiv:2506.13559v1 Announce Type: cross 
Abstract: Pragmatics, the ability to infer meaning beyond literal interpretation, is crucial for social cognition and communication. While LLMs have been benchmarked for their pragmatic understanding, improving their performance remains underexplored. Existing methods rely on annotated labels but overlook the reasoning process humans naturally use to interpret implicit meaning. To bridge this gap, we introduce a novel pragmatic dataset, ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both correct and incorrect interpretations. Through preference-tuning and supervised fine-tuning, we demonstrate that thought-based learning significantly enhances LLMs' pragmatic understanding, improving accuracy by 11.12% across model families. We further discuss a transfer-learning study where we evaluate the performance of thought-based training for the other tasks of pragmatics (presupposition, deixis) that are not seen during the training time and observe an improvement of 16.10% compared to label-trained models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints</title>
<link>https://arxiv.org/abs/2506.13566</link>
<guid>https://arxiv.org/abs/2506.13566</guid>
<content:encoded><![CDATA[
arXiv:2506.13566v1 Announce Type: cross 
Abstract: The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing makespan under deterministic constraints. Real-world production environments introduce additional complexities that cause traditional scheduling approaches to be less effective. Reinforcement learning (RL) holds potential in addressing these challenges, as it allows agents to learn adaptive scheduling strategies. However, there is a lack of a comprehensive, general-purpose frameworks for effectively training and evaluating RL agents under real-world constraints. To address this gap, we propose a modular framework that extends classical JSSP formulations by incorporating key \mbox{real-world} constraints inherent to the shopfloor, including transport logistics, buffer management, machine breakdowns, setup times, and stochastic processing conditions, while also supporting multi-objective optimization. The framework is a customizable solution that offers flexibility in defining problem instances and configuring simulation parameters, enabling adaptation to diverse production scenarios. A standardized interface ensures compatibility with various RL approaches, providing a robust environment for training RL agents and facilitating the standardized comparison of different scheduling methods under dynamic and uncertain conditions. We release JobShopLab as an open-source tool for both research and industrial applications, accessible at: https://github.com/proto-lab-ro/jobshoplab
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible-length Text Infilling for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2506.13579</link>
<guid>https://arxiv.org/abs/2506.13579</guid>
<content:encoded><![CDATA[
arXiv:2506.13579v1 Announce Type: cross 
Abstract: Discrete diffusion models are a new class of text generators that offer advantages such as bidirectional context use, parallelizable generation, and flexible prompting compared to autoregressive models. However, a critical limitation of discrete diffusion models is their inability to perform flexible-length or flexible-position text infilling without access to ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete \textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling), the first discrete diffusion model to overcome this challenge. DDOT jointly denoises token values and token positions, employing a novel sample-level Optimal Transport (OT) coupling. This coupling preserves relative token ordering while dynamically adjusting the positions and length of infilled segments, a capability previously missing in text diffusion. Our method is orthogonal to existing discrete text diffusion methods and is compatible with various pretrained text denoisers. Extensive experiments on text infilling benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms naive diffusion baselines. Furthermore, DDOT achieves performance on par with state-of-the-art non-autoregressive models and enables significant improvements in training efficiency and flexibility.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes</title>
<link>https://arxiv.org/abs/2506.13583</link>
<guid>https://arxiv.org/abs/2506.13583</guid>
<content:encoded><![CDATA[
arXiv:2506.13583v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) agents often exhibit learning behaviors that are not intuitively interpretable by human observers, which can result in suboptimal feedback in collaborative teaching settings. Yet, how humans perceive and interpret RL agent's learning behavior is largely unknown. In a bottom-up approach with two experiments, this work provides a data-driven understanding of the factors of human observers' understanding of the agent's learning process. A novel, observation-based paradigm to directly assess human inferences about agent learning was developed. In an exploratory interview study (\textit{N}=9), we identify four core themes in human interpretations: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second confirmatory study (\textit{N}=34) applied an expanded version of the paradigm across two tasks (navigation/manipulation) and two RL algorithms (tabular/function approximation). Analyses of 816 responses confirmed the reliability of the paradigm and refined the thematic framework, revealing how these themes evolve over time and interrelate. Our findings provide a human-centered understanding of how people make sense of agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation</title>
<link>https://arxiv.org/abs/2506.13599</link>
<guid>https://arxiv.org/abs/2506.13599</guid>
<content:encoded><![CDATA[
arXiv:2506.13599v1 Announce Type: cross 
Abstract: Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \textbf{C}ityGPT-Powered \textbf{A}gentic framework for \textbf{M}obility \textbf{S}imulation (\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \textbf{CAMS} generates more realistic and plausible trajectories. In general, \textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Artificial Intelligence Method for Estimating Flicker in Power Systems</title>
<link>https://arxiv.org/abs/2506.13611</link>
<guid>https://arxiv.org/abs/2506.13611</guid>
<content:encoded><![CDATA[
arXiv:2506.13611v1 Announce Type: cross 
Abstract: This paper introduces a novel hybrid AI method combining H filtering and an adaptive linear neuron network for flicker component estimation in power distribution systems.The proposed method leverages the robustness of the H filter to extract the voltage envelope under uncertain and noisy conditions followed by the use of ADALINE to accurately identify flicker frequencies embedded in the envelope.This synergy enables efficient time domain estimation with rapid convergence and noise resilience addressing key limitations of existing frequency domain approaches.Unlike conventional techniques this hybrid AI model handles complex power disturbances without prior knowledge of noise characteristics or extensive training.To validate the method performance we conduct simulation studies based on IEC Standard 61000 4 15 supported by statistical analysis Monte Carlo simulations and real world data.Results demonstrate superior accuracy robustness and reduced computational load compared to Fast Fourier Transform and Discrete Wavelet Transform based estimators.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning</title>
<link>https://arxiv.org/abs/2506.13612</link>
<guid>https://arxiv.org/abs/2506.13612</guid>
<content:encoded><![CDATA[
arXiv:2506.13612v1 Announce Type: cross 
Abstract: Despite federated learning (FL)'s potential in collaborative learning, its performance has deteriorated due to the data heterogeneity of distributed users. Recently, clustered federated learning (CFL) has emerged to address this challenge by partitioning users into clusters according to their similarity. However, CFL faces difficulties in training when users are unwilling to share their cluster identities due to privacy concerns. To address these issues, we present an innovative Efficient and Robust Secure Aggregation scheme for CFL, dubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while maintaining users' cluster identity confidentially. Moreover, it detects potential poisonous attacks without compromising individual client gradients by discarding negatively correlated gradients and aggregating positively correlated ones using a weighted approach. The server also authenticates correct gradient encoding by clients. EBS-CFL has high efficiency with client-side overhead O(ml + m^2) for communication and O(m^2l) for computation, where m is the number of cluster identities, and l is the gradient size. When m = 1, EBS-CFL's computational efficiency of client is at least O(log n) times better than comparison schemes, where n is the number of clients.In addition, we validate the scheme through extensive experiments. Finally, we theoretically prove the scheme's security.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Convolution-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation</title>
<link>https://arxiv.org/abs/2506.13628</link>
<guid>https://arxiv.org/abs/2506.13628</guid>
<content:encoded><![CDATA[
arXiv:2506.13628v1 Announce Type: cross 
Abstract: Synthetic data generation plays a crucial role in medical research by mitigating privacy concerns and enabling large-scale patient data analysis. This study presents a beta-Variational Autoencoder Graph Convolutional Neural Network framework for generating synthetic Abdominal Aorta Aneurysms (AAA). Using a small real-world dataset, our approach extracts key anatomical features and captures complex statistical relationships within a compact disentangled latent space. To address data limitations, low-impact data augmentation based on Procrustes analysis was employed, preserving anatomical integrity. The generation strategies, both deterministic and stochastic, manage to enhance data diversity while ensuring realism. Compared to PCA-based approaches, our model performs more robustly on unseen data by capturing complex, nonlinear anatomical variations. This enables more comprehensive clinical and statistical analyses than the original dataset alone. The resulting synthetic AAA dataset preserves patient privacy while providing a scalable foundation for medical research, device testing, and computational modeling.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.13638</link>
<guid>https://arxiv.org/abs/2506.13638</guid>
<content:encoded><![CDATA[
arXiv:2506.13638v1 Announce Type: cross 
Abstract: Model editing aims to efficiently update a pre-trained model's knowledge without the need for time-consuming full retraining. While existing pioneering editing methods achieve promising results, they primarily focus on editing single-modal language models (LLMs). However, for vision-language models (VLMs), which involve multiple modalities, the role and impact of each modality on editing performance remain largely unexplored. To address this gap, we explore the impact of textual and visual modalities on model editing and find that: (1) textual and visual representations reach peak sensitivity at different layers, reflecting their varying importance; and (2) editing both modalities can efficiently update knowledge, but this comes at the cost of compromising the model's original capabilities. Based on our findings, we propose DualEdit, an editor that modifies both textual and visual modalities at their respective key layers. Additionally, we introduce a gating module within the more sensitive textual modality, allowing DualEdit to efficiently update new knowledge while preserving the model's original information. We evaluate DualEdit across multiple VLM backbones and benchmark datasets, demonstrating its superiority over state-of-the-art VLM editing baselines as well as adapted LLM editing methods on different evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning</title>
<link>https://arxiv.org/abs/2506.13654</link>
<guid>https://arxiv.org/abs/2506.13654</guid>
<content:encoded><![CDATA[
arXiv:2506.13654v1 Announce Type: cross 
Abstract: We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems</title>
<link>https://arxiv.org/abs/2506.13666</link>
<guid>https://arxiv.org/abs/2506.13666</guid>
<content:encoded><![CDATA[
arXiv:2506.13666v1 Announce Type: cross 
Abstract: The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at https://github.com/littlelittlenine/SafeMCP.git.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent Prefix Data</title>
<link>https://arxiv.org/abs/2506.13674</link>
<guid>https://arxiv.org/abs/2506.13674</guid>
<content:encoded><![CDATA[
arXiv:2506.13674v1 Announce Type: cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between input and prefix significance within the attention head. This motivates us to introduce Prefix-Tuning+, a novel architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself. We further provide an overview of our construction process to guide future users when constructing their own context-based methods. Our experiments show that, across a diverse set of benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning methods. Notably, it achieves performance on par with the widely adopted LoRA method on several general benchmarks, highlighting the potential modern extension of Prefix-Tuning approaches. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSA: Harnessing Robot States for Vision-Language and Action Alignment</title>
<link>https://arxiv.org/abs/2506.13679</link>
<guid>https://arxiv.org/abs/2506.13679</guid>
<content:encoded><![CDATA[
arXiv:2506.13679v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A fundamental challenge in developing such models is effectively aligning the vision-language space with the robotic action space. Existing approaches typically rely on directly fine-tuning VLMs using expert demonstrations. However, this strategy suffers from a spatio-temporal gap, resulting in considerable data inefficiency and heavy reliance on human labor. Spatially, VLMs operate within a high-level semantic space, whereas robotic actions are grounded in low-level 3D physical space; temporally, VLMs primarily interpret the present, while VLA models anticipate future actions. To overcome these challenges, we propose a novel training paradigm, ROSA, which leverages robot state estimation to improve alignment between vision-language and action spaces. By integrating robot state estimation data obtained via an automated process, ROSA enables the VLA model to gain enhanced spatial understanding and self-awareness, thereby boosting performance and generalization. Extensive experiments in both simulated and real-world environments demonstrate the effectiveness of ROSA, particularly in low-data regimes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning how to Share Credit among Macro-Actions</title>
<link>https://arxiv.org/abs/2506.13690</link>
<guid>https://arxiv.org/abs/2506.13690</guid>
<content:encoded><![CDATA[
arXiv:2506.13690v1 Announce Type: cross 
Abstract: One proposed mechanism to improve exploration in reinforcement learning is through the use of macro-actions. Paradoxically though, in many scenarios the naive addition of macro-actions does not lead to better exploration, but rather the opposite. It has been argued that this was caused by adding non-useful macros and multiple works have focused on mechanisms to discover effectively environment-specific useful macros. In this work, we take a slightly different perspective. We argue that the difficulty stems from the trade-offs between reducing the average number of decisions per episode versus increasing the size of the action space. Namely, one typically treats each potential macro-action as independent and atomic, hence strictly increasing the search space and making typical exploration strategies inefficient. To address this problem we propose a novel regularization term that exploits the relationship between actions and macro-actions to improve the credit assignment mechanism by reducing the effective dimension of the action space and, therefore, improving exploration. The term relies on a similarity matrix that is meta-learned jointly with learning the desired policy. We empirically validate our strategy looking at macro-actions in Atari games, and the StreetFighter II environment. Our results show significant improvements over the Rainbow-DQN baseline in all environments. Additionally, we show that the macro-action similarity is transferable to related environments. We believe this work is a small but important step towards understanding how the similarity-imposed geometry on the action space can be exploited to improve credit assignment and exploration, therefore making learning more effective.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems</title>
<link>https://arxiv.org/abs/2506.13692</link>
<guid>https://arxiv.org/abs/2506.13692</guid>
<content:encoded><![CDATA[
arXiv:2506.13692v1 Announce Type: cross 
Abstract: With the advancement of large language models, many dialogue systems are now capable of providing reasonable and informative responses to patients' medical conditions. However, when patients consult their doctor, they may experience negative emotions due to the severity and urgency of their situation. If the model can provide appropriate comfort and empathy based on the patient's negative emotions while answering medical questions, it will likely offer a more reassuring experience during the medical consultation process. To address this issue, our paper explores the balance between knowledge sharing and emotional support in the healthcare dialogue process. We utilize a large language model to rewrite a real-world interactive medical dialogue dataset, generating patient queries with negative emotions and corresponding medical responses aimed at soothing the patient's emotions while addressing their concerns. The modified data serves to refine the latest large language models with various fine-tuning methods, enabling them to accurately provide sentences with both emotional reassurance and constructive suggestions in response to patients' questions. Compared to the original LLM model, our experimental results demonstrate that our methodology significantly enhances the model's ability to generate emotional responses while maintaining its original capability to provide accurate knowledge-based answers.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value-Free Policy Optimization via Reward Partitioning</title>
<link>https://arxiv.org/abs/2506.13702</link>
<guid>https://arxiv.org/abs/2506.13702</guid>
<content:encoded><![CDATA[
arXiv:2506.13702v1 Announce Type: cross 
Abstract: Single-trajectory reinforcement learning (RL) methods aim to optimize policies from datasets consisting of (prompt, response, reward) triplets, where scalar rewards are directly available. This supervision format is highly practical, as it mirrors real-world human feedback, such as thumbs-up/down signals, and avoids the need for structured preference annotations. In contrast, pairwise preference-based methods like Direct Preference Optimization (DPO) rely on datasets with both preferred and dispreferred responses, which are harder to construct and less natural to collect. Among single-trajectory approaches, Direct Reward Optimization (DRO) has shown strong empirical performance due to its simplicity and stability. However, DRO requires approximating a value function, which introduces several limitations: high off-policy variance, coupling between policy and value learning, and a lack of absolute supervision on the policy itself. We introduce Reward Partitioning Optimization (RPO), a new method that resolves these limitations by removing the need to model the value function. Instead, RPO normalizes observed rewards using a partitioning approach estimated directly from data. This leads to a straightforward supervised learning objective on the policy, with no auxiliary models and no joint optimization. RPO provides direct and stable supervision on the policy, making it robust and easy to implement in practice. We validate RPO on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder models. Our results demonstrate that RPO outperforms existing single-trajectory baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings confirm that RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.13705</link>
<guid>https://arxiv.org/abs/2506.13705</guid>
<content:encoded><![CDATA[
arXiv:2506.13705v1 Announce Type: cross 
Abstract: Time-series reasoning remains a significant challenge in multimodal large language models (MLLMs) due to the dynamic temporal patterns, ambiguous semantics, and lack of temporal priors. In this work, we introduce TimeMaster, a reinforcement learning (RL)-based method that enables time-series MLLMs to perform structured, interpretable reasoning directly over visualized time-series inputs and task prompts. TimeMaster adopts a three-part structured output format, reasoning, classification, and domain-specific extension, and is optimized via a composite reward function that aligns format adherence, prediction accuracy, and open-ended insight quality. The model is trained using a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish a good initialization, followed by Group Relative Policy Optimization (GRPO) at the token level to enable stable and targeted reward-driven improvement in time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster achieves state-of-the-art performance, outperforming both classical time-series models and few-shot GPT-4o by over 14.6% and 7.3% performance gain, respectively. Notably, TimeMaster goes beyond time-series classification: it also exhibits expert-like reasoning behavior, generates context-aware explanations, and delivers domain-aligned insights. Our results highlight that reward-driven RL can be a scalable and promising path toward integrating temporal understanding into time-series MLLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Self-Supervised Learning As Neural Manifold Packing</title>
<link>https://arxiv.org/abs/2506.13717</link>
<guid>https://arxiv.org/abs/2506.13717</guid>
<content:encoded><![CDATA[
arXiv:2506.13717v1 Announce Type: cross 
Abstract: Contrastive self-supervised learning based on point-wise comparisons has been widely studied for vision tasks. In the visual cortex of the brain, neuronal responses to distinct stimulus classes are organized into geometric structures known as neural manifolds. Accurate classification of stimuli can be achieved by effectively separating these manifolds, akin to solving a packing problem. We introduce Contrastive Learning As Manifold Packing (CLAMP), a self-supervised framework that recasts representation learning as a manifold packing problem. CLAMP introduces a loss function inspired by the potential energy of short-range repulsive particle systems, such as those encountered in the physics of simple liquids and jammed packings. In this framework, each class consists of sub-manifolds embedding multiple augmented views of a single image. The sizes and positions of the sub-manifolds are dynamically optimized by following the gradient of a packing loss. This approach yields interpretable dynamics in the embedding space that parallel jamming physics, and introduces geometrically meaningful hyperparameters within the loss function. Under the standard linear evaluation protocol, which freezes the backbone and trains only a linear classifier, CLAMP achieves competitive performance with state-of-the-art self-supervised models. Furthermore, our analysis reveals that neural manifolds corresponding to different categories emerge naturally and are effectively separated in the learned representation space, highlighting the potential of CLAMP to bridge insights from physics, neural science, and machine learning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs</title>
<link>https://arxiv.org/abs/2506.13727</link>
<guid>https://arxiv.org/abs/2506.13727</guid>
<content:encoded><![CDATA[
arXiv:2506.13727v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanditWare: A Contextual Bandit-based Framework for Hardware Prediction</title>
<link>https://arxiv.org/abs/2506.13730</link>
<guid>https://arxiv.org/abs/2506.13730</guid>
<content:encoded><![CDATA[
arXiv:2506.13730v1 Announce Type: cross 
Abstract: Distributed computing systems are essential for meeting the demands of modern applications, yet transitioning from single-system to distributed environments presents significant challenges. Misallocating resources in shared systems can lead to resource contention, system instability, degraded performance, priority inversion, inefficient utilization, increased latency, and environmental impact.
  We present BanditWare, an online recommendation system that dynamically selects the most suitable hardware for applications using a contextual multi-armed bandit algorithm. BanditWare balances exploration and exploitation, gradually refining its hardware recommendations based on observed application performance while continuing to explore potentially better options. Unlike traditional statistical and machine learning approaches that rely heavily on large historical datasets, BanditWare operates online, learning and adapting in real-time as new workloads arrive.
  We evaluated BanditWare on three workflow applications: Cycles (an agricultural science scientific workflow) BurnPro3D (a web-based platform for fire science) and a matrix multiplication application. Designed for seamless integration with the National Data Platform (NDP), BanditWare enables users of all experience levels to optimize resource allocation efficiently.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Following by Boosting Attention of Large Language Models</title>
<link>https://arxiv.org/abs/2506.13734</link>
<guid>https://arxiv.org/abs/2506.13734</guid>
<content:encoded><![CDATA[
arXiv:2506.13734v1 Announce Type: cross 
Abstract: Controlling the generation of large language models (LLMs) remains a central challenge to ensure their safe and reliable deployment. While prompt engineering and finetuning are common approaches, recent work has explored latent steering, a lightweight technique that alters LLM internal activations to guide generation. However, subsequent studies revealed latent steering's effectiveness to be limited, often underperforming simple instruction prompting. To address this limitation, we first establish a benchmark across diverse behaviors for standardized evaluation of steering techniques. Building on insights from this benchmark, we introduce Instruction Attention Boosting (InstABoost), a latent steering method that boosts the strength of instruction prompting by altering the model's attention during generation. InstABoost combines the strengths of existing approaches and is theoretically supported by prior work that suggests that in-context rule following in transformer-based models can be controlled by manipulating attention on instructions. Empirically, InstABoost demonstrates superior control success compared to both traditional prompting and latent steering.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability</title>
<link>https://arxiv.org/abs/2506.13746</link>
<guid>https://arxiv.org/abs/2506.13746</guid>
<content:encoded><![CDATA[
arXiv:2506.13746v1 Announce Type: cross 
Abstract: Phishing attacks remain one of the most prevalent and persistent cybersecurity threat with attackers continuously evolving and intensifying tactics to evade the general detection system. Despite significant advances in artificial intelligence and machine learning, faithfully reproducing the interpretable reasoning with classification and explainability that underpin phishing judgments remains challenging. Due to recent advancement in Natural Language Processing, Large Language Models (LLMs) show a promising direction and potential for improving domain specific phishing classification tasks. However, enhancing the reliability and robustness of classification models requires not only accurate predictions from LLMs but also consistent and trustworthy explanations aligning with those predictions. Therefore, a key question remains: can LLMs not only classify phishing emails accurately but also generate explanations that are reliably aligned with their predictions and internally self-consistent? To answer these questions, we have fine-tuned transformer based models, including BERT, Llama models, and Wizard, to improve domain relevance and make them more tailored to phishing specific distinctions, using Binary Sequence Classification, Contrastive Learning (CL) and Direct Preference Optimization (DPO). To that end, we examined their performance in phishing classification and explainability by applying the ConsistenCy measure based on SHAPley values (CC SHAP), which measures prediction explanation token alignment to test the model's internal faithfulness and consistency and uncover the rationale behind its predictions and reasoning. Overall, our findings show that Llama models exhibit stronger prediction explanation token alignment with higher CC SHAP scores despite lacking reliable decision making accuracy, whereas Wizard achieves better prediction accuracy but lower CC SHAP scores.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction</title>
<link>https://arxiv.org/abs/2506.13751</link>
<guid>https://arxiv.org/abs/2506.13751</guid>
<content:encoded><![CDATA[
arXiv:2506.13751v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action "vocabulary" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Thinking with Budget Guidance</title>
<link>https://arxiv.org/abs/2506.13752</link>
<guid>https://arxiv.org/abs/2506.13752</guid>
<content:encoded><![CDATA[
arXiv:2506.13752v1 Announce Type: cross 
Abstract: Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models</title>
<link>https://arxiv.org/abs/2506.13754</link>
<guid>https://arxiv.org/abs/2506.13754</guid>
<content:encoded><![CDATA[
arXiv:2506.13754v1 Announce Type: cross 
Abstract: We present a unified framework for solving partial differential equations (PDEs) using video-inpainting diffusion transformer models. Unlike existing methods that devise specialized strategies for either forward or inverse problems under full or partial observation, our approach unifies these tasks under a single, flexible generative framework. Specifically, we recast PDE-solving as a generalized inpainting problem, e.g., treating forward prediction as inferring missing spatiotemporal information of future states from initial conditions. To this end, we design a transformer-based architecture that conditions on arbitrary patterns of known data to infer missing values across time and space. Our method proposes pixel-space video diffusion models for fine-grained, high-fidelity inpainting and conditioning, while enhancing computational efficiency through hierarchical modeling. Extensive experiments show that our video inpainting-based diffusion model offers an accurate and versatile solution across a wide range of PDEs and problem setups, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
arXiv:2506.13759v1 Announce Type: cross 
Abstract: In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value</title>
<link>https://arxiv.org/abs/2506.13763</link>
<guid>https://arxiv.org/abs/2506.13763</guid>
<content:encoded><![CDATA[
arXiv:2506.13763v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in generative modeling. Despite more stable training, the loss of diffusion models is not indicative of absolute data-fitting quality, since its optimal value is typically not zero but unknown, leading to confusion between large optimal loss and insufficient model capacity. In this work, we advocate the need to estimate the optimal loss value for diagnosing and improving diffusion models. We first derive the optimal loss in closed form under a unified formulation of diffusion models, and develop effective estimators for it, including a stochastic variant scalable to large datasets with proper control of variance and bias. With this tool, we unlock the inherent metric for diagnosing the training quality of mainstream diffusion model variants, and develop a more performant training schedule based on the optimal loss. Moreover, using models with 120M to 1.5B parameters, we find that the power law is better demonstrated after subtracting the optimal loss from the actual training loss, suggesting a more principled setting for investigating the scaling law for diffusion models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A philosophical and ontological perspective on Artificial General Intelligence and the Metaverse</title>
<link>https://arxiv.org/abs/2402.06660</link>
<guid>https://arxiv.org/abs/2402.06660</guid>
<content:encoded><![CDATA[
arXiv:2402.06660v4 Announce Type: replace 
Abstract: This paper leverages various philosophical and ontological frameworks to explore the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. Several theoretical frameworks underpin this exploration, such as embodied cognition, Michael Levin's computational boundary of a "Self," and Donald D. Hoffman's Interface Theory of Perception, which lead to considering human perceived outer reality as a symbolic representation of alternate inner states of being, and where AGI could embody a different form of consciousness with a larger computational boundary. The paper further discusses the necessary architecture for the emergence of an embodied AGI, how to calibrate an AGI's symbolic interface, and the key role played by the Metaverse, decentralized systems and open-source blockchain technology. The paper concludes by emphasizing the importance of achieving a certain degree of harmony in human relations and recognizing the interconnectedness of humanity at a global level, as key prerequisites for the emergence of a stable embodied AGI.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roadmap on Incentive Compatibility for AI Alignment and Governance in Sociotechnical Systems</title>
<link>https://arxiv.org/abs/2402.12907</link>
<guid>https://arxiv.org/abs/2402.12907</guid>
<content:encoded><![CDATA[
arXiv:2402.12907v3 Announce Type: replace 
Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference</title>
<link>https://arxiv.org/abs/2406.15513</link>
<guid>https://arxiv.org/abs/2406.15513</guid>
<content:encoded><![CDATA[
arXiv:2406.15513v3 Announce Type: replace 
Abstract: In this study, we introduce the safety human preference dataset, PKU-SafeRLHF, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs. Data is available at https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can a Bayesian Oracle Prevent Harm from an Agent?</title>
<link>https://arxiv.org/abs/2408.05284</link>
<guid>https://arxiv.org/abs/2408.05284</guid>
<content:encoded><![CDATA[
arXiv:2408.05284v3 Announce Type: replace 
Abstract: Is there a way to design powerful AI systems based on machine learning methods that would satisfy probabilistic safety guarantees? With the long-term goal of obtaining a probabilistic guarantee that would apply in every context, we consider estimating a context-dependent bound on the probability of violating a given safety specification. Such a risk evaluation would need to be performed at run-time to provide a guardrail against dangerous actions of an AI. Noting that different plausible hypotheses about the world could produce very different outcomes, and because we do not know which one is right, we derive bounds on the safety violation probability predicted under the true but unknown hypothesis. Such bounds could be used to reject potentially dangerous actions. Our main results involve searching for cautious but plausible hypotheses, obtained by a maximization that involves Bayesian posteriors over hypotheses. We consider two forms of this result, in the i.i.d. case and in the non-i.i.d. case, and conclude with open problems towards turning such theoretical results into practical AI guardrails.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUnified Framework for Next-Gen Urban Forecasting via LLM-driven Dependency Retrieval and GeoTransformer</title>
<link>https://arxiv.org/abs/2408.08852</link>
<guid>https://arxiv.org/abs/2408.08852</guid>
<content:encoded><![CDATA[
arXiv:2408.08852v3 Announce Type: replace 
Abstract: Urban forecasting has increasingly benefited from high-dimensional spatial data through two primary approaches: graph-based methods that rely on predefined spatial structures, and region-based methods that focus on learning expressive urban representations. Although these methods have laid a strong foundation, they either rely heavily on structured spatial data, struggle to adapt to task-specific dependencies, or fail to integrate holistic urban context. Moreover, no existing framework systematically integrates these two paradigms and overcomes their respective limitations. To address this gap, we propose a novel, unified framework for high-dimensional urban forecasting, composed of three key components: (1) the Urban Region Representation Module that organizes latent embeddings and semantic descriptions for each region, (2) the Task-aware Dependency Retrieval module that selects relevant context regions based on natural language prompts, and (3) the Prediction Module, exemplified by our proposed GeoTransformer architecture, which adopts a novel geospatial attention mechanism to incorporate spatial proximity and information entropy as priors. Our framework is modular, supports diverse representation methods and forecasting models, and can operate even with minimal input. Quantitative experiments and qualitative analysis across six urban forecasting tasks demonstrate strong task generalization and validate the framework's effectiveness.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning</title>
<link>https://arxiv.org/abs/2410.01044</link>
<guid>https://arxiv.org/abs/2410.01044</guid>
<content:encoded><![CDATA[
arXiv:2410.01044v2 Announce Type: replace 
Abstract: The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stepwise Reasoning Error Disruption Attack of LLMs</title>
<link>https://arxiv.org/abs/2412.11934</link>
<guid>https://arxiv.org/abs/2412.11934</guid>
<content:encoded><![CDATA[
arXiv:2412.11934v5 Announce Type: replace 
Abstract: Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Think with Tables: Tabular Structures Enhance LLM Comprehension for Data-Analytics Requests</title>
<link>https://arxiv.org/abs/2412.17189</link>
<guid>https://arxiv.org/abs/2412.17189</guid>
<content:encoded><![CDATA[
arXiv:2412.17189v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) often struggle with data-analytics requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we introduce Thinking with Tables, where we inject tabular structures into LLMs for data-analytics requests. Through comprehensive evaluations across various request types, we show that providing tabular structures yields a 40.29 percent average performance gain along with better robustness and token efficiency. Through attention-value analysis, we uncover that tables help LLMs better attend to relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON, are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. These advantages remain consistent under increased task complexity and even when all input data cannot be structured. Finally, as data analytics typically relies on structured factual inputs, our text-to-table conversion demonstrates the method's applicability to text-compatible data sources.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Precise Computation of Shannon Entropy</title>
<link>https://arxiv.org/abs/2502.01160</link>
<guid>https://arxiv.org/abs/2502.01160</guid>
<content:encoded><![CDATA[
arXiv:2502.01160v2 Announce Type: replace 
Abstract: Quantitative information flow analyses (QIF) are a class of techniques for measuring the amount of confidential information leaked by a program to its public outputs. Shannon entropy is an important method to quantify the amount of leakage in QIF. This paper focuses on the programs modeled in Boolean constraints and optimizes the two stages of the Shannon entropy computation to implement a scalable precise tool PSE. In the first stage, we design a knowledge compilation language called \ADDAND that combines Algebraic Decision Diagrams and conjunctive decomposition. \ADDAND avoids enumerating possible outputs of a program and supports tractable entropy computation. In the second stage, we optimize the model counting queries that are used to compute the probabilities of outputs. We compare PSE with the state-of-the-art probabilistic approximately correct tool EntropyEstimation, which was shown to significantly outperform the previous precise tools. The experimental results demonstrate that PSE solved 56 more benchmarks compared to EntropyEstimation in a total of 459. For 98\% of the benchmarks that both PSE and EntropyEstimation solved, PSE is at least $10\times$ as efficient as EntropyEstimation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation and Interpretation in Artificial and Natural Computing</title>
<link>https://arxiv.org/abs/2502.10383</link>
<guid>https://arxiv.org/abs/2502.10383</guid>
<content:encoded><![CDATA[
arXiv:2502.10383v2 Announce Type: replace 
Abstract: Artificial computing machinery transforms representations through an objective process, to be interpreted subjectively by humans, so the machine and the interpreter are different entities, but in the putative natural computing both processes are performed by the same agent. The method or process that transforms a representation is called here the mode of computing. The mode used by digital computers is the algorithmic one, but there are others, such as quantum computers and diverse forms of non-conventional computing, and there is an open-ended set of representational formats and modes that could be used in artificial and natural computing. A mode based on a notion of computing different from Turing's may perform feats beyond what the Turing Machine does but the modes would not be of the same kind and could not be compared. For a mode of computing to be more powerful than the algorithmic one, it ought to compute functions lacking an effective algorithm, and Church Thesis would not hold. Here, a thought experiment including a computational demon using a hypothetical mode for such an effect is presented. If there is natural computing, there is a mode of natural computing whose properties may be causal to the phenomenological experience. Discovering it would come with solving the hard problem of consciousness; but if it turns out that such a mode does not exist, there is no such thing as natural computing, and the mind is not a computational process.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From System 1 to System 2: A Survey of Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2502.17419</link>
<guid>https://arxiv.org/abs/2502.17419</guid>
<content:encoded><![CDATA[
arXiv:2502.17419v5 Announce Type: replace 
Abstract: Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reference-Aligned Retrieval-Augmented Question Answering over Heterogeneous Proprietary Documents</title>
<link>https://arxiv.org/abs/2502.19596</link>
<guid>https://arxiv.org/abs/2502.19596</guid>
<content:encoded><![CDATA[
arXiv:2502.19596v3 Announce Type: replace 
Abstract: Proprietary corporate documents contain rich domain-specific knowledge, but their overwhelming volume and disorganized structure make it difficult even for employees to access the right information when needed. For example, in the automotive industry, vehicle crash-collision tests, each costing hundreds of thousands of dollars, produce highly detailed documentation. However, retrieving relevant content during decision-making remains time-consuming due to the scale and complexity of the material. While Retrieval-Augmented Generation (RAG)-based Question Answering (QA) systems offer a promising solution, building an internal RAG-QA system poses several challenges: (1) handling heterogeneous multi-modal data sources, (2) preserving data confidentiality, and (3) enabling traceability between each piece of information in the generated answer and its original source document. To address these, we propose a RAG-QA framework for internal enterprise use, consisting of: (1) a data pipeline that converts raw multi-modal documents into a structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving architecture, and (3) a lightweight reference matcher that links answer segments to supporting content. Applied to the automotive domain, our system improves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16), and helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale ratings from both human and LLM judge.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engineering Scientific Assistants using Interactive Structured Induction of Programs</title>
<link>https://arxiv.org/abs/2503.14488</link>
<guid>https://arxiv.org/abs/2503.14488</guid>
<content:encoded><![CDATA[
arXiv:2503.14488v2 Announce Type: replace 
Abstract: We are interested in the construction of software that can act as scientific assistants to domain specialists. It is expected that such assistants will be needed to accelerate the identification of ways to address complex problems requiring urgent solutions. In this paper, our focus is not on a specific scientific problem, but on the software-engineering of such 'science accelerators'. Recent developments in 'No Code' techniques would seem to suggest that scientist can simply hypothesise solutions simply by conversing with a large language model (LLM). However, for complex scientific problems, this seems unlikely given the current state of LLM technology. What does appear feasible is that a software engineer can use LLMs to rapidly construct programs for use by a domain-specialist, including the specialist's requirements expressed in natural language. We propose the design of an interactive form of 'structured' inductive programming in which a software-engineer and an LLM collaboratively construct an 'assistant' for a scientific data analysis. The paper describes a simple implementation called iStrucInd that adapts a '2-way Intelligibility' protocol to implement the interaction between the software engineer and the LLM. We test the tool on two different non-trivial scientific data analysis tasks. Specifically, we compare the system constructed by iStrucInd against systems constructed manually and by Low Code/No Code methods along dimensions of: (a) program performance; (b) program quality; and (c) programming effort. The results show iStrucInd allows a software engineer to develop better programs faster suggesting interactive structured induction can play a useful role in the rapid construction of scientific assistants.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordable AI Assistants with Knowledge Graph of Thoughts</title>
<link>https://arxiv.org/abs/2504.02670</link>
<guid>https://arxiv.org/abs/2504.02670</guid>
<content:encoded><![CDATA[
arXiv:2504.02670v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Requirements for Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope</title>
<link>https://arxiv.org/abs/2504.12497</link>
<guid>https://arxiv.org/abs/2504.12497</guid>
<content:encoded><![CDATA[
arXiv:2504.12497v2 Announce Type: replace 
Abstract: Regardless of past learning, an agent in an open world will face unfamiliar events outside of prior experience, existing models, or policies. Further, the agent will sometimes lack relevant knowledge and/or sufficient time to assess the situation and evaluate response options. How can an agent respond reasonably to situations that are outside of its original design scope? How can it recognize such situations sufficiently quickly and reliably to determine reasonable, adaptive courses of action? We identify key characteristics needed for solutions, review the state-of-the-art, and outline a proposed, novel approach that combines domain-general meta-knowledge (inspired by human cognition) and metareasoning. This approach offers potential for fast, adaptive responses to unfamiliar situations, more fully meeting the performance characteristics required for open-world, general agents.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Cognitive Design Patterns to General LLM Agents</title>
<link>https://arxiv.org/abs/2505.07087</link>
<guid>https://arxiv.org/abs/2505.07087</guid>
<content:encoded><![CDATA[
arXiv:2505.07087v2 Announce Type: replace 
Abstract: One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or "cognitive design patterns" that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanisms and representations available for exploring the possibilities of general intelligence. This paper outlines a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive ("agentic") use cases. Examining and applying these recurring patterns enables predictions of gaps or deficiencies in today's Agentic LLM Systems and identification of subjects of future research towards general intelligence using generative foundation models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations</title>
<link>https://arxiv.org/abs/2505.21318</link>
<guid>https://arxiv.org/abs/2505.21318</guid>
<content:encoded><![CDATA[
arXiv:2505.21318v2 Announce Type: replace 
Abstract: While large language models (LLMs) with Chain-of-Thought (CoT) reasoning excel in mathematics and coding, their potential for systematic reasoning in chemistry, a domain demanding rigorous structural analysis for real-world tasks like drug design and reaction engineering, remains untapped. Current benchmarks focus on simple knowledge retrieval, neglecting step-by-step reasoning required for complex tasks such as molecular optimization and reaction prediction. To address this, we introduce ChemCoTBench, a reasoning framework that bridges molecular structure understanding with arithmetic-inspired operations, including addition, deletion, and substitution, to formalize chemical problem-solving into transparent, step-by-step workflows. By treating molecular transformations as modular "chemical operations", the framework enables slow-thinking reasoning, mirroring the logic of mathematical proofs while grounding solutions in real-world chemical constraints. We evaluate models on two high-impact tasks: Molecular Property Optimization and Chemical Reaction Prediction. These tasks mirror real-world challenges while providing structured evaluability. By providing annotated datasets, a reasoning taxonomy, and baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning methods and practical chemical discovery, establishing a foundation for advancing LLMs as tools for AI-driven scientific innovation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding</title>
<link>https://arxiv.org/abs/2505.23990</link>
<guid>https://arxiv.org/abs/2505.23990</guid>
<content:encoded><![CDATA[
arXiv:2505.23990v2 Announce Type: replace 
Abstract: To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information understanding can serve as a foundation for adaptive robotic assistance in dynamic, human-centered situations. To evaluate its capability in a realistic human-assistance proxy task, we benchmarked Multi-RAG on the MMBench-Video dataset, a challenging multimodal video understanding benchmark. Our system achieves superior performance compared to existing open-source video large language models (Video-LLMs) and large vision-language models (LVLMs), while utilizing fewer resources and less input data. The results demonstrate Multi- RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures</title>
<link>https://arxiv.org/abs/2506.01438</link>
<guid>https://arxiv.org/abs/2506.01438</guid>
<content:encoded><![CDATA[
arXiv:2506.01438v2 Announce Type: replace 
Abstract: The emergence of large language models has catalyzed two distinct yet interconnected paradigms in artificial intelligence: standalone AI Agents and collaborative Agentic AI ecosystems. This comprehensive study establishes a definitive framework for distinguishing these architectures through systematic analysis of their operational principles, structural compositions, and deployment methodologies. We characterize AI Agents as specialized, tool-enhanced systems leveraging foundation models for targeted automation within constrained environments. Conversely, Agentic AI represents sophisticated multi-entity frameworks where distributed agents exhibit emergent collective intelligence through coordinated interaction protocols. Our investigation traces the evolutionary trajectory from traditional rule-based systems through generative AI foundations to contemporary agent architectures. We present detailed architectural comparisons examining planning mechanisms, memory systems, coordination protocols, and decision-making processes. The study categorizes application landscapes, contrasting single-agent implementations in customer service and content management with multi-agent deployments in research automation and complex decision support. We identify critical challenges including reliability issues, coordination complexities, and scalability constraints, while proposing innovative solutions through enhanced reasoning frameworks, robust memory architectures, and improved coordination mechanisms. This framework provides essential guidance for practitioners selecting appropriate agentic approaches and establishes foundational principles for next-generation intelligent system development.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents need world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[
arXiv:2506.01622v2 Announce Type: replace 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control Tax: The Price of Keeping AI in Check</title>
<link>https://arxiv.org/abs/2506.05296</link>
<guid>https://arxiv.org/abs/2506.05296</guid>
<content:encoded><![CDATA[
arXiv:2506.05296v2 Announce Type: replace 
Abstract: The rapid integration of agentic AI into high-stakes real-world applications requires robust oversight mechanisms. The emerging field of AI Control (AIC) aims to provide such an oversight mechanism, but practical adoption depends heavily on implementation overhead. To study this problem better, we introduce the notion of Control tax -- the operational and financial cost of integrating control measures into AI pipelines. Our work makes three key contributions to the field of AIC: (1) we introduce a theoretical framework that quantifies the Control Tax and maps classifier performance to safety assurances; (2) we conduct comprehensive evaluations of state-of-the-art language models in adversarial settings, where attacker models insert subtle backdoors into code while monitoring models attempt to detect these vulnerabilities; and (3) we provide empirical financial cost estimates for control protocols and develop optimized monitoring strategies that balance safety and cost-effectiveness while accounting for practical constraints like auditing budgets. Our framework enables practitioners to make informed decisions by systematically connecting safety guarantees with their costs, advancing AIC through principled economic feasibility assessment across different deployment contexts.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unreal Patterns</title>
<link>https://arxiv.org/abs/2506.06284</link>
<guid>https://arxiv.org/abs/2506.06284</guid>
<content:encoded><![CDATA[
arXiv:2506.06284v2 Announce Type: replace 
Abstract: This paper introduces a framework for representing information about entities that do not exist or may never exist, such as those involving fictional entities, blueprints, simulations, and future scenarios. Traditional approaches that introduce "dummy instances" or rely on modal logic are criticized, and a proposal is defended in which such cases are modeled using the intersections of actual types rather than specific non existent tokens. The paper positions itself within the Basic Formal Ontology and its realist commitments, emphasizing the importance of practical, implementable solutions over purely metaphysical or philosophical proposals, arguing that existing approaches to non existent entities either overcommit to metaphysical assumptions or introduce computational inefficiencies that hinder applications. By developing a structured ontology driven approach to unreal patterns, the paper aims to provide a useful and computationally viable means of handling references to hypothetical or non existent entities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regret Minimization and Convergence to Equilibria in General-sum Markov Games</title>
<link>https://arxiv.org/abs/2207.14211</link>
<guid>https://arxiv.org/abs/2207.14211</guid>
<content:encoded><![CDATA[
arXiv:2207.14211v3 Announce Type: replace-cross 
Abstract: An abundance of recent impossibility results establish that regret minimization in Markov games with adversarial opponents is both statistically and computationally intractable. Nevertheless, none of these results preclude the possibility of regret minimization under the assumption that all parties adopt the same learning procedure. In this work, we present the first (to our knowledge) algorithm for learning in general-sum Markov games that provides sublinear regret guarantees when executed by all agents. The bounds we obtain are for swap regret, and thus, along the way, imply convergence to a correlated equilibrium. Our algorithm is decentralized, computationally efficient, and does not require any communication between agents. Our key observation is that online learning via policy optimization in Markov games essentially reduces to a form of weighted regret minimization, with unknown weights determined by the path length of the agents' policy sequence. Consequently, controlling the path length leads to weighted regret objectives for which sufficiently adaptive algorithms provide sublinear regret guarantees.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Risk Control</title>
<link>https://arxiv.org/abs/2208.02814</link>
<guid>https://arxiv.org/abs/2208.02814</guid>
<content:encoded><![CDATA[
arXiv:2208.02814v4 Announce Type: replace-cross 
Abstract: We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MogaNet: Multi-order Gated Aggregation Network</title>
<link>https://arxiv.org/abs/2211.03295</link>
<guid>https://arxiv.org/abs/2211.03295</guid>
<content:encoded><![CDATA[
arXiv:2211.03295v4 Announce Type: replace-cross 
Abstract: By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks. However, recent progress on multi-order game-theoretic interaction within deep neural networks (DNNs) reveals the representation bottleneck of modern ConvNets, where the expressive interactions have not been effectively encoded with the increased kernel size. To tackle this challenge, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. MogaNet exhibits great scalability, impressive efficiency of parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D&3D human pose estimation, and video prediction. Notably, MogaNet hits 80.0% and 87.8% accuracy with 5.2M and 181M parameters on ImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59% FLOPs and 17M parameters, respectively. The source code is available at https://github.com/Westlake-AI/MogaNet.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Deep Learning</title>
<link>https://arxiv.org/abs/2301.00314</link>
<guid>https://arxiv.org/abs/2301.00314</guid>
<content:encoded><![CDATA[
arXiv:2301.00314v4 Announce Type: replace-cross 
Abstract: We derive a set of causal deep neural networks whose architectures are a consequence of tensor (multilinear) factor analysis, a framework that facilitates causal inference. Forward causal questions are addressed with a neural network architecture composed of causal capsules and a tensor transformer. Causal capsules compute a set of invariant causal factor representations, whose interactions are governed by a tensor transformation. Inverse causal questions are addressed with a neural network that implements the multilinear projection algorithm. The architecture reverses the order of operations of a forward neural network and estimates the causes of effects. As an alternative to aggressive bottleneck dimension reduction or regularized regression that may camouflage an inherently underdetermined inverse problem, we prescribe modeling different aspects of the mechanism of data formation with piecewise tensor models whose multilinear projections produce multiple candidate solutions. Our forward and inverse questions may be addressed with shallow architectures, but for computationally scalable solutions, we derive a set of deep neural networks by taking advantage of block algebra. An interleaved kernel hierarchy results in doubly non-linear tensor factor models. The causal neural networks that are a consequence of tensor factor analysis are data agnostic, but are illustrated with facial images. Sequential, parallel and asynchronous parallel computation strategies are described.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge</title>
<link>https://arxiv.org/abs/2310.11703</link>
<guid>https://arxiv.org/abs/2310.11703</guid>
<content:encoded><![CDATA[
arXiv:2310.11703v2 Announce Type: replace-cross 
Abstract: Vector databases (VDBs) have emerged to manage high-dimensional data that exceed the capabilities of traditional database management systems, and are now tightly integrated with large language models as well as widely applied in modern artificial intelligence systems. Although relatively few studies describe existing or introduce new vector database architectures, the core technologies underlying VDBs, such as approximate nearest neighbor search, have been extensively studied and are well documented in the literature. In this work, we present a comprehensive review of the relevant algorithms to provide a general understanding of this booming research area. Specifically, we first provide a review of storage and retrieval techniques in VDBs, with detailed design principles and technological evolution. Then, we conduct an in-depth comparison of several advanced VDB solutions with their strengths, limitations, and typical application scenarios. Finally, we also outline emerging opportunities for coupling VDBs with large language models, including open research problems and trends, such as novel indexing strategies. This survey aims to serve as a practical resource, enabling readers to quickly gain an overall understanding of the current knowledge landscape in this rapidly developing area.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Completeness of Invariant Geometric Deep Learning Models</title>
<link>https://arxiv.org/abs/2402.04836</link>
<guid>https://arxiv.org/abs/2402.04836</guid>
<content:encoded><![CDATA[
arXiv:2402.04836v4 Announce Type: replace-cross 
Abstract: Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features in point clouds. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of a wide range of invariant models under fully-connected conditions. We first rigorously characterize the expressiveness of the most classic invariant model, message-passing neural networks incorporating distance (DisGNN), restricting its unidentifiable cases to be only highly symmetric point clouds. We then prove that GeoNGNN, the geometric counterpart of one of the simplest subgraph graph neural networks, can effectively break these corner cases' symmetry and thus achieve E(3)-completeness. By leveraging GeoNGNN as a theoretical tool, we further prove that: 1) most subgraph GNNs developed in traditional graph learning can be seamlessly extended to geometric scenarios with E(3)-completeness; 2) DimeNet, GemNet and SphereNet, three well-established invariant models, are also all capable of achieving E(3)-completeness. Our theoretical results fill the gap in the expressive power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual contrastive learning: robust representations via causal image synthesis</title>
<link>https://arxiv.org/abs/2403.09605</link>
<guid>https://arxiv.org/abs/2403.09605</guid>
<content:encoded><![CDATA[
arXiv:2403.09605v3 Announce Type: replace-cross 
Abstract: Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline</title>
<link>https://arxiv.org/abs/2403.14941</link>
<guid>https://arxiv.org/abs/2403.14941</guid>
<content:encoded><![CDATA[
arXiv:2403.14941v2 Announce Type: replace-cross 
Abstract: Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. In this paper, we present the first systematic classification framework for lane-level traffic prediction, offering a structured taxonomy and analysis of existing methods. We construct three representative datasets from two real-world road networks, covering both regular and irregular lane configurations, and make them publicly available to support future research. We further establishes a unified spatial topology structure and prediction task formulation, and proposes a simple yet effective baseline model, GraphMLP, based on graph structure and MLP networks. This unified framework enables consistent evaluation across datasets and modeling paradigms. We also reproduce previously unavailable code from existing studies and conduct extensive experiments to assess a range of models in terms of accuracy, efficiency, and applicability, providing the first benchmark that jointly considers predictive performance and training cost for lane-level traffic scenarios. All datasets and code are released at https://github.com/ShuhaoLii/LaneLevel-Traffic-Benchmark.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Wireless Federated Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2404.13238</link>
<guid>https://arxiv.org/abs/2404.13238</guid>
<content:encoded><![CDATA[
arXiv:2404.13238v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have driven profound transformations in wireless networks. However, within wireless environments, the training of LLMs faces significant challenges related to security and privacy. Federated Learning (FL), with its decentralized architecture, offers enhanced data privacy protection. Nevertheless, when integrated with LLMs, FL still struggles with several critical limitations, including large-scale and heterogeneous data, resource-intensive training, and substantial communication overhead. To address these challenges, this paper first presents a systematic analysis of the distinct training stages of LLMs in wireless networks, including pre-training, instruction tuning, and alignment tuning. Building upon this foundation, we propose a Personalized Wireless Federated Fine-tuning (PWFF) framework. Initially, we utilize the adapter and Low-Rank Adaptation (LoRA) techniques to decrease energy consumption, while employing global partial aggregation to reduce communication delay. Subsequently, we develop two reward models and design a personalized loss function to fulfill the goal of personalized learning. Furthermore, we implement a local multi-objective alignment to ensure the stability and effectiveness of the FL process. Finally, we conduct a series of simulations to validate the performance of the proposed PWFF method and provide an in-depth discussion of the open issues.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aptly: Making Mobile Apps from Natural Language</title>
<link>https://arxiv.org/abs/2405.00229</link>
<guid>https://arxiv.org/abs/2405.00229</guid>
<content:encoded><![CDATA[
arXiv:2405.00229v2 Announce Type: replace-cross 
Abstract: This paper introduces Aptly, a platform designed to democratize mobile app development, particularly for young learners. Aptly integrates a Large Language Model (LLM) with App Inventor, enabling users to create apps using their natural language. User's description is translated into a programming language that corresponds with App Inventor's visual blocks. A preliminary study with high school students demonstrated the usability and potential of the platform. Prior programming experience influenced how users interact with Aptly. Participants identified areas for improvement and expressed a shift in perspective regarding programming accessibility and AI's role in creative endeavors.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Multi-Objective Reinforcement Learning with Envelope Updates in URLLC-enabled Vehicular Networks</title>
<link>https://arxiv.org/abs/2405.11331</link>
<guid>https://arxiv.org/abs/2405.11331</guid>
<content:encoded><![CDATA[
arXiv:2405.11331v3 Announce Type: replace-cross 
Abstract: We develop a novel multi-objective reinforcement learning (MORL) framework to jointly optimize wireless network selection and autonomous driving policies in a multi-band vehicular network operating on conventional sub-6GHz spectrum and Terahertz frequencies. The proposed framework is designed to 1. maximize the traffic flow and minimize collisions by controlling the vehicle's motion dynamics (i.e., speed and acceleration), and 2. enhance the ultra-reliable low-latency communication (URLLC) while minimizing handoffs (HOs). We cast this problem as a multi-objective Markov Decision Process (MOMDP) and develop solutions for both predefined and unknown preferences of the conflicting objectives. Specifically, we develop a novel envelope MORL solution which develops policies that address multiple objectives with unknown preferences to the agent. While this approach reduces reliance on scalar rewards, policy effectiveness varying with different preferences is a challenge. To address this, we apply a generalized version of the Bellman equation and optimize the convex envelope of multi-objective Q values to learn a unified parametric representation capable of generating optimal policies across all possible preference configurations. Following an initial learning phase, our agent can execute optimal policies under any specified preference or infer preferences from minimal data samples. Numerical results validate the efficacy of the envelope-based MORL solution and demonstrate interesting insights related to the inter-dependency of vehicle motion dynamics, HOs, and the communication data rate. The proposed policies enable autonomous vehicles (AVs) to adopt safe driving behaviors with improved connectivity.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency of Neural Causal Partial Identification</title>
<link>https://arxiv.org/abs/2405.15673</link>
<guid>https://arxiv.org/abs/2405.15673</guid>
<content:encoded><![CDATA[
arXiv:2405.15673v3 Announce Type: replace-cross 
Abstract: Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove the consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization this method may not be asymptotically consistent. Our results are enabled by new results on the approximability of Structural Causal Models (SCMs) via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does learning the right latent variables necessarily improve in-context learning?</title>
<link>https://arxiv.org/abs/2405.19162</link>
<guid>https://arxiv.org/abs/2405.19162</guid>
<content:encoded><![CDATA[
arXiv:2405.19162v2 Announce Type: replace-cross 
Abstract: Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks. For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients. While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or if they instead exploit heuristics and statistical shortcuts enabled by attention layers. Both scenarios have inspired active ongoing work. In this paper, we systematically investigate the effect of explicitly inferring task latents. We minimally modify the Transformer architecture with a bottleneck designed to prevent shortcuts in favor of more structured solutions, and then compare performance against standard Transformers across various ICL tasks. Contrary to intuition and some recent works, we find little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general. Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction. Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in the Wild</title>
<link>https://arxiv.org/abs/2405.19996</link>
<guid>https://arxiv.org/abs/2405.19996</guid>
<content:encoded><![CDATA[
arXiv:2405.19996v5 Announce Type: replace-cross 
Abstract: Blind image quality assessment (IQA) in the wild, which assesses the quality of images with complex authentic distortions and no reference images, presents significant challenges. Given the difficulty in collecting large-scale training data, leveraging limited data to develop a model with strong generalization remains an open problem. Motivated by the robust image perception capabilities of pre-trained text-to-image (T2I) diffusion models, we propose a novel IQA method, diffusion priors-based IQA (DP-IQA), to utilize the T2I model's prior for improved performance and generalization ability. Specifically, we utilize pre-trained Stable Diffusion as the backbone, extracting multi-level features from the denoising U-Net guided by prompt embeddings through a tunable text adapter. Simultaneously, an image adapter compensates for information loss introduced by the lossy pre-trained encoder. Unlike T2I models that require full image distribution modeling, our approach targets image quality assessment, which inherently requires fewer parameters. To improve applicability, we distill the knowledge into a lightweight CNN-based student model, significantly reducing parameters while maintaining or even enhancing generalization performance. Experimental results demonstrate that DP-IQA achieves state-of-the-art performance on various in-the-wild datasets, highlighting the superior generalization capability of T2I priors in blind IQA tasks. To our knowledge, DP-IQA is the first method to apply pre-trained diffusion priors in blind IQA. Codes and checkpoints are available at https://github.com/RomGai/DP-IQA.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OR-Bench: An Over-Refusal Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2405.20947</link>
<guid>https://arxiv.org/abs/2405.20947</guid>
<content:encoded><![CDATA[
arXiv:2405.20947v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating LLM Ethics: Advancements, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2406.18841</link>
<guid>https://arxiv.org/abs/2406.18841</guid>
<content:encoded><![CDATA[
arXiv:2406.18841v5 Announce Type: replace-cross 
Abstract: This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrialBench: Multi-Modal Artificial Intelligence-Ready Clinical Trial Datasets</title>
<link>https://arxiv.org/abs/2407.00631</link>
<guid>https://arxiv.org/abs/2407.00631</guid>
<content:encoded><![CDATA[
arXiv:2407.00631v3 Announce Type: replace-cross 
Abstract: Clinical trials are pivotal for developing new medical treatments but typically carry risks such as patient mortality and enrollment failure that waste immense efforts spanning over a decade. Applying artificial intelligence (AI) to predict key events in clinical trials holds great potential for providing insights to guide trial designs. However, complex data collection and question definition requiring medical expertise have hindered the involvement of AI thus far. This paper tackles these challenges by presenting a comprehensive suite of 23 meticulously curated AI-ready datasets covering multi-modal input features and 8 crucial prediction challenges in clinical trial design, encompassing prediction of trial duration, patient dropout rate, serious adverse event, mortality rate, trial approval outcome, trial failure reason, drug dose finding, design of eligibility criteria. Furthermore, we provide basic validation methods for each task to ensure the datasets' usability and reliability. We anticipate that the availability of such open-access datasets will catalyze the development of advanced AI approaches for clinical trial design, ultimately advancing clinical trial research and accelerating medical solution development.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoQA in the Era of LLMs: An Empirical Study</title>
<link>https://arxiv.org/abs/2408.04223</link>
<guid>https://arxiv.org/abs/2408.04223</guid>
<content:encoded><![CDATA[
arXiv:2408.04223v2 Announce Type: replace-cross 
Abstract: Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks. As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing. This work conducts a timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering. Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments. Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also, they do not necessarily generalize better. The findings demonstrate Video-LLMs' QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability, suggesting the urgent need on rationales in Video-LLM developing.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Text-to-SQL in the Era of LLMs: Where are we, and where are we going?</title>
<link>https://arxiv.org/abs/2408.05109</link>
<guid>https://arxiv.org/abs/2408.05109</guid>
<content:encoded><![CDATA[
arXiv:2408.05109v5 Announce Type: replace-cross 
Abstract: Translating users' natural language queries (NL) into SQL queries (i.e., Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of Text-to-SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: Text-to-SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks; (3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL solutions. Finally, we discuss the research challenges and open problems of Text-to-SQL in the LLMs era.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents</title>
<link>https://arxiv.org/abs/2408.08089</link>
<guid>https://arxiv.org/abs/2408.08089</guid>
<content:encoded><![CDATA[
arXiv:2408.08089v2 Announce Type: replace-cross 
Abstract: Current research in LLM-based simulation systems lacks comprehensive solutions for modeling real-world court proceedings, while existing legal language models struggle with dynamic courtroom interactions. We present AgentCourt, a comprehensive legal simulation framework that addresses these challenges through adversarial evolution of LLM-based agents. Our AgentCourt introduces a new adversarial evolutionary approach for agents called AdvEvol, which performs dynamic knowledge learning and evolution through structured adversarial interactions in a simulated courtroom program, breaking the limitations of the traditional reliance on static knowledge bases or manual annotations. By simulating 1,000 civil cases, we construct an evolving knowledge base that enhances the agents' legal reasoning abilities. The evolved lawyer agents demonstrated outstanding performance on our newly introduced CourtBench benchmark, achieving a 12.1% improvement in performance compared to the original lawyer agents. Evaluations by professional lawyers confirm the effectiveness of our approach across three critical dimensions: cognitive agility, professional knowledge, and logical rigor. Beyond outperforming specialized legal models in interactive reasoning tasks, our findings emphasize the importance of adversarial learning in legal AI and suggest promising directions for extending simulation-based legal reasoning to broader judicial and regulatory contexts. The project's code is available at: https://github.com/relic-yuexi/AgentCourt
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward End-to-End Bearing Fault Diagnosis for Industrial Scenarios with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2408.11067</link>
<guid>https://arxiv.org/abs/2408.11067</guid>
<content:encoded><![CDATA[
arXiv:2408.11067v2 Announce Type: replace-cross 
Abstract: This paper explores the application of spiking neural networks (SNNs), known for their low-power binary spikes, to bearing fault diagnosis, bridging the gap between high-performance AI algorithms and real-world industrial scenarios. In particular, we identify two key limitations of existing SNN fault diagnosis methods: inadequate encoding capacity that necessitates cumbersome data preprocessing, and non-spike-oriented architectures that constrain the performance of SNNs. To alleviate these problems, we propose a Multi-scale Residual Attention SNN (MRA-SNN) to simultaneously improve the efficiency, performance, and robustness of SNN methods. By incorporating a lightweight attention mechanism, we have designed a multi-scale attention encoding module to extract multiscale fault features from vibration signals and encode them as spatio-temporal spikes, eliminating the need for complicated preprocessing. Then, the spike residual attention block extracts high-dimensional fault features and enhances the expressiveness of sparse spikes with the attention mechanism for end-to-end diagnosis. In addition, the performance and robustness of MRA-SNN is further enhanced by introducing the lightweight attention mechanism within the spiking neurons to simulate the biological dendritic filtering effect. Extensive experiments on MFPT, JNU, Bearing, and Gearbox benchmark datasets demonstrate that MRA-SNN significantly outperforms existing methods in terms of accuracy, energy consumption, and noise robustness, and is more feasible for deployment in real-world industrial scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Clinical Note Generation from Complex Doctor-Patient Conversation</title>
<link>https://arxiv.org/abs/2408.14568</link>
<guid>https://arxiv.org/abs/2408.14568</guid>
<content:encoded><![CDATA[
arXiv:2408.14568v2 Announce Type: replace-cross 
Abstract: Writing clinical notes and documenting medical exams is a critical task for healthcare professionals, serving as a vital component of patient care documentation. However, manually writing these notes is time-consuming and can impact the amount of time clinicians can spend on direct patient interaction and other tasks. Consequently, the development of automated clinical note generation systems has emerged as a clinically meaningful area of research within AI for health. In this paper, we present three key contributions to the field of clinical note generation using large language models (LLMs). First, we introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex doctor-patient conversations paired with their full clinical notes. This dataset, created and curated by medical experts with the help of modern neural networks, provides a valuable resource for training and evaluating models in clinical note generation tasks. Second, we propose the K-SOAP (Keyword, Subjective, Objective, Assessment, and Plan) note format, which enhances traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and Plan) notes by adding a keyword section at the top, allowing for quick identification of essential information. Third, we develop an automatic pipeline to generate K-SOAP notes from doctor-patient conversations and benchmark various modern LLMs using various metrics. Our results demonstrate significant improvements in efficiency and performance compared to standard LLM finetuning methods.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Feasibility of Fully AI-automated Vishing Attacks</title>
<link>https://arxiv.org/abs/2409.13793</link>
<guid>https://arxiv.org/abs/2409.13793</guid>
<content:encoded><![CDATA[
arXiv:2409.13793v2 Announce Type: replace-cross 
Abstract: A vishing attack is a form of social engineering where attackers use phone calls to deceive individuals into disclosing sensitive information, such as personal data, financial information, or security credentials. Attackers exploit the perceived urgency and authenticity of voice communication to manipulate victims, often posing as legitimate entities like banks or tech support. Vishing is a particularly serious threat as it bypasses security controls designed to protect information. In this work, we study the potential for vishing attacks to escalate with the advent of AI. In theory, AI-powered software bots may have the ability to automate these attacks by initiating conversations with potential victims via phone calls and deceiving them into disclosing sensitive information. To validate this thesis, we introduce ViKing, an AI-powered vishing system developed using publicly available AI technology. It relies on a Large Language Model (LLM) as its core cognitive processor to steer conversations with victims, complemented by a pipeline of speech-to-text and text-to-speech modules that facilitate audio-text conversion in phone calls. Through a controlled social experiment involving 240 participants, we discovered that ViKing has successfully persuaded many participants to reveal sensitive information, even those who had been explicitly warned about the risk of vishing campaigns. Interactions with ViKing's bots were generally considered realistic. From these findings, we conclude that tools like ViKing may already be accessible to potential malicious actors, while also serving as an invaluable resource for cyber awareness programs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much Can We Forget about Data Contamination?</title>
<link>https://arxiv.org/abs/2410.03249</link>
<guid>https://arxiv.org/abs/2410.03249</guid>
<content:encoded><![CDATA[
arXiv:2410.03249v4 Announce Type: replace-cross 
Abstract: The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B, have forgotten the data seen at the beginning of training.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Representation Condition Improves Equivariant Molecule Generation</title>
<link>https://arxiv.org/abs/2410.03655</link>
<guid>https://arxiv.org/abs/2410.03655</guid>
<content:encoded><![CDATA[
arXiv:2410.03655v3 Announce Type: replace-cross 
Abstract: Recent advances in molecular generative models have demonstrated great promise for accelerating scientific discovery, particularly in drug design. However, these models often struggle to generate high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to improve molecular generative models by integrating geometric representation conditions with provable theoretical guarantees. We decompose the generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared with single-stage generation, the easy-to-generate representation in the first stage guides the second stage generation toward a high-quality molecule in a goal-oriented way. Leveraging EDM and SemlaFlow as base generators, we observe significant quality improvements in unconditional molecule generation on the widely used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 50\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations. Furthermore, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while largely preserving the generation quality achieved with 1,000 steps, thereby significantly reducing the generation iterations needed.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Upcycling Large Language Models into Mixture of Experts</title>
<link>https://arxiv.org/abs/2410.07524</link>
<guid>https://arxiv.org/abs/2410.07524</guid>
<content:encoded><![CDATA[
arXiv:2410.07524v2 Announce Type: replace-cross 
Abstract: Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel "virtual group" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models. Code is available.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Gesture Recognition in Autism: Integrating YOLOv7, Video Augmentation and VideoMAE for Video Analysis</title>
<link>https://arxiv.org/abs/2410.09339</link>
<guid>https://arxiv.org/abs/2410.09339</guid>
<content:encoded><![CDATA[
arXiv:2410.09339v2 Announce Type: replace-cross 
Abstract: Deep learning and advancements in contactless sensors have significantly enhanced our ability to understand complex human activities in healthcare settings. In particular, deep learning models utilizing computer vision have been developed to enable detailed analysis of human gesture recognition, especially repetitive gestures which are commonly observed behaviors in children with autism. This research work aims to identify repetitive behaviors indicative of autism by analyzing videos captured in natural settings as children engage in daily activities. The focus is on accurately categorizing real-time repetitive gestures such as spinning, head banging, and arm flapping. To this end, we utilize the publicly accessible Self-Stimulatory Behavior Dataset (SSBD) to classify these stereotypical movements. A key component of the proposed methodology is the use of \textbf{VideoMAE}, a model designed to improve both spatial and temporal analysis of video data through a masking and reconstruction mechanism. This model significantly outperformed traditional methods, achieving an accuracy of 97.7\%, a 14.7\% improvement over the previous state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Shielding and Reinforcement Learning for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2410.10460</link>
<guid>https://arxiv.org/abs/2410.10460</guid>
<content:encoded><![CDATA[
arXiv:2410.10460v2 Announce Type: replace-cross 
Abstract: Deep reinforcement learning has emerged as a powerful tool for obtaining high-performance policies. However, the safety of these policies has been a long-standing issue. One promising paradigm to guarantee safety is a shield, which shields a policy from making unsafe actions. However, computing a shield scales exponentially in the number of state variables. This is a particular concern in multi-agent systems with many agents. In this work, we propose a novel approach for multi-agent shielding. We address scalability by computing individual shields for each agent. The challenge is that typical safety specifications are global properties, but the shields of individual agents only ensure local properties. Our key to overcome this challenge is to apply assume-guarantee reasoning. Specifically, we present a sound proof rule that decomposes a (global, complex) safety specification into (local, simple) obligations for the shields of the individual agents. Moreover, we show that applying the shields during reinforcement learning significantly improves the quality of the policies obtained for a given training budget. We demonstrate the effectiveness and scalability of our multi-agent shielding framework in two case studies, reducing the computation time from hours to seconds and achieving fast learning convergence.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Information-Theoretic Measures of Predictive Uncertainty</title>
<link>https://arxiv.org/abs/2410.10786</link>
<guid>https://arxiv.org/abs/2410.10786</guid>
<content:encoded><![CDATA[
arXiv:2410.10786v2 Announce Type: replace-cross 
Abstract: Reliable estimation of predictive uncertainty is crucial for machine learning applications, particularly in high-stakes scenarios where hedging against risks is essential. Despite its significance, there is no universal agreement on how to best quantify predictive uncertainty. In this work, we revisit core concepts to propose a framework for information-theoretic measures of predictive uncertainty. Our proposed framework categorizes predictive uncertainty measures according to two factors: (I) The predicting model (II) The approximation of the true predictive distribution. Examining all possible combinations of these two factors, we derive a set of predictive uncertainty measures that includes both known and newly introduced ones. We extensively evaluate these measures across a broad set of tasks, identifying conditions under which certain measures excel. Our findings show the importance of aligning the choice of uncertainty measure with the predicting model on in-distribution (ID) data, the limitations of epistemic uncertainty measures for out-of-distribution (OOD) data, and that the disentanglement between measures varies substantially between ID and OOD data. Together, these insights provide a more comprehensive understanding of predictive uncertainty measures, revealing their implicit assumptions and relationships.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoH: Multi-Head Attention as Mixture-of-Head Attention</title>
<link>https://arxiv.org/abs/2410.11842</link>
<guid>https://arxiv.org/abs/2410.11842</guid>
<content:encoded><![CDATA[
arXiv:2410.11842v2 Announce Type: replace-cross 
Abstract: In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering</title>
<link>https://arxiv.org/abs/2410.12846</link>
<guid>https://arxiv.org/abs/2410.12846</guid>
<content:encoded><![CDATA[
arXiv:2410.12846v4 Announce Type: replace-cross 
Abstract: Question answering on free-form tables (a.k.a. TableQA) is a challenging task because of the flexible structure and complex schema of tables. Recent studies use Large Language Models (LLMs) for this task, exploiting their capability in understanding the questions and tabular data, which are typically given in natural language and contain many textual fields, respectively. While this approach has shown promising results, it overlooks the challenges brought by numerical values which are common in tabular data, and LLMs are known to struggle with such values. We aim to address this issue, and we propose a model named TabLaP that uses LLMs as a planner rather than an answer generator. This approach exploits LLMs' capability in multi-step reasoning while leaving the actual numerical calculations to a Python interpreter for accurate calculation. Recognizing the inaccurate nature of LLMs, we further make a first attempt to quantify the trustworthiness of the answers produced by TabLaP, such that users can use TabLaP in a regret-aware manner. Experimental results on two benchmark datasets show that TabLaP is substantially more accurate than the state-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the two datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrameBridge: Improving Image-to-Video Generation with Bridge Models</title>
<link>https://arxiv.org/abs/2410.15371</link>
<guid>https://arxiv.org/abs/2410.15371</guid>
<content:encoded><![CDATA[
arXiv:2410.15371v2 Announce Type: replace-cross 
Abstract: Diffusion models have achieved remarkable progress on image-to-video (I2V) generation, while their noise-to-data generation process is inherently mismatched with this task, which may lead to suboptimal synthesis quality. In this work, we present FrameBridge. By modeling the frame-to-frames generation process with a bridge model based data-to-data generative process, we are able to fully exploit the information contained in the given image and improve the consistency between the generation process and I2V task. Moreover, we propose two novel techniques toward the two popular settings of training I2V models, respectively. Firstly, we propose SNR-Aligned Fine-tuning (SAF), making the first attempt to fine-tune a diffusion model to a bridge model and, therefore, allowing us to utilize the pre-trained diffusion-based text-to-video (T2V) models. Secondly, we propose neural prior, further improving the synthesis quality of FrameBridge when training from scratch. Experiments conducted on WebVid-2M and UCF-101 demonstrate the superior quality of FrameBridge in comparison with the diffusion counterpart (zero-shot FVD 95 vs. 192 on MSR-VTT and non-zero-shot FVD 122 vs. 171 on UCF-101), and the advantages of our proposed SAF and neural prior for bridge-based I2V models. The project page: https://framebridge-icml.github.io/.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.16600</link>
<guid>https://arxiv.org/abs/2410.16600</guid>
<content:encoded><![CDATA[
arXiv:2410.16600v3 Announce Type: replace-cross 
Abstract: Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Augmented Code Generation Using Programming Knowledge Graphs</title>
<link>https://arxiv.org/abs/2410.18251</link>
<guid>https://arxiv.org/abs/2410.18251</guid>
<content:encoded><![CDATA[
arXiv:2410.18251v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly improved code generation, but, they frequently face difficulties when dealing with challenging and complex problems. Retrieval-Augmented Generation (RAG) addresses this issue by retrieving and integrating external knowledge at the inference time. However, retrieval models often fail to find most relevant context, and generation models, with limited context capacity, can hallucinate when given irrelevant data. We present a novel framework that leverages a Programming Knowledge Graph (PKG) to semantically represent and retrieve code. This approach enables fine-grained code retrieval by focusing on the most relevant segments while reducing irrelevant context through a tree-pruning technique. PKG is coupled with a re-ranking mechanism to reduce even more hallucinations by selectively integrating non-RAG solutions. We propose two retrieval approaches-block-wise and function-wise-based on the PKG, optimizing context granularity. Evaluations on the HumanEval and MBPP benchmarks show our method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art models by up to 34% on MBPP. Our contributions include PKG-based retrieval, tree pruning to enhance retrieval precision, a re-ranking method for robust solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic code augmentation with relevant comments and docstrings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chemical Language Model Linker: blending text and molecules with modular adapters</title>
<link>https://arxiv.org/abs/2410.20182</link>
<guid>https://arxiv.org/abs/2410.20182</guid>
<content:encoded><![CDATA[
arXiv:2410.20182v3 Announce Type: replace-cross 
Abstract: The development of large language models and multi-modal models has enabled the appealing idea of generating novel molecules from text descriptions. Generative modeling would shift the paradigm from relying on large-scale chemical screening to find molecules with desired properties to directly generating those molecules. However, multi-modal models combining text and molecules are often trained from scratch, without leveraging existing high-quality pretrained models. Training from scratch consumes more computational resources and prohibits model scaling. In contrast, we propose a lightweight adapter-based strategy named Chemical Language Model Linker (ChemLML). ChemLML blends the two single domain models and obtains conditional molecular generation from text descriptions while still operating in the specialized embedding spaces of the molecular domain. ChemLML can tailor diverse pretrained text models for molecule generation by training relatively few adapter parameters. We find that the choice of molecular representation used within ChemLML, SMILES versus SELFIES, has a strong influence on conditional molecular generation performance. SMILES is often preferable despite not guaranteeing valid molecules. We raise issues in using the entire PubChem dataset of molecules and their associated descriptions for evaluating molecule generation and provide a filtered version of the dataset as a generation test set. To demonstrate how ChemLML could be used in practice, we generate candidate protein inhibitors and use docking to assess their quality and also generate candidate membrane permeable molecules.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences</title>
<link>https://arxiv.org/abs/2410.21332</link>
<guid>https://arxiv.org/abs/2410.21332</guid>
<content:encoded><![CDATA[
arXiv:2410.21332v2 Announce Type: replace-cross 
Abstract: Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables. HVM efficiently organizes memory while uncovering abstractions, leading to compact sequence representations. When learning on language datasets such as babyLM, HVM learns a more efficient dictionary than standard compression algorithms such as Lempel-Ziv. In a sequence recall task requiring the acquisition and transfer of variables embedded in sequences, we demonstrate HVM's sequence likelihood correlates with human recall times. In contrast, large language models (LLMs) struggle to transfer abstract variables as effectively as humans. From HVM's adjustable layer of abstraction, we demonstrate that the model realizes a precise trade-off between compression and generalization. Our work offers a cognitive model that captures the learning and transfer of abstract representations in human cognition and differentiates itself from LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse</title>
<link>https://arxiv.org/abs/2410.21333</link>
<guid>https://arxiv.org/abs/2410.21333</guid>
<content:encoded><![CDATA[
arXiv:2410.21333v4 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting has become a widely used strategy for improving large language and multimodal model performance. However, it is still an open question under which settings CoT systematically reduces performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, focusing on six representative tasks from the psychological literature where deliberation hurts performance in humans. In three of these tasks, state-of-the-art models exhibit significant performance drop-offs with CoT (up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o), while in others, CoT effects are mixed, with positive, neutral, and negative changes. While models and humans do not exhibit perfectly parallel cognitive processes, considering cases where thinking has negative consequences for humans helps identify settings where it negatively impacts models. By connecting the literature on human verbal thinking and deliberation with evaluations of CoT, we offer a perspective for understanding the impact of inference-time reasoning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
<link>https://arxiv.org/abs/2411.07611</link>
<guid>https://arxiv.org/abs/2411.07611</guid>
<content:encoded><![CDATA[
arXiv:2411.07611v4 Announce Type: replace-cross 
Abstract: Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable multimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in the same encoding space, enabling it to be naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dataset of questions on decision-theoretic reasoning in Newcomb-like problems</title>
<link>https://arxiv.org/abs/2411.10588</link>
<guid>https://arxiv.org/abs/2411.10588</guid>
<content:encoded><![CDATA[
arXiv:2411.10588v4 Announce Type: replace-cross 
Abstract: We introduce a dataset of natural-language questions in the decision theory of so-called Newcomb-like problems. Newcomb-like problems include, for instance, decision problems in which an agent interacts with a similar other agent, and thus has to reason about the fact that the other agent will likely reason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is important because interactions between foundation-model-based agents will often be Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow for greater cooperation between models.
  Our dataset contains both capabilities questions (i.e., questions with a unique, uncontroversially correct answer) and attitude questions (i.e., questions about which decision theorists would disagree). We use our dataset for an investigation of decision-theoretical capabilities and expressed attitudes and their interplay in existing models (different models by OpenAI, Anthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based interventions. We find, among other things, that attitudes vary significantly between existing models; that high capabilities are associated with attitudes more favorable toward so-called evidential decision theory; and that attitudes are consistent across different types of questions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastRAG: Retrieval Augmented Generation for Semi-structured Data</title>
<link>https://arxiv.org/abs/2411.13773</link>
<guid>https://arxiv.org/abs/2411.13773</guid>
<content:encoded><![CDATA[
arXiv:2411.13773v2 Announce Type: replace-cross 
Abstract: Efficiently processing and interpreting network data is critical for the operation of increasingly complex networks. Recent advances in Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved data processing in network management. However, existing RAG methods like VectorRAG and GraphRAG struggle with the complexity and implicit nature of semi-structured technical data, leading to inefficiencies in time, cost, and retrieval. This paper introduces FastRAG, a novel RAG approach designed for semi-structured data. FastRAG employs schema learning and script learning to extract and structure data without needing to submit entire data sources to an LLM. It integrates text search with knowledge graph (KG) querying to improve accuracy in retrieving context-rich information. Evaluation results demonstrate that FastRAG provides accurate question answering, while improving up to 90% in time and 85% in cost compared to GraphRAG.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2411.18688</link>
<guid>https://arxiv.org/abs/2411.18688</guid>
<content:encoded><![CDATA[
arXiv:2411.18688v5 Announce Type: replace-cross 
Abstract: With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AccDiffusion v2: Towards More Accurate Higher-Resolution Diffusion Extrapolation</title>
<link>https://arxiv.org/abs/2412.02099</link>
<guid>https://arxiv.org/abs/2412.02099</guid>
<content:encoded><![CDATA[
arXiv:2412.02099v2 Announce Type: replace-cross 
Abstract: Diffusion models suffer severe object repetition and local distortion when the inference resolution differs from its pre-trained resolution. We propose AccDiffusion v2, an accurate method for patch-wise higher-resolution diffusion extrapolation without training. Our in-depth analysis in this paper shows that using an identical text prompt for different patches leads to repetitive generation, while the absence of a prompt undermines image details. In response, our AccDiffusion v2 novelly decouples the vanilla image-content-aware prompt into a set of patch-content-aware prompts, each of which serves as a more precise description of a patch. Further analysis reveals that local distortion arises from inaccurate descriptions in prompts about the local structure of higher-resolution images. To address this issue, AccDiffusion v2, for the first time, introduces an auxiliary local structural information through ControlNet during higher-resolution diffusion extrapolation aiming to mitigate the local distortions. Finally, our analysis indicates that global semantic information is conducive to suppressing both repetitive generation and local distortion. Hence, our AccDiffusion v2 further proposes dilated sampling with window interaction for better global semantic information during higher-resolution diffusion extrapolation. We conduct extensive experiments, including both quantitative and qualitative comparisons, to demonstrate the efficacy of our AccDiffusion v2. The quantitative comparison shows that AccDiffusion v2 achieves state-of-the-art performance in image generation extrapolation without training. The qualitative comparison intuitively illustrates that AccDiffusion v2 effectively suppresses the issues of repetitive generation and local distortion in image generation extrapolation. Our code is available at https://github.com/lzhxmu/AccDiffusion_v2.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression</title>
<link>https://arxiv.org/abs/2412.03213</link>
<guid>https://arxiv.org/abs/2412.03213</guid>
<content:encoded><![CDATA[
arXiv:2412.03213v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been widely deployed in a variety of applications, and the context length is rapidly increasing to handle tasks such as long-document QA and complex logical reasoning. However, long context poses significant challenges for inference efficiency, including high memory costs of key-value (KV) cache and increased latency due to extensive memory accesses. Recent works have proposed compressing KV cache to approximate computation, but these methods either evict tokens permanently, never recalling them for later inference, or recall previous tokens at the granularity of pages divided by textual positions. Both approaches degrade the model accuracy and output quality. To achieve efficient and accurate recallable KV cache compression, we introduce ClusterKV, which recalls tokens at the granularity of semantic clusters. We design and implement efficient algorithms and systems for clustering, selection, indexing and caching. Experiment results show that ClusterKV attains negligible accuracy loss across various tasks with 32k context lengths, using only a 1k to 2k KV cache budget, and achieves up to a 2$\times$ speedup in latency and a 2.5$\times$ improvement in decoding throughput. Compared to SoTA recallable KV compression methods, ClusterKV demonstrates higher model accuracy and output quality, while maintaining or exceeding inference efficiency. Our code is available at https://github.com/sjtu-zhao-lab/ClusterKV.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing</title>
<link>https://arxiv.org/abs/2412.12326</link>
<guid>https://arxiv.org/abs/2412.12326</guid>
<content:encoded><![CDATA[
arXiv:2412.12326v2 Announce Type: replace-cross 
Abstract: In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose a novel multi-agent reinforcement learning (MARL) method to address this issue - learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Unlike traditional cooperative MARL solutions that involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies, we propose a novel MARL approach where agents exchange action suggestions. Our method reveals less private information compared to sharing rewards, values, or policies, while enabling effective cooperation without the need to design intrinsic rewards. Our algorithm is supported by our theoretical analysis that establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that our algorithm performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnalogXpert: Automating Analog Topology Synthesis by Incorporating Circuit Design Expertise into Large Language Models</title>
<link>https://arxiv.org/abs/2412.19824</link>
<guid>https://arxiv.org/abs/2412.19824</guid>
<content:encoded><![CDATA[
arXiv:2412.19824v2 Announce Type: replace-cross 
Abstract: Analog circuits are crucial in modern electronic systems, and automating their design has attracted significant research interest. One of major challenges is topology synthesis, which determines circuit components and their connections. Recent studies explore large language models (LLM) for topology synthesis. However, the scenarios addressed by these studies do not align well with practical applications. Specifically, existing work uses vague design requirements as input and outputs an ideal model, but detailed structural requirements and device-level models are more practical. Moreover, current approaches either formulate topology synthesis as graph generation or Python code generation, whereas practical topology design is a complex process that demands extensive design knowledge. In this work, we propose AnalogXpert, a LLM-based agent aiming at solving practical topology synthesis problem by incorporating circuit design expertise into LLMs. First, we represent analog topology as SPICE code and introduce a subcircuit library to reduce the design space, in the same manner as experienced designers. Second, we decompose the problem into two sub-task (i.e., block selection and block connection) through the use of CoT and incontext learning techniques, to mimic the practical design process. Third, we introduce a proofreading strategy that allows LLMs to incrementally correct the errors in the initial design, akin to human designers who iteratively check and adjust the initial topology design to ensure accuracy. Finally, we construct a high-quality benchmark containing both real data (30) and synthetic data (2k). AnalogXpert achieves 40% and 23% success rates on the synthetic dataset and real dataset respectively, which is markedly better than those of GPT-4o (3% on both the synthetic dataset and the real dataset).
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors</title>
<link>https://arxiv.org/abs/2501.00741</link>
<guid>https://arxiv.org/abs/2501.00741</guid>
<content:encoded><![CDATA[
arXiv:2501.00741v3 Announce Type: replace-cross 
Abstract: Neuromorphic cameras, also known as event cameras, are asynchronous brightness-change sensors that can capture extremely fast motion without suffering from motion blur, making them particularly promising for 3D reconstruction in extreme environments. However, existing research on 3D reconstruction using monocular neuromorphic cameras is limited, and most of the methods rely on estimating physical priors and employ complex multi-step pipelines. In this work, we propose an end-to-end method for dense voxel 3D reconstruction using neuromorphic cameras that eliminates the need to estimate physical priors. Our method incorporates a novel event representation to enhance edge features, enabling the proposed feature-enhancement model to learn more effectively. Additionally, we introduced Optimal Binarization Threshold Selection Principle as a guideline for future related work, using the optimal reconstruction results achieved with threshold optimization as the benchmark. Our method achieves a 54.6% improvement in reconstruction accuracy compared to the baseline method.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning</title>
<link>https://arxiv.org/abs/2501.01120</link>
<guid>https://arxiv.org/abs/2501.01120</guid>
<content:encoded><![CDATA[
arXiv:2501.01120v2 Announce Type: replace-cross 
Abstract: Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT's robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems. The code of our work and prompt-based baselines is available at https://github.com/Jian-Lang/RAGPT.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage</title>
<link>https://arxiv.org/abs/2501.02039</link>
<guid>https://arxiv.org/abs/2501.02039</guid>
<content:encoded><![CDATA[
arXiv:2501.02039v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Rotary Position Embeddings for Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2501.06051</link>
<guid>https://arxiv.org/abs/2501.06051</guid>
<content:encoded><![CDATA[
arXiv:2501.06051v2 Announce Type: replace-cross 
Abstract: Self-attention relies on positional embeddings to encode input order. Relative Position (RelPos) embeddings are widely used in Automatic Speech Recognition (ASR). However, RelPos has quadratic time complexity to input length and is often incompatible with fast GPU implementations of attention. In contrast, Rotary Positional Embedding (RoPE) rotates each input vector based on its absolute position, taking linear time to sequence length, implicitly encoding relative distances through self-attention dot products. Thus, it is usually compatible with efficient attention. However, its use in ASR remains underexplored. This work evaluates RoPE across diverse ASR tasks with training data ranging from 100 to 50,000 hours, covering various speech types (read, spontaneous, clean, noisy) and different accents in both streaming and non-streaming settings. ASR error rates are similar or better than RelPos, while training time is reduced by up to 21%. Code is available via the SpeechBrain toolkit.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of Large Language Models</title>
<link>https://arxiv.org/abs/2501.09223</link>
<guid>https://arxiv.org/abs/2501.09223</guid>
<content:encoded><![CDATA[
arXiv:2501.09223v2 Announce Type: replace-cross 
Abstract: This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into five main chapters, each exploring a key area: pre-training, generative models, prompting, alignment, and inference. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</title>
<link>https://arxiv.org/abs/2501.12375</link>
<guid>https://arxiv.org/abs/2501.12375</guid>
<content:encoded><![CDATA[
arXiv:2501.12375v3 Announce Type: replace-cross 
Abstract: Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (< 10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concurrent Learning with Aggregated States via Randomized Least Squares Value Iteration</title>
<link>https://arxiv.org/abs/2501.13394</link>
<guid>https://arxiv.org/abs/2501.13394</guid>
<content:encoded><![CDATA[
arXiv:2501.13394v3 Announce Type: replace-cross 
Abstract: Designing learning agents that explore efficiently in a complex environment has been widely recognized as a fundamental challenge in reinforcement learning. While a number of works have demonstrated the effectiveness of techniques based on randomized value functions on a single agent, it remains unclear, from a theoretical point of view, whether injecting randomization can help a society of agents {\it concurently} explore an environment. The theoretical results %that we established in this work tender an affirmative answer to this question. We adapt the concurrent learning framework to \textit{randomized least-squares value iteration} (RLSVI) with \textit{aggregated state representation}. We demonstrate polynomial worst-case regret bounds in both finite- and infinite-horizon environments. In both setups the per-agent regret decreases at an optimal rate of $\Theta\left(\frac{1}{\sqrt{N}}\right)$, highlighting the advantage of concurent learning. Our algorithm exhibits significantly lower space complexity compared to \cite{russo2019worst} and \cite{agrawal2021improved}. We reduce the space complexity by a factor of $K$ while incurring only a $\sqrt{K}$ increase in the worst-case regret bound, compared to \citep{agrawal2021improved,russo2019worst}. Additionally, we conduct numerical experiments to demonstrate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2501.14693</link>
<guid>https://arxiv.org/abs/2501.14693</guid>
<content:encoded><![CDATA[
arXiv:2501.14693v2 Announce Type: replace-cross 
Abstract: Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Optimizer Works Best for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks?</title>
<link>https://arxiv.org/abs/2501.16371</link>
<guid>https://arxiv.org/abs/2501.16371</guid>
<content:encoded><![CDATA[
arXiv:2501.16371v4 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) have revolutionized the computation of PDE solutions by integrating partial differential equations (PDEs) into the neural network's training process as soft constraints, becoming an important component of the scientific machine learning (SciML) ecosystem. More recently, physics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be effective and comparable in accuracy with PINNs. In their current implementation, both PINNs and PIKANs are mainly optimized using first-order methods like Adam, as well as quasi-Newton methods such as BFGS and its low-memory variant, L-BFGS. However, these optimizers often struggle with highly non-linear and non-convex loss landscapes, leading to challenges such as slow convergence, local minima entrapment, and (non)degenerate saddle points. In this study, we investigate the performance of Self-Scaled BFGS (SSBFGS), Self-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton schemes, including BFGS and L-BFGS with different line search strategies approaches. These methods dynamically rescale updates based on historical gradient information, thus enhancing training efficiency and accuracy. We systematically compare these optimizers -- using both PINNs and PIKANs -- on key challenging linear, stiff, multi-scale and non-linear PDEs, including the Burgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our findings provide state-of-the-art results with orders-of-magnitude accuracy improvements without the use of adaptive weights or any other enhancements typically employed in PINNs. More broadly, our results reveal insights into the effectiveness of second-order optimization strategies in significantly improving the convergence and accurate generalization of PINNs and PIKANs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASAP: Learning Generalizable Online Bin Packing via Adaptive Selection After Proposal</title>
<link>https://arxiv.org/abs/2501.17377</link>
<guid>https://arxiv.org/abs/2501.17377</guid>
<content:encoded><![CDATA[
arXiv:2501.17377v2 Announce Type: replace-cross 
Abstract: Recently, deep reinforcement learning (DRL) has achieved promising results in solving online 3D Bin Packing Problems (3D-BPP). However, these DRL-based policies may perform poorly on new instances due to distribution shift. Besides generalization, we also consider adaptation, completely overlooked by previous work, which aims at rapidly fine-tuning these policies to a new test distribution. To tackle both generalization and adaptation issues, we propose ASAP, which decomposes a solver's decision-making into two policies, one for proposal and one for selection. The role of the proposal policy is to suggest promising actions, which allows the selection policy to choose among them. To effectively learn these policies, we introduce a training framework that combines pre-training and post-training, enhanced by meta-learning. During online adaptation, we only fine-tune the selection policy to rapidly adapt to a test distribution. Our experiments demonstrate that ASAP exhibits excellent generalization and adaptation capabilities on in-distribution and out-of-distribution instances for both discrete and continuous setups.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation by Interval-wise Dropout: A Simple Way to Prevent Neural Networks from Plasticity Loss</title>
<link>https://arxiv.org/abs/2502.01342</link>
<guid>https://arxiv.org/abs/2502.01342</guid>
<content:encoded><![CDATA[
arXiv:2502.01342v2 Announce Type: replace-cross 
Abstract: Plasticity loss, a critical challenge in neural network training, limits a model's ability to adapt to new tasks or shifts in data distribution. This paper introduces AID (Activation by Interval-wise Dropout), a novel method inspired by Dropout, designed to address plasticity loss. Unlike Dropout, AID generates subnetworks by applying Dropout with different probabilities on each preactivation interval. Theoretical analysis reveals that AID regularizes the network, promoting behavior analogous to that of deep linear networks, which do not suffer from plasticity loss. We validate the effectiveness of AID in maintaining plasticity across various benchmarks, including continual learning tasks on standard image classification datasets such as CIFAR10, CIFAR100, and TinyImageNet. Furthermore, we show that AID enhances reinforcement learning performance in the Arcade Learning Environment benchmark.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Adversarial Estimates for Improving Sample Efficiency in Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.01558</link>
<guid>https://arxiv.org/abs/2502.01558</guid>
<content:encoded><![CDATA[
arXiv:2502.01558v2 Announce Type: replace-cross 
Abstract: Sample inefficiency is a long-lasting challenge in deep reinforcement learning (DRL). Despite dramatic improvements have been made, the problem is far from being solved and is especially challenging in environments with sparse or delayed rewards. In our work, we propose to use Adversarial Estimates as a new, simple and efficient approach to mitigate this problem for a class of feedback-based DRL algorithms. Our approach leverages latent similarity search from a small set of human-collected trajectories to boost learning, using only five minutes of human-recorded experience. The results of our study show algorithms trained with Adversarial Estimates converge faster than their original version. Moreover, we discuss how our approach could enable learning in feedback-based algorithms in extreme scenarios with very sparse rewards.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer by Layer: Uncovering Hidden Representations in Language Models</title>
<link>https://arxiv.org/abs/2502.02013</link>
<guid>https://arxiv.org/abs/2502.02013</guid>
<content:encoded><![CDATA[
arXiv:2502.02013v2 Announce Type: replace-cross 
Abstract: From extracting features to generating text, the outputs of large language models (LLMs) typically rely on the final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Informed Merging of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02421</link>
<guid>https://arxiv.org/abs/2502.02421</guid>
<content:encoded><![CDATA[
arXiv:2502.02421v2 Announce Type: replace-cross 
Abstract: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs, with up to a 40% increase in benchmark performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</title>
<link>https://arxiv.org/abs/2502.02508</link>
<guid>https://arxiv.org/abs/2502.02508</guid>
<content:encoded><![CDATA[
arXiv:2502.02508v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Other Side of the Coin: Unveiling the Downsides of Model Aggregation in Federated Learning from a Layer-peeled Perspective</title>
<link>https://arxiv.org/abs/2502.03231</link>
<guid>https://arxiv.org/abs/2502.03231</guid>
<content:encoded><![CDATA[
arXiv:2502.03231v2 Announce Type: replace-cross 
Abstract: It is often observed that the aggregated model in FL underperforms on local data until after several rounds of local training. This temporary performance drop can potentially slow down the convergence of the FL model. Prior work regards this performance drop as an inherent cost of knowledge sharing among clients and does not give it special attention. While some studies directly focus on designing techniques to alleviate the issue, its root causes remain poorly understood. To bridge this gap, we construct a framework that enables layer-peeled analysis of how feature representations evolve during model aggregation in FL. It focuses on two key aspects: (1) the intrinsic quality of extracted features, and (2) the alignment between features and their subsequent parameters -- both of which are critical to downstream performance. Using this framework, we first investigate how model aggregation affects internal feature extraction process. Our analysis reveals that aggregation degrades feature quality and weakens the coupling between intermediate features and subsequent layers, both of which are well shaped during local training. More importantly, this degradation is not confined to specific layers but progressively accumulates with network depth -- a phenomenon we term Cumulative Feature Degradation (CFD). CFD significantly impairs the quality of penultimate-layer features and weakens their coupling with the classifier, ultimately degrading model performance. We further revisit several widely adopted solutions through the lens of layer-peeled feature extraction to understand why they are effective in addressing aggregation-induced performance drop. Our results show that their effectiveness lies in mitigating the feature degradation described above, which is well aligned with our observations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[
arXiv:2502.03460v2 Announce Type: replace-cross 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title>
<link>https://arxiv.org/abs/2502.04322</link>
<guid>https://arxiv.org/abs/2502.04322</guid>
<content:encoded><![CDATA[
arXiv:2502.04322v2 Announce Type: replace-cross 
Abstract: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Temperature for Language Models with Multi-Sample Inference</title>
<link>https://arxiv.org/abs/2502.05234</link>
<guid>https://arxiv.org/abs/2502.05234</guid>
<content:encoded><![CDATA[
arXiv:2502.05234v2 Announce Type: replace-cross 
Abstract: Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature's role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is attention all you need to solve the correlated electron problem?</title>
<link>https://arxiv.org/abs/2502.05383</link>
<guid>https://arxiv.org/abs/2502.05383</guid>
<content:encoded><![CDATA[
arXiv:2502.05383v3 Announce Type: replace-cross 
Abstract: The attention mechanism has transformed artificial intelligence research by its ability to learn relations between objects. In this work, we explore how a many-body wavefunction ansatz constructed from a large-parameter self-attention neural network can be used to solve the interacting electron problem in solids. By a systematic neural-network variational Monte Carlo study on a moir\'e quantum material, we demonstrate that the self-attention ansatz provides an accurate and efficient solution without human bias. Moreover, our numerical study finds that the required number of variational parameters scales roughly as $N^2$ with the number of electrons, which opens a path towards efficient large-scale simulations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification</title>
<link>https://arxiv.org/abs/2502.07299</link>
<guid>https://arxiv.org/abs/2502.07299</guid>
<content:encoded><![CDATA[
arXiv:2502.07299v2 Announce Type: replace-cross 
Abstract: The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Knowledge-oriented Nighttime Haze Imaging Enhancer for Vision-driven Intelligent Systems</title>
<link>https://arxiv.org/abs/2502.07351</link>
<guid>https://arxiv.org/abs/2502.07351</guid>
<content:encoded><![CDATA[
arXiv:2502.07351v4 Announce Type: replace-cross 
Abstract: Salient object detection (SOD) plays a critical role in Intelligent Imaging, facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality and hinder reliable object detection in real-world scenarios. To address these challenges, we propose a multi-knowledge-oriented nighttime haze imaging enhancer (MKoIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MKoIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and night-time haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead to meet the requirements of real-time imaging deployment. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather/imaging conditions illustrate that MKoIE surpasses existing methods, enhancing the reliability, accuracy, and operational efficiency of intelligent imaging.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2502.08106</link>
<guid>https://arxiv.org/abs/2502.08106</guid>
<content:encoded><![CDATA[
arXiv:2502.08106v3 Announce Type: replace-cross 
Abstract: Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth Knows No Language: Evaluating Truthfulness Beyond English</title>
<link>https://arxiv.org/abs/2502.09387</link>
<guid>https://arxiv.org/abs/2502.09387</guid>
<content:encoded><![CDATA[
arXiv:2502.09387v3 Announce Type: replace-cross 
Abstract: We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
arXiv:2502.09604v3 Announce Type: replace-cross 
Abstract: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. The source code is available at https://github.com/facebookresearch/SelfCite
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARBOR: Exploring Persona Dynamics in Multi-Agent Competition</title>
<link>https://arxiv.org/abs/2502.12149</link>
<guid>https://arxiv.org/abs/2502.12149</guid>
<content:encoded><![CDATA[
arXiv:2502.12149v2 Announce Type: replace-cross 
Abstract: We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Generalization in Diffusion-Based Neural Combinatorial Solver via Inference Time Adaptation</title>
<link>https://arxiv.org/abs/2502.12188</link>
<guid>https://arxiv.org/abs/2502.12188</guid>
<content:encoded><![CDATA[
arXiv:2502.12188v2 Announce Type: replace-cross 
Abstract: Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated effectiveness in solving NP-complete (NPC) problems by learning discrete diffusion models for solution generation, eliminating hand-crafted domain knowledge. Despite their success, existing NCO methods face significant challenges in both cross-scale and cross-problem generalization, and high training costs compared to traditional solvers. While recent studies on diffusion models have introduced training-free guidance approaches that leverage pre-defined guidance functions for conditional generation, such methodologies have not been extensively explored in combinatorial optimization. To bridge this gap, we propose a training-free inference time adaptation framework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and cross-scale generalization capabilities of diffusion-based NCO solvers without requiring additional training. We provide theoretical analysis that helps understanding the cross-problem transfer capability. Our experimental results demonstrate that a diffusion solver, trained exclusively on the Traveling Salesman Problem (TSP), can achieve competitive zero-shot transfer performance across different problem scales on TSP variants, such as Prize Collecting TSP (PCTSP) and the Orienteering Problem (OP), through inference time adaptation.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMedTS: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series</title>
<link>https://arxiv.org/abs/2502.13509</link>
<guid>https://arxiv.org/abs/2502.13509</guid>
<content:encoded><![CDATA[
arXiv:2502.13509v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data, such as lab test results, capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative prompt embeddings. These prompt embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Memorization and Parametric Response Rates in Retrieval-Augmented Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.13836</link>
<guid>https://arxiv.org/abs/2502.13836</guid>
<content:encoded><![CDATA[
arXiv:2502.13836v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. In line with existing work, we find that finetuned models rely more heavily on memorization than retrieval-augmented VLMs, and achieve higher accuracy as a result (72% vs 52% on WebQA test set). Finally, we present the first empirical comparison of the parametric effect between text and visual modalities. Here, we find that image-based questions have parametric response rates that are consistently 15-25% higher than for text-based questions in the WebQA dataset. As such, our measures pose a challenge for future work, both to account for differences in model memorization across different modalities and more generally to reconcile memorization and generalization in joint Retrieval-QA tasks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Improving LLM Alignment via Preference Data Selection</title>
<link>https://arxiv.org/abs/2502.14560</link>
<guid>https://arxiv.org/abs/2502.14560</guid>
<content:encoded><![CDATA[
arXiv:2502.14560v3 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability. Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\% of the Ultrafeedback dataset, our approach achieves 3\% to 8\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\% improvement with 25\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C2-DPO: Constrained Controlled Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.17507</link>
<guid>https://arxiv.org/abs/2502.17507</guid>
<content:encoded><![CDATA[
arXiv:2502.17507v2 Announce Type: replace-cross 
Abstract: Direct preference optimization (\texttt{DPO}) has emerged as a promising approach for solving the alignment problem in AI. In this paper, we make two counter-intuitive observations about \texttt{DPO}. First, we show that \texttt{DPO} loss could be derived by starting from an alternative optimization problem that only defines the KL guardrail on in-sample responses, unlike the original RLHF problem where guardrails are defined on the entire distribution. Second, we prove a surprising property of this alternative optimization problem, namely that under its optimal policy, both preferred and rejected responses tend to decrease in probability, a phenomenon typically displayed by DPO in practice. To control this behavior, we propose a set of constraints designed to limit the displacement of probability mass between the preferred and rejected responses in the reference and target policies. The resulting algorithm, which we call Constrained Controlled DPO (\texttt{C2-DPO}), has a meaningful RLHF interpretation. By hedging against the displacement, \texttt{C2-DPO} provides practical improvements over vanilla \texttt{DPO} when aligning several language models using standard preference datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Euler to AI: Unifying Formulas for Mathematical Constants</title>
<link>https://arxiv.org/abs/2502.17533</link>
<guid>https://arxiv.org/abs/2502.17533</guid>
<content:encoded><![CDATA[
arXiv:2502.17533v2 Announce Type: replace-cross 
Abstract: The constant $\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines large language models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 407 distinct formulas for $\pi$ and prove relations between 381 (94%) of them, of which 188 (46%) can be derived from a single mathematical object$\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine. Our method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model</title>
<link>https://arxiv.org/abs/2503.02969</link>
<guid>https://arxiv.org/abs/2503.02969</guid>
<content:encoded><![CDATA[
arXiv:2503.02969v2 Announce Type: replace-cross 
Abstract: Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code and demo at https://github.com/LeiLiLab/InfiniSST
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Trustworthiness Challenges in Deep Learning Models for Continental-Scale Water Quality Prediction</title>
<link>https://arxiv.org/abs/2503.09947</link>
<guid>https://arxiv.org/abs/2503.09947</guid>
<content:encoded><![CDATA[
arXiv:2503.09947v2 Announce Type: replace-cross 
Abstract: Water quality is foundational to environmental sustainability, ecosystem resilience, and public health. Deep learning models, particularly Long Short-Term Memory (LSTM) networks, offer transformative potential for large-scale water quality prediction and scientific insights generation. However, their widespread adoption in high-stakes decision-making, such as pollution mitigation and equitable resource allocation, is prevented by unresolved trustworthiness challenges including fairness, uncertainty, interpretability, robustness, generalizability, and reproducibility. In this work, we present the first comprehensive evaluation of trustworthiness in a continental-scale multi-task LSTM model predicting 20 water quality variables (encompassing physical/chemical processes, geochemical weathering, and nutrient cycling) across 482 U.S. basins. Our investigation uncovers systematic patterns of model performance disparities linked to basin characteristics, the inherent complexity of biogeochemical processes, and variable predictability, emphasizing critical performance fairness concerns. We further propose methodological frameworks for quantitatively evaluating critical aspects of trustworthiness, including uncertainty, interpretability, and robustness, identifying key limitations that could challenge reliable real-world deployment. This work serves as a timely call to action for advancing trustworthy data-driven methods for water resources management and provides a pathway to offering critical insights for researchers, decision-makers, and practitioners seeking to leverage artificial intelligence (AI) responsibly in environmental management.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compute Optimal Scaling of Skills: Knowledge vs Reasoning</title>
<link>https://arxiv.org/abs/2503.10061</link>
<guid>https://arxiv.org/abs/2503.10061</guid>
<content:encoded><![CDATA[
arXiv:2503.10061v3 Announce Type: replace-cross 
Abstract: Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers without Normalization</title>
<link>https://arxiv.org/abs/2503.10622</link>
<guid>https://arxiv.org/abs/2503.10622</guid>
<content:encoded><![CDATA[
arXiv:2503.10622v2 Announce Type: replace-cross 
Abstract: Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathFusion: Enhancing Mathematical Problem-solving of LLM through Instruction Fusion</title>
<link>https://arxiv.org/abs/2503.16212</link>
<guid>https://arxiv.org/abs/2503.16212</guid>
<content:encoded><![CDATA[
arXiv:2503.16212v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents</title>
<link>https://arxiv.org/abs/2503.16465</link>
<guid>https://arxiv.org/abs/2503.16465</guid>
<content:encoded><![CDATA[
arXiv:2503.16465v2 Announce Type: replace-cross 
Abstract: Autonomous graphical user interface (GUI) agents powered by multimodal large language models have shown great promise. However, a critical yet underexplored issue persists: over-execution, where the agent executes tasks in a fully autonomous way, without adequate assessment of its action confidence to compromise an adaptive human-agent collaboration. This poses substantial risks in complex scenarios, such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks. To address the issue, we introduce OS-Kairos, an adaptive GUI agent capable of predicting confidence levels at each interaction step and efficiently deciding whether to act autonomously or seek human intervention. OS-Kairos is developed through two key mechanisms: (i) collaborative probing that annotates confidence scores at each interaction step; (ii) confidence-driven interaction that leverages these confidence scores to elicit the ability of adaptive interaction. Experimental results show that OS-Kairos substantially outperforms existing models on our curated dataset featuring complex scenarios, as well as on established benchmarks such as AITZ and Meta-GUI, with 24.59\%$\sim$87.29\% improvements in task success rate. OS-Kairos facilitates an adaptive human-agent collaboration, prioritizing effectiveness, generality, scalability, and efficiency for real-world GUI interaction. The dataset and codes are available at https://github.com/Wuzheng02/OS-Kairos.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices</title>
<link>https://arxiv.org/abs/2503.18242</link>
<guid>https://arxiv.org/abs/2503.18242</guid>
<content:encoded><![CDATA[
arXiv:2503.18242v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities on a broad array of NLP tasks, but their tendency to produce hallucinations$\unicode{x2013}$plausible-sounding but factually incorrect content$\unicode{x2013}$poses severe challenges in high-stakes domains. Existing hallucination detection methods either bear the computational cost of multiple inference passes or sacrifice accuracy for efficiency with single-pass approaches, neither of which is ideal in resource-constrained environments such as edge devices. We propose the Shannon Entropy Distribution Hallucination Detector (ShED-HD), a novel hallucination detection framework that bridges this gap by classifying sequence-level entropy patterns using a lightweight BiLSTM architecture with single-headed attention. In contrast to prior approaches, ShED-HD efficiently detects distinctive uncertainty patterns across entire output sequences, preserving contextual awareness. Through in-depth evaluation on three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that ShED-HD significantly outperforms other computationally efficient approaches in the out-of-distribution setting, while achieving comparable performance in the in-distribution setting. ShED-HD facilitates hallucination detection that is low-cost, accurate, and generalizable, improving the credibility of content generated by LLMs in resource-constrained environments where trustworthy AI functionality is crucial.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions</title>
<link>https://arxiv.org/abs/2503.20290</link>
<guid>https://arxiv.org/abs/2503.20290</guid>
<content:encoded><![CDATA[
arXiv:2503.20290v3 Announce Type: replace-cross 
Abstract: This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating how LLM annotations represent diverse views on contentious topics</title>
<link>https://arxiv.org/abs/2503.23243</link>
<guid>https://arxiv.org/abs/2503.23243</guid>
<content:encoded><![CDATA[
arXiv:2503.23243v2 Announce Type: replace-cross 
Abstract: Researchers have proposed the use of generative large language models (LLMs) to label data for research and applied settings. This literature emphasizes the improved performance of these models relative to other natural language models, noting that generative LLMs typically outperform other models and even humans across several metrics. Previous literature has examined bias across many applications and contexts, but less work has focused specifically on bias in generative LLMs' responses to subjective annotation tasks. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show systematic substantial disagreement with annotators on the basis of demographics. Rather, we find that multiple LLMs tend to be biased in the same directions on the same demographic categories within the same datasets. Moreover, the disagreement between human annotators on the labeling task -- a measure of item difficulty -- is far more predictive of LLM agreement with human annotators. We conclude with a discussion of the implications for researchers and practitioners using LLMs for automated data annotation tasks. Specifically, we emphasize that fairness evaluations must be contextual, model choice alone will not solve potential issues of bias, and item difficulty must be integrated into bias assessments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthesizing Data for Context Attribution in Question Answering</title>
<link>https://arxiv.org/abs/2504.05317</link>
<guid>https://arxiv.org/abs/2504.05317</guid>
<content:encoded><![CDATA[
arXiv:2504.05317v2 Announce Type: replace-cross 
Abstract: Question Answering (QA) accounts for a significant portion of LLM usage "in the wild". However, LLMs sometimes produce false or misleading responses, also known as "hallucinations". Therefore, grounding the generated answers in contextually provided information -- i.e., providing evidence for the generated text -- is paramount for LLMs' trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs' natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scholar Inbox: Personalized Paper Recommendations for Scientists</title>
<link>https://arxiv.org/abs/2504.08385</link>
<guid>https://arxiv.org/abs/2504.08385</guid>
<content:encoded><![CDATA[
arXiv:2504.08385v2 Announce Type: replace-cross 
Abstract: Scholar Inbox is a new open-access platform designed to address the challenges researchers face in staying current with the rapidly expanding volume of scientific literature. We provide personalized recommendations, continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual paper summaries, semantic search, and a range of tools to streamline research workflows and promote open research access. The platform's personalized recommendation system is trained on user ratings, ensuring that recommendations are tailored to individual researchers' interests. To further enhance the user experience, Scholar Inbox also offers a map of science that provides an overview of research across domains, enabling users to easily explore specific topics. We use this map to address the cold start problem common in recommender systems, as well as an active learning strategy that iteratively prompts users to rate a selection of papers, allowing the system to learn user preferences quickly. We evaluate the quality of our recommendation system on a novel dataset of 800k user ratings, which we make publicly available, as well as via an extensive user study. https://www.scholar-inbox.com/
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design</title>
<link>https://arxiv.org/abs/2504.10112</link>
<guid>https://arxiv.org/abs/2504.10112</guid>
<content:encoded><![CDATA[
arXiv:2504.10112v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. Due to the opaque nature of LLMs, empirical methods are typically used to analyze their efficacy. The quality of this analysis is highly dependent on the chosen testbed, captured metrics and analysis methods employed.
  This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We review 19 research papers detailing 18 prototypes and their respective testbeds.
  We detail our findings and provide actionable recommendations for future research, emphasizing the importance of extending existing testbeds, creating baselines, and including comprehensive metrics and qualitative analysis. We also note the distinction between security research and practice, suggesting that CTF-based challenges may not fully represent real-world penetration testing scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture</title>
<link>https://arxiv.org/abs/2504.10512</link>
<guid>https://arxiv.org/abs/2504.10512</guid>
<content:encoded><![CDATA[
arXiv:2504.10512v2 Announce Type: replace-cross 
Abstract: Language representation learning has emerged as a promising approach for sequential recommendation, thanks to its ability to learn generalizable representations. However, despite its advantages, this approach still struggles with data sparsity and a limited understanding of common-sense user preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a framework that combines $\textbf{J}$oint $\textbf{E}$mbedding $\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item textual descriptions. JEPA4Rec captures semantically rich and transferable representations, improving recommendation performance and reducing reliance on large-scale pre-training data. Specifically, JEPA4Rec represents items as text sentences by flattening descriptive information such as $\textit{title, category}$, and other attributes. To encode these sentences, we employ a bidirectional Transformer encoder with modified embedding layers tailored for capturing item information in recommendation datasets. We apply masking to text sentences and use them to predict the representations of the unmasked sentences, helping the model learn generalizable item embeddings. To further improve recommendation performance and language understanding, we employ a two-stage training strategy incorporating self-supervised learning losses. Experiments on six real-world datasets demonstrate that JEPA4Rec consistently outperforms state-of-the-art methods, particularly in cross-domain, cross-platform, and low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Time Schedulers for Generative Diffusion Models</title>
<link>https://arxiv.org/abs/2504.13612</link>
<guid>https://arxiv.org/abs/2504.13612</guid>
<content:encoded><![CDATA[
arXiv:2504.13612v3 Announce Type: replace-cross 
Abstract: The practical performance of generative diffusion models depends on the appropriate choice of the noise scheduling function, which can also be equivalently expressed as a time reparameterization. In this paper, we present a time scheduler that selects sampling points based on entropy rather than uniform time spacing, ensuring that each point contributes an equal amount of information to the final generation. We prove that this time reparameterization does not depend on the initial choice of time. Furthermore, we provide a tractable exact formula to estimate this \emph{entropic time} for a trained model using the training loss without substantial overhead. Alongside the entropic time, inspired by the optimality results, we introduce a rescaled entropic time. In our experiments with mixtures of Gaussian distributions and ImageNet, we show that using the (rescaled) entropic times greatly improves the inference performance of trained models. In particular, we found that the image quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can be substantially increased by the rescaled entropic time reparameterization without increasing the number of function evaluations, with greater improvements in the few NFEs regime.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Recommendations and Non-instrumental Image Concerns</title>
<link>https://arxiv.org/abs/2504.19047</link>
<guid>https://arxiv.org/abs/2504.19047</guid>
<content:encoded><![CDATA[
arXiv:2504.19047v2 Announce Type: replace-cross 
Abstract: There is growing enthusiasm about the potential for humans and AI to collaborate by leveraging their respective strengths. Yet in practice, this promise often falls short. This paper uses an online experiment to identify non-instrumental image concerns as a key reason individuals underutilize AI recommendations. I show that concerns about how one is perceived, even when those perceptions carry no monetary consequences, lead participants to disregard AI advice and reduce task performance.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents</title>
<link>https://arxiv.org/abs/2505.05849</link>
<guid>https://arxiv.org/abs/2505.05849</guid>
<content:encoded><![CDATA[
arXiv:2505.05849v4 Announce Type: replace-cross 
Abstract: The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentVigil exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAEGER: Dual-Level Humanoid Whole-Body Controller</title>
<link>https://arxiv.org/abs/2505.06584</link>
<guid>https://arxiv.org/abs/2505.06584</guid>
<content:encoded><![CDATA[
arXiv:2505.06584v2 Announce Type: replace-cross 
Abstract: This paper presents JAEGER, a dual-level whole-body controller for humanoid robots that addresses the challenges of training a more robust and versatile policy. Unlike traditional single-controller approaches, JAEGER separates the control of the upper and lower bodies into two independent controllers, so that they can better focus on their distinct tasks. This separation alleviates the dimensionality curse and improves fault tolerance. JAEGER supports both root velocity tracking (coarse-grained control) and local joint angle tracking (fine-grained control), enabling versatile and stable movements. To train the controller, we utilize a human motion dataset (AMASS), retargeting human poses to humanoid poses through an efficient retargeting network, and employ a curriculum learning approach. This method performs supervised learning for initialization, followed by reinforcement learning for further exploration. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-art methods in both simulation and real environments.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</title>
<link>https://arxiv.org/abs/2505.07096</link>
<guid>https://arxiv.org/abs/2505.07096</guid>
<content:encoded><![CDATA[
arXiv:2505.07096v3 Announce Type: replace-cross 
Abstract: Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixed Point Explainability</title>
<link>https://arxiv.org/abs/2505.12421</link>
<guid>https://arxiv.org/abs/2505.12421</guid>
<content:encoded><![CDATA[
arXiv:2505.12421v2 Announce Type: replace-cross 
Abstract: This paper introduces a formal notion of fixed point explanations, inspired by the "why regress" principle, to assess, through recursive applications, the stability of the interplay between a model and its explainer. Fixed point explanations satisfy properties like minimality, stability, and faithfulness, revealing hidden model behaviours and explanatory weaknesses. We define convergence conditions for several classes of explainers, from feature-based to mechanistic tools like Sparse AutoEncoders, and we report quantitative and qualitative results.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision</title>
<link>https://arxiv.org/abs/2505.14999</link>
<guid>https://arxiv.org/abs/2505.14999</guid>
<content:encoded><![CDATA[
arXiv:2505.14999v2 Announce Type: replace-cross 
Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oversmoothing, Oversquashing, Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning</title>
<link>https://arxiv.org/abs/2505.15547</link>
<guid>https://arxiv.org/abs/2505.15547</guid>
<content:encoded><![CDATA[
arXiv:2505.15547v2 Announce Type: replace-cross 
Abstract: After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this position paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records</title>
<link>https://arxiv.org/abs/2505.16941</link>
<guid>https://arxiv.org/abs/2505.16941</guid>
<content:encoded><![CDATA[
arXiv:2505.16941v3 Announce Type: replace-cross 
Abstract: Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained on structured electronic health record (EHR) data, even in settings with limited labeled data, a prevalent challenge in healthcare. However, there is little consensus on these models' potential for clinical utility due to the lack of desiderata of comprehensive and meaningful tasks and sufficiently diverse evaluations to characterize the benefit over conventional supervised learning. To address this gap, we propose a suite of clinically meaningful tasks spanning patient outcomes, early prediction of acute and chronic conditions, including desiderata for robust evaluations. We evaluate state-of-the-art foundation models on EHR data consisting of 5 million patients from Columbia University Irving Medical Center (CUMC), a large urban academic medical center in New York City, across 14 clinically relevant tasks. We measure overall accuracy, calibration, and subpopulation performance to surface tradeoffs based on the choice of pre-training, tokenization, and data representation strategies. Our study aims to advance the empirical evaluation of structured EHR foundation models and guide the development of future healthcare foundation models.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs</title>
<link>https://arxiv.org/abs/2505.17105</link>
<guid>https://arxiv.org/abs/2505.17105</guid>
<content:encoded><![CDATA[
arXiv:2505.17105v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) plays an essential role in healthcare and is pervasively incorporated into medical software and equipment. In the European Union, healthcare is a high-risk application domain for AI, and providers must prepare Instructions for Use (IFU) according to the European regulation 2024/1689 (AI Act). To this regulation, the principle of transparency is cardinal and requires the IFU to be clear and relevant to the users. This study tests whether these latter requirements are satisfied by the IFU structure. A survey was administered online via the Qualtrics platform to four types of direct stakeholders, i.e., managers (N = 238), healthcare professionals (N = 115), patients (N = 229), and Information Technology experts (N = 230). The participants rated the relevance of a set of transparency needs and indicated the IFU section addressing them. The results reveal differentiated priorities across stakeholders and a troubled mapping of transparency needs onto the IFU structure. Recommendations to build a locally meaningful IFU are derived.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding</title>
<link>https://arxiv.org/abs/2505.20759</link>
<guid>https://arxiv.org/abs/2505.20759</guid>
<content:encoded><![CDATA[
arXiv:2505.20759v2 Announce Type: replace-cross 
Abstract: Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge</title>
<link>https://arxiv.org/abs/2505.21605</link>
<guid>https://arxiv.org/abs/2505.21605</guid>
<content:encoded><![CDATA[
arXiv:2505.21605v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning</title>
<link>https://arxiv.org/abs/2505.22389</link>
<guid>https://arxiv.org/abs/2505.22389</guid>
<content:encoded><![CDATA[
arXiv:2505.22389v3 Announce Type: replace-cross 
Abstract: Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose \textbf{Perturb-and-Merge (P\&amp;M)}, a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\&amp;M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, we minimize the total loss increase across all tasks and derive an analytical solution for the optimal merging coefficient. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\&amp;M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the performance of machine-learning-assisted Monte Carlo in sampling from simple statistical physics models</title>
<link>https://arxiv.org/abs/2505.22598</link>
<guid>https://arxiv.org/abs/2505.22598</guid>
<content:encoded><![CDATA[
arXiv:2505.22598v3 Announce Type: replace-cross 
Abstract: Recent years have seen a rise in the application of machine learning techniques to aid the simulation of hard-to-sample systems that cannot be studied using traditional methods. Despite the introduction of many different architectures and procedures, a wide theoretical understanding is still lacking, with the risk of suboptimal implementations. As a first step to address this gap, we provide here a complete analytic study of the widely-used Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model. The contribution of this work is twofold: firstly, we give a description of the optimal weights and of the training under Gradient Descent optimization. Secondly, we compare what happens in Sequential Tempering with and without the addition of local Metropolis Monte Carlo steps. We are thus able to give theoretical predictions on the best procedure to apply in this case. This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum computing and artificial intelligence: status and perspectives</title>
<link>https://arxiv.org/abs/2505.23860</link>
<guid>https://arxiv.org/abs/2505.23860</guid>
<content:encoded><![CDATA[
arXiv:2505.23860v2 Announce Type: replace-cross 
Abstract: This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title>
<link>https://arxiv.org/abs/2505.24133</link>
<guid>https://arxiv.org/abs/2505.24133</guid>
<content:encoded><![CDATA[
arXiv:2505.24133v3 Announce Type: replace-cross 
Abstract: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</title>
<link>https://arxiv.org/abs/2506.00424</link>
<guid>https://arxiv.org/abs/2506.00424</guid>
<content:encoded><![CDATA[
arXiv:2506.00424v2 Announce Type: replace-cross 
Abstract: Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Graph Neural Networks for Robustness in Olfaction Sensors and Datasets</title>
<link>https://arxiv.org/abs/2506.00455</link>
<guid>https://arxiv.org/abs/2506.00455</guid>
<content:encoded><![CDATA[
arXiv:2506.00455v2 Announce Type: replace-cross 
Abstract: Robotic odour source localization (OSL) is a critical capability for autonomous systems operating in complex environments. However, current OSL methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address this challenge, we introduce a novel machine learning method using diffusion-based molecular generation to enhance odour localization accuracy that can be used by itself or with automated olfactory dataset construction pipelines with vision-language models (VLMs) This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and the training data of VLMs, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors which emulate human olfactory recognition through electronic sensor arrays. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making through better sensor selection for a target compound. Our methodology represents a foundational advancement in the field of artificial olfaction, offering a scalable solution to the challenges posed by limited olfactory data and sensor ambiguities.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Incremental Framework for Topological Dialogue Semantics: Efficient Reasoning in Discrete Spaces</title>
<link>https://arxiv.org/abs/2506.00615</link>
<guid>https://arxiv.org/abs/2506.00615</guid>
<content:encoded><![CDATA[
arXiv:2506.00615v2 Announce Type: replace-cross 
Abstract: We present a tractable, incremental framework for topological dialogue semantics based on finite, discrete semantic spaces. Building on the intuition that utterances correspond to open sets and their combinatorial relations form a simplicial complex (the dialogue nerve), we give a rigorous foundation, a provably correct incremental algorithm for nerve updates, and a reference implementation in the Wolfram Language. The framework supports negative nerve computation (inconsistency tracking), consequence extraction, and a transparent, set-theoretic ranking of entailments. We clarify which combinatorial properties hold in the discrete case, provide motivating examples, and outline limitations and prospects for richer logical and categorical extensions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</title>
<link>https://arxiv.org/abs/2506.00713</link>
<guid>https://arxiv.org/abs/2506.00713</guid>
<content:encoded><![CDATA[
arXiv:2506.00713v2 Announce Type: replace-cross 
Abstract: This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider</title>
<link>https://arxiv.org/abs/2506.02634</link>
<guid>https://arxiv.org/abs/2506.02634</guid>
<content:encoded><![CDATA[
arXiv:2506.02634v2 Announce Type: replace-cross 
Abstract: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population</title>
<link>https://arxiv.org/abs/2506.03177</link>
<guid>https://arxiv.org/abs/2506.03177</guid>
<content:encoded><![CDATA[
arXiv:2506.03177v2 Announce Type: replace-cross 
Abstract: This study presents a deep learning system for breast cancer detection in mammography, developed using a modified EfficientNetV2 architecture with enhanced attention mechanisms. The model was trained on mammograms from a major Thai medical center and validated on three distinct datasets: an in-domain test set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain generalizability set (761 cases) collected from two different hospitals. For cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the respective datasets. The system's lesion localization capability, evaluated using metrics including Lesion Localization Fraction (LLF) and Non-Lesion Localization Fraction (NLF), demonstrated robust performance in identifying suspicious regions. Clinical validation through concordance tests showed strong agreement with radiologists: 83.5% classification and 84.0% localization concordance for biopsy-confirmed cases, and 78.1% classification and 79.6% localization concordance for out-of-domain cases. Expert radiologists' acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for out-of-domain cases. The system achieved a System Usability Scale score of 74.17 for source hospital, and 69.20 for validation hospitals, indicating good clinical acceptance. These results demonstrate the model's effectiveness in assisting mammogram interpretation, with the potential to enhance breast cancer screening workflows in clinical practice.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP</title>
<link>https://arxiv.org/abs/2506.04385</link>
<guid>https://arxiv.org/abs/2506.04385</guid>
<content:encoded><![CDATA[
arXiv:2506.04385v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more "traditional" language modelling approaches.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System</title>
<link>https://arxiv.org/abs/2506.05020</link>
<guid>https://arxiv.org/abs/2506.05020</guid>
<content:encoded><![CDATA[
arXiv:2506.05020v2 Announce Type: replace-cross 
Abstract: Heterogeneous multi-robot systems show great potential in complex tasks requiring hybrid cooperation. However, traditional approaches relying on static models often struggle with task diversity and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical framework integrating a prompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision Language Model (VLM). The LLM decomposes tasks and constructs a global semantic map, while the VLM extracts task-specified semantic labels and 2D spatial information from aerial images to support local planning. Within this framework, the aerial robot follows an optimized global semantic path and continuously provides bird-view images, guiding the ground robot's local semantic navigation and manipulation, including target-absent scenarios where implicit alignment is maintained. Experiments on real-world cube or object arrangement tasks demonstrate the framework's adaptability and robustness in dynamic environments. To the best of our knowledge, this is the first demonstration of an aerial-ground heterogeneous system integrating VLM-based perception with LLM-driven task reasoning and motion planning.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code</title>
<link>https://arxiv.org/abs/2506.05692</link>
<guid>https://arxiv.org/abs/2506.05692</guid>
<content:encoded><![CDATA[
arXiv:2506.05692v2 Announce Type: replace-cross 
Abstract: The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tactile MNIST: Benchmarking Active Tactile Perception</title>
<link>https://arxiv.org/abs/2506.06361</link>
<guid>https://arxiv.org/abs/2506.06361</guid>
<content:encoded><![CDATA[
arXiv:2506.06361v2 Announce Type: replace-cross 
Abstract: Tactile perception has the potential to significantly enhance dexterous robotic manipulation by providing rich local information that can complement or substitute for other sensory modalities such as vision. However, because tactile sensing is inherently local, it is not well-suited for tasks that require broad spatial awareness or global scene understanding on its own. A human-inspired strategy to address this issue is to consider active perception techniques instead. That is, to actively guide sensors toward regions with more informative or significant features and integrate such information over time in order to understand a scene or complete a task. Both active perception and different methods for tactile sensing have received significant attention recently. Yet, despite advancements, both fields lack standardized benchmarks. To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an open-source, Gymnasium-compatible benchmark specifically designed for active tactile perception tasks, including localization, classification, and volume estimation. Our benchmark suite offers diverse simulation scenarios, from simple toy environments all the way to complex tactile perception tasks using vision-based tactile sensors. Furthermore, we also offer a comprehensive dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600 real-world tactile samples collected from 600 3D printed digits. Using this dataset, we train a CycleGAN for realistic tactile simulation rendering. By providing standardized protocols and reproducible evaluation frameworks, our benchmark suite facilitates systematic progress in the fields of tactile sensing and active perception.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFE: Finding Sparse and Flat Minima to Improve Pruning</title>
<link>https://arxiv.org/abs/2506.06866</link>
<guid>https://arxiv.org/abs/2506.06866</guid>
<content:encoded><![CDATA[
arXiv:2506.06866v2 Announce Type: replace-cross 
Abstract: Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress. Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time. Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective. We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines. In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Physics-informed Diffusion for Anomaly Detection in Trajectories</title>
<link>https://arxiv.org/abs/2506.06999</link>
<guid>https://arxiv.org/abs/2506.06999</guid>
<content:encoded><![CDATA[
arXiv:2506.06999v2 Announce Type: replace-cross 
Abstract: Given trajectory data, a domain-specific study area, and a user-defined threshold, we aim to find anomalous trajectories indicative of possible GPS spoofing (e.g., fake trajectory). The problem is societally important to curb illegal activities in international waters, such as unauthorized fishing and illicit oil transfers. The problem is challenging due to advances in AI generated in deep fakes generation (e.g., additive noise, fake trajectories) and lack of adequate amount of labeled samples for ground-truth verification. Recent literature shows promising results for anomalous trajectory detection using generative models despite data sparsity. However, they do not consider fine-scale spatiotemporal dependencies and prior physical knowledge, resulting in higher false-positive rates. To address these limitations, we propose a physics-informed diffusion model that integrates kinematic constraints to identify trajectories that do not adhere to physical laws. Experimental results on real-world datasets in the maritime and urban domains show that the proposed framework results in higher prediction accuracy and lower estimation error rate for anomaly detection and trajectory generation methods, respectively. Our implementation is available at https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks</title>
<link>https://arxiv.org/abs/2506.07016</link>
<guid>https://arxiv.org/abs/2506.07016</guid>
<content:encoded><![CDATA[
arXiv:2506.07016v2 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audio-visual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance. Project: https://schowdhury671.github.io/magnet_project/
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments</title>
<link>https://arxiv.org/abs/2506.07355</link>
<guid>https://arxiv.org/abs/2506.07355</guid>
<content:encoded><![CDATA[
arXiv:2506.07355v2 Announce Type: replace-cross 
Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model adaptation framework for Split Computing under closed constraints, where the head and tail networks are proprietary and inaccessible to users. In such closed environments, conventional adaptation methods are infeasible since they require access to model parameters or architectures. SALT addresses this challenge by introducing a compact, trainable adapter on the client side to refine latent features from the head network, enabling user-specific adaptation without modifying the original models or increasing communication overhead. We evaluate SALT on user-specific classification tasks with CIFAR-10 and CIFAR-100, demonstrating improved accuracy with lower training latency compared to fine-tuning methods. Furthermore, SALT facilitates model adaptation for robust inference over lossy networks, a common challenge in edge-cloud environments. With minimal deployment overhead, SALT offers a practical solution for personalized inference in edge AI systems under strict system constraints.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement</title>
<link>https://arxiv.org/abs/2506.07431</link>
<guid>https://arxiv.org/abs/2506.07431</guid>
<content:encoded><![CDATA[
arXiv:2506.07431v2 Announce Type: replace-cross 
Abstract: Accurate ultrasound image segmentation is a prerequisite for precise biometrics and accurate assessment. Relying on manual delineation introduces significant errors and is time-consuming. However, existing segmentation models are designed based on objects in natural scenes, making them difficult to adapt to ultrasound objects with high noise and high similarity. This is particularly evident in small object segmentation, where a pronounced jagged effect occurs. Therefore, this paper proposes a fetal femur and cranial ultrasound image segmentation model based on feature perception and Mamba enhancement to address these challenges. Specifically, a longitudinal and transverse independent viewpoint scanning convolution block and a feature perception module were designed to enhance the ability to capture local detail information and improve the fusion of contextual information. Combined with the Mamba-optimized residual structure, this design suppresses the interference of raw noise and enhances local multi-dimensional scanning. The system builds global information and local feature dependencies, and is trained with a combination of different optimizers to achieve the optimal solution. After extensive experimental validation, the FAMSeg network achieved the fastest loss reduction and the best segmentation performance across images of varying sizes and orientations.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeVo: High-Quality Song Generation with Multi-Preference Alignment</title>
<link>https://arxiv.org/abs/2506.07520</link>
<guid>https://arxiv.org/abs/2506.07520</guid>
<content:encoded><![CDATA[
arXiv:2506.07520v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) and audio language models have significantly improved music generation, particularly in lyrics-to-song generation. However, existing approaches still struggle with the complex composition of songs and the scarcity of high-quality data, leading to limitations in sound quality, musicality, instruction following, and vocal-instrument harmony. To address these challenges, we introduce LeVo, an LM-based framework consisting of LeLM and a music codec. LeLM is capable of parallelly modeling two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation. It employs two decoder-only transformers and a modular extension training strategy to prevent interference between different token types. To further enhance musicality and instruction following, we introduce a multi-preference alignment method based on Direct Preference Optimization (DPO). This method handles diverse human preferences through a semi-automatic data construction process and DPO post-training. Experimental results demonstrate that LeVo consistently outperforms existing methods on both objective and subjective metrics. Ablation studies further justify the effectiveness of our designs. Audio examples are available at https://levo-demo.github.io/. Code is released at https://github.com/tencent-ailab/songgeneration.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries</title>
<link>https://arxiv.org/abs/2506.07555</link>
<guid>https://arxiv.org/abs/2506.07555</guid>
<content:encoded><![CDATA[
arXiv:2506.07555v2 Announce Type: replace-cross 
Abstract: Generating high fidelity, differentially private (DP) synthetic images offers a promising route to share and analyze sensitive visual data without compromising individual privacy. However, existing DP image synthesis methods struggle to produce high resolution outputs that faithfully capture the structure of the original data. In this paper, we introduce a novel method, referred to as Synthesis via Private Textual Intermediaries (SPTI), that can generate high resolution DP images with easy adoption. The key idea is to shift the challenge of DP image synthesis from the image domain to the text domain by leveraging state of the art DP text generation methods. SPTI first summarizes each private image into a concise textual description using image to text models, then applies a modified Private Evolution algorithm to generate DP text, and finally reconstructs images using text to image models. Notably, SPTI requires no model training, only inference with off the shelf models. Given a private dataset, SPTI produces synthetic images of substantially higher quality than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less than or equal to 26.71 under epsilon equal to 1.0, improving over Private Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine tuning baselines. Overall, our results demonstrate that Synthesis via Private Textual Intermediaries provides a resource efficient and proprietary model compatible framework for generating high resolution DP synthetic images, greatly expanding access to private visual datasets.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</title>
<link>https://arxiv.org/abs/2506.07603</link>
<guid>https://arxiv.org/abs/2506.07603</guid>
<content:encoded><![CDATA[
arXiv:2506.07603v2 Announce Type: replace-cross 
Abstract: Surgical video understanding is pivotal for enabling automated intraoperative decision-making, skill assessment, and postoperative quality improvement. However, progress in developing surgical video foundation models (FMs) remains hindered by the scarcity of large-scale, diverse datasets for pretraining and systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a unified surgical video benchmarking framework comprising a pretraining dataset, \textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}. SurgBench offers extensive coverage of diverse surgical scenarios, with SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11 specialties, and SurgBench-E providing robust evaluation across six categories (phase classification, camera motion, tool recognition, disease diagnosis, action classification, and organ detection) spanning 72 fine-grained tasks. Extensive experiments reveal that existing video FMs struggle to generalize across varied surgical video analysis tasks, whereas pretraining on SurgBench-P yields substantial performance improvements and superior cross-domain generalization to unseen procedures and modalities. Our dataset and code are available upon request.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes</title>
<link>https://arxiv.org/abs/2506.07864</link>
<guid>https://arxiv.org/abs/2506.07864</guid>
<content:encoded><![CDATA[
arXiv:2506.07864v2 Announce Type: replace-cross 
Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous monitoring to prevent severe hypo- and hyperglycemic events. While continuous glucose monitoring has improved blood glucose management, deploying predictive models on wearable devices remains challenging due to computational and memory constraints. To address this, we propose a novel Lightweight Sequential Transformer model designed for blood glucose prediction in T1D. By integrating the strengths of Transformers' attention mechanisms and the sequential processing of recurrent neural networks, our architecture captures long-term dependencies while maintaining computational efficiency. The model is optimized for deployment on resource-constrained edge devices and incorporates a balanced loss function to handle the inherent data imbalance in hypo- and hyperglycemic events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend, demonstrate that the proposed model outperforms state-of-the-art methods in predicting glucose levels and detecting adverse events. This work fills the gap between high-performance modeling and practical deployment, providing a reliable and efficient T1D management solution.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
arXiv:2506.08001v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Semantics, Semantic Spacetime, and Graphical Reasoning</title>
<link>https://arxiv.org/abs/2506.07756</link>
<guid>https://arxiv.org/abs/2506.07756</guid>
<content:encoded><![CDATA[
<div> representation, knowledge, process modelling, Semantic Spacetime, information leakage

Summary:
The article discusses the formal aspects of the Semantic Spacetime graph model, particularly focusing on its use for directed knowledge representations and process modelling. It introduces the concept of a finite $\gamma(3,4)$ representation that allows operations to scale to any level of semantic complexity. The Semantic Spacetime postulates aim to provide predictability with minimal constraints in graph pathways. The presence of absorbing states in partial graphs leads to information leakage, similar to the issue of division by zero, indicating a loss of closure and the need for manual insertion of remedial information. The origins of the Semantic Spacetime model, along with its Promise Theory, shed light on how absorbing states relate to boundary information where intentionality can be incorporated. <div>
arXiv:2506.07756v2 Announce Type: replace 
Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its use for directed knowledge representations and process modelling. A finite $\gamma(3,4)$ representation is defined to form a closed set of operations that can scale to any degree of semantic complexity. The Semantic Spacetime postulates bring predictability with minimal constraints to pathways in graphs. The ubiquitous appearance of absorbing states in any partial graph means that a graph process leaks information. The issue is closely associated with the issue of division by zero, which signals a loss of closure and the need for manual injection of remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify how such absorbing states are associated with boundary information where intentionality can enter.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DURA-CPS: A Multi-Role Orchestrator for Dependability Assurance in LLM-Enabled Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2506.06381</link>
<guid>https://arxiv.org/abs/2506.06381</guid>
<content:encoded><![CDATA[
<div> framework, DURA-CPS, multi-role orchestration, AI-powered CPS, verification and validation

Summary: 
The paper introduces the DURA-CPS framework, which utilizes multi-role orchestration to automate the assurance process for AI-powered Cyber-Physical Systems (CPS). Specialized roles are assigned to dedicated agents within a simulated environment to continuously evaluate AI behavior against various dependability requirements. Through a case study involving an autonomous vehicle navigating an intersection with an AI-based planner, DURA-CPS effectively detects vulnerabilities, manages performance impacts, and supports adaptive recovery strategies. The framework offers a structured and extensible solution for rigorous verification and validation in safety- and security-critical systems. <div>
arXiv:2506.06381v2 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to operate in critical applications. However, traditional verification and validation methods often struggle to handle the unpredictable and dynamic nature of AI components. In this paper, we introduce DURA-CPS, a novel framework that employs multi-role orchestration to automate the iterative assurance process for AI-powered CPS. By assigning specialized roles (e.g., safety monitoring, security assessment, fault injection, and recovery planning) to dedicated agents within a simulated environment, DURA-CPS continuously evaluates and refines AI behavior against a range of dependability requirements. We demonstrate the framework through a case study involving an autonomous vehicle navigating an intersection with an AI-based planner. Our results show that DURA-CPS effectively detects vulnerabilities, manages performance impacts, and supports adaptive recovery strategies, thereby offering a structured and extensible solution for rigorous V&amp;V in safety- and security-critical systems.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects</title>
<link>https://arxiv.org/abs/2506.11012</link>
<guid>https://arxiv.org/abs/2506.11012</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, knowledge graph reasoning, static single-step KGR, static multi-step KGR, dynamic KGR<br />
<br />
Summary: Knowledge graphs are crucial for cognitive intelligence systems, with knowledge graph reasoning (KGR) playing a key role in various applications. This survey categorizes KGR approaches into static single-step, multi-step, dynamic, multi-modal, few-shot, and inductive reasoning. It also explores downstream application tasks and challenging reasoning paradigms. The survey discusses advanced techniques like large language models (LLMs) and their impact on KGR, providing a comprehensive perspective on current research trends in the field. Promising future directions for KGR are outlined, highlighting the importance of understanding and advancing knowledge graph reasoning methodologies. <div>
arXiv:2506.11012v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring and leveraging diverse real-world knowledge, which serve as a fundamental technology for enabling cognitive intelligence systems with advanced understanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims to infer new knowledge based on existing facts in KGs, playing a crucial role in applications such as public security intelligence, intelligent healthcare, and financial risk assessment. From a task-centric perspective, existing KGR approaches can be broadly classified into static single-step KGR, static multi-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR. While existing surveys have covered these six types of KGR tasks, a comprehensive review that systematically summarizes all KGR tasks particularly including downstream applications and more challenging reasoning paradigms remains lacking. In contrast to previous works, this survey provides a more comprehensive perspective on the research of KGR by categorizing approaches based on primary reasoning tasks, downstream application tasks, and potential challenging reasoning tasks. Besides, we explore advanced techniques, such as large language models (LLMs), and their impact on KGR. This work aims to highlight key research trends and outline promising future directions in the field of KGR.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoGSN: An Ontology for Dynamic Management of Assurance Cases</title>
<link>https://arxiv.org/abs/2506.11023</link>
<guid>https://arxiv.org/abs/2506.11023</guid>
<content:encoded><![CDATA[
<div> OntoGSN; ontology; assurance cases; GSN standard; dynamic AC management

Summary:
The article introduces OntoGSN, an ontology and middleware designed to manage Assurance Cases (ACs) within the Goal Structuring Notation (GSN) standard. ACs are crucial for ensuring system properties like safety and robustness. Existing tools support static applications, but dynamic contexts like autonomous driving pose challenges. OntoGSN offers a knowledge representation graph that can be automatically updated and evaluated, making AC management more efficient. The ontology formalizes the GSN Community Standard v3, adhering strictly to the standard's text. It includes a helper ontology, parser for integration with AC tools, SPARQL query library with automation patterns, and a prototypical interface. The development process follows FAIR principles, the OOPS framework, and community feedback. The article demonstrates OntoGSN's utility with an example of ensuring adversarial robustness in large language models. Ongoing evaluations and community input guide the development of additional middleware elements to meet evolving needs. 

<br /><br />Summary: <div>
arXiv:2506.11023v1 Announce Type: new 
Abstract: Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard's text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic</title>
<link>https://arxiv.org/abs/2506.11221</link>
<guid>https://arxiv.org/abs/2506.11221</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical communication skills, LLM-powered simulations, fuzzy logic, automated evaluation, medical education

Summary: 

This paper presents a novel approach called LLM-as-a-Fuzzy-Judge which combines fuzzy logic and Large Language Models (LLMs) to automate the evaluation of medical students' clinical communication skills. By fine-tuning LLMs to assess students' utterances based on human annotations from fuzzy sets, the system can align with physicians' subjective preferences in assessing professionalism, medical relevance, ethical behavior, and contextual distractions. The methodology involved data collection, annotation, prompt engineering, and supervised fine-tuning of pre-trained LLMs. Results show that the LLM-as-a-Fuzzy-Judge achieves high accuracy, demonstrating its effectiveness in delivering human-aligned assessments. By leveraging fuzzy logic and LLMs, this work advances automated evaluation in medical education, supporting more robust assessment practices. The GitHub repository for this project is also available for further exploration and development. 

Summary: <div>
arXiv:2506.11221v1 Announce Type: new 
Abstract: Clinical communication skills are critical in medical education, and practicing and assessing clinical communication skills on a scale is challenging. Although LLM-powered clinical scenario simulations have shown promise in enhancing medical students' clinical practice, providing automated and scalable clinical evaluation that follows nuanced physician judgment is difficult. This paper combines fuzzy logic and Large Language Model (LLM) and proposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the automated evaluation of medical students' clinical skills with subjective physicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is fine-tuned to evaluate medical students' utterances within student-AI patient conversation scripts based on human annotations from four fuzzy sets, including Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction. The methodology of this paper started from data collection from the LLM-powered medical education system, data annotation based on multidimensional fuzzy sets, followed by prompt engineering and the supervised fine-tuning (SFT) of the pre-trained LLMs using these human annotations. The results show that the LLM-as-a-Fuzzy-Judge achieves over 80\% accuracy, with major criteria items over 90\%, effectively leveraging fuzzy logic and LLM as a solution to deliver interpretable, human-aligned assessment. This work suggests the viability of leveraging fuzzy logic and LLM to align with human preferences, advances automated evaluation in medical education, and supports more robust assessment and judgment practices. The GitHub repository of this work is available at https://github.com/2sigmaEdTech/LLMAsAJudge
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUDAS: Mote-scale Unsupervised Domain Adaptation in Multi-label Sound Classification</title>
<link>https://arxiv.org/abs/2506.11331</link>
<guid>https://arxiv.org/abs/2506.11331</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised Domain Adaptation, Multi-label sound classification, IoT devices, Resource-constrained, MUDAS

Summary:
MUDAS is a new Unsupervised Domain Adaptation (UDA) framework designed for multi-label sound classification on resource-constrained IoT devices. Existing UDA algorithms are limited in their adaptability to multi-label tasks and in their ability to run efficiently on low-power devices. MUDAS addresses these constraints by selectively retraining the classifier in situ using high-confidence data, minimizing computational and memory requirements. The framework also implements class-specific adaptive thresholds for generating reliable pseudo-labels and incorporates diversity regularization to enhance multi-label classification accuracy. Evaluation on the SONYC Urban Sound Tagging dataset shows that MUDAS outperforms existing UDA algorithms in urban sound classification, particularly in resource-constrained IoT environments. This framework demonstrates significant improvements in classification accuracy and is well-suited for on-device deployment in challenging acoustic environments. 

<br /><br />Summary: <div>
arXiv:2506.11331v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation (UDA) is essential for adapting machine learning models to new, unlabeled environments where data distribution shifts can degrade performance. Existing UDA algorithms are designed for single-label tasks and rely on significant computational resources, limiting their use in multi-label scenarios and in resource-constrained IoT devices. Overcoming these limitations is particularly challenging in contexts such as urban sound classification, where overlapping sounds and varying acoustics require robust, adaptive multi-label capabilities on low-power, on-device systems. To address these limitations, we introduce Mote-scale Unsupervised Domain Adaptation for Sounds (MUDAS), a UDA framework developed for multi-label sound classification in resource-constrained IoT settings. MUDAS efficiently adapts models by selectively retraining the classifier in situ using high-confidence data, minimizing computational and memory requirements to suit on-device deployment. Additionally, MUDAS incorporates class-specific adaptive thresholds to generate reliable pseudo-labels and applies diversity regularization to improve multi-label classification accuracy. In evaluations on the SONYC Urban Sound Tagging (SONYC-UST) dataset recorded at various New York City locations, MUDAS demonstrates notable improvements in classification accuracy over existing UDA algorithms, achieving good performance in a resource-constrained IoT setting.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables</title>
<link>https://arxiv.org/abs/2506.11375</link>
<guid>https://arxiv.org/abs/2506.11375</guid>
<content:encoded><![CDATA[
<div> ChemTable, benchmark, chemical tables, multimodal, language models<br />
<br />
Summary: <br />
ChemTable introduces a benchmark for chemical tables extracted from literature. It includes expert-annotated cell polygons, logical layouts, and domain-specific labels for tasks like Table Recognition and Table Understanding. Models struggle with descriptive and reasoning-oriented question answering tasks compared to humans, with significant gaps between open-source and closed-source models. The study highlights challenges in chemistry-aware table understanding and positions ChemTable as a realistic benchmark for advancing scientific reasoning. <div>
arXiv:2506.11375v1 Announce Type: new 
Abstract: Chemical tables encode complex experimental knowledge through symbolic expressions, structured variables, and embedded molecular graphics. Existing benchmarks largely overlook this multimodal and domain-specific complexity, limiting the ability of multimodal large language models to support scientific understanding in chemistry. In this work, we introduce ChemTable, a large-scale benchmark of real-world chemical tables curated from the experimental sections of literature. ChemTable includes expert-annotated cell polygons, logical layouts, and domain-specific labels, including reagents, catalysts, yields, and graphical components and supports two core tasks: (1) Table Recognition, covering structure parsing and content extraction; and (2) Table Understanding, encompassing both descriptive and reasoning-oriented question answering grounded in table structure and domain semantics. We evaluated a range of representative multimodal models, including both open-source and closed-source models, on ChemTable and reported a series of findings with practical and conceptual insights. Although models show reasonable performance on basic layout parsing, they exhibit substantial limitations on both descriptive and inferential QA tasks compared to human performance, and we observe significant performance gaps between open-source and closed-source models across multiple dimensions. These results underscore the challenges of chemistry-aware table understanding and position ChemTable as a rigorous and realistic benchmark for advancing scientific reasoning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning</title>
<link>https://arxiv.org/abs/2506.11376</link>
<guid>https://arxiv.org/abs/2506.11376</guid>
<content:encoded><![CDATA[
<div> Empathy, Therapeutic Alliance, Large Language Model, Family Caregivers, Mental Health Support  
Summary:  
The study examined the effectiveness of a large language model (LLM) conversational agent in delivering mental health support to family caregivers. The agent combined Problem-Solving Therapy (PST) with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). Through a within-subject experiment with 28 caregivers, the models utilizing Few-Shot and Retrieval-Augmented Generation (RAG) techniques showed improved contextual understanding and personalized support. Participants appreciated the model's ability to validate emotions, explore unexpressed feelings, and offer actionable strategies. However, there were challenges in balancing thorough assessments with efficient advice delivery. Overall, the study suggests that LLMs have the potential to provide empathetic and tailored support to family caregivers.  
Summary: <div>
arXiv:2506.11376v1 Announce Type: new 
Abstract: Family caregivers often face substantial mental health challenges due to their multifaceted roles and limited resources. This study explored the potential of a large language model (LLM)-powered conversational agent to deliver evidence-based mental health support for caregivers, specifically Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted with 28 caregivers interacting with four LLM configurations to evaluate empathy and therapeutic alliance. The best-performing models incorporated Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques, alongside clinician-curated examples. The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants valued the model's ability to validate emotions, explore unexpressed feelings, and provide actionable strategies. However, balancing thorough assessment with efficient advice delivery remains a challenge. This work highlights the potential of LLMs in delivering empathetic and tailored support for family caregivers.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocalAD: Local Motion Planning for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.11419</link>
<guid>https://arxiv.org/abs/2506.11419</guid>
<content:encoded><![CDATA[
<div> local interactions, autonomous driving, motion prediction, end-to-end framework, collision rate<br />
Summary:<br />
In the field of end-to-end autonomous driving, motion prediction is crucial for ego-vehicle planning. Existing methods often overlook the significance of locally interacting agents in planning decisions, which can lead to overlooked risks and decreased reliability. To address this, FocalAD introduces the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss) modules. ELAI focuses on local interactions to enhance motion representations, while FLA Loss prioritizes decision-critical neighboring agents in planning. Experimental results show that FocalAD outperforms state-of-the-art methods on various datasets, including nuScenes and Bench2Drive. Particularly, on the Adv-nuScenes dataset, FocalAD significantly reduces the average collision rate, showcasing its robustness and effectiveness in enhancing autonomous driving systems. <br /><br />Summary: <div>
arXiv:2506.11419v1 Announce Type: new 
Abstract: In end-to-end autonomous driving,the motion prediction plays a pivotal role in ego-vehicle planning. However, existing methods often rely on globally aggregated motion features, ignoring the fact that planning decisions are primarily influenced by a small number of locally interacting agents. Failing to attend to these critical local interactions can obscure potential risks and undermine planning reliability. In this work, we propose FocalAD, a novel end-to-end autonomous driving framework that focuses on critical local neighbors and refines planning by enhancing local motion representations. Specifically, FocalAD comprises two core modules: the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a graph-based ego-centric interaction representation that captures motion dynamics with local neighbors to enhance both ego planning and agent motion queries. FLA Loss increases the weights of decision-critical neighboring agents, guiding the model to prioritize those more relevant to planning. Extensive experiments show that FocalAD outperforms existing state-of-the-art methods on the open-loop nuScenes datasets and closed-loop Bench2Drive benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD achieves even greater improvements, reducing the average colilision rate by 41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</title>
<link>https://arxiv.org/abs/2506.11445</link>
<guid>https://arxiv.org/abs/2506.11445</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, multi-agent reinforcement learning, local state attention, traffic situations, merging efficiency

Summary:

In mixed-traffic environments, autonomous vehicles need to interact with human-controlled vehicles and navigate unusual driving scenarios. This study approaches the problem as a multi-agent reinforcement learning (MARL) task with full cooperative reward among autonomous vehicles. The authors propose a Local State Attention module that utilizes self-attention to compress essential information from nearby agents and resolve conflicts in traffic situations. Through simulations of a highway merging scenario with a priority vehicle as an unexpected event, the module successfully manages the merging process by prioritizing information from other vehicles. Results show significant improvements in merging efficiency, especially in high-density traffic conditions. This approach addresses challenges faced by autonomous vehicles in mixed-traffic environments and highlights the effectiveness of utilizing MARL with attention mechanisms in resolving conflicts and adapting to stochastic events. 

<br /><br />Summary: <div>
arXiv:2506.11445v1 Announce Type: new 
Abstract: In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aware Automatic Channel Pruning by Searching with Graph Embedding</title>
<link>https://arxiv.org/abs/2506.11469</link>
<guid>https://arxiv.org/abs/2506.11469</guid>
<content:encoded><![CDATA[
<div> pruning, deep neural networks, graph convolutional networks, compression efficiency, accuracy retention
<br />
Automated channel pruning for deep neural networks is crucial for optimizing computational efficiency. The proposed Structure-Aware Automatic Channel Pruning (SACP) framework utilizes graph convolutional networks to understand network topology and determine the global importance of each channel. By considering structural dependencies, SACP enables topology-aware pruning without the need for manual intervention. The framework restricts the space of pruning rate combinations and uses a search-based approach to find the optimal combination. Experimental results on CIFAR-10 and ImageNet datasets with models like ResNet and VGG16 show that SACP outperforms existing pruning methods in terms of compression efficiency while maintaining competitive accuracy levels. SACP proves to be an effective and automated solution for channel pruning in deep neural networks.
<br /><br />Summary: <div>
arXiv:2506.11469v1 Announce Type: new 
Abstract: Channel pruning is a powerful technique to reduce the computational overhead of deep neural networks, enabling efficient deployment on resource-constrained devices. However, existing pruning methods often rely on local heuristics or weight-based criteria that fail to capture global structural dependencies within the network, leading to suboptimal pruning decisions and degraded model performance. To address these limitations, we propose a novel structure-aware automatic channel pruning (SACP) framework that utilizes graph convolutional networks (GCNs) to model the network topology and learn the global importance of each channel. By encoding structural relationships within the network, our approach implements topology-aware pruning and this pruning is fully automated, reducing the need for human intervention. We restrict the pruning rate combinations to a specific space, where the number of combinations can be dynamically adjusted, and use a search-based approach to determine the optimal pruning rate combinations. Extensive experiments on benchmark datasets (CIFAR-10, ImageNet) with various models (ResNet, VGG16) demonstrate that SACP outperforms state-of-the-art pruning methods on compression efficiency and competitive on accuracy retention.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models</title>
<link>https://arxiv.org/abs/2506.11487</link>
<guid>https://arxiv.org/abs/2506.11487</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, neuro-symbolic coordination, automated theorem proving, symbolic search methods, proof patterns<br />
Summary:<br />
The paper introduces DSP+, an enhanced version of the Draft, Sketch, and Prove framework, which combines neuro-symbolic techniques with existing reasoning models and tactic step provers for automated theorem proving. The framework improves performance in three phases: drafting concise natural-language subgoals, autoformalizing subgoals with hypotheses, and integrating symbolic search methods with step provers. Without training, DSP+ solves a significant number of problems from various benchmarks, including an unsolved IMO problem. The framework also identifies formalization errors in proof patterns, making them comprehensible to human experts. This study demonstrates the effectiveness of classical reasoning patterns in automated theorem proving alongside reinforcement learning techniques. <div>
arXiv:2506.11487v1 Announce Type: new 
Abstract: Recent advancements, such as DeepSeek-Prover-V2-671B and Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging reinforcement learning (RL)-based large-scale training for automated theorem proving. Surprisingly, we discover that even without any training, careful neuro-symbolic coordination of existing off-the-shelf reasoning models and tactic step provers can achieve comparable performance. This paper introduces \textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework, featuring a \emph{fine-grained and integrated} neuro-symbolic enhancement for each phase: (1) In the draft phase, we prompt reasoning models to generate concise natural-language subgoals to benefit the sketch phase, removing thinking tokens and references to human-written proofs; (2) In the sketch phase, subgoals are autoformalized with hypotheses to benefit the proving phase, and sketch lines containing syntactic errors are masked according to predefined rules; (3) In the proving phase, we tightly integrate symbolic search methods like Aesop with step provers to establish proofs for the sketch subgoals. Experimental results show that, without any additional model training or fine-tuning, DSP+ solves 80.7\%, 32.8\%, and 24 out of 644 problems from miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets compared to state-of-the-arts. DSP+ proves \texttt{imo\_2019\_p1}, an IMO problem in miniF2F that is not solved by any prior work. Additionally, DSP+ generates proof patterns comprehensible by human experts, facilitating the identification of formalization errors; For example, eight wrongly formalized statements in miniF2F are discovered. Our results highlight the potential of classical reasoning patterns besides the RL-based training. All components will be open-sourced.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning</title>
<link>https://arxiv.org/abs/2506.11555</link>
<guid>https://arxiv.org/abs/2506.11555</guid>
<content:encoded><![CDATA[
<div> cognitive, reasoning, Retrieval-Augmented Generation (RAG), knowledge integration, language models <br />
<br />
Summary: In this work, the authors introduce RAG+, an extension of the Retrieval-Augmented Generation (RAG) framework that incorporates application-aware reasoning into the retrieval process. RAG+ utilizes a dual corpus of knowledge and application examples to enhance large language models' (LLMs) performance on knowledge-intensive tasks. By retrieving both knowledge and examples jointly during inference, LLMs are not only able to access relevant information but also apply it within structured reasoning processes. Experimental results across mathematical, legal, and medical domains show that RAG+ consistently outperforms standard RAG variants, with average improvements of 3-5% and peak gains up to 7.5% in complex scenarios. This approach bridges the gap between retrieved facts and task-specific reasoning, advancing a more cognitively grounded framework for knowledge integration in LLMs. <div>
arXiv:2506.11555v1 Announce Type: new 
Abstract: The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative LLM Inference via Planning for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2506.11578</link>
<guid>https://arxiv.org/abs/2506.11578</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, test-time collaboration, planner model, reasoner model, cross-model inference  
Summary:  
In the study, the researchers address the challenge of balancing the cost and effectiveness of utilizing large language models (LLMs) by proposing a test-time collaboration framework. This framework involves a planner model generating a plan as a high-level abstraction of the problem, guiding a reasoner model to generate a complete solution. Small and large LLMs collaborate by taking turns as planner and reasoner, exchanging plans in multiple rounds to solve complex tasks effectively. The proposed method achieves accuracy comparable to expensive proprietary models while reducing dependence on paid inference, showcasing planning as a valuable tool for orchestrating cost-effective cross-model inference under real-world deployment constraints.<br /><br />Summary: <div>
arXiv:2506.11578v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at complex reasoning tasks, but those with strong capabilities (e.g., whose numbers of parameters are larger than 100B) are often accessible only through paid APIs, making them too costly for applications of frequent use. In contrast, smaller open-sourced LLMs (e.g., whose numbers of parameters are less than 3B) are freely available and easy to deploy locally (e.g., under a single GPU having 8G VRAM), but lack suff icient reasoning ability. This trade-off raises a natural question: can small (free) and large (costly) models collaborate at test time to combine their strengths? We propose a test-time collaboration framework in which a planner model first generates a plan, defined as a distilled and high-level abstraction of the problem.
  This plan serves as a lightweight intermediate that guides a reasoner model, which generates a complete solution. Small and large models take turns acting as planner and reasoner, exchanging plans in a multi-round cascade to collaboratively solve complex tasks. Our method achieves accuracy comparable to strong proprietary models alone, while significantly reducing reliance on paid inference. These results highlight planning as an effective prior for orchestrating cost-aware, cross-model inference under real-world deployment constraints.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM@school -- Evaluation of AI image understanding on German middle school knowledge</title>
<link>https://arxiv.org/abs/2506.11604</link>
<guid>https://arxiv.org/abs/2506.11604</guid>
<content:encoded><![CDATA[
<div> dataset, Vision Language Models, German language, middle school, multimodal understanding <br />
<br />
This paper introduces a new benchmark dataset for evaluating Vision Language Models (VLMs) in the German language. The dataset includes over 2,000 open-ended questions based on real middle school curricula in various subjects. Thirteen state-of-the-art VLMs were evaluated on the dataset, showing overall low accuracy, particularly in music, mathematics, and adversarial scenarios. The study highlights the discrepancies between performance on popular benchmarks and real-world tasks. Middle school-level tasks are proposed as a valuable avenue for testing VLMs, especially in non-English contexts. The dataset and evaluation protocol provide a thorough testbed for improving the visual and linguistic reasoning capabilities of AI systems. <br /><br />Summary: <div>
arXiv:2506.11604v1 Announce Type: new 
Abstract: This paper introduces a novel benchmark dataset designed to evaluate the capabilities of Vision Language Models (VLMs) on tasks that combine visual reasoning with subject-specific background knowledge in the German language. In contrast to widely used English-language benchmarks that often rely on artificially difficult or decontextualized problems, this dataset draws from real middle school curricula across nine domains including mathematics, history, biology, and religion. The benchmark includes over 2,000 open-ended questions grounded in 486 images, ensuring that models must integrate visual interpretation with factual reasoning rather than rely on superficial textual cues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple dimensions, including domain-specific accuracy and performance on adversarial crafted questions. Our findings reveal that even the strongest models achieve less than 45% overall accuracy, with particularly poor performance in music, mathematics, and adversarial settings. Furthermore, the results indicate significant discrepancies between success on popular benchmarks and real-world multimodal understanding. We conclude that middle school-level tasks offer a meaningful and underutilized avenue for stress-testing VLMs, especially in non-English contexts. The dataset and evaluation protocol serve as a rigorous testbed to better understand and improve the visual and linguistic reasoning capabilities of future AI systems.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization</title>
<link>https://arxiv.org/abs/2506.11712</link>
<guid>https://arxiv.org/abs/2506.11712</guid>
<content:encoded><![CDATA[
<div> supervised learning, multimodal, optimization, large language models, hallucination<br />
Summary:<br />
Direct Preference Optimization (DPO) has been effective in reducing hallucination in Multimodal Large Language Models (MLLMs) by improving their attention to visual inputs. Existing methods have shown progress in this area but suffer from non-rigorous optimization and indirect preference supervision. To address these limitations, Symmetric Multimodal Preference Optimization (SymMPO) is proposed. SymMPO utilizes symmetric preference learning with direct preference supervision to enhance visual understanding in MLLMs while maintaining alignment with DPO. It introduces a preference margin consistency loss to regulate the preference gap between pairs. Evaluation across five benchmarks shows that SymMPO outperforms existing methods, demonstrating its effectiveness in mitigating hallucination in MLLMs. <br /><br />Summary: <div>
arXiv:2506.11712v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational GNNs Cannot Learn $C_2$ Features for Planning</title>
<link>https://arxiv.org/abs/2506.11721</link>
<guid>https://arxiv.org/abs/2506.11721</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Planning Domain, Value Functions, Expressive Power, First-Order Logic <br />
<br />
Relational Graph Neural Networks (R-GNNs) were proposed for learning value functions in planning domains but fail to learn value functions defined by $C_2 features. The connection between GNN expressive power and $C_2 first-order logic with two variables and counting motivated R-GNNs. However, R-GNNs struggle to generalize to unseen problems and learn optimal value functions decomposed as arithmetic expressions of $C_2 features. Empirical results show that prior GNN architectures for planning may be more effective in learning value functions defined by $C_2 features. Further exploration of GNN architectures for planning domains is needed to improve generalization and capability to learn value functions in line with $C_2 features. <br /><br />Summary: <div>
arXiv:2506.11721v1 Announce Type: new 
Abstract: Relational Graph Neural Networks (R-GNNs) are a GNN-based approach for learning value functions that can generalise to unseen problems from a given planning domain. R-GNNs were theoretically motivated by the well known connection between the expressive power of GNNs and $C_2$, first-order logic with two variables and counting. In the context of planning, $C_2$ features refer to the set of formulae in $C_2$ with relations defined by the unary and binary predicates of a planning domain. Some planning domains exhibit optimal value functions that can be decomposed as arithmetic expressions of $C_2$ features. We show that, contrary to empirical results, R-GNNs cannot learn value functions defined by $C_2$ features. We also identify prior GNN architectures for planning that may better learn value functions defined by $C_2$ features.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Effect Identification in Heterogeneous Environments from Higher-Order Moments</title>
<link>https://arxiv.org/abs/2506.11756</link>
<guid>https://arxiv.org/abs/2506.11756</guid>
<content:encoded><![CDATA[
<div> causal effect, latent confounder, multiple environments, moment-based algorithm, identifiability

Summary:
The article investigates the estimation of the causal effect of a treatment variable on an outcome in the presence of a latent confounder. It establishes the identifiability of the causal effect under certain conditions when data is available from multiple environments, assuming the target causal effect remains consistent across these environments. A moment-based algorithm is proposed for estimating the causal effect, with the restriction that only one parameter of the data-generating mechanism varies across environments. The study demonstrates the loss of identifiability when both exogenous noise distributions of the latent and treatment variables vary across environments. Additionally, a procedure is outlined to identify the varying parameter of the data-generating mechanism across environments. Experimental evaluation on synthetic data validates the performance of the proposed methods. <div>
arXiv:2506.11756v1 Announce Type: new 
Abstract: We investigate the estimation of the causal effect of a treatment variable on an outcome in the presence of a latent confounder. We first show that the causal effect is identifiable under certain conditions when data is available from multiple environments, provided that the target causal effect remains invariant across these environments. Secondly, we propose a moment-based algorithm for estimating the causal effect as long as only a single parameter of the data-generating mechanism varies across environments -- whether it be the exogenous noise distribution or the causal relationship between two variables. Conversely, we prove that identifiability is lost if both exogenous noise distributions of both the latent and treatment variables vary across environments. Finally, we propose a procedure to identify which parameter of the data-generating mechanism has varied across the environments and evaluate the performance of our proposed methods through experiments on synthetic data.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Performance of LLMs for Real Estate Appraisal</title>
<link>https://arxiv.org/abs/2506.11812</link>
<guid>https://arxiv.org/abs/2506.11812</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, real estate market, house price estimates, In-Context Learning, interpretability

Summary:<br /><br />This study explores the use of Large Language Models (LLMs) to provide accessible and interpretable estimates of house prices in the real estate market. By optimizing In-Context Learning (ICL) strategies, LLMs show effectiveness in leveraging hedonic variables for meaningful price estimates. While traditional machine learning models excel in predictive accuracy, LLMs offer a more interactive and interpretable alternative. The study finds that LLMs can explain their predictions in line with state-of-the-art models, enhancing trustworthiness. However, LLMs struggle with overconfidence in price intervals and spatial reasoning limitations. By providing practical guidance for structured prediction tasks, the study highlights the potential of LLMs in enhancing transparency in real estate appraisal. Stakeholders can benefit from actionable insights generated by LLMs in the real estate market. <div>
arXiv:2506.11812v1 Announce Type: new 
Abstract: The real estate market is vital to global economies but suffers from significant information asymmetry. This study examines how Large Language Models (LLMs) can democratize access to real estate insights by generating competitive and interpretable house price estimates through optimized In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs on diverse international housing datasets, comparing zero-shot, few-shot, market report-enhanced, and hybrid prompting techniques. Our results show that LLMs effectively leverage hedonic variables, such as property size and amenities, to produce meaningful estimates. While traditional machine learning models remain strong for pure predictive accuracy, LLMs offer a more accessible, interactive and interpretable alternative. Although self-explanations require cautious interpretation, we find that LLMs explain their predictions in agreement with state-of-the-art models, confirming their trustworthiness. Carefully selected in-context examples based on feature similarity and geographic proximity, significantly enhance LLM performance, yet LLMs struggle with overconfidence in price intervals and limited spatial reasoning. We offer practical guidance for structured prediction tasks through prompt optimization. Our findings highlight LLMs' potential to improve transparency in real estate appraisal and provide actionable insights for stakeholders.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Political Bias in LLMs through Structured Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2506.11825</link>
<guid>https://arxiv.org/abs/2506.11825</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, political bias, agent gender, multi-agent debate, echo chambers

Summary:
Large language models (LLMs) are used to study social behavior, with a focus on political biases and interaction dynamics in debates. A study explores how LLM type and agent gender affect political bias through structured debates involving Neutral, Republican, and Democrat American LLM agents. Results show Neutral agents align with Democrats, while Republicans shift towards neutrality. Agent gender influences attitudes, with agents adjusting opinions based on others' genders. Echo chambers form as agents with shared political affiliations intensify attitudes during debates, contrary to previous beliefs. This research sheds light on the complexities of political bias and interaction dynamics within debates involving LLMs and highlights the impact of model provenance and agent personas on political attitudes. <div>
arXiv:2506.11825v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment</title>
<link>https://arxiv.org/abs/2506.11880</link>
<guid>https://arxiv.org/abs/2506.11880</guid>
<content:encoded><![CDATA[
<div> Keywords: language technologies, Large Language Models, demographic biases, accountability, privacy  
Summary:  
- The use of language technologies, particularly Large Language Models (LLMs), in high-stake settings is growing.  
- However, LLMs are prone to ethical concerns like demographic biases, accountability, and privacy issues.  
- This study focuses on analyzing Transformers-based systems' ability to learn demographic biases in data, specifically in AI-based automated recruitment.  
- The researchers propose a framework to enhance privacy by removing gender information from the learning process to minimize biased outcomes in the final tools.  
- Experiments conducted demonstrate the impact of data biases on systems using two different LLMs and show that the proposed framework effectively prevents the trained systems from replicating biases present in the data.  

<br /><br />Summary: <div>
arXiv:2506.11880v1 Announce Type: new 
Abstract: The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making</title>
<link>https://arxiv.org/abs/2506.11887</link>
<guid>https://arxiv.org/abs/2506.11887</guid>
<content:encoded><![CDATA[
<div> Keywords: human-AI decision-making, cascaded LLM framework, deferral policy, abstention policy, online learning mechanism

Summary:
In this work, a cascaded LLM decision framework is proposed for effective human-AI decision-making. The framework involves delegating tasks across multiple tiers of expertise, including a base model, a large model, and a human expert. A deferral policy determines when to switch between models, while an abstention policy decides whether human intervention is necessary. An online learning mechanism is incorporated to improve decision quality over time. The approach is demonstrated in general and medical question-answering tasks, showing improved accuracy compared to single-model baselines, reduced cost, and a systematic way to handle abstentions.<br /><br />Summary: <div>
arXiv:2506.11887v1 Announce Type: new 
Abstract: Effective human-AI decision-making balances three key factors: the \textit{correctness} of predictions, the \textit{cost} of knowledge and reasoning complexity, and the confidence about whether to \textit{abstain} automated answers or involve human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise -- a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base model's answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, we incorporate an online learning mechanism in the framework that can leverage human feedback to improve decision quality over time. We demonstrate this approach to general question-answering (ARC-Easy and ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results show that our cascaded strategy outperforms in most cases single-model baselines in accuracy while reducing cost and providing a principled way to handle abstentions.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task</title>
<link>https://arxiv.org/abs/2506.11986</link>
<guid>https://arxiv.org/abs/2506.11986</guid>
<content:encoded><![CDATA[
<div> Keywords: Schema linking, Text-to-SQL, Reinforcement learning, Reasoning ability, Fine-tuning

Summary:
Schema-R1 is a new approach for schema linking in the Text-to-SQL task, focusing on improving reasoning ability. This model uses reinforcement learning and consists of three key steps: constructing high-quality reasoning samples, supervised fine-tuning for initialization, and rule-based reinforcement learning training. The goal is to enhance the schema linking model's ability to accurately predict table and column names in SQL queries. By optimizing for reasoning instead of rote-learning, Schema-R1 achieves a 10% improvement in filter accuracy compared to existing methods. The code for Schema-R1 is available on GitHub for further exploration and use. This new approach shows promise in advancing the field of schema linking and improving the performance of Text-to-SQL models. 

<br /><br />Summary: <div>
arXiv:2506.11986v1 Announce Type: new 
Abstract: Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10\% improvement in filter accuracy compared to the existing method. Our code is available at https://github.com/hongWin/Schema-R1/.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making</title>
<link>https://arxiv.org/abs/2506.12012</link>
<guid>https://arxiv.org/abs/2506.12012</guid>
<content:encoded><![CDATA[
<div> Evaluation, Language models, Strategic games, Reasoning, Decision making  
Summary:  
- The study focuses on evaluating large language models (LLMs) for complex reasoning tasks through the lens of strategic games, measuring planning, revision, and decision making under resource constraints.  
- A framework is proposed to assess LLMs based on metrics like win rate, correction success rate, and improvement slope, with ChatGPT-o3-mini emerging as the top performer.  
- The analysis reveals a negative correlation between overcorrection risk rate and correction success rate, suggesting that frequent edits may not always lead to better outcomes.  
- Qwen-Plus struggles due to excessive resource use, despite a high overcorrection risk rate.  
- The results underscore the importance of understanding the internal processes of LLMs beyond just their final decisions, showcasing the need for evaluating not just the outcomes but also the reasoning behind them.
<br /><br />Summary: <div>
arXiv:2506.12012v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality</title>
<link>https://arxiv.org/abs/2506.10984</link>
<guid>https://arxiv.org/abs/2506.10984</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-assisted code generation, application modernization, large language models, code reasoning, human expertise

Summary:
AI-assisted code generation tools have improved efficiency in software development but face challenges such as security vulnerabilities and reliability issues. This paper explores leveraging large language models (LLMs) for application modernization, focusing on code reasoning and generation. It emphasizes the importance of human expertise in guiding AI processes. A case study demonstrates the framework's application in a real-world scenario, showcasing alternative approaches. The paper provides actionable insights for AI-driven application modernization research. The reference implementation is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2506.10984v1 Announce Type: cross 
Abstract: AI-assisted code generation tools have revolutionized software development, offering unprecedented efficiency and scalability. However, multiple studies have consistently highlighted challenges such as security vulnerabilities, reliability issues, and inconsistencies in the generated code. Addressing these concerns is crucial to unlocking the full potential of this transformative technology. While advancements in foundational and code-specialized language models have made notable progress in mitigating some of these issues, significant gaps remain, particularly in ensuring high-quality, trustworthy outputs.
  This paper builds upon existing research on leveraging large language models (LLMs) for application modernization. It explores an opinionated approach that emphasizes two core capabilities of LLMs: code reasoning and code generation. The proposed framework integrates these capabilities with human expertise to tackle application modernization challenges effectively. It highlights the indispensable role of human involvement and guidance in ensuring the success of AI-assisted processes.
  To demonstrate the framework's utility, this paper presents a detailed case study, walking through its application in a real-world scenario. The analysis includes a step-by-step breakdown, assessing alternative approaches where applicable. This work aims to provide actionable insights and a robust foundation for future research in AI-driven application modernization. The reference implementation created for this paper is available on GitHub.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs</title>
<link>https://arxiv.org/abs/2506.10989</link>
<guid>https://arxiv.org/abs/2506.10989</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Python code, prompting approach, code generation, AI-driven programming tasks

Summary:<br /><br />In this paper, a novel prompting approach is introduced to enhance Large Language Models' (LLMs) ability to generate accurate Python code. The proposed prompt template aims to improve code snippet quality and correctness, ensuring reliable results that pass tests. Experiments on two state-of-the-art LLMs using the HumanEval dataset show that the new approach outperforms zero-shot and Chain-of-Thought (CoT) methods in the Pass@k metric. Additionally, the method achieves these improvements with reduced token usage compared to the CoT approach, making it both effective and resource-efficient. This not only lowers computational demands but also improves the eco-footprint of LLM capabilities. The study demonstrates the potential of tailored prompting strategies to optimize code generation performance, with implications for broader applications in AI-driven programming tasks.<br /><br />Summary: <div>
arXiv:2506.10989v1 Announce Type: cross 
Abstract: In this paper, we propose a novel prompting approach aimed at enhancing the ability of Large Language Models (LLMs) to generate accurate Python code. Specifically, we introduce a prompt template designed to improve the quality and correctness of generated code snippets, enabling them to pass tests and produce reliable results. Through experiments conducted on two state-of-the-art LLMs using the HumanEval dataset, we demonstrate that our approach outperforms widely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the Pass@k metric. Furthermore, our method achieves these improvements with significantly reduced token usage compared to the CoT approach, making it both effective and resource-efficient, thereby lowering the computational demands and improving the eco-footprint of LLM capabilities. These findings highlight the potential of tailored prompting strategies to optimize code generation performance, paving the way for broader applications in AI-driven programming tasks.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances</title>
<link>https://arxiv.org/abs/2506.10990</link>
<guid>https://arxiv.org/abs/2506.10990</guid>
<content:encoded><![CDATA[
<div> Keywords: Follow-the-Sun, carbon footprint, Artificial Intelligence, anomaly detection, experimental results

Summary:<br /><br />
The 'Follow-the-Sun' (FtS) computational model aims to reduce the carbon footprint of computer workloads by dynamically shifting to cleaner energy sources. An experiment was conducted to evaluate FtS's effectiveness in minimizing carbon emissions in AI model training. Benchmarking four AI algorithms for anomaly detection showed FtS achieving up to 14.6% reduction in carbon emissions and aiding in training time preservation. Comparisons with other strategies like Flexible Start and Pause and Resume also revealed FtS as a promising approach. Utilizing historical carbon intensity data from seven European cities in the year 2021, the experiment highlighted FtS's potential to significantly lower carbon emissions in AI workloads, thus contributing to environmental sustainability in the field of artificial intelligence.  

Summary: <div>
arXiv:2506.10990v1 Announce Type: cross 
Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads. It involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels. With the significant power consumption of Artificial Intelligence (AI) being a subject of extensive debate, FtS is proposed as a strategy to mitigate the carbon footprint of training AI models. However, the literature lacks scientific evidence on the advantages of FtS to mitigate the carbon footprint of AI workloads. In this paper, we present the results of an experiment conducted in a partial synthetic scenario to address this research gap. We benchmarked four AI algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases: no strategy, FtS, and two strategies previously introduced in the state of the art, namely Flexible Start and Pause and Resume. To conduct our experiment, we utilized historical carbon intensity data from the year 2021 for seven European cities. Our results demonstrate that the FtS strategy not only achieves average reductions of up to 14.6% in carbon emissions (with peaks of 16.3%) but also helps in preserving the time needed for training.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs for Visualization Tasks</title>
<link>https://arxiv.org/abs/2506.10996</link>
<guid>https://arxiv.org/abs/2506.10996</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Visualization, Large Language Models, Code Generation, Understanding Visualizations, Limitations

Summary:
Large Language Models (LLMs) have shown impressive performance in various tasks, including generating code for visualizations and understanding common visualizations through simple prompts and questions. The study highlights the potential of LLMs in assisting with Information Visualization systems by showcasing their ability to generate code for visualizations and provide insights through answering questions. However, it also identifies limitations in the capabilities of LLMs in these tasks. The findings suggest opportunities for further enhancing LLMs and Information Visualization systems by leveraging the strengths of both fields and addressing existing limitations. This research contributes valuable insights for advancing LLMs and enhancing the efficiency and effectiveness of Information Visualization techniques.<br /><br />Summary: <div>
arXiv:2506.10996v1 Announce Type: cross 
Abstract: Information Visualization has been utilized to gain insights from complex data. In recent times, Large Language Models (LLMs) have performed very well in many tasks. In this paper, we showcase the capabilities of different popular LLMs to generate code for visualization based on simple prompts. We also analyze the power of LLMs to understand some common visualizations by answering simple questions. Our study shows that LLMs could generate code for some visualizations as well as answer questions about them. However, LLMs also have several limitations. We believe that our insights can be used to improve both LLMs and Information Visualization systems.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Formal Verification of Backend Systems with LLMs</title>
<link>https://arxiv.org/abs/2506.10998</link>
<guid>https://arxiv.org/abs/2506.10998</guid>
<content:encoded><![CDATA[
<div> framework, functional programming, type systems, Scala, formal verification

Summary:
The article introduces a novel framework that uses functional programming and type systems to automatically generate theorems from Scala backend code and verify them using LLM-based provers. The proposed method can verify over 50% of test requirements, potentially automating half of a testing engineer's workload. With an average cost of $2.19 per API, the LLM-based verification is cost-effective compared to manual testing and can be easily scaled through parallel execution. The framework has the potential to greatly improve engineering productivity by automating testing processes and identifying bugs accurately. The results demonstrate a promising direction towards scalable, AI-powered software testing, as advancements in models continue to evolve.  

<br /><br />Summary: <div>
arXiv:2506.10998v1 Announce Type: cross 
Abstract: Software testing plays a critical role in ensuring that systems behave as intended. However, existing automated testing approaches struggle to match the capabilities of human engineers due to key limitations such as test locality, lack of general reliability, and business logic blindness. In this work, we propose a novel framework that leverages functional programming and type systems to translate Scala backend code into formal Lean representations. Our pipeline automatically generates theorems that specify the intended behavior of APIs and database operations, and uses LLM-based provers to verify them. When a theorem is proved, the corresponding logic is guaranteed to be correct and no further testing is needed. If the negation of a theorem is proved instead, it confirms a bug. In cases where neither can be proved, human intervention is required. We evaluate our method on realistic backend systems and find that it can formally verify over 50% of the test requirements, which suggests that half of a testing engineer's workload can be automated. Additionally, with an average cost of only $2.19 per API, LLM-based verification is significantly more cost-effective than manual testing and can be scaled easily through parallel execution. Our results indicate a promising direction for scalable, AI-powered software testing, with the potential to greatly improve engineering productivity as models continue to advance.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Validation of COBOL to Java Transformation</title>
<link>https://arxiv.org/abs/2506.10999</link>
<guid>https://arxiv.org/abs/2506.10999</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, code translation, COBOL, Java, symbolic execution
Summary:
Recent advancements in Large Language Model (LLM) technology have enabled the translation of enterprise-level COBOL code to modern languages like Java or Python. However, ensuring the accuracy of the translated code remains a challenge. This study introduces a framework and tool for validating the equivalence between COBOL and translated Java code. By employing symbolic-execution-based test generation, unit tests are automatically generated for the original COBOL programs. These tests also simulate external resource calls for comprehensive coverage. Equivalent JUnit test cases with matching mock behaviors as in the COBOL code are generated and executed to verify semantic equivalence between the original and translated programs. The results can aid in code repair and provide valuable feedback for enhancing AI models. <br /><br />Summary: Recent advancements in Large Language Model technology have facilitated the translation of COBOL code to Java. A framework and tool for validating code equivalence and automatically generating comprehensive unit tests have been proposed. The approach aims to enhance the trustworthiness of translated code and enhance AI models through feedback. <div>
arXiv:2506.10999v1 Announce Type: cross 
Abstract: Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Technological Readiness in the Era of AI Uncertainty</title>
<link>https://arxiv.org/abs/2506.11001</link>
<guid>https://arxiv.org/abs/2506.11001</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, military combat systems, technology readiness assessment, AI Readiness Framework, defense technology management

Summary:
The article discusses the importance of ensuring the readiness of AI-enabled capabilities in military combat systems. It highlights the limitations of current technology readiness assessments in capturing critical AI-specific factors, potentially leading to deployment risks. The proposed AI Readiness Framework aims to evaluate the maturity and trustworthiness of AI components in military systems by assessing reliability, safety, and suitability for combat use. By expanding on traditional Technology Readiness Levels (TRL) and utilizing existing data evaluation tools and testing practices, the framework offers a structured approach for decision-makers to determine if an AI-enabled system meets performance, transparency, and human integration standards for deployment with confidence. Implementation of this framework can advance defense technology management and risk assessment in the military sector. 

<br /><br />Summary: <div>
arXiv:2506.11001v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is poised to revolutionize military combat systems, but ensuring these AI-enabled capabilities are truly mission-ready presents new challenges. We argue that current technology readiness assessments fail to capture critical AI-specific factors, leading to potential risks in deployment. We propose a new AI Readiness Framework to evaluate the maturity and trustworthiness of AI components in military systems. The central thesis is that a tailored framework - analogous to traditional Technology Readiness Levels (TRL) but expanded for AI - can better gauge an AI system's reliability, safety, and suitability for combat use. Using current data evaluation tools and testing practices, we demonstrate the framework's feasibility for near-term implementation. This structured approach provides military decision-makers with clearer insight into whether an AI-enabled system has met the necessary standards of performance, transparency, and human integration to be deployed with confidence, thus advancing the field of defense technology management and risk assessment.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbedAgent: Benchmarking Large Language Models in Embedded System Development</title>
<link>https://arxiv.org/abs/2506.11003</link>
<guid>https://arxiv.org/abs/2506.11003</guid>
<content:encoded><![CDATA[
<div> Embedded system development, Large Language Models, benchmark, EmbedAgent, Embedbench <br />
Summary:<br />
In this paper, a new paradigm called EmbedAgent is introduced to assess Large Language Models (LLMs) in tasks related to embedded system development. A comprehensive benchmark called Embedbench is proposed to evaluate LLMs on tasks such as embedded system programming, circuit design, and cross-platform migration. Experiment results on 10 mainstream LLMs reveal that DeepSeek-R1 struggles with schematic information and generation tasks, while LLMs perform better on MicroPython compared to ESP-IDF in cross-platform migration. Chat LLMs like DeepSeek-V3 often fail to utilize relevant pre-trained knowledge, while reasoning LLMs can overlook efficient knowledge during pretraining. Two strategies, retrieval augmented generation and compiler feedback, are proposed to enhance LLM performance, resulting in significant improvements in accuracy for tasks such as schematic generation and cross-platform migration from Arduino to ESP32. <div>
arXiv:2506.11003v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in various tasks, yet few benchmarks assess their capabilities in embedded system development.In this paper, we introduce EmbedAgent, a paradigm designed to simulate real-world roles in embedded system development, such as Embedded System Programmer, Architect, and Integrator. This paradigm enables LLMs to be tested in tasks that bridge the gap between digital and physical systems, allowing for a more comprehensive assessment of their capabilities. To evaluate LLMs on these tasks, we propose Embedbench, the first comprehensive benchmark for embedded system programming, circuit design, and cross-platform migration.Embedbench consists of 126 cases, covering 9 electronic components across 3 hardware platforms. Through extensive experiments on 10 mainstream LLMs, we uncover several key findings. Surprisingly, despite the simplicity of the cases, DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic information, and 50.0% when tasked with generating the schematics itself. In the cross-platform migration tasks, LLMs show relatively strong performance with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8% pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4% pass@1.Interestingly, we observe that general-purpose chat LLMs like DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this domain, while reasoning LLMs tend to overthink and overlook efficient knowledge during pretraining. Based on these insights, we propose two strategies: retrieval augmented generation and compiler feedback-to enhance LLM performance. These strategies result in significant improvements, with Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without. Additionally, the accuracy of the Arduino to ESP32 migration task improves from 21.4% to 27.8%.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing a Dyslexia Indicator Using Eye Tracking</title>
<link>https://arxiv.org/abs/2506.11004</link>
<guid>https://arxiv.org/abs/2506.11004</guid>
<content:encoded><![CDATA[
<div> eye-tracking, machine learning, dyslexia, early detection, non-invasive 

Summary: 
- The study explores the use of eye-tracking technology and machine learning algorithms for early dyslexia detection.
- Analysis of eye movement patterns, such as fixation durations and saccades, helped identify dyslexia features.
- A Random Forest Classifier achieved an accuracy of 88.58% in detecting dyslexia.
- Hierarchical clustering methods were used to classify varying severity levels of dyslexia.
- The technology demonstrated potential for identifying individuals with dyslexia through non-invasive means, including borderline cases. 

<br /><br /> <div>
arXiv:2506.11004v1 Announce Type: cross 
Abstract: Dyslexia, affecting an estimated 10% to 20% of the global population, significantly impairs learning capabilities, highlighting the need for innovative and accessible diagnostic methods. This paper investigates the effectiveness of eye-tracking technology combined with machine learning algorithms as a cost-effective alternative for early dyslexia detection. By analyzing general eye movement patterns, including prolonged fixation durations and erratic saccades, we proposed an enhanced solution for determining eye-tracking-based dyslexia features. A Random Forest Classifier was then employed to detect dyslexia, achieving an accuracy of 88.58\%. Additionally, hierarchical clustering methods were applied to identify varying severity levels of dyslexia. The analysis incorporates diverse methodologies across various populations and settings, demonstrating the potential of this technology to identify individuals with dyslexia, including those with borderline traits, through non-invasive means. Integrating eye-tracking with machine learning represents a significant advancement in the diagnostic process, offering a highly accurate and accessible method in clinical research.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Comments on LLM Comprehension of Legacy Code</title>
<link>https://arxiv.org/abs/2506.11007</link>
<guid>https://arxiv.org/abs/2506.11007</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, legacy languages, comprehension, evaluation, documentation

Summary:<br />
Large language models (LLMs) have shown high performance in software engineering tasks but face challenges in comprehending legacy code due to lack of accurate documentation. The study aims to objectively evaluate LLM comprehension of legacy languages using multiple-choice question answering (MCQA). Preliminary findings highlight the impact of documentation, including comment prevalence and accuracy, on LLM comprehension of legacy code. To address this gap, future work will focus on developing efficient evaluation methods for LLM comprehension of legacy languages. Overall, the research emphasizes the importance of accurate documentation in enhancing LLM understanding of legacy code and sets strategic objectives for further investigation in this area.<br /><br />Summary: <div>
arXiv:2506.11007v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been increasingly integrated into software engineering and maintenance tasks due to their high performance with software engineering tasks and robust understanding of modern programming languages. However, the ability of LLMs to comprehend code written with legacy languages remains a research gap challenged by real-world legacy systems lacking or containing inaccurate documentation that may impact LLM comprehension. To assess LLM comprehension of legacy languages, there is a need for objective LLM evaluation. In order to objectively measure LLM comprehension of legacy languages, we need an efficient, quantitative evaluation method. We leverage multiple-choice question answering (MCQA), an emerging LLM evaluation methodology, to evaluate LLM comprehension of legacy code and the impact of comment prevalence and inaccurate comments. In this work, we present preliminary findings on the impact of documentation on LLM comprehension of legacy code and outline strategic objectives for future work.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>