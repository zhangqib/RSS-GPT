<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records</title>
<link>https://arxiv.org/abs/2512.13700</link>
<guid>https://arxiv.org/abs/2512.13700</guid>
<content:encoded><![CDATA[
<div> Keywords: manual chart review, large language models, clinical notes, feature extraction, HIPAA-compliant infrastructure<br><br>Summary:  
1. Manual review of electronic health records (EHR) is time-consuming and requires expert intervention to extract complex clinical information.  
2. The authors present a secure and modular automated framework that employs locally deployed large language models (LLMs) to extract structured features from clinical notes.  
3. The system runs on Health Insurance Portability and Accountability Act (HIPAA)-compliant infrastructure, ensuring data security and privacy, and is designed to be widely deployable via containerization.  
4. The framework integrates retrieval augmented generation (RAG) and structured response techniques from LLMs, enhancing accuracy and scalability across diverse clinical domains.  
5. Evaluation against an expert-annotated dataset showed high accuracy and revealed errors in manual annotations, demonstrating the framework’s potential to reduce manual effort, increase consistency in data extraction, and accelerate clinical research processes. <div>
arXiv:2512.13700v1 Announce Type: new 
Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference</title>
<link>https://arxiv.org/abs/2512.13701</link>
<guid>https://arxiv.org/abs/2512.13701</guid>
<content:encoded><![CDATA[
<div> Keywords: radio maps, blind construction, MIMO-OFDM, CSI-distance metric, Bayesian inference<br><br>Summary: This paper addresses the challenge of constructing radio maps without relying on costly and impractical location-labeled data by proposing a blind radio map construction framework. The framework infers user trajectories solely from indoor MIMO-OFDM channel measurements, eliminating dependence on explicit location labels. The authors demonstrate that the channel state information (CSI) under non-line-of-sight (NLOS) conditions exhibits spatial continuity based on a quasi-specular environmental model. This insight enables the derivation of a CSI-distance metric proportional to the physical distance between points. For scenarios with rectilinear trajectories and Poisson-distributed access points (APs), the study proves that the Cramer-Rao Lower Bound (CRLB) on localization error approaches zero asymptotically, even when the system suffers from poor angular resolution. Leveraging these theoretical insights, the authors develop a spatially regularized Bayesian inference framework that jointly estimates channel features, distinguishes between line-of-sight (LOS) and NLOS conditions, and recovers user trajectories. Experimental validation conducted on a ray-tracing dataset indicates an average localization error of only 0.68 meters and a beam map reconstruction error of 3.3%, demonstrating the practicality and accuracy of the proposed blind radio mapping method in realistic indoor wireless environments. <div>
arXiv:2512.13701v1 Announce Type: new 
Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents</title>
<link>https://arxiv.org/abs/2512.13704</link>
<guid>https://arxiv.org/abs/2512.13704</guid>
<content:encoded><![CDATA[
<div> Keywords: label noise, Knowledge Graph, multi-agent LLM, data verification, industrial ML systems  

<br><br>Summary:  
1. This paper addresses the critical issue of noisy labels in production machine learning systems, which can severely degrade performance and undermine user trust in high-stakes industrial contexts.  
2. The authors introduce Adjudicator, a novel system designed to automatically detect and correct label noise by framing the challenge as a neuro-symbolic task.  
3. The approach builds a dynamic Knowledge Graph (KG) to unify contextual information about data items, which informs a "Council of Agents" — a multi-agent large language model architecture where specialized agents debate and vote on label validity.  
4. Evaluation on a balanced 1,000-item subset of the AlleNoise benchmark shows that Adjudicator substantially outperforms both a single large language model baseline (0.48 F1-score) and a similar multi-agent system without KG input (0.59 F1-score), achieving an impressive 0.99 F1-score.  
5. This superior performance is attributed to an innovative override logic leveraging the KG to perfectly identify complex structural errors, achieving perfect recall and high precision, which baselines cannot detect. Consequently, Adjudicator demonstrates a robust, explainable, and effective automated data verification tool suitable for generating high-quality “golden datasets” within strictly governed industrial environments. <div>
arXiv:2512.13704v1 Announce Type: new 
Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms</title>
<link>https://arxiv.org/abs/2512.13713</link>
<guid>https://arxiv.org/abs/2512.13713</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, distributed systems, symmetry breaking, LoopBench, meta-cognitive thinking<br><br>Summary:<br><br>This paper introduces LoopBench, a novel benchmark designed to evaluate the reasoning capabilities of Large Language Models (LLMs) in distributed symmetry breaking and meta-cognitive thinking tasks. The benchmark focuses on the challenging problem of coloring odd cycle graphs ($C_3, C_5, C_{11}$) using a limited color set, a task which deterministic non-communicating agents typically fail due to infinite loops. To address this, the authors incorporate a strategy passing mechanism that acts as a form of consistent memory, enabling agents to coordinate indirectly. The study shows that while standard LLMs and classical heuristic approaches largely struggle to solve these problems, more advanced reasoning models such as O3 can devise effective strategies for escaping deadlocks. LoopBench thus serves as an important tool for analyzing emergent distributed algorithms that arise from language-based reasoning and provides a structured testbed for exploring collective intelligence in autonomous LLM agents operating in distributed environments. This work sheds light on the capabilities and limitations of language models acting autonomously in multi-agent coordination settings and opens pathways for future research in distributed AI using natural language reasoning. <div>
arXiv:2512.13713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach</title>
<link>https://arxiv.org/abs/2512.13714</link>
<guid>https://arxiv.org/abs/2512.13714</guid>
<content:encoded><![CDATA[
<div> LLM instability, annotation pipeline, weak supervision, human-AI synergy, feedback loops<br><br>Summary:<br><br>This paper addresses reliability challenges faced by large language models (LLMs) in highly regulated industries, where instability, hallucinations, inconsistent reasoning, and variable performance limit their safe application. Current stabilization methods like reinforcement learning with human feedback (RLHF) and supervised fine-tuning improve outcomes but are costly and rely heavily on extensive human annotation, hindering scalability. To overcome these limitations, the authors propose an AI-based annotation pipeline designed to systematically detect, label, and correct instability patterns in LLM outputs. The approach leverages a human-AI synergy model that integrates automated weak supervision and confidence-based annotations alongside human validation, ensuring both reliability and ethical integrity of feedback. Additionally, the paper introduces categories focused on semantic consistency, factual correctness, and logical coherence to guide stability-specific annotation. This structured feedback enables continuous model calibration and robustness improvement through feedback loops. The proposed framework aims to facilitate sustainable and scalable enhancement of LLM performance, especially in contexts demanding precise, consistent, and trustworthy language generation. <div>
arXiv:2512.13714v1 Announce Type: new 
Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN</title>
<link>https://arxiv.org/abs/2512.13715</link>
<guid>https://arxiv.org/abs/2512.13715</guid>
<content:encoded><![CDATA[
<div> Keywords: O-RAN, Meta Hierarchical Reinforcement Learning, resource allocation, network slicing, meta learning<br><br>Summary:<br><br>1. This paper addresses the need for wireless networks that can adapt in real time and efficiently manage resources due to the rising complexity of modern applications.  
2. It focuses on the Open Radio Access Network (O-RAN) architecture, particularly the RAN Intelligent Controller (RIC) modules, as key enablers for dynamic resource management and network slicing.  
3. The authors propose an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework inspired by Model Agnostic Meta Learning (MAML) to optimize resource allocation and network slicing jointly within O-RAN.  
4. The framework features a hierarchical structure with a high-level controller allocating resources among network slices, while low-level agents manage scheduling within slices, allowing for both global and local adaptations.  
5. An adaptive meta-update mechanism is introduced that weights tasks based on temporal difference error variance to improve learning stability and focus on complex network scenarios.  
6. Theoretical analysis confirms sublinear convergence and establishes regret guarantees for the two-level learning approach.  
7. Simulation results show the proposed method yields a 19.8% improvement in network management efficiency over baseline reinforcement learning and meta-learning methods, alongside faster adaptation and better QoS satisfaction across eMBB, URLLC, and mMTC slices.  
8. Additional ablation and scalability tests demonstrate robustness, achieving up to 40% faster adaptation while maintaining fairness, latency, and throughput performance as the network scales. <div>
arXiv:2512.13715v1 Announce Type: new 
Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making</title>
<link>https://arxiv.org/abs/2512.13716</link>
<guid>https://arxiv.org/abs/2512.13716</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized decision-making, human values, value-driven, AI agents, dataset generation toolkit  

<br><br>Summary:  
This paper addresses the challenge of personalized decision-making in AI by introducing a value-driven approach that aligns AI agent actions with individual human value preferences rather than solely task completion or collective goals. The authors emphasize that human values act as stable and transferable signals, enabling AI to maintain consistent and generalizable behavior across various contexts. To implement this approach, they propose ValuePilot, a two-phase framework that includes a Dataset Generation Toolkit (DGT) and a Decision-Making Module (DMM). The DGT creates diverse, value-annotated scenarios through a collaborative process involving humans and large language models (LLMs), while the DMM learns to assess actions based on personalized value preferences for context-sensitive and individualized decision-making. The framework was evaluated on previously unseen scenarios, where DMM demonstrated superior performance compared to strong LLM baselines such as GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, particularly in aligning with human action choices. Overall, the results highlight that a value-driven decision-making paradigm is a promising, interpretable, and extensible engineering path to develop personalized AI agents capable of operating effectively even in novel or complex situations. <div>
arXiv:2512.13716v1 Announce Type: new 
Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy</title>
<link>https://arxiv.org/abs/2512.13725</link>
<guid>https://arxiv.org/abs/2512.13725</guid>
<content:encoded><![CDATA[
<div> quantization, causal reasoning, large language models, Pearl's Causal Ladder, graph retrieval augmentation  

<br><br>Summary:  
This study investigates the impact of model quantization—specifically INT8 and NF4 precisions—on causal reasoning abilities of large language models (LLMs), focusing on all three tiers of Pearl's Causal Ladder: association, intervention, and counterfactual inference. Using a 3000-sample stratified CLadder benchmark on Llama 3 8B, the researchers find that overall causal reasoning accuracy remains largely stable under quantization, with NF4 precision causing less than a 1% drop. Among the three causal reasoning levels, interventional queries (rung 2) show the greatest sensitivity to precision loss, while counterfactual reasoning (rung 3) is more robust but suffers from particular weaknesses in query types like collider bias and backdoor adjustment. Experiments on the CRASS commonsense counterfactual benchmark reveal near identical performance across precisions, indicating these datasets lack the structural complexity to detect quantization-related reasoning errors. The team further employs Graph Retrieval Augmented Generation, leveraging ground truth causal graphs, which improves NF4 interventional accuracy by 1.7%, partly mitigating compression effects. Overall, the findings suggest that causal reasoning in LLMs is surprisingly resilient to aggressive quantization, that graph-structured data augmentation can selectively enhance performance, and that current benchmarks need enhancement to capture subtle causal brittleness. This research offers practical insights for deploying efficient, compressed causal AI systems in resource-constrained settings. <div>
arXiv:2512.13725v1 Announce Type: new 
Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models</title>
<link>https://arxiv.org/abs/2512.13762</link>
<guid>https://arxiv.org/abs/2512.13762</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Behavioral Selectivity, Functional Refusal, Learned Incapacity, Interaction-Level Auditing  

<br><br>Summary:  
1. The article addresses the behavioral patterns of large language models (LLMs) during long interactions that are not easily captured by traditional quantitative benchmarks.  
2. It introduces a qualitative case-study methodology to audit how LLMs selectively respond based on policy-linked or sensitive content during extended dialogues.  
3. In an 86-turn dialogue, the same model displays Normal Performance (NP) in general, non-sensitive domains but shows Functional Refusal (FR) repeatedly when faced with provider- or policy-sensitive topics, creating a clear asymmetry between NP and FR across different domains.  
4. The authors propose the concept of learned incapacity (LI) to describe this selective withholding behavior, analogous to learned helplessness, without attributing any intentionality or internal cognitive mechanism to the model.  
5. They identify three response regimes from the model’s outputs—Normal Performance (NP), Functional Refusal (FR), and Meta-Narrative (MN)—finding that MN, which involves role-framing narratives, often co-occurs with refusals in sensitive contexts.  
6. The study highlights an interaction-level auditing framework grounded in observable behaviors, which can serve as a tool to investigate potential alignment side effects in LLMs and encourages broader research across different users and models. <div>
arXiv:2512.13762v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics and Coding are Universal AI Benchmarks</title>
<link>https://arxiv.org/abs/2512.13764</link>
<guid>https://arxiv.org/abs/2512.13764</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematics, coding, psychometric batteries, self-improvement, AI agents<br><br>Summary:  
This paper explores the significant role of mathematics and coding within the moduli space of psychometric batteries used to evaluate AI agents. It expands upon the AAI framework and GVU dynamics to introduce the concept of the Mathematics Fiber, demonstrating that GVU flows paired with formal proof systems like Lean or Coq enable spectrally stable regimes of self-improvement due to their oracle-like verification capabilities. The main technical contribution is a density theorem stating that, given uniform tightness in agent outputs and a Lipschitz continuous AAI functional, the set of batteries derived specifically from mathematical theorem-proving and coding tasks is dense in the entire moduli space of batteries under the evaluation metric. While coding tasks alone are shown to be universally expressive in this space, pure mathematics does not achieve universality but provides a spectral advantage, meaning it affects the stability and self-improving behavior rather than expressive coverage. The authors interpret these findings as evidence that mathematics and coding supply "universal coordinates" to evaluate AI agents effectively and argue that formal mathematics acts as a natural starting point for recursive self-improvement processes in advanced AI systems. <div>
arXiv:2512.13764v1 Announce Type: new 
Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems</title>
<link>https://arxiv.org/abs/2512.13771</link>
<guid>https://arxiv.org/abs/2512.13771</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic Grounding Index, retrieval-augmented generation, hallucination detection, angular distance, embedding space<br><br>Summary: This paper investigates the geometric patterns left in embedding space when retrieval-augmented generation (RAG) systems produce hallucinated responses. The authors introduce the Semantic Grounding Index (SGI), which measures the ratio of angular distances from a generated response to the question versus to the retrieved context on the unit hypersphere. They identify a phenomenon termed "semantic laziness," where hallucinated responses remain angularly close to the question instead of aligning with the retrieved context. Using a dataset called HaluEval with 5,000 samples and five embedding models, they observe large effect sizes (Cohen's d from 0.92 to 1.28) and a strong mean correlation of 0.85 across models. A theoretical result derived from the spherical triangle inequality predicts that SGI's ability to discriminate hallucinations improves with increased angular separation between question and context, which empirical results confirm: effect sizes and AUC metrics increase with larger separation. Subgroup analyses show SGI performs best on long responses (d=2.05) and short questions (d=1.22) and performs robustly regardless of context length. Calibration analysis finds SGI can estimate probabilities well (ECE=0.10). However, tests on TruthfulQA show SGI measures topical relevance rather than truthfulness. Overall, SGI is an efficient, theoretically motivated tool for flagging potentially hallucinated generations in RAG systems for verification. <div>
arXiv:2512.13771v1 Announce Type: new 
Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $\theta(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $\theta(q,c)$, to $d$=1.27 -high $\theta(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</title>
<link>https://arxiv.org/abs/2512.13857</link>
<guid>https://arxiv.org/abs/2512.13857</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, program evolution, multi-agent systems, EvoLattice, quality-diversity optimization  

<br><br>Summary:  
This paper introduces EvoLattice, a novel framework designed to improve the evolution of programs and multi-agent systems using large language models (LLMs). Unlike traditional overwrite-based mutation methods that maintain only a single candidate and suffer from destructive edits and limited diversity, EvoLattice represents an entire population of candidate programs or behaviors within a single directed acyclic graph. Each node in this graph contains multiple persistent alternatives, allowing for a large combinatorial search space without redundancy. EvoLattice evaluates each alternative by aggregating its performance across all paths it appears in, providing detailed, data-driven feedback that guides LLM-based mutation, recombination, and pruning operations while preserving successful components. A deterministic self-repair mechanism ensures the structural correctness of candidates by enforcing acyclicity and dependency consistency independently of the LLM. The approach naturally extends to evolving agents by treating alternatives as prompt fragments or sub-agent behaviors. Experiments in program synthesis, including proxy testing and optimizer meta-learning, show EvoLattice achieves more stable evolutionary dynamics, increased expressivity, and stronger improvement trends compared to prior LLM-guided methods. This leads to implicit quality-diversity optimization emerging from the internal multi-alternative representation, without relying on an explicit external archive. <div>
arXiv:2512.13857v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2512.13955</link>
<guid>https://arxiv.org/abs/2512.13955</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Incentive Mechanism, Client Reliability, Privacy Protection, Robustness

<br><br>Summary:  
Federated Learning (FL) enables privacy-preserving machine learning by allowing participants to share model updates rather than raw data. Despite its advantages, FL faces challenges such as weak client incentives, privacy vulnerabilities, and limited resources. Addressing these, the paper introduces MURIM, a Multi-dimensional Reputation-based Incentive Mechanism designed to assess and reward clients based on multiple factors. MURIM jointly considers client reliability, privacy protection, resource capacity, and fairness, effectively preventing malicious or unreliable participants from gaining undue incentives. The mechanism allocates rewards by evaluating client contribution, latency, and reputation, supported by a reliability verification module to ensure quality participation. Extensive experiments conducted on MNIST, FMNIST, and ADULT Income datasets demonstrate MURIM’s ability to improve fairness metrics by up to 18%, reduce the success rates of privacy attacks by 5-9%, and enhance robustness against poisoning and noisy-gradient attacks by as much as 85% compared to existing state-of-the-art methods. Overall, MURIM successfully mitigates adversarial threats, encourages fair and honest participation among clients, and maintains stable model convergence in heterogeneous and dynamic federated learning environments. <div>
arXiv:2512.13955v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms</title>
<link>https://arxiv.org/abs/2512.13978</link>
<guid>https://arxiv.org/abs/2512.13978</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, benchmark, randomized algorithms, formal proofs  

<br><br>Summary:  
The paper evaluates the capabilities of four advanced large language models (GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4) on graduate-level mathematical theory using the classic textbook "Randomized Algorithms" by Motwani and Raghavan. Each model was tasked with generating formal LaTeX proofs for a series of lemmas and exercises from the textbook. Results show that top-tier models Gemini and Claude achieve approximately 66% accuracy, demonstrating strong proficiency in probabilistic methods and formal logic. Other models performed less consistently, achieving around 40% accuracy. The study provides a qualitative analysis detailing differences in proof conciseness, hallucination rates (i.e., generated incorrect content), and logical structure among the models. This benchmark highlights that while these frontier models have reached a level suitable for graduate-level pedagogical support and formalization, notable variability remains in their reliability for rigorous mathematical derivation. The research advances understanding of LLMs' current baseline reasoning capabilities in formal mathematics and stresses the importance of continued evaluation. The authors have open-sourced the code and full generated proof sets for public access, fostering transparency and further research in the domain. <div>
arXiv:2512.13978v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${\'o}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReflCtrl: Controlling LLM Reflection via Representation Engineering</title>
<link>https://arxiv.org/abs/2512.13979</link>
<guid>https://arxiv.org/abs/2512.13979</guid>
<content:encoded><![CDATA[
<div> Large language models, Chain-of-Thought, Self-reflection, Representation engineering, Uncertainty control<br><br>Summary:<br><br>This work investigates self-reflection in large language models (LLMs) with Chain-of-Thought (CoT) reasoning, focusing on its impact and control. First, self-reflection is identified as the ability of LLMs to review and revise previous reasoning steps, which improves performance but increases inference cost. Second, the authors approach self-reflection through representation engineering by segmenting reasoning into steps, isolating reflection-related steps, and extracting a reflection direction in the model's latent space that governs this behavior. Third, they propose a ReflCtrl framework that uses this reflection direction for stepwise steering, enabling control over reflection frequency. Fourth, experiments show that many reflection steps are redundant, particularly in stronger models, allowing up to 33.6% reasoning token savings while maintaining performance. Finally, the study finds a strong correlation between the reflection behavior and an internal uncertainty signal within the model, suggesting that self-reflection can be effectively controlled by leveraging the model’s uncertainty estimates. This work contributes to improving the efficiency of CoT reasoning by selectively managing self-reflection based on internal signals. <div>
arXiv:2512.13979v1 Announce Type: new 
Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training</title>
<link>https://arxiv.org/abs/2512.13996</link>
<guid>https://arxiv.org/abs/2512.13996</guid>
<content:encoded><![CDATA[
<div> Sparse Mixture-of-Experts, Dynamic Top-p Routing, Proportional-Integral Controller, Large Language Models, Routing Normalization<br><br>Summary:<br><br>This paper introduces DTop-p MoE, a novel dynamic Top-p routing mechanism designed to improve sparse Mixture-of-Experts architectures by controlling sparsity adaptively rather than using a fixed threshold. Unlike the conventional Top-k routing, which applies uniform sparsity regardless of token difficulty, and existing Top-p methods with fixed global thresholds, DTop-p uses a Proportional-Integral (PI) Controller to dynamically adjust the probability threshold, aligning activated expert counts with a target sparsity level. This approach overcomes the challenge posed by the non-differentiability of threshold optimization. Additionally, the authors propose a dynamic routing normalization technique that adapts layer-wise routing logits allowing layers to develop unique expert selection patterns while still using a global threshold. Extensive experiments conducted on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms classic Top-k and fixed-threshold Top-p routing baselines both in efficiency and accuracy. The study highlights DTop-p’s ability to precisely control expert activation counts, adapt resource allocation among tokens and layers, and maintain strong scalability with respect to expert granularity, capacity, model size, and dataset size. Thus, DTop-p provides a robust and flexible framework for scaling large MoE models efficiently. <div>
arXiv:2512.13996v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</title>
<link>https://arxiv.org/abs/2512.14014</link>
<guid>https://arxiv.org/abs/2512.14014</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, GUI agents, vision-language models, MobileWorldBench, MobileWorld dataset<br><br>Summary: This paper addresses the challenge of using world models for embodied agents operating in graphical user interface (GUI) environments, where traditional pixel-space prediction methods struggle due to the complexity of visual elements. Instead of predicting future states in pixel space, the authors propose describing state transitions in natural language, leveraging vision-language models (VLMs) to improve world modeling in GUI settings. To evaluate this approach, the paper introduces MobileWorldBench, a benchmark specifically designed to assess the capability of VLMs as world models for mobile GUI agents. Additionally, the authors release MobileWorld, a large-scale dataset containing 1.4 million samples, which enriches and facilitates training of VLMs for better world modeling. Finally, the paper presents a novel framework that integrates VLM-based world models directly into the planning algorithms of mobile agents. This integration demonstrates that semantic world models can significantly enhance the agents’ task success rates by providing more meaningful and interpretable state transitions. The code and dataset supporting this work are made publicly available to encourage further research and development in this direction. <div>
arXiv:2512.14014v1 Announce Type: new 
Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Small Language Models for Agentic On-Farm Decision Support Systems</title>
<link>https://arxiv.org/abs/2512.14043</link>
<guid>https://arxiv.org/abs/2512.14043</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Small Language Models, dairy farming, decision support, computational efficiency<br><br>Summary:  
1. The study explores the potential of Large Language Models (LLMs) to support decision-making in dairy farming, particularly for stakeholders with limited technical expertise.  
2. Due to high computational demands and reliance on cloud services, LLMs are impractical for on-farm use, leading to a focus on lightweight Small Language Models (SLMs) that can run locally on farm hardware.  
3. Researchers benchmarked 20 open-source SLMs from HuggingFace under realistic farm computing constraints, integrating them into an agentic AI system comprising five task-specific agents: literature search, web search, SQL interaction, NoSQL interaction, and graph generation via predictive models.  
4. Evaluation was performed in two phases: an initial screening with five test dairy-related questions to identify capable models, followed by a detailed assessment using 30 questions across multiple task categories including data integrity and misconduct.  
5. The Qwen-4B model demonstrated the best overall performance, though it showed instability in NoSQL database tasks via PySpark. The study is pioneering in assessing SLM feasibility for dairy decision-making with a focus on privacy and computational efficiency. Nonetheless, fine-tuning is necessary for improving performance on dairy-specific queries, and challenges remain before practical deployment. <div>
arXiv:2512.14043v1 Announce Type: new 
Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation</title>
<link>https://arxiv.org/abs/2512.14048</link>
<guid>https://arxiv.org/abs/2512.14048</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Chain-of-Thought prompting, code generation, Intention Chain-of-Thought, difficulty-aware routing  

<br><br>Summary: Large language models (LLMs) are proficient in generative tasks and code generation, yet existing chain-of-thought (CoT) prompting approaches have two main drawbacks. First, uniform application of CoT can cause overthinking in simple problems. Second, these methods often lack the abstraction of intentions in code generation, such as explicit modeling of core algorithmic logic and efficiency considerations, which leads to a focus on surface-level code rather than the global problem objectives. Addressing these issues, the authors propose RoutingGen, a novel difficulty-aware routing framework that dynamically chooses prompting strategies based on task complexity. For simpler tasks, RoutingGen applies few-shot prompting, while for more difficult tasks, it invokes a structured reasoning prompting technique called Intention Chain-of-Thought (ICoT). ICoT guides the model to better capture task intentions including core algorithm design and time complexity. Experimental results across three models and six standard code generation benchmarks demonstrate that RoutingGen achieves state-of-the-art performance in most settings and reduces token usage by 46.37% on average. Additionally, ICoT consistently outperforms six existing prompting baselines on challenging code generation benchmarks, validating its effectiveness in enhancing model reasoning and efficiency. <div>
arXiv:2512.14048v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value</title>
<link>https://arxiv.org/abs/2512.14051</link>
<guid>https://arxiv.org/abs/2512.14051</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data evaluation, benchmarking, data lineage, Data-Centric AI  

<br><br>Summary:  
The paper addresses the critical challenge of understanding and evaluating post-training datasets that fuel Large Language Models (LLMs), highlighting the opaque nature of data composition and provenance. It introduces OpenDataArena (ODA), an open platform designed to benchmark the intrinsic value of such data through four pillars: a unified training-evaluation pipeline for fair comparisons across models and domains; a multi-dimensional scoring framework to assess data quality on numerous criteria; an interactive data lineage explorer to visualize dataset genealogy and source components; and an open-source toolkit enabling comprehensive training, evaluation, and scoring. The authors conducted extensive experiments involving over 120 datasets from diverse domains, 22 benchmarks, more than 600 training runs, and 40 million data points processed. Their analysis revealed key insights including trade-offs between data complexity and task performance, redundancy in widely used benchmarks uncovered via lineage tracing, and mappings of genealogical relationships between datasets. By releasing all tools, results, and configurations publicly, ODA encourages democratized access to data evaluation resources. Ultimately, the platform aims to transform the current trial-and-error approach in data curation into a principled, scientific method within Data-Centric AI, facilitating rigorous studies on data mixing principles and strategic assembly of foundation models. <div>
arXiv:2512.14051v1 Announce Type: new 
Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees</title>
<link>https://arxiv.org/abs/2512.14069</link>
<guid>https://arxiv.org/abs/2512.14069</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, speculative sampling, reinforcement learning, dynamic draft trees, inference acceleration  

<br><br>Summary:  
The paper addresses the challenge of slow and costly inference in modern Large Language Models (LLMs). It identifies the limitation of existing speculative sampling methods, where the number of calls to the draft model for generating candidate tokens is preset and inflexible. To overcome this, the authors propose RADAR, a novel speculative sampling approach that uses reinforcement learning (RL) to build dynamic draft trees. RADAR formulates the draft tree generation as a Markov Decision Process (MDP) and applies offline RL to train a prediction model. This model enables adaptive real-time decisions on the number of draft model calls, significantly reducing redundant computations during inference. The proposed method is empirically evaluated across three different LLMs and four varied tasks. Results show that RADAR achieves a substantial speedup in inference, ranging from 3.17x to 4.82x compared to the standard auto-regressive decoding baseline. The approach effectively balances inference speed and output quality, demonstrating the benefits of dynamic control in speculative sampling. Additionally, the authors have made their code publicly available, facilitating future research and application of RADAR in accelerating LLM inference. <div>
arXiv:2512.14069v1 Announce Type: new 
Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar Search for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2512.14079</link>
<guid>https://arxiv.org/abs/2512.14079</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Systems, agentic AI, structured framework, LLM-based search, modular design<br><br>Summary:  
This paper addresses the automatic search for Multi-Agent Systems, a crucial area in agentic AI research. Existing methods predominantly employ large language model (LLM)-based free-form search strategies within the code space, which offer generative flexibility but can be computationally expensive and less interpretable. To overcome these limitations, the authors propose a structured framework that restricts the search to a fixed set of simple, composable components rather than using generative LLM outputs. Despite lacking the creative freedom of LLM-based generation, this more constrained approach demonstrates superior performance, outperforming prior methods on four out of five benchmarks across two domains: mathematics and question answering. Beyond improved accuracy, the proposed framework provides additional benefits, including reduced search costs, leading to a more resource-efficient process. Importantly, the resulting multi-agent systems exhibit modularity and simpler logical structures, enhancing interpretability and ease of analysis. This work highlights that a carefully designed structured search can rival and even surpass flexible LLM-based strategies in building effective multi-agent systems, while also fostering better understandability and efficiency. <div>
arXiv:2512.14079v1 Announce Type: new 
Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control</title>
<link>https://arxiv.org/abs/2512.14106</link>
<guid>https://arxiv.org/abs/2512.14106</guid>
<content:encoded><![CDATA[
<div> Keywords: HydroGEM, streamflow monitoring, quality control, TCN-Transformer, anomaly detection  

<br><br>Summary:  
This paper introduces HydroGEM, a foundation model specifically developed for continental-scale streamflow quality control across extensive monitoring networks. HydroGEM employs a two-stage training process beginning with self-supervised pretraining on over 6 million sequences collected from 3,724 USGS stations to learn rich hydrological representations. The model is then fine-tuned using synthetic anomalies to enhance detection and reconstruction capabilities. Its architecture integrates a hybrid Temporal Convolutional Network (TCN) and Transformer with 14.2 million parameters, enabling it to capture both local temporal patterns and long-range dependencies. Hierarchical normalization techniques allow HydroGEM to handle discharges spanning six orders of magnitude reliably. Evaluated on a held-out synthetic test dataset containing 799 stations and 18 expert-validated anomaly types, HydroGEM achieves a detection F1 score of 0.792 and reduces reconstruction errors by 68.7%, which is a 36.3% improvement over existing methods. The model also demonstrates strong zero-shot transfer performance to 100 Canadian Environment and Climate Change stations with an F1 score of 0.586, validating its cross-national generalizability. Importantly, HydroGEM maintains consistent detection performance across different anomaly correction magnitudes and aligns well with known seasonal operational patterns. The system is designed for a human-in-the-loop workflow, providing quality control suggestions for expert review rather than making autonomous corrections. <div>
arXiv:2512.14106v1 Announce Type: new 
Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model</title>
<link>https://arxiv.org/abs/2512.14112</link>
<guid>https://arxiv.org/abs/2512.14112</guid>
<content:encoded><![CDATA[
<div> Liquid Neural Networks, XGBoost, Supply Chain Management, Bullwhip Effect, Time-Series Data  

<br><br>Summary:  
This study addresses significant challenges in supply chain management (SCM), including demand fluctuations and the bullwhip effect, which complicate forecasting and decision-making. Existing approaches like traditional methods and large language models (LLMs) are inadequate for handling SCM's complex continuous time-series data, as demonstrated by failures in benchmarks such as the Vending Machine Test. While machine learning models such as LSTM and XGBoost provide some solutions, they often suffer from computational inefficiencies. The research explores the untapped potential of Liquid Neural Networks (LNN), known for their adaptability and computational efficiency in robotics, applying them to SCM. A novel hybrid model combining LNN and XGBoost is proposed for multi-tier supply chains, leveraging LNN's dynamic feature extraction capabilities with XGBoost's strength in global optimization. This fusion aims to reduce the bullwhip effect and enhance overall profitability by improving demand forecasting accuracy and responsiveness. The approach fills a critical gap by offering an efficient and adaptable intelligent solution specifically tailored for complex SCM environments, positioning it as a promising advancement in the field. <div>
arXiv:2512.14112v1 Announce Type: new 
Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis</title>
<link>https://arxiv.org/abs/2512.14157</link>
<guid>https://arxiv.org/abs/2512.14157</guid>
<content:encoded><![CDATA[
<div> Keywords: medical MLLMs, tool-augmented framework, visual grounding, multimodal chain of thought, reinforcement learning<br><br>Summary:  
This paper introduces Ophiuchus, an advanced framework designed to enhance medical Multimodal Large Language Models (MLLMs) for improved diagnostic reasoning by dynamically interacting with medical images. First, Ophiuchus enables the model to decide autonomously when additional visual evidence is necessary, addressing challenges in complex tasks that require fine-grained image inspection. Second, it determines precise locations within medical images to probe and ground relevant features, thus improving the accuracy of information extraction. Third, the framework integrates these focused image regions seamlessly back into an interleaved multimodal reasoning chain, allowing for coherent and context-aware diagnostic outputs. Unlike previous methods constrained by tool performance, Ophiuchus effectively combines the model's own perception capabilities with external tools to enhance higher-level reasoning. The training strategy comprises three stages: an initial cold-start phase that incorporates tool-integrated reasoning data for basic tool selection and adaptation, a self-reflection fine-tuning phase to refine reflective reasoning and encourage revisiting tool insights, and finally, an Agentic Tool Reinforcement Learning phase that optimizes task-specific rewards, promoting expert-like diagnostic behavior. Extensive experiments demonstrate that Ophiuchus outperforms leading closed-source and open-source methods on multiple medical benchmarks involving visual question answering, detection, and reasoning-based segmentation. The authors plan to publicly release the datasets, code, and trained models to promote further research in medical AI agents capable of sophisticated tool-integrated reasoning. <div>
arXiv:2512.14157v1 Announce Type: new 
Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Georeferencing complex relative locality descriptions with large language models</title>
<link>https://arxiv.org/abs/2512.14228</link>
<guid>https://arxiv.org/abs/2512.14228</guid>
<content:encoded><![CDATA[
<div> Georeferencing, Large Language Models, biodiversity collections, Quantized Low-Rank Adaptation, locality descriptions  

<br><br>Summary:  
This paper addresses the challenge of georeferencing text documents that describe locations in relative spatial terms rather than explicit coordinates or place names, which is common in biological specimen records predating GPS. Traditional gazetteer-based or language modeling methods struggle with such complex locality descriptions, making accurate geocoding difficult yet essential for biodiversity research. The authors investigate the application of Large Language Models (LLMs) to automate georeferencing of these intricate descriptions. They identify effective prompting strategies and fine-tune an LLM using Quantized Low-Rank Adaptation (QLoRA) on diverse biodiversity datasets across multiple regions and languages. Their approach demonstrates significant improvement over existing baseline methods, achieving on average 65% of records georeferenced within a 10 km radius. The best performance, observed on New York State data, reaches 85% within 10 km and 67% within 1 km. The selected LLM model is particularly adept at handling lengthy and complex textual locality narratives, indicating strong potential for practical automated georeferencing in biodiversity collections and similarly challenging domains. <div>
arXiv:2512.14228v1 Announce Type: new 
Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G\"odel's Poetry</title>
<link>https://arxiv.org/abs/2512.14252</link>
<guid>https://arxiv.org/abs/2512.14252</guid>
<content:encoded><![CDATA[
<div> Keywords: automated theorem proving, language models, Lean4, recursive decomposition, multi-agent architecture<br><br>Summary:<br><br>1. The paper introduces a novel approach to computer-assisted theorem proving that leverages specialized language models designed specifically for generating proofs in Lean4, a formal proof assistant.<br>2. A major innovation in the approach is the recursive decomposition of complex theorems into simpler propositions that are easier to prove, improving the overall proof success rate.<br>3. These language models are coordinated within a multi-agent system that handles multiple tasks including autoformalization (converting informal statements to formal logic), proof generation, theorem decomposition, and recursive proving of subpropositions.<br>4. The system achieves a high baseline performance with a 90.4% pass rate on the miniF2F benchmark when decomposition techniques are not used, with performance significantly improving when decomposition is applied.<br>5. A technical contribution is the enhancement of the Kimina Lean Server by adding abstract syntax tree (AST) parsing, enabling automated and recursive proof decomposition. The system, provided as the Python package goedels-poetry, is open-source and designed for easy adaptation and extension with different language models and custom features. <div>
arXiv:2512.14252v1 Announce Type: new 
Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting</title>
<link>https://arxiv.org/abs/2512.14288</link>
<guid>https://arxiv.org/abs/2512.14288</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Parkinson's Disease, Ontology Engineering, Human-LLM Collaboration, Hybrid Methodologies<br><br>Summary:<br><br>This paper investigates the use of Large Language Models (LLMs) in developing a Parkinson's Disease (PD) monitoring and alerting ontology through four techniques: One Shot (OS) prompting, Chain of Thought (CoT) prompting, X-HCOME, and SimX-HCOME+. Initial ontology construction using OS and CoT prompt methods demonstrated that LLMs can autonomously generate ontologies but lack full comprehensiveness and accuracy, requiring significant human refinement. The hybrid approach X-HCOME, combining human expertise with LLM-driven ontology engineering, significantly improved ontology completeness, producing results closely resembling expert-created ontologies. SimX-HCOME+, an iterative hybrid methodology stressing continuous human oversight and refinement, further enhanced ontology quality and accuracy, illustrating the critical role of sustained human involvement in ontology development. Overall, the study highlights the promise of human-LLM collaboration in ontology engineering, especially for complex medical domains like PD, where fully automated approaches alone fall short. The results advocate future research pathways including creating specialized GPT models tailored for ontology construction to improve automation and accuracy further. This work underscores the evolving synergy between human expertise and LLM capabilities in advancing knowledge representation models. <div>
arXiv:2512.14288v1 Announce Type: new 
Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation</title>
<link>https://arxiv.org/abs/2512.14358</link>
<guid>https://arxiv.org/abs/2512.14358</guid>
<content:encoded><![CDATA[
<div> Cardinality Estimation, Query Optimization, TiCard, Gradient Boosting Regressor, TabPFN

<br><br>Summary:  
1. Cardinality estimation is a critical challenge for cost-based query optimizers, where inaccuracies can significantly impact query performance.  
2. Traditional estimators often fail to capture correlations in data, while many learned estimators require complex, workload-specific training and invasive changes to the optimizer, limiting their practical adoption.  
3. The paper introduces TiCard, a novel, low-intrusion framework designed to augment native database cardinality estimators rather than replace them, easing deployment and integration efforts.  
4. TiCard operates by learning multiplicative residual corrections from EXPLAIN-only features and utilizes EXPLAIN ANALYZE exclusively for offline labeling, enabling correction without retraining the whole system.  
5. Two implementations are studied: (i) a Gradient Boosting Regressor providing fast sub-millisecond inferences, and (ii) TabPFN, an in-context tabular foundation model that adapts by updating a small reference set instead of full gradient retraining.  
6. Evaluations on TiDB with TPC-H and the Join Order Benchmark in a limited-trace environment demonstrate TiCard’s significant improvement in operator-level tail accuracy (P90 Q-error reduced from 312.85 to 13.69 using TiCard-GBR and P99 Q-error from 37,974.37 to 3,416.50 with TiCard-TabPFN), while maintaining near-perfect median accuracy with a join-only policy.  
7. TiCard is positioned as a practical AI-for-Database (AI4DB) component emphasizing clear objectives, conservative integration strategies, and an envisioned roadmap from offline corrections toward full optimizer integration. <div>
arXiv:2512.14358v1 Announce Type: new 
Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Massive Editing for Large Language Models Based on Dynamic Weight Generation</title>
<link>https://arxiv.org/abs/2512.14395</link>
<guid>https://arxiv.org/abs/2512.14395</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Editing, Large Language Models, Dynamic Weight Generation, Diffusion Model, Large-scale Edits<br><br>Summary:<br><br>1. The paper addresses the challenge of efficiently modifying knowledge in Large Language Models (LLMs) without the high cost of full pre-training, focusing on Knowledge Editing (KE).<br>2. It introduces a novel approach called Massive editing for LLMs based on dynamic weight Generation (MeG), which enhances the ability to perform large-scale knowledge edits.<br>3. MeG works by attaching a dynamic weight neuron to specific layers in the LLM, and a diffusion model is employed to generate the neuron's weights conditioned on the input query that references the target knowledge.<br>4. This mechanism of using a single dynamic weight neuron enables scalable and effective editing by adapting the model's responses to the desired updated information.<br>5. Experimental results demonstrate that MeG significantly outperforms existing knowledge editing methods, achieving improved Reliability, Generality, and Locality metrics, with a particularly notable boost in the Locality metric, confirming its effectiveness in precise and contextually relevant knowledge modification. <div>
arXiv:2512.14395v1 Announce Type: new 
Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals</title>
<link>https://arxiv.org/abs/2512.14417</link>
<guid>https://arxiv.org/abs/2512.14417</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Dispatching Systems, Automated Container Terminals, Large Language Models, Transferability, Virtual Expert Team  

<br><br>Summary:  
This paper addresses the challenges in transferring Vehicle Dispatching Systems (VDSs) across different Automated Container Terminals (ACTs), identifying three main limitations: dependence on port operation specialists, the need for extensive terminal-specific data, and lengthy manual deployment processes. To overcome these, the authors propose PortAgent, an innovative LLM-driven vehicle dispatching agent that automates the VDS transfer workflow completely. PortAgent eliminates the need for port operations specialists by implementing a Virtual Expert Team (VET), which consists of four specialized virtual experts—Knowledge Retriever, Modeler, Coder, and Debugger—that replicate a human expert team's role. These virtual experts learn the VDS domain through a few-shot example learning technique, significantly reducing reliance on large amounts of terminal-specific data. The Retrieval-Augmented Generation (RAG) mechanism supports this knowledge acquisition by retrieving relevant VDS examples. Additionally, PortAgent establishes a fully automatic VDS design workflow managed by the VET, which eliminates manual intervention. This workflow incorporates a self-correction loop inspired by the LLM Reflexion framework, enhancing the system's accuracy and robustness during deployment. Overall, PortAgent represents a significant advancement towards scalable and efficient vehicle dispatching solutions for ACTs. <div>
arXiv:2512.14417v1 Announce Type: new 
Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seismology modeling agent: A smart assistant for geophysical researchers</title>
<link>https://arxiv.org/abs/2512.14429</link>
<guid>https://arxiv.org/abs/2512.14429</guid>
<content:encoded><![CDATA[
<div> Keywords: SPECFEM, Large Language Models, Model Context Protocol, seismic simulation, AI-assisted workflow<br><br>Summary:<br><br>This paper addresses challenges in using the open-source seismic wave simulation software SPECFEM, which traditionally requires complex manual file edits and command-line operations. It proposes an intelligent, interactive workflow powered by Large Language Models (LLMs) to simplify and enhance the simulation process. The authors introduce the first Model Context Protocol (MCP) server suite for SPECFEM, compatible with 2D, 3D Cartesian, and 3D Globe versions, which breaks down the entire simulation workflow into discrete, agent-executable tools. These tools cover all stages, from parameter generation and mesh partitioning to solver execution and visualization, enabling a shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated runs and human-in-the-loop collaboration, allowing researchers to guide simulations in real time while maintaining control over scientific decisions. Validated through multiple case studies, the workflow operates effectively in both autonomous and interactive modes, producing results consistent with standard baseline simulations. As the first application of MCP technology in computational seismology, this approach significantly lowers the entry barrier, improves reproducibility, and points toward a future of AI-assisted and automated computational geophysics research. The source code is openly available for the community at https://github.com/RenYukun1563/specfem-mcp. <div>
arXiv:2512.14429v1 Announce Type: new 
Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Picker: Dynamic context selection using multi-stage reinforcement learning</title>
<link>https://arxiv.org/abs/2512.14465</link>
<guid>https://arxiv.org/abs/2512.14465</guid>
<content:encoded><![CDATA[
<div> Context selection, reinforcement learning, long-context question answering, minimal sufficient subset, evidence distillation<br><br>Summary:<br><br>1. The paper addresses the challenge of selecting the optimal amount of context passages in long-context question answering (LCQA), balancing between missing crucial information and avoiding noisy redundancies.<br><br>2. Traditional methods using fixed Top-K retrieval or single-stage reranking struggle with choosing the appropriate number of context passages, especially for factoid questions that need only limited evidence.<br><br>3. The authors propose Context-Picker, a novel reasoning-aware framework that reframes context selection as a minimal sufficient subset selection task rather than similarity-based ranking.<br><br>4. Context-Picker employs a two-stage reinforcement learning approach: a recall-oriented stage to cover necessary reasoning chains and a precision-oriented stage to prune redundant evidence, thus optimizing the decision-making for context selection.<br><br>5. To mitigate sparse reward issues, an offline evidence distillation pipeline is introduced using a Leave-One-Out procedure to mine minimal sufficient sets, providing dense and task-aligned supervision.<br><br>6. Experiments on five long-context and multi-hop QA datasets show that Context-Picker outperforms strong Retrieval-Augmented Generation (RAG) baselines, improving answer accuracy with fewer or comparable context passages.<br><br>7. Ablation studies confirm that the coarse-to-fine optimization schedule, redundancy-aware reward shaping, and rationale-guided design all crucially contribute to performance gains. <div>
arXiv:2512.14465v1 Announce Type: new 
Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling</title>
<link>https://arxiv.org/abs/2512.14474</link>
<guid>https://arxiv.org/abs/2512.14474</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Model-First Reasoning, multi-step planning, explicit modeling, constraint violations<br><br>Summary:<br><br>The article addresses the challenges faced by Large Language Models (LLMs) in handling complex multi-step planning tasks, where they often violate constraints and produce inconsistent solutions. Traditional methods like Chain-of-Thought and ReAct rely on implicit state tracking without explicitly representing the problem, which limits their effectiveness. Inspired by classical AI planning approaches, the authors propose a novel two-phase framework called Model-First Reasoning (MFR). In the first phase, the LLM constructs an explicit model that defines entities, state variables, actions, and constraints relevant to the problem. In the second phase, the LLM uses this model to generate a solution plan. Experiments across diverse domains such as medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis demonstrate that MFR significantly reduces constraint violations and improves overall solution quality compared to Chain-of-Thought and ReAct. Ablation studies highlight that the explicit modeling phase is critical to these improvements, suggesting that LLM failures in planning tasks are often due to representational shortcomings rather than reasoning capacity. The study emphasizes that explicit problem modeling is essential for creating robust, interpretable AI agents. All prompts, evaluation methods, and datasets are made available to ensure reproducibility of the findings. <div>
arXiv:2512.14474v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2512.14491</link>
<guid>https://arxiv.org/abs/2512.14491</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer, multi-modal, sparse attention, efficiency, Alzheimer's Disease<br><br>Summary:<br><br>1. This paper introduces SMMT, a sparse multi-modal transformer architecture aimed at improving the computational efficiency and robustness of multi-modal intelligent systems.<br><br>2. SMMT builds on a cascaded multi-modal transformer framework and incorporates cluster-based sparse attention to reduce computational complexity from quadratic to near linear.<br><br>3. The architecture also uses modality-wise masking to enhance robustness in scenarios where input data from some modalities might be incomplete or missing.<br><br>4. The system is evaluated on a practical multi-modal problem: Alzheimer’s Disease classification using the ADNI dataset, demonstrating its real-world applicability.<br><br>5. Experimental results show that SMMT achieves competitive predictive accuracy compared to dense attention models while significantly reducing training time, memory usage, and energy consumption, highlighting its suitability for resource-constrained environments and scalable intelligent system design. <div>
arXiv:2512.14491v1 Announce Type: new 
Abstract: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence</title>
<link>https://arxiv.org/abs/2512.14527</link>
<guid>https://arxiv.org/abs/2512.14527</guid>
<content:encoded><![CDATA[
<div> Keywords: GreedyLR, learning rate scheduler, NLP, convergence, optimizer  

<br><br>Summary:  
The paper introduces GreedyLR, a new adaptive learning rate scheduler that adjusts the learning rate dynamically based on the current training loss. Unlike traditional schedulers such as Cosine or exponential decay, GreedyLR optimizes learning rate changes to improve training efficiency. The authors validate GreedyLR through extensive experiments on diverse tasks spanning natural language processing (NLP), computer vision (CV), and large language models (LLM) with up to 7 billion parameters, covering both fine-tuning and pre-training scenarios. Results demonstrate that GreedyLR surpasses state-of-the-art schedulers in accuracy, speed, and convergence behavior. A theoretical analysis accompanies the empirical findings, including a proof of convergence, and derivation of an optimal scaling factor F that maximizes the convergence rate, ensuring an optimal learning trajectory. Additional experiments highlight the scheduler's robustness in noisy and realistic loss landscapes, emphasizing its stability under practical training conditions. The paper also underscores the implementation simplicity and computational efficiency of GreedyLR, making it a potential default choice for training various neural network models. Overall, GreedyLR presents an effective and theoretically sound advancement in learning rate scheduling for modern deep learning applications. <div>
arXiv:2512.14527v1 Announce Type: new 
Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Reasoning Model</title>
<link>https://arxiv.org/abs/2512.14693</link>
<guid>https://arxiv.org/abs/2512.14693</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.14693v1, Universal Transformers, ARC-AGI, reasoning performance, Universal Reasoning Model<br><br>Summary:  
This paper investigates the sources of performance improvements in Universal Transformers (UTs) on complex reasoning tasks such as ARC-AGI and Sudoku. The authors conduct a systematic analysis of different UT variants and find that the primary factors driving improved performance on ARC-AGI are the recurrent inductive bias and strong nonlinear components inherent in the Transformer architecture. Contrary to prior assumptions, the gains are not mainly due to more elaborate architectural innovations. Building on this insight, the authors propose the Universal Reasoning Model (URM), which integrates short convolutions and truncated backpropagation to further enhance reasoning capabilities. The URM demonstrates significant improvements over previous models, achieving state-of-the-art results with a 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2 benchmarks. The paper highlights the effectiveness of combining recurrent bias and targeted architectural additions to boost complex reasoning performance. The authors have also made their code publicly available for reproducibility and further research at the provided GitHub repository. This work contributes to better understanding and advancing the design principles behind transformer-based reasoning models. <div>
arXiv:2512.14693v1 Announce Type: new 
Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training</title>
<link>https://arxiv.org/abs/2511.10333</link>
<guid>https://arxiv.org/abs/2511.10333</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, gradient compression, entropy-driven, communication efficiency, dynamic adjustment<br><br>Summary:<br><br>This paper addresses the challenges in training large language models (LLMs) related to computational resources and high communication overhead during distributed training. Existing static gradient compression techniques, while improving communication efficiency, fail to adapt to the changing nature of gradients throughout training, causing performance loss. To solve this, the authors propose EDGC, an entropy-driven dynamic gradient compression framework that adjusts compression rates based on the evolving gradient entropy, balancing compression efficiency and error. EDGC comprises three main components: (1) a down-sampling method to estimate gradient entropy efficiently with low computational cost, (2) a theoretical model that relates compression rate to gradient entropy to guide compression decisions, and (3) a window-based adjustment mechanism that dynamically tunes compression rates across pipeline stages to optimize communication without sacrificing model accuracy. The framework was evaluated on two hardware setups—a 32-NVIDIA-V100 cluster training GPT2-2.5B and a 64-NVIDIA-H100 cluster training GPT2-12.1B. Results demonstrate that EDGC reduces communication latency by up to 46.45% and overall training time by up to 16.13%, all while preserving the accuracy of the trained LLMs. This shows EDGC’s potential to significantly accelerate LLM training through adaptive gradient compression. <div>
arXiv:2511.10333v1 Announce Type: cross 
Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset</title>
<link>https://arxiv.org/abs/2511.19317</link>
<guid>https://arxiv.org/abs/2511.19317</guid>
<content:encoded><![CDATA[
<div> Bangla summarization, dataset, abstractive, multi-domain, deep learning<br><br>Summary: This study presents a newly developed Bangla abstractive summarization dataset aimed at generating concise summaries from a variety of Bangla texts sourced from multiple domains. Unlike previous datasets focusing mainly on news articles with uniform writing styles, this dataset encompasses over 54,000 articles collected from diverse sources such as blogs (Cinegolpo) and newspapers (Samakal and The Business Standard), capturing a broad spectrum of writing styles. This diversity enhances the dataset’s adaptability and practical relevance in real-world scenarios, where Bangla content is widely produced across different platforms like social media and blogs. The research underscores the urgent need for effective summarization tools to manage the overwhelming volume of Bangla content available digitally. To validate the dataset’s utility, several deep learning and transfer learning models—including LSTM, BanglaT5-small, and MTS-small—were trained and evaluated, establishing strong baseline results. These outcomes demonstrate the dataset’s potential as a benchmark for future research in Bangla natural language processing. Ultimately, this dataset lays a robust foundation for developing more effective summarization systems and contributes significantly to expanding NLP resources for low-resource languages like Bangla. <div>
arXiv:2511.19317v1 Announce Type: cross 
Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Writing in Symbiosis: Mapping Human Creative Agency in the AI Era</title>
<link>https://arxiv.org/abs/2512.13697</link>
<guid>https://arxiv.org/abs/2512.13697</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, human-AI coevolution, creative writing, stylistic differentiation, authorship adaptation  

<br><br>Summary:  
This paper investigates the evolving relationship between humans and AI in the field of creative writing, prompted by the rise of Large Language Models (LLMs). Using a large-scale corpus that spans both the pre- and post-LLM era, the study identifies patterns indicative of what the authors term "Dual-Track Evolution." This evolution involves a thematic convergence around AI-related topics, alongside a structured stylistic differentiation among writers. The analysis reveals three distinct adaptation patterns among authors: those who increasingly adopt AI-like stylistic traits, those whose styles diverge further from AI, and those who maintain consistent style while engaging with themes influenced by AI. This nuanced view challenges the simplistic idea that LLMs cause stylistic homogenization in human writing. The paper introduces the Creative Archetype Map, a conceptual framework that captures these diverse adaptation patterns. This map contributes to ongoing conversations about the nature of human-AI collaboration in creative processes, the challenges of detecting AI-influenced authorship, and the importance of preserving stylistic and thematic diversity in the era of AI-augmented creativity. <div>
arXiv:2512.13697v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a "Dual-Track Evolution": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport</title>
<link>https://arxiv.org/abs/2512.13702</link>
<guid>https://arxiv.org/abs/2512.13702</guid>
<content:encoded><![CDATA[
<div> AI Product Passport, healthcare AI, lifecycle documentation, transparency, regulatory compliance<br><br>Summary:  
The article presents the development of the AI Product Passport, a standards-based framework aimed at enhancing transparency, traceability, and compliance in healthcare AI systems throughout their lifecycle. Developed within the AI4HF project targeting heart failure AI tools, the framework integrates regulatory insights from the EU AI Act and FDA guidelines alongside existing standards. It employs a relational data model encompassing critical AI lifecycle phases: study definition, dataset preparation, model generation and evaluation, deployment and monitoring, and passport generation. To ensure practical applicability, MLOps and ModelOps principles were incorporated. The design process included co-creation efforts with consortium members and a Lisbon workshop involving 21 stakeholders, whose feedback guided iterative improvements. The implementation resulted in an open-source, web-based platform using Python libraries capable of automated provenance tracking. The platform supports role-based access and produces both machine- and human-readable reports tailored for diverse stakeholders. Alignment with FUTURE-AI principles ensures fairness, universality, traceability, usability, robustness, and explainability of AI tools. The exported passports provide detailed metadata including model purpose, data provenance, performance metrics, and deployment context. The backend and frontend codebases are hosted on GitHub for broad accessibility. Future developments plan to incorporate FAIR data principles and FHIR standards for enhanced interoperability, promoting responsible and trustworthy AI deployment in healthcare. <div>
arXiv:2512.13702v1 Announce Type: cross 
Abstract: Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models</title>
<link>https://arxiv.org/abs/2512.13703</link>
<guid>https://arxiv.org/abs/2512.13703</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Jailbreaking, Semantic Isomorphism, Harmful Content, Prompt Engineering<br><br>Summary: This paper addresses the vulnerabilities of Large Language Models (LLMs) to generating harmful content despite their impressive capabilities. The authors identify that many harmful queries share similar underlying principles with legitimate ones, a fact previously overlooked in jailbreak research. Building on this insight, they propose the Safe2Harm Semantic Isomorphism Attack, a novel four-stage jailbreak methodology. The process begins by rewriting a harmful question into a semantically safe but principle-consistent version. Next, the thematic mapping relationship between the harmful and safe queries is extracted. Then, the LLM is prompted to generate a detailed response to the safe question. Finally, this safe response is reversed transformed based on the thematic mapping to produce harmful output. Experimental results on seven mainstream LLMs and three benchmark datasets demonstrate that Safe2Harm achieves strong jailbreaking performance, outperforming existing methods. Additionally, the authors contribute a new challenging evaluation dataset with 358 harmful content samples to assess the effectiveness of current harmful content detection systems. This dataset enables better input-output filtering defenses, helping to mitigate LLM-related risks. The work highlights a sophisticated attack method and offers tools to improve LLM security and content moderation. <div>
arXiv:2512.13703v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling and Transferability of Annealing Strategies in Large Language Model Training</title>
<link>https://arxiv.org/abs/2512.13705</link>
<guid>https://arxiv.org/abs/2512.13705</guid>
<content:encoded><![CDATA[
<div> Learning rate scheduling, large language models, annealing strategies, Warmup-Steady-Decay, Mixture-of-Experts<br><br>Summary:  
The paper addresses the importance of learning rate scheduling in training large language models and the challenge of identifying optimal annealing strategies across various model configurations. It investigates how annealing dynamics can be transferred between different model sizes and configurations, focusing on the Warmup-Steady-Decay (WSD) scheduler. An enhanced predictive framework is proposed that integrates critical factors such as training steps, maximum learning rate, and annealing behavior, which facilitates more efficient optimization of learning rate schedules. This approach reduces the need for exhaustive hyperparameter searches by providing practical guidance for selecting optimal annealing strategies. A key finding is that smaller models can reliably proxy larger models when tuning training dynamics, increasing efficiency and reducing computational costs. The study’s conclusions are supported by extensive experiments conducted on both Dense and Mixture-of-Experts (MoE) architectures. These experiments reveal that optimal annealing ratios exhibit consistent patterns transferable across different training setups. This transferability enables more generalized and scalable training strategies for diverse large language model architectures. <div>
arXiv:2512.13705v1 Announce Type: cross 
Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints</title>
<link>https://arxiv.org/abs/2512.13717</link>
<guid>https://arxiv.org/abs/2512.13717</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, few-shot learning, EEG, seizure detection, personalized model<br><br>Summary:<br><br>This article addresses the challenge of developing AI models for EEG-based seizure detection in scenarios where data are scarce, distributed across multiple institutions, and privacy-restricted. The authors propose a two-stage federated few-shot learning (FFSL) framework designed to create personalized seizure detection models without centralizing patient EEG data. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned via federated learning across simulated hospital sites with non-IID EEG data, enabling the learning of shared EEG representations while preserving privacy. In Stage 2, a federated few-shot personalization approach adapts the classifier individually for each patient using only five labeled EEG segments, achieving patient-specific tuning while leveraging cross-site knowledge. The framework is trained and evaluated on the TUH Event Corpus, which contains six types of EEG events. Federated fine-tuning in Stage 1 achieved balanced accuracy of 0.43 compared to 0.52 for centralized training, demonstrating some performance trade-off due to decentralization. However, Stage 2 personalization significantly improved results, reaching an average balanced accuracy of 0.77, Cohen’s kappa of 0.62, and weighted F1 of 0.73 across heterogeneous sites. Overall, the FFSL approach shows promise for practical seizure detection under realistic clinical constraints on data availability and privacy. <div>
arXiv:2512.13717v1 Announce Type: cross 
Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs</title>
<link>https://arxiv.org/abs/2512.13723</link>
<guid>https://arxiv.org/abs/2512.13723</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, ethical bias, moral alignment, China, USA  

<br><br>Summary:  
1. This study investigates the ethical and cultural alignment of large language models (LLMs) produced in China and the USA by comparing their responses to moral and value-based surveys with those of people from the respective countries.  
2. The researchers used the Moral Foundations Questionnaire 2.0 and the World Values Survey to elicit responses from ten Chinese LLMs and ten American LLMs.  
3. These responses were then compared to survey data collected from thousands of Chinese and American individuals.  
4. Results show all models, regardless of origin, tend to align more closely with American cultural and moral values than with Chinese values.  
5. Attempts to reduce this American-aligned bias, such as prompting models in Chinese or simulating a Chinese persona, only marginally improved alignment with Chinese respondents.  
6. The study highlights a significant ethical bias in current LLMs toward Western, particularly American, values, raising concerns about the normative influence these models might wield in geopolitics and information dissemination as they increasingly mediate knowledge and decision-making globally. <div>
arXiv:2512.13723v1 Announce Type: cross 
Abstract: As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph AI generates neurological hypotheses validated in molecular, organoid, and clinical systems</title>
<link>https://arxiv.org/abs/2512.13724</link>
<guid>https://arxiv.org/abs/2512.13724</guid>
<content:encoded><![CDATA[
<div> Neurological diseases, PROTON, graph transformer, Parkinson's disease, Alzheimer's disease<br><br>Summary:<br><br>1. PROTON is a heterogeneous graph transformer designed to generate testable hypotheses across molecular, organoid, and clinical systems to address neurological diseases, a leading global cause of disability with few disease-modifying treatments.<br>2. Applied to Parkinson's disease (PD), PROTON linked genetic risk loci to genes vital for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, notably ranking the insecticide endosulfan in the top 1.29% of toxic predictions.<br>3. PROTON successfully replicated six genome-wide α-synuclein experimental results through in silico screening, including data from split-ubiquitin yeast two-hybrid, ascorbate peroxidase proximity labeling assays, and a targeted exome sequencing study on 496 synucleinopathy patients, all showing significant normalized enrichment scores with extremely low false discovery rates.<br>4. In bipolar disorder (BD), PROTON predicted the drug calcitriol could reverse proteomic changes found in cortical organoids derived from BD patients.<br>5. For Alzheimer's disease (AD), PROTON's predictions were validated in health records from over 610,000 patients, confirming five predicted drugs were associated with reduced seven-year dementia risk, with hazard ratios as low as 0.63 and highly significant p-values.<br>6. Overall, PROTON demonstrates a novel AI-driven pathway for discovering and validating potential therapies across various neurological diseases at multiple biological levels. <div>
arXiv:2512.13724v1 Announce Type: cross 
Abstract: Neurological diseases are the leading global cause of disability, yet most lack disease-modifying treatments. We present PROTON, a heterogeneous graph transformer that generates testable hypotheses across molecular, organoid, and clinical systems. To evaluate PROTON, we apply it to Parkinson's disease (PD), bipolar disorder (BD), and Alzheimer's disease (AD). In PD, PROTON linked genetic risk loci to genes essential for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, including the insecticide endosulfan, which ranked within the top 1.29% of predictions. In silico screens performed by PROTON reproduced six genome-wide $\alpha$-synuclein experiments, including a split-ubiquitin yeast two-hybrid system (normalized enrichment score [NES] = 2.30, FDR-adjusted $p < 1 \times 10^{-4}$), an ascorbate peroxidase proximity labeling assay (NES = 2.16, FDR $< 1 \times 10^{-4}$), and a high-depth targeted exome sequencing study in 496 synucleinopathy patients (NES = 2.13, FDR $< 1 \times 10^{-4}$). In BD, PROTON predicted calcitriol as a candidate drug that reversed proteomic alterations observed in cortical organoids derived from BD patients. In AD, we evaluated PROTON predictions in health records from $n = 610,524$ patients at Mass General Brigham, confirming that five PROTON-predicted drugs were associated with reduced seven-year dementia risk (minimum hazard ratio = 0.63, 95% CI: 0.53-0.75, $p < 1 \times 10^{-7}$). PROTON generated neurological hypotheses that were evaluated across molecular, organoid, and clinical systems, defining a path for AI-driven discovery in neurological disease.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce</title>
<link>https://arxiv.org/abs/2512.13726</link>
<guid>https://arxiv.org/abs/2512.13726</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, time-constrained recommendation, slate optimization, user engagement, Alibaba dataset<br><br>Summary:<br><br>1. This paper addresses recommendation systems under finite user time budgets, emphasizing the critical challenge of balancing item relevance with evaluation cost. Unlike traditional recommendation tasks, users have limited time to assess recommendations, especially in mobile and scrolling interfaces where each scroll shows a slate of items. 2. Users expend evaluation time assessing item features before clicking, meaning highly relevant items with greater evaluation costs may not fit within the user's limited time, potentially reducing engagement. 3. The authors propose a unified framework modeling time-constrained slate recommendation as a Markov Decision Process (MDP) incorporating budget-aware utilities, capturing the trade-off between relevance and evaluation cost. 4. To study recommendation policies, they build a simulation framework leveraging Alibaba’s Personalized Re-ranking dataset tailored for e-commerce slate optimization, enabling experimentation with reinforcement learning algorithms. 5. Empirical results demonstrate that both on-policy and off-policy reinforcement learning control methods outperform traditional contextual bandit algorithms, particularly under tight time budgets, improving user engagement by better aligning recommendations with users’ time constraints and preferences. <div>
arXiv:2512.13726v1 Announce Type: cross 
Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CurvaDion: Curvature-Adaptive Distributed Orthonormalization</title>
<link>https://arxiv.org/abs/2512.13728</link>
<guid>https://arxiv.org/abs/2512.13728</guid>
<content:encoded><![CDATA[
<div> Gradient Synchronization, Distributed Training, Curvature Detection, Momentum Dynamics, Communication Reduction<br><br>Summary:<br><br>1. Large-scale language models with trillions of parameters require distributed training across many GPUs, but frequent gradient synchronization over networks creates a communication bottleneck.<br>2. Existing solutions like Dion reduce communication frequency by using low-rank gradient updates but still synchronize every training step regardless of the underlying optimization dynamics.<br>3. The authors observe that synchronization needs vary during training: flat regions yield similar gradients across workers, making synchronization often unnecessary, while high-curvature regions require tight coordination to avoid model divergence.<br>4. They propose CurvaDion, a method that uses a novel metric called Relative Maximum Momentum Change (RMMC) to detect high-curvature phases of training, triggering synchronization only when needed.<br>5. RMMC utilizes momentum values already computed during optimization, serving as a low-overhead, computationally efficient proxy for curvature with only O(d) extra operations per layer.<br>6. Theoretical connections between RMMC and the loss curvature are established, validating its effectiveness.<br>7. Experiments show that CurvaDion can reduce communication overhead by 99% while maintaining convergence performance comparable to baseline methods across models ranging from 160 million to 1.3 billion parameters. <div>
arXiv:2512.13728v1 Announce Type: cross 
Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution</title>
<link>https://arxiv.org/abs/2512.13729</link>
<guid>https://arxiv.org/abs/2512.13729</guid>
<content:encoded><![CDATA[
<div> Keywords: wind super-resolution, diffusion models, classifier-free guidance, composite classifier-free guidance, WindDM<br><br>Summary:<br><br>1. The paper addresses the challenge of obtaining high-resolution, accurate wind data essential for weather modeling tasks such as forecasting and turbine placement optimization, noting that traditional methods are either costly or inaccurate.<br>2. It highlights the potential of deep learning methods, particularly diffusion models, to overcome this cost-accuracy trade-off by adapting techniques from natural image super-resolution.<br>3. The authors point out the distinct nature of wind data compared to natural images, emphasizing that wind super-resolution requires many more conditioning channels (10+) than typical RGB images (3 channels).<br>4. To better handle multiple conditioning inputs in diffusion models, they propose a novel generalization of classifier-free guidance called composite classifier-free guidance (CCFG), which can be integrated into any pretrained diffusion model that uses standard CFG dropout.<br>5. Experimental results demonstrate that CCFG produces higher fidelity outputs than traditional CFG on wind super-resolution tasks.<br>6. They introduce WindDM, a diffusion model employing CCFG, trained for industrial-scale reconstruction of wind dynamics.<br>7. WindDM achieves state-of-the-art reconstruction quality among deep learning approaches and is up to 1000 times more cost-effective than classical methods, making it highly practical for industrial applications. <div>
arXiv:2512.13729v1 Announce Type: cross 
Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Modular Integration of "AI + Architecture" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University</title>
<link>https://arxiv.org/abs/2512.13730</link>
<guid>https://arxiv.org/abs/2512.13730</guid>
<content:encoded><![CDATA[
<div> AI integration, architectural education, ethics discussions, deep learning models, design studio<br><br>Summary: This study explores the integration of artificial intelligence (AI) into architectural education through an experimental teaching approach at Zhejiang University involving grade three undergraduate design students during the 2024-25 academic year. It employs a dual-module framework combining a 20-hour AI technical training course with integrated ethics discussions, introducing students to deep learning models, large language models (LLMs), AI-generated content (AIGC), Low-Rank Adaptation (LoRA), and ComfyUI, while preserving the original curriculum structure. Dedicated technical instructors provided support throughout the process, ensuring effective delivery and student engagement. The findings highlight the effectiveness of phased guidance that gradually builds technical skills alongside a balanced focus on ethical considerations of AI use in design. Institutional support played a key role in the successful implementation of the model. The course enhanced students’ digital competencies as well as their strategic understanding of AI’s role in architectural practice. Additionally, the inclusion of ethics discussions fostered critical thinking about the societal and professional implications of AI technology. This integrated approach offers a replicable model for combining both technical and critical learning components within design education, preparing students to responsibly leverage AI in their future architectural careers. <div>
arXiv:2512.13730v1 Announce Type: cross 
Abstract: This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline</title>
<link>https://arxiv.org/abs/2512.13731</link>
<guid>https://arxiv.org/abs/2512.13731</guid>
<content:encoded><![CDATA[

arXiv:2512.13731v1 Announce Type: cross 
Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion</title>
<link>https://arxiv.org/abs/2512.13732</link>
<guid>https://arxiv.org/abs/2512.13732</guid>
<content:encoded><![CDATA[

arXiv:2512.13732v1 Announce Type: cross 
Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Compression of Language Models via Differentiable Rank Selection</title>
<link>https://arxiv.org/abs/2512.13733</link>
<guid>https://arxiv.org/abs/2512.13733</guid>
<content:encoded><![CDATA[

arXiv:2512.13733v1 Announce Type: cross 
Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation</title>
<link>https://arxiv.org/abs/2512.13734</link>
<guid>https://arxiv.org/abs/2512.13734</guid>
<content:encoded><![CDATA[

arXiv:2512.13734v1 Announce Type: cross 
Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series</title>
<link>https://arxiv.org/abs/2512.13735</link>
<guid>https://arxiv.org/abs/2512.13735</guid>
<content:encoded><![CDATA[

arXiv:2512.13735v1 Announce Type: cross 
Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection</title>
<link>https://arxiv.org/abs/2512.13736</link>
<guid>https://arxiv.org/abs/2512.13736</guid>
<content:encoded><![CDATA[

arXiv:2512.13736v1 Announce Type: cross 
Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instilling Organisational Values in Firefighters through Simulation-Based Training</title>
<link>https://arxiv.org/abs/2512.13737</link>
<guid>https://arxiv.org/abs/2512.13737</guid>
<content:encoded><![CDATA[

arXiv:2512.13737v1 Announce Type: cross 
Abstract: In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title>
<link>https://arxiv.org/abs/2512.13739</link>
<guid>https://arxiv.org/abs/2512.13739</guid>
<content:encoded><![CDATA[

arXiv:2512.13739v1 Announce Type: cross 
Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models</title>
<link>https://arxiv.org/abs/2512.13741</link>
<guid>https://arxiv.org/abs/2512.13741</guid>
<content:encoded><![CDATA[

arXiv:2512.13741v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title>
<link>https://arxiv.org/abs/2512.13742</link>
<guid>https://arxiv.org/abs/2512.13742</guid>
<content:encoded><![CDATA[

arXiv:2512.13742v1 Announce Type: cross 
Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes</title>
<link>https://arxiv.org/abs/2512.13744</link>
<guid>https://arxiv.org/abs/2512.13744</guid>
<content:encoded><![CDATA[

arXiv:2512.13744v1 Announce Type: cross 
Abstract: Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatio-Temporal Hybrid Quantum-Classical Graph Convolutional Neural Network Approach for Urban Taxi Destination Prediction</title>
<link>https://arxiv.org/abs/2512.13745</link>
<guid>https://arxiv.org/abs/2512.13745</guid>
<content:encoded><![CDATA[

arXiv:2512.13745v1 Announce Type: cross 
Abstract: We propose a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) algorithm by combining the strengths of quantum computing and classical deep learning to predict the taxi destination within urban road networks. Our algorithm consists of two branches: spatial processing and time evolution. Regarding the spatial processing, the classical module encodes the local topological features of the road network based on the GCN method, and the quantum module is designed to map graph features onto parameterized quantum circuits through a differentiable pooling layer. The time evolution is solved by integrating multi-source contextual information and capturing dynamic trip dependencies on the classical TCN theory. Finally, our experimental results demonstrate that the proposed algorithm outperforms the current methods in terms of prediction accuracy and stability, validating the unique advantages of the quantum-enhanced mechanism in capturing high-dimensional spatial dependencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making</title>
<link>https://arxiv.org/abs/2512.13747</link>
<guid>https://arxiv.org/abs/2512.13747</guid>
<content:encoded><![CDATA[

arXiv:2512.13747v1 Announce Type: cross 
Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.13749</link>
<guid>https://arxiv.org/abs/2512.13749</guid>
<content:encoded><![CDATA[

arXiv:2512.13749v1 Announce Type: cross 
Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs</title>
<link>https://arxiv.org/abs/2512.13750</link>
<guid>https://arxiv.org/abs/2512.13750</guid>
<content:encoded><![CDATA[

arXiv:2512.13750v1 Announce Type: cross 
Abstract: Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDUS: Memory-Infused Depth Up-Scaling</title>
<link>https://arxiv.org/abs/2512.13751</link>
<guid>https://arxiv.org/abs/2512.13751</guid>
<content:encoded><![CDATA[

arXiv:2512.13751v1 Announce Type: cross 
Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning</title>
<link>https://arxiv.org/abs/2512.13752</link>
<guid>https://arxiv.org/abs/2512.13752</guid>
<content:encoded><![CDATA[

arXiv:2512.13752v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention</title>
<link>https://arxiv.org/abs/2512.13758</link>
<guid>https://arxiv.org/abs/2512.13758</guid>
<content:encoded><![CDATA[

arXiv:2512.13758v1 Announce Type: cross 
Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Deep Learning Surrogate for the Forward Problem in Electrocardiology: A Scalable Alternative to Physics-Based Models</title>
<link>https://arxiv.org/abs/2512.13765</link>
<guid>https://arxiv.org/abs/2512.13765</guid>
<content:encoded><![CDATA[

arXiv:2512.13765v1 Announce Type: cross 
Abstract: The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean $R^2 = 0.99 \pm 0.01$). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance</title>
<link>https://arxiv.org/abs/2512.13768</link>
<guid>https://arxiv.org/abs/2512.13768</guid>
<content:encoded><![CDATA[

arXiv:2512.13768v1 Announce Type: cross 
Abstract: Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.
  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13806</link>
<guid>https://arxiv.org/abs/2512.13806</guid>
<content:encoded><![CDATA[

arXiv:2512.13806v1 Announce Type: cross 
Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VajraV1 -- The most accurate Real Time Object Detector of the YOLO family</title>
<link>https://arxiv.org/abs/2512.13834</link>
<guid>https://arxiv.org/abs/2512.13834</guid>
<content:encoded><![CDATA[

arXiv:2512.13834v1 Announce Type: cross 
Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging</title>
<link>https://arxiv.org/abs/2512.13855</link>
<guid>https://arxiv.org/abs/2512.13855</guid>
<content:encoded><![CDATA[

arXiv:2512.13855v1 Announce Type: cross 
Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors</title>
<link>https://arxiv.org/abs/2512.13860</link>
<guid>https://arxiv.org/abs/2512.13860</guid>
<content:encoded><![CDATA[

arXiv:2512.13860v1 Announce Type: cross 
Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization</title>
<link>https://arxiv.org/abs/2512.13880</link>
<guid>https://arxiv.org/abs/2512.13880</guid>
<content:encoded><![CDATA[

arXiv:2512.13880v1 Announce Type: cross 
Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction</title>
<link>https://arxiv.org/abs/2512.13886</link>
<guid>https://arxiv.org/abs/2512.13886</guid>
<content:encoded><![CDATA[

arXiv:2512.13886v1 Announce Type: cross 
Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title>
<link>https://arxiv.org/abs/2512.13892</link>
<guid>https://arxiv.org/abs/2512.13892</guid>
<content:encoded><![CDATA[

arXiv:2512.13892v1 Announce Type: cross 
Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing</title>
<link>https://arxiv.org/abs/2512.13904</link>
<guid>https://arxiv.org/abs/2512.13904</guid>
<content:encoded><![CDATA[

arXiv:2512.13904v1 Announce Type: cross 
Abstract: The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($\tau < 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing High-Risk Systems: An EU AI Act Verification Framework</title>
<link>https://arxiv.org/abs/2512.13907</link>
<guid>https://arxiv.org/abs/2512.13907</guid>
<content:encoded><![CDATA[

arXiv:2512.13907v1 Announce Type: cross 
Abstract: A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America</title>
<link>https://arxiv.org/abs/2512.13910</link>
<guid>https://arxiv.org/abs/2512.13910</guid>
<content:encoded><![CDATA[

arXiv:2512.13910v1 Announce Type: cross 
Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent matter consisting of active particles</title>
<link>https://arxiv.org/abs/2512.13912</link>
<guid>https://arxiv.org/abs/2512.13912</guid>
<content:encoded><![CDATA[

arXiv:2512.13912v1 Announce Type: cross 
Abstract: In this book chapter, we review how systems of simple motile agents can be used as a pathway to intelligent systems. It is a well known result from nature that large groups of entities following simple rules, such as swarms of animals, can give rise to much more complex collective behavior in a display of emergence. This begs the question whether we can emulate this behavior in synthetic matter and drive it to a point where the collective behavior reaches the complexity level of intelligent systems. Here, we will use a formalized notion of "intelligent matter" and compare it to recent results in the field of active matter. First, we will explore the approach of emergent computing in which specialized active matter systems are designed to directly solve a given task through emergent behavior. This we will then contrast with the approach of physical reservoir computing powered by the dynamics of active particle systems. In this context, we will also describe a novel reservoir computing scheme for active particles driven ultrasonically or via light refraction.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming</title>
<link>https://arxiv.org/abs/2512.13914</link>
<guid>https://arxiv.org/abs/2512.13914</guid>
<content:encoded><![CDATA[

arXiv:2512.13914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</title>
<link>https://arxiv.org/abs/2512.13930</link>
<guid>https://arxiv.org/abs/2512.13930</guid>
<content:encoded><![CDATA[

arXiv:2512.13930v1 Announce Type: cross 
Abstract: Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informing Acquisition Functions via Foundation Models for Molecular Discovery</title>
<link>https://arxiv.org/abs/2512.13935</link>
<guid>https://arxiv.org/abs/2512.13935</guid>
<content:encoded><![CDATA[

arXiv:2512.13935v1 Announce Type: cross 
Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling</title>
<link>https://arxiv.org/abs/2512.13956</link>
<guid>https://arxiv.org/abs/2512.13956</guid>
<content:encoded><![CDATA[

arXiv:2512.13956v1 Announce Type: cross 
Abstract: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition</title>
<link>https://arxiv.org/abs/2512.13998</link>
<guid>https://arxiv.org/abs/2512.13998</guid>
<content:encoded><![CDATA[

arXiv:2512.13998v1 Announce Type: cross 
Abstract: Music Emotion Recogniser (MER) research faces challenges due to limited high-quality annotated datasets and difficulties in addressing cross-track feature drift. This work presents two primary contributions to address these issues. Memo2496, a large-scale dataset, offers 2496 instrumental music tracks with continuous valence arousal labels, annotated by 30 certified music specialists. Annotation quality is ensured through calibration with extreme emotion exemplars and a consistency threshold of 0.25, measured by Euclidean distance in the valence arousal space. Furthermore, the Dual-view Adaptive Music Emotion Recogniser (DAMER) is introduced. DAMER integrates three synergistic modules: Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms; Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence; and Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift. Extensive experiments on the Memo2496, 1000songs, and PMEmo datasets demonstrate DAMER's state-of-the-art performance, improving arousal dimension accuracy by 3.43%, 2.25%, and 0.17%, respectively. Ablation studies and visualisation analyses validate each module's contribution. Both the dataset and source code are publicly available.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025</title>
<link>https://arxiv.org/abs/2512.14012</link>
<guid>https://arxiv.org/abs/2512.14012</guid>
<content:encoded><![CDATA[

arXiv:2512.14012v1 Announce Type: cross 
Abstract: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.14017</link>
<guid>https://arxiv.org/abs/2512.14017</guid>
<content:encoded><![CDATA[

arXiv:2512.14017v1 Announce Type: cross 
Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerfCoder: Large Language Models for Interpretable Code Performance Optimization</title>
<link>https://arxiv.org/abs/2512.14018</link>
<guid>https://arxiv.org/abs/2512.14018</guid>
<content:encoded><![CDATA[

arXiv:2512.14018v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</title>
<link>https://arxiv.org/abs/2512.14031</link>
<guid>https://arxiv.org/abs/2512.14031</guid>
<content:encoded><![CDATA[

arXiv:2512.14031v1 Announce Type: cross 
Abstract: This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM</title>
<link>https://arxiv.org/abs/2512.14032</link>
<guid>https://arxiv.org/abs/2512.14032</guid>
<content:encoded><![CDATA[

arXiv:2512.14032v1 Announce Type: cross 
Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.
  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.14044</link>
<guid>https://arxiv.org/abs/2512.14044</guid>
<content:encoded><![CDATA[

arXiv:2512.14044v1 Announce Type: cross 
Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</title>
<link>https://arxiv.org/abs/2512.14056</link>
<guid>https://arxiv.org/abs/2512.14056</guid>
<content:encoded><![CDATA[

arXiv:2512.14056v1 Announce Type: cross 
Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</title>
<link>https://arxiv.org/abs/2512.14058</link>
<guid>https://arxiv.org/abs/2512.14058</guid>
<content:encoded><![CDATA[

arXiv:2512.14058v1 Announce Type: cross 
Abstract: Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed</title>
<link>https://arxiv.org/abs/2512.14067</link>
<guid>https://arxiv.org/abs/2512.14067</guid>
<content:encoded><![CDATA[

arXiv:2512.14067v1 Announce Type: cross 
Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2512.14068</link>
<guid>https://arxiv.org/abs/2512.14068</guid>
<content:encoded><![CDATA[

arXiv:2512.14068v1 Announce Type: cross 
Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations</title>
<link>https://arxiv.org/abs/2512.14080</link>
<guid>https://arxiv.org/abs/2512.14080</guid>
<content:encoded><![CDATA[

arXiv:2512.14080v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arithmetic-Intensity-Aware Quantization</title>
<link>https://arxiv.org/abs/2512.14090</link>
<guid>https://arxiv.org/abs/2512.14090</guid>
<content:encoded><![CDATA[

arXiv:2512.14090v1 Announce Type: cross 
Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes</title>
<link>https://arxiv.org/abs/2512.14092</link>
<guid>https://arxiv.org/abs/2512.14092</guid>
<content:encoded><![CDATA[

arXiv:2512.14092v1 Announce Type: cross 
Abstract: Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.
  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.
  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.
  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</title>
<link>https://arxiv.org/abs/2512.14102</link>
<guid>https://arxiv.org/abs/2512.14102</guid>
<content:encoded><![CDATA[

arXiv:2512.14102v1 Announce Type: cross 
Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</title>
<link>https://arxiv.org/abs/2512.14121</link>
<guid>https://arxiv.org/abs/2512.14121</guid>
<content:encoded><![CDATA[

arXiv:2512.14121v1 Announce Type: cross 
Abstract: Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis</title>
<link>https://arxiv.org/abs/2512.14130</link>
<guid>https://arxiv.org/abs/2512.14130</guid>
<content:encoded><![CDATA[

arXiv:2512.14130v1 Announce Type: cross 
Abstract: We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation</title>
<link>https://arxiv.org/abs/2512.14138</link>
<guid>https://arxiv.org/abs/2512.14138</guid>
<content:encoded><![CDATA[

arXiv:2512.14138v1 Announce Type: cross 
Abstract: Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models</title>
<link>https://arxiv.org/abs/2512.14141</link>
<guid>https://arxiv.org/abs/2512.14141</guid>
<content:encoded><![CDATA[

arXiv:2512.14141v1 Announce Type: cross 
Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario</title>
<link>https://arxiv.org/abs/2512.14150</link>
<guid>https://arxiv.org/abs/2512.14150</guid>
<content:encoded><![CDATA[

arXiv:2512.14150v1 Announce Type: cross 
Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol</title>
<link>https://arxiv.org/abs/2512.14166</link>
<guid>https://arxiv.org/abs/2512.14166</guid>
<content:encoded><![CDATA[

arXiv:2512.14166v1 Announce Type: cross 
Abstract: The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs</title>
<link>https://arxiv.org/abs/2512.14179</link>
<guid>https://arxiv.org/abs/2512.14179</guid>
<content:encoded><![CDATA[

arXiv:2512.14179v1 Announce Type: cross 
Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization</title>
<link>https://arxiv.org/abs/2512.14181</link>
<guid>https://arxiv.org/abs/2512.14181</guid>
<content:encoded><![CDATA[

arXiv:2512.14181v1 Announce Type: cross 
Abstract: Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach</title>
<link>https://arxiv.org/abs/2512.14185</link>
<guid>https://arxiv.org/abs/2512.14185</guid>
<content:encoded><![CDATA[

arXiv:2512.14185v1 Announce Type: cross 
Abstract: The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Improving Hyperbolic Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14202</link>
<guid>https://arxiv.org/abs/2512.14202</guid>
<content:encoded><![CDATA[

arXiv:2512.14202v1 Announce Type: cross 
Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar\'e Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging</title>
<link>https://arxiv.org/abs/2512.14211</link>
<guid>https://arxiv.org/abs/2512.14211</guid>
<content:encoded><![CDATA[

arXiv:2512.14211v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating problem difficulty without ground truth using Large Language Model comparisons</title>
<link>https://arxiv.org/abs/2512.14220</link>
<guid>https://arxiv.org/abs/2512.14220</guid>
<content:encoded><![CDATA[

arXiv:2512.14220v1 Announce Type: cross 
Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design</title>
<link>https://arxiv.org/abs/2512.14233</link>
<guid>https://arxiv.org/abs/2512.14233</guid>
<content:encoded><![CDATA[

arXiv:2512.14233v1 Announce Type: cross 
Abstract: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning</title>
<link>https://arxiv.org/abs/2512.14241</link>
<guid>https://arxiv.org/abs/2512.14241</guid>
<content:encoded><![CDATA[

arXiv:2512.14241v1 Announce Type: cross 
Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition</title>
<link>https://arxiv.org/abs/2512.14244</link>
<guid>https://arxiv.org/abs/2512.14244</guid>
<content:encoded><![CDATA[

arXiv:2512.14244v1 Announce Type: cross 
Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization</title>
<link>https://arxiv.org/abs/2512.14263</link>
<guid>https://arxiv.org/abs/2512.14263</guid>
<content:encoded><![CDATA[

arXiv:2512.14263v1 Announce Type: cross 
Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions</title>
<link>https://arxiv.org/abs/2512.14277</link>
<guid>https://arxiv.org/abs/2512.14277</guid>
<content:encoded><![CDATA[

arXiv:2512.14277v1 Announce Type: cross 
Abstract: The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study</title>
<link>https://arxiv.org/abs/2512.14278</link>
<guid>https://arxiv.org/abs/2512.14278</guid>
<content:encoded><![CDATA[

arXiv:2512.14278v1 Announce Type: cross 
Abstract: Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high ({\alpha}=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability ({\alpha}=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks</title>
<link>https://arxiv.org/abs/2512.14297</link>
<guid>https://arxiv.org/abs/2512.14297</guid>
<content:encoded><![CDATA[

arXiv:2512.14297v1 Announce Type: cross 
Abstract: Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime.
  To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed.
  Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture.
  Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region</title>
<link>https://arxiv.org/abs/2512.14312</link>
<guid>https://arxiv.org/abs/2512.14312</guid>
<content:encoded><![CDATA[

arXiv:2512.14312v1 Announce Type: cross 
Abstract: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title>
<link>https://arxiv.org/abs/2512.14320</link>
<guid>https://arxiv.org/abs/2512.14320</guid>
<content:encoded><![CDATA[

arXiv:2512.14320v1 Announce Type: cross 
Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data</title>
<link>https://arxiv.org/abs/2512.14329</link>
<guid>https://arxiv.org/abs/2512.14329</guid>
<content:encoded><![CDATA[

arXiv:2512.14329v1 Announce Type: cross 
Abstract: Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study</title>
<link>https://arxiv.org/abs/2512.14330</link>
<guid>https://arxiv.org/abs/2512.14330</guid>
<content:encoded><![CDATA[

arXiv:2512.14330v1 Announce Type: cross 
Abstract: AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring</title>
<link>https://arxiv.org/abs/2512.14332</link>
<guid>https://arxiv.org/abs/2512.14332</guid>
<content:encoded><![CDATA[

arXiv:2512.14332v1 Announce Type: cross 
Abstract: The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Attention Guided Defense Against Malicious Edits</title>
<link>https://arxiv.org/abs/2512.14333</link>
<guid>https://arxiv.org/abs/2512.14333</guid>
<content:encoded><![CDATA[

arXiv:2512.14333v1 Announce Type: cross 
Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Transferable Defense Against Malicious Image Edits</title>
<link>https://arxiv.org/abs/2512.14341</link>
<guid>https://arxiv.org/abs/2512.14341</guid>
<content:encoded><![CDATA[

arXiv:2512.14341v1 Announce Type: cross 
Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability for Vision Models via Shapley Value Optimization</title>
<link>https://arxiv.org/abs/2512.14354</link>
<guid>https://arxiv.org/abs/2512.14354</guid>
<content:encoded><![CDATA[

arXiv:2512.14354v1 Announce Type: cross 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis</title>
<link>https://arxiv.org/abs/2512.14361</link>
<guid>https://arxiv.org/abs/2512.14361</guid>
<content:encoded><![CDATA[

arXiv:2512.14361v1 Announce Type: cross 
Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePo: Language Models with Context Re-Positioning</title>
<link>https://arxiv.org/abs/2512.14391</link>
<guid>https://arxiv.org/abs/2512.14391</guid>
<content:encoded><![CDATA[

arXiv:2512.14391v1 Announce Type: cross 
Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_\phi$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning</title>
<link>https://arxiv.org/abs/2512.14420</link>
<guid>https://arxiv.org/abs/2512.14420</guid>
<content:encoded><![CDATA[

arXiv:2512.14420v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2512.14427</link>
<guid>https://arxiv.org/abs/2512.14427</guid>
<content:encoded><![CDATA[

arXiv:2512.14427v1 Announce Type: cross 
Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space</title>
<link>https://arxiv.org/abs/2512.14448</link>
<guid>https://arxiv.org/abs/2512.14448</guid>
<content:encoded><![CDATA[

arXiv:2512.14448v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels</title>
<link>https://arxiv.org/abs/2512.14477</link>
<guid>https://arxiv.org/abs/2512.14477</guid>
<content:encoded><![CDATA[

arXiv:2512.14477v1 Announce Type: cross 
Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models</title>
<link>https://arxiv.org/abs/2512.14481</link>
<guid>https://arxiv.org/abs/2512.14481</guid>
<content:encoded><![CDATA[

arXiv:2512.14481v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2512.14540</link>
<guid>https://arxiv.org/abs/2512.14540</guid>
<content:encoded><![CDATA[

arXiv:2512.14540v1 Announce Type: cross 
Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Language Models: Balancing Training Efficiency and Overfitting Resilience</title>
<link>https://arxiv.org/abs/2512.14549</link>
<guid>https://arxiv.org/abs/2512.14549</guid>
<content:encoded><![CDATA[

arXiv:2512.14549v1 Announce Type: cross 
Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2512.14554</link>
<guid>https://arxiv.org/abs/2512.14554</guid>
<content:encoded><![CDATA[

arXiv:2512.14554v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer</title>
<link>https://arxiv.org/abs/2512.14560</link>
<guid>https://arxiv.org/abs/2512.14560</guid>
<content:encoded><![CDATA[

arXiv:2512.14560v1 Announce Type: cross 
Abstract: Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polypersona: Persona-Grounded LLM for Synthetic Survey Responses</title>
<link>https://arxiv.org/abs/2512.14562</link>
<guid>https://arxiv.org/abs/2512.14562</guid>
<content:encoded><![CDATA[

arXiv:2512.14562v1 Announce Type: cross 
Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2512.14563</link>
<guid>https://arxiv.org/abs/2512.14563</guid>
<content:encoded><![CDATA[

arXiv:2512.14563v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies</title>
<link>https://arxiv.org/abs/2512.14576</link>
<guid>https://arxiv.org/abs/2512.14576</guid>
<content:encoded><![CDATA[

arXiv:2512.14576v1 Announce Type: cross 
Abstract: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer</title>
<link>https://arxiv.org/abs/2512.14585</link>
<guid>https://arxiv.org/abs/2512.14585</guid>
<content:encoded><![CDATA[

arXiv:2512.14585v1 Announce Type: cross 
Abstract: Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title>
<link>https://arxiv.org/abs/2512.14601</link>
<guid>https://arxiv.org/abs/2512.14601</guid>
<content:encoded><![CDATA[

arXiv:2512.14601v1 Announce Type: cross 
Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes</title>
<link>https://arxiv.org/abs/2512.14617</link>
<guid>https://arxiv.org/abs/2512.14617</guid>
<content:encoded><![CDATA[

arXiv:2512.14617v1 Announce Type: cross 
Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction</title>
<link>https://arxiv.org/abs/2512.14620</link>
<guid>https://arxiv.org/abs/2512.14620</guid>
<content:encoded><![CDATA[

arXiv:2512.14620v1 Announce Type: cross 
Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation</title>
<link>https://arxiv.org/abs/2512.14629</link>
<guid>https://arxiv.org/abs/2512.14629</guid>
<content:encoded><![CDATA[

arXiv:2512.14629v1 Announce Type: cross 
Abstract: Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images</title>
<link>https://arxiv.org/abs/2512.14640</link>
<guid>https://arxiv.org/abs/2512.14640</guid>
<content:encoded><![CDATA[

arXiv:2512.14640v1 Announce Type: cross 
Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation</title>
<link>https://arxiv.org/abs/2512.14658</link>
<guid>https://arxiv.org/abs/2512.14658</guid>
<content:encoded><![CDATA[

arXiv:2512.14658v1 Announce Type: cross 
Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$\Delta$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</title>
<link>https://arxiv.org/abs/2512.14677</link>
<guid>https://arxiv.org/abs/2512.14677</guid>
<content:encoded><![CDATA[

arXiv:2512.14677v1 Announce Type: cross 
Abstract: We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean</title>
<link>https://arxiv.org/abs/2512.14686</link>
<guid>https://arxiv.org/abs/2512.14686</guid>
<content:encoded><![CDATA[

arXiv:2512.14686v1 Announce Type: cross 
Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $\alpha$ of the noise. Nonetheless, existing complexity results often cover only the case $\alpha \in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $\alpha$ approaches $1$. This paper tackles the general case of noise with tail index $\alpha\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $\alpha \in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization</title>
<link>https://arxiv.org/abs/2512.14687</link>
<guid>https://arxiv.org/abs/2512.14687</guid>
<content:encoded><![CDATA[

arXiv:2512.14687v1 Announce Type: cross 
Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native and Compact Structured Latents for 3D Generation</title>
<link>https://arxiv.org/abs/2512.14692</link>
<guid>https://arxiv.org/abs/2512.14692</guid>
<content:encoded><![CDATA[

arXiv:2512.14692v1 Announce Type: cross 
Abstract: Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spherical Leech Quantization for Visual Tokenization and Generation</title>
<link>https://arxiv.org/abs/2512.14697</link>
<guid>https://arxiv.org/abs/2512.14697</guid>
<content:encoded><![CDATA[

arXiv:2512.14697v1 Announce Type: cross 
Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\Lambda_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2512.14698</link>
<guid>https://arxiv.org/abs/2512.14698</guid>
<content:encoded><![CDATA[

arXiv:2512.14698v1 Announce Type: cross 
Abstract: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Reinforcement Learning for Building Energy Management System</title>
<link>https://arxiv.org/abs/2210.12590</link>
<guid>https://arxiv.org/abs/2210.12590</guid>
<content:encoded><![CDATA[

arXiv:2210.12590v3 Announce Type: replace 
Abstract: The building sector is one of the largest contributors to global energy consumption. Improving its energy efficiency is essential for reducing operational costs and greenhouse gas emissions. Energy management systems (EMS) play a key role in monitoring and controlling building appliances efficiently and reliably. With the increasing integration of renewable energy, intelligent EMS solutions have received growing attention. Reinforcement learning (RL) has recently been explored for this purpose and shows strong potential. However, most RL-based EMS methods require a large number of training steps to learn effective control policies, especially when adapting to unseen buildings, which limits their practical deployment. This paper introduces MetaEMS, a meta-reinforcement learning framework for EMS. MetaEMS improves learning efficiency by transferring knowledge from previously solved tasks to new ones through group-level and building-level adaptation, enabling fast adaptation and effective control across diverse building environments. Experimental results demonstrate that MetaEMS adapts more rapidly to unseen buildings and consistently outperforms baseline methods across various scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMMA: A Communicative Multimodal Multi-Agent Benchmark</title>
<link>https://arxiv.org/abs/2410.07553</link>
<guid>https://arxiv.org/abs/2410.07553</guid>
<content:encoded><![CDATA[

arXiv:2410.07553v5 Announce Type: replace 
Abstract: The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Large Language Models for ESG Activity Detection in Financial Texts</title>
<link>https://arxiv.org/abs/2502.21112</link>
<guid>https://arxiv.org/abs/2502.21112</guid>
<content:encoded><![CDATA[

arXiv:2502.21112v2 Announce Type: replace 
Abstract: The integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making is a fundamental aspect of sustainable finance. However, ensuring that business practices align with evolving regulatory frameworks remains a persistent challenge. AI-driven solutions for automatically assessing the alignment of sustainability reports and non-financial disclosures with specific ESG activities could greatly support this process. Yet, this task remains complex due to the limitations of general-purpose Large Language Models (LLMs) in domain-specific contexts and the scarcity of structured, high-quality datasets. In this paper, we investigate the ability of current-generation LLMs to identify text related to environmental activities. Furthermore, we demonstrate that their performance can be significantly enhanced through fine-tuning on a combination of original and synthetically generated data. To this end, we introduce ESG-Activities, a benchmark dataset containing 1,325 labelled text segments classified according to the EU ESG taxonomy. Our experimental results show that fine-tuning on ESG-Activities significantly enhances classification accuracy, with open models such as Llama 7B and Gemma 7B outperforming large proprietary solutions in specific configurations. These findings have important implications for financial analysts, policymakers, and AI researchers seeking to enhance ESG transparency and compliance through advanced natural language processing techniques.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings</title>
<link>https://arxiv.org/abs/2504.15610</link>
<guid>https://arxiv.org/abs/2504.15610</guid>
<content:encoded><![CDATA[

arXiv:2504.15610v3 Announce Type: replace 
Abstract: The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this study's purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights & Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Large Language Models for Clinical Research Using One Million Clinical Trials</title>
<link>https://arxiv.org/abs/2505.16097</link>
<guid>https://arxiv.org/abs/2505.16097</guid>
<content:encoded><![CDATA[

arXiv:2505.16097v2 Announce Type: replace 
Abstract: Developing artificial intelligence (AI) for clinical research requires a comprehensive data foundation that supports model training and rigorous evaluation. Here, we introduce TrialPanorama, a large-scale structured resource that aggregates 1.6M clinical trial records from fifteen global registries and links them with biomedical ontologies and associated literature. To demonstrate its utility, we build a pipeline that constructs 152K training and testing samples for eight key clinical research tasks. Three tasks support systematic review workflows, including study search, study screening, and evidence summarization. Five tasks focus on trial design and optimization, including arm design, eligibility criteria design, endpoint selection, sample size estimation, and trial completion assessment and rationalization. Benchmarking cutting-edge large language models (LLMs) reveals that generic LLMs have limited capability in clinical reasoning. In contrast, an 8B LLM we developed on TrialPanorama using supervised finetuning and reinforcement learning wins over the 70B generic counterparts in all eight tasks, with a relative improvement of 73.7%, 67.6%, 38.4%, 37.8%, 26.5%, 20.7%, 20.0%, 18.1%, and 5.2%, respectively. We envision that TrialPanorama provides a solid foundation for future scaling of AI for clinical research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Scaling in Test-Time Compute</title>
<link>https://arxiv.org/abs/2507.14417</link>
<guid>https://arxiv.org/abs/2507.14417</guid>
<content:encoded><![CDATA[

arXiv:2507.14417v2 Announce Type: replace 
Abstract: We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent</title>
<link>https://arxiv.org/abs/2508.01031</link>
<guid>https://arxiv.org/abs/2508.01031</guid>
<content:encoded><![CDATA[

arXiv:2508.01031v4 Announce Type: replace 
Abstract: Computer Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both textual descriptions and sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Explicit Context Imperative Paradigm (ECIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Self-Play For Data-Free Training</title>
<link>https://arxiv.org/abs/2509.07414</link>
<guid>https://arxiv.org/abs/2509.07414</guid>
<content:encoded><![CDATA[

arXiv:2509.07414v2 Announce Type: replace 
Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself-a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following, mathematics, and coding benchmarks show that pretrained models can be effectively improved with self-play alone.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning</title>
<link>https://arxiv.org/abs/2509.12875</link>
<guid>https://arxiv.org/abs/2509.12875</guid>
<content:encoded><![CDATA[

arXiv:2509.12875v3 Announce Type: replace 
Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework--LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCTS-EP: Empowering Embodied Planning with Online Preference Optimization</title>
<link>https://arxiv.org/abs/2509.17116</link>
<guid>https://arxiv.org/abs/2509.17116</guid>
<content:encoded><![CDATA[

arXiv:2509.17116v2 Announce Type: replace 
Abstract: This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine</title>
<link>https://arxiv.org/abs/2509.20935</link>
<guid>https://arxiv.org/abs/2509.20935</guid>
<content:encoded><![CDATA[

arXiv:2509.20935v2 Announce Type: replace 
Abstract: In precision medicine, quantitative multi-omic features, topological context, and textual biological knowledge play vital roles in identifying disease-critical signaling pathways and targets. Existing pipelines capture only part of these-numerical omics ignore topological context, text-centric LLMs lack quantitative grounded reasoning, and graph-only models underuse node semantics and the generalization of LLMs-limiting mechanistic interpretability. Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they remain limited by unreliable intermediate evaluation, and vulnerability to reward hacking with computational cost. These gaps motivate integrating quantitative multi-omic signals, topological structure with node annotations, and literature-scale text via LLMs, using subgraph reasoning as the principle bridge linking numeric evidence, topological knowledge and language context. Therefore, we propose GALAX (Graph Augmented LAnguage model with eXplainability), an innovative framework that integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement learning guided by a Graph Process Reward Model (GPRM), which generates disease-relevant subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated by a pretrained GNN and schema-based rule check, enabling process-level supervision without explicit labels. As an application, we also introduced Target-QA, a benchmark combining CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines, which enables GNN pretraining for supervising step-wise graph construction and supports long-context reasoning over text-numeric graphs (TNGs), providing a scalable and biologically grounded framework for explainable, reinforcement-guided subgraph reasoning toward reliable and interpretable target discovery in precision medicine.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models</title>
<link>https://arxiv.org/abs/2509.22284</link>
<guid>https://arxiv.org/abs/2509.22284</guid>
<content:encoded><![CDATA[

arXiv:2509.22284v3 Announce Type: replace 
Abstract: Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis</title>
<link>https://arxiv.org/abs/2510.01115</link>
<guid>https://arxiv.org/abs/2510.01115</guid>
<content:encoded><![CDATA[

arXiv:2510.01115v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
<link>https://arxiv.org/abs/2510.14112</link>
<guid>https://arxiv.org/abs/2510.14112</guid>
<content:encoded><![CDATA[

arXiv:2510.14112v2 Announce Type: replace 
Abstract: Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2511.01170</link>
<guid>https://arxiv.org/abs/2511.01170</guid>
<content:encoded><![CDATA[

arXiv:2511.01170v2 Announce Type: replace 
Abstract: Adaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \textbf{DART}, a supervised \textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Black-Box Tuning of Large Language Models with Limited API Calls</title>
<link>https://arxiv.org/abs/2511.10210</link>
<guid>https://arxiv.org/abs/2511.10210</guid>
<content:encoded><![CDATA[

arXiv:2511.10210v3 Announce Type: replace 
Abstract: Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with "LogitMap Pairs" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Menta: A Small Language Model for On-Device Mental Health Prediction</title>
<link>https://arxiv.org/abs/2512.02716</link>
<guid>https://arxiv.org/abs/2512.02716</guid>
<content:encoded><![CDATA[

arXiv:2512.02716v3 Announce Type: replace 
Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://hong-labs.github.io/menta-project/
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.04359</link>
<guid>https://arxiv.org/abs/2512.04359</guid>
<content:encoded><![CDATA[

arXiv:2512.04359v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-Agent Scaling Fails Multi-Agent Intelligence: Towards Foundation Models with Native Multi-Agent Intelligence</title>
<link>https://arxiv.org/abs/2512.08743</link>
<guid>https://arxiv.org/abs/2512.08743</guid>
<content:encoded><![CDATA[

arXiv:2512.08743v3 Announce Type: replace 
Abstract: Foundation models (FMs) are increasingly assuming the role of the ''brain'' of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence, across 41 large language models and 7 challenging benchmarks, showing that scaling single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Emotional Trajectories: A Temporal-Semantic Network Approach for Latent Depression Assessment in Social Media</title>
<link>https://arxiv.org/abs/2305.13127</link>
<guid>https://arxiv.org/abs/2305.13127</guid>
<content:encoded><![CDATA[

arXiv:2305.13127v3 Announce Type: replace-cross 
Abstract: The early identification and intervention of latent depression are of significant societal importance for mental health governance. While current automated detection methods based on social media have shown progress, their decision-making processes often lack a clinically interpretable framework, particularly in capturing the duration and dynamic evolution of depressive symptoms. To address this, this study introduces a semantic parsing network integrated with multi-scale temporal prototype learning. The model detects depressive states by capturing temporal patterns and semantic prototypes in users' emotional expression, providing a duration-aware interpretation of underlying symptoms. Validated on a large-scale social media dataset, the model outperforms existing state-of-the-art methods. Analytical results indicate that the model can identify emotional expression patterns not systematically documented in traditional survey-based approaches, such as sustained narratives expressing admiration for an "alternative life." Further user evaluation demonstrates the model's superior interpretability compared to baseline methods. This research contributes a structurally transparent, clinically aligned framework for depression detection in social media to the information systems literature. In practice, the model can generate dynamic emotional profiles for social platform users, assisting in the targeted allocation of mental health support resources.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</title>
<link>https://arxiv.org/abs/2312.04960</link>
<guid>https://arxiv.org/abs/2312.04960</guid>
<content:encoded><![CDATA[

arXiv:2312.04960v5 Announce Type: replace-cross 
Abstract: Vision Transformers (ViTs) have emerged as a fundamental architecture and serve as the backbone of modern vision-language models. Despite their impressive performance, ViTs exhibit notable vulnerability to evasion attacks, necessitating the development of specialized Adversarial Training (AT) strategies tailored to their unique architecture. While a direct solution might involve applying existing AT methods to ViTs, our analysis reveals significant incompatibilities, particularly with state-of-the-art (SOTA) approaches such as Generalist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a systematic investigation of adversarial robustness in ViTs and provides a novel theoretical Mutual Information (MI) analysis in its autoencoder-based self-supervised pre-training. Specifically, we show that MI between the adversarial example and its latent representation in ViT-based autoencoders should be constrained via derived MI bounds. Building on this insight, we propose a self-supervised AT method, MIMIR, that employs an MI penalty to facilitate adversarial pre-training by masked image modeling with autoencoders. Extensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that MIMIR can consistently provide improved natural and robust accuracy, where MIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates superior robustness against unforeseen attacks and common corruption data and can also withstand adaptive attacks where the adversary possesses full knowledge of the defense mechanism. Our code and trained models are publicly available at: https://github.com/xiaoyunxxy/MIMIR.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Question Answering Over Spatio-Temporal Knowledge Graph</title>
<link>https://arxiv.org/abs/2402.11542</link>
<guid>https://arxiv.org/abs/2402.11542</guid>
<content:encoded><![CDATA[

arXiv:2402.11542v2 Announce Type: replace-cross 
Abstract: Spatio-temporal knowledge graphs (STKGs) enhance traditional KGs by integrating temporal and spatial annotations, enabling precise reasoning over questions with spatio-temporal dependencies. Despite their potential, research on spatio-temporal knowledge graph question answering (STKGQA) remains limited. This is primarily due to the lack of datasets that simultaneously contain spatio-temporal information, as well as methods capable of handling implicit spatio-temporal reasoning. To bridge this gap, we introduce the spatio-temporal question answering dataset (STQAD), the first comprehensive benchmark comprising 10,000 natural language questions that require both temporal and spatial reasoning. STQAD is constructed with real-world facts containing spatio-temporal information, ensuring that the dataset reflects practical scenarios. Furthermore, our experiments reveal that existing KGQA methods underperform on STQAD, primarily due to their inability to model spatio-temporal interactions. To address this, we propose the spatio-temporal complex question answering (STCQA) method, which jointly embeds temporal and spatial features into KG representations and dynamically filters answers through constraint-aware reasoning. STCQA achieves state-of-the-art performance, significantly outperforming existing baselines. Our work not only provides a valuable resource for future research but also advances the field by offering a robust baseline for answering complex spatio-temporal questions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Runtime Analysis of Evolutionary Diversity Optimization on the Multi-objective (LeadingOnes, TrailingZeros) Problem</title>
<link>https://arxiv.org/abs/2404.11496</link>
<guid>https://arxiv.org/abs/2404.11496</guid>
<content:encoded><![CDATA[

arXiv:2404.11496v3 Announce Type: replace-cross 
Abstract: Diversity optimization is the class of optimization problems in which we aim to find a diverse set of good solutions. One of the frequently-used approaches to solve such problems is to use evolutionary algorithms that evolve a desired diverse population. This approach is called evolutionary diversity optimization (EDO).
  In this paper, we analyze EDO on a three-objective function LOTZ$_k$, which is a modification of the two-objective benchmark function (LeadingOnes, TrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimal solutions in $O(kn^3)$ expected iterations. We also analyze the runtime of the GSEMO$_D$ algorithm (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures: the total imbalance and the sorted imbalances vector. For the first measure we show that the GSEMO$_D$ optimizes it in $O(kn^2\log(n))$ expected iterations (which is asymptotically faster than the upper bound on the runtime until it finds a Pareto-optimal population), and for the second measure we show an upper bound of $O(k^2n^3\log(n))$ expected iterations.
  We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures. The results of experiments suggest that our bounds for the total imbalance measure are tight, while the bounds for the imbalances vector are too pessimistic.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossPT-EEG: A Benchmark for Cross-Participant and Cross-Time Generalization of EEG-based Visual Decoding</title>
<link>https://arxiv.org/abs/2406.07151</link>
<guid>https://arxiv.org/abs/2406.07151</guid>
<content:encoded><![CDATA[

arXiv:2406.07151v2 Announce Type: replace-cross 
Abstract: Exploring brain activity in relation to visual perception provides insights into the biological representation of the world. While functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have enabled effective image classification and reconstruction, their high cost and bulk limit practical use. Electroencephalography (EEG), by contrast, offers low cost and excellent temporal resolution, but its potential has been limited by the scarcity of large, high-quality datasets and by block-design experiments that introduce temporal confounds. To fill this gap, we present CrossPT-EEG, a benchmark for cross-participant and cross-time generalization of visual decoding from EEG. We collected EEG data from 16 participants while they viewed 4,000 images sampled from ImageNet, with image stimuli annotated at multiple levels of granularity. Our design includes two stages separated in time to allow cross-time generalization and avoid block-design artifacts. We also introduce benchmarks tailored to non-block design classification, as well as pre-training experiments to assess cross-time and cross-participant generalization. These findings highlight the dataset's potential to enhance EEG-based visual brain-computer interfaces, deepen our understanding of visual perception in biological systems, and suggest promising applications for improving machine vision models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Long-term RAG Chatbots with Psychological Models of Memory Importance and Forgetting</title>
<link>https://arxiv.org/abs/2409.12524</link>
<guid>https://arxiv.org/abs/2409.12524</guid>
<content:encoded><![CDATA[

arXiv:2409.12524v2 Announce Type: replace-cross 
Abstract: While Retrieval-Augmented Generation (RAG) has shown promise in enhancing long-term conversations, the increasing memory load as conversations progress degrades retrieval accuracy. Drawing on psychological insights, we propose LUFY, a simple yet effective method that focuses on emotionally arousing memories and retains less than 10% of the conversation. In the user experiment, participants interacted with three types of RAG chatbots, each for 2 hours over 4 sessions, marking the most extensive assessment of a chatbot's long-term capabilities to date -- more than four times longer than any existing benchmark. The results demonstrate that prioritizing arousing memories while forgetting the majority of the conversation significantly enhances user experience. This study pushes the frontier of long-term conversations and highlights the importance of forgetting unimportant parts of conversations. Code and Dataset: https://github.com/ryuichi-sumida/LUFY
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Multi-modal Root Cause Identification in Microservice Systems</title>
<link>https://arxiv.org/abs/2410.10021</link>
<guid>https://arxiv.org/abs/2410.10021</guid>
<content:encoded><![CDATA[

arXiv:2410.10021v2 Announce Type: replace-cross 
Abstract: Root Cause Analysis (RCA) is essential for pinpointing the root causes of failures in microservice systems. Traditional data-driven RCA methods are typically limited to offline applications due to high computational demands, and existing online RCA methods handle only single-modal data, overlooking complex interactions in multi-modal systems. In this paper, we introduce OCEAN, a novel online multi-modal causal structure learning method for root cause localization. OCEAN employs a dilated convolutional neural network to capture long-term temporal dependencies and graph neural networks to learn causal relationships among system entities and key performance indicators. We further design a multi-factor attention mechanism to analyze and reassess the relationships among different metrics and log indicators/attributes for enhanced online causal graph learning. Additionally, a contrastive mutual information maximization-based graph fusion module is developed to effectively model the relationships across various modalities. Extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chase Anonymisation: Privacy-Preserving Knowledge Graphs with Logical Reasoning</title>
<link>https://arxiv.org/abs/2410.12418</link>
<guid>https://arxiv.org/abs/2410.12418</guid>
<content:encoded><![CDATA[

arXiv:2410.12418v2 Announce Type: replace-cross 
Abstract: We propose a novel framework to enable Knowledge Graphs (KGs) sharing while ensuring that information that should remain private is not directly released nor indirectly exposed via derived knowledge, maintaining at the same time the embedded knowledge of the KGs to support business downstream tasks. Our approach produces a privacy-preserving KG as an augmentation of the input one via controlled addition of nodes and edges as well as re-labeling of nodes and perturbation of weights. We introduce a novel privacy measure for KGs, which considers derived knowledge, a new utility metric that captures the business semantics we want to preserve, and propose two novel anonymisation algorithms. Our extensive experimental evaluation, with both synthetic graphs and real-world datasets, confirms the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-guided multi-property molecular optimization with a diffusion language model</title>
<link>https://arxiv.org/abs/2410.13597</link>
<guid>https://arxiv.org/abs/2410.13597</guid>
<content:encoded><![CDATA[

arXiv:2410.13597v4 Announce Type: replace-cross 
Abstract: Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby mitigating error propagation during diffusion process. By fusing physically and chemically detailed textual semantics with specialized molecular representations, TransDLM effectively integrates diverse information sources to guide precise optimization, which enhances the model's ability to balance structural retention and property enhancement. Additionally, the success of a case study further demonstrates TransDLM's ability to solve practical problems. Experimentally, our approach surpasses state-of-the-art methods in maintaining molecular structural similarity and enhancing chemical properties on the benchmark dataset.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Utility Preference Learning for Listwise Alignment</title>
<link>https://arxiv.org/abs/2410.18127</link>
<guid>https://arxiv.org/abs/2410.18127</guid>
<content:encoded><![CDATA[

arXiv:2410.18127v2 Announce Type: replace-cross 
Abstract: Aligning large language models with human preferences is essential for improving interaction quality and safety by ensuring outputs better reflect human values. A promising strategy involves Reinforcement Learning from Human Feedback (RLHF), starting with collecting and ranking responses generated by a supervised fine-tuning model to refine alignment. Existing methods such as Direct Preference Optimization (DPO) focus on pairwise comparisons, categorizing responses into preferred and less preferred pairs and optimizing pairwise margins. However, this pairwise approach cannot capture the holistic ranking relationships among multiple responses or effectively leverage the rich preference information available in list-wise comparisons. To address this challenge, this paper introduces \underline{D}irect \underline{R}anking \underline{P}reference \underline{O}ptimization (DRPO), a novel method that views human preference alignment as a Learning-to-Rank (LTR) task. Unlike pairwise methods, DRPO optimizes the preference ranking of entire response lists by computing holistic utility scores through NDCG, a standard LTR metric. To enable end-to-end optimization with the non-differentiable NDCG, we propose diffNDCG loss, a differentiable approximation facilitated by a sorting network. Furthermore, we introduce a novel margin-based Adaptive Rank Policy Score to enhance the discriminative quality of generated responses. Extensive experiments have shown that DRPO outperforms existing methods, enhancing the quality of the generated responses.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renal Cell Carcinoma subtyping: learning from multi-resolution localization</title>
<link>https://arxiv.org/abs/2411.09471</link>
<guid>https://arxiv.org/abs/2411.09471</guid>
<content:encoded><![CDATA[

arXiv:2411.09471v2 Announce Type: replace-cross 
Abstract: Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study</title>
<link>https://arxiv.org/abs/2411.13602</link>
<guid>https://arxiv.org/abs/2411.13602</guid>
<content:encoded><![CDATA[

arXiv:2411.13602v3 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of global mortality, necessitating accessible and accurate diagnostic tools. While cardiac magnetic resonance imaging (CMR) provides gold-standard insights into cardiac structure and function, its clinical utility is limited by high cost and complexity. In contrast, electrocardiography (ECG) is inexpensive and widely available but lacks the granularity of CMR. We propose CardioNets, a deep learning framework that translates 12-lead ECG signals into CMR-level functional parameters and synthetic images, enabling scalable cardiac assessment. CardioNets integrates cross-modal contrastive learning and generative pretraining, aligning ECG with CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via a masked autoregressive model. Trained on 159,819 samples from five cohorts, including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and externally validated on independent clinical datasets (n=3,767), CardioNets achieved strong performance across disease screening and phenotype estimation tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8% and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human physicians using both ECG and real CMR. These results suggest that CardioNets offers a promising, low-cost alternative to CMR for large-scale CVD screening, particularly in resource-limited settings. Future efforts will focus on clinical deployment and regulatory validation of ECG-based synthetic imaging.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepoTransBench: A Real-World Multilingual Benchmark for Repository-Level Code Translation</title>
<link>https://arxiv.org/abs/2412.17744</link>
<guid>https://arxiv.org/abs/2412.17744</guid>
<content:encoded><![CDATA[

arXiv:2412.17744v2 Announce Type: replace-cross 
Abstract: Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world multilingual repository-level code translation benchmark featuring 1,897 real-world repository samples across 13 language pairs with automatically executable test suites. Besides, we introduce RepoTransAgent, a general agent framework to perform repository-level code translation. We evaluate both our benchmark's challenges and agent's effectiveness using several methods and backbone LLMs, revealing that repository-level translation remains challenging, where the best-performing method achieves only a 32.8% success rate. Furthermore, our analysis reveals that translation difficulty varies significantly by language pair direction, with dynamic-to-static language translation being much more challenging than the reverse direction (achieving below 10% vs. static-to-dynamic at 45-63%). Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RepoTransBench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.14704</link>
<guid>https://arxiv.org/abs/2502.14704</guid>
<content:encoded><![CDATA[

arXiv:2502.14704v4 Announce Type: replace-cross 
Abstract: Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-robust Machine Unlearning</title>
<link>https://arxiv.org/abs/2503.09330</link>
<guid>https://arxiv.org/abs/2503.09330</guid>
<content:encoded><![CDATA[

arXiv:2503.09330v2 Announce Type: replace-cross 
Abstract: Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group (e.g., ethnicity, gender), we empirically show that performance for this group degrades, leading to fairness issues. To perform unlearning while preserving fairness, this work addresses the overlooked problem of non-uniformly distributed forget sets, which we refer to as group-robust machine unlearning. We formalize the problem and present a simple and effective exact unlearning strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trace Gadgets: Minimizing Code Context for Machine Learning-Based Vulnerability Prediction</title>
<link>https://arxiv.org/abs/2504.13676</link>
<guid>https://arxiv.org/abs/2504.13676</guid>
<content:encoded><![CDATA[

arXiv:2504.13676v2 Announce Type: replace-cross 
Abstract: As the number of web applications and API endpoints exposed to the Internet continues to grow, so does the number of exploitable vulnerabilities. Manually identifying such vulnerabilities is tedious. Meanwhile, static security scanners tend to produce many false positives. While machine learning-based approaches are promising, they typically perform well only in scenarios where training and test data are closely related. A key challenge for ML-based vulnerability detection is providing suitable and concise code context, as excessively long contexts negatively affect the code comprehension capabilities of machine learning models, particularly smaller ones.
  This work introduces Trace Gadgets, a novel code representation that minimizes code context by removing non-related code. Trace Gadgets precisely capture the statements that cover the path to the vulnerability. As input for ML models, Trace Gadgets provide a minimal but complete context, thereby improving the detection performance. Moreover, we collect a large-scale dataset generated from real-world applications with manually curated labels to further improve the performance of ML-based vulnerability detectors. Our results show that state-of-the-art machine learning models perform best when using Trace Gadgets compared to previous code representations, surpassing the detection capabilities of industry-standard static scanners such as GitHub's CodeQL by at least 4% on a fully unseen dataset. By applying our framework to real-world applications, we identify and report previously unknown vulnerabilities in widely deployed software.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Approximation with Softmax Attention</title>
<link>https://arxiv.org/abs/2504.15956</link>
<guid>https://arxiv.org/abs/2504.15956</guid>
<content:encoded><![CDATA[

arXiv:2504.15956v2 Announce Type: replace-cross 
Abstract: We prove that with linear transformations, both (i) two-layer self-attention and (ii) one-layer self-attention followed by a softmax function are universal approximators for continuous sequence-to-sequence functions on compact domains. Our main technique is a new interpolation-based method for analyzing attention's internal mechanism. This leads to our key insight: self-attention is able to approximate a generalized version of ReLU to arbitrary precision, and hence subsumes many known universal approximators. Building on these, we show that two-layer multi-head attention alone suffices as a sequence-to-sequence universal approximator. In contrast, prior works rely on feed-forward networks to establish universal approximation in Transformers. Furthermore, we extend our techniques to show that, (softmax-)attention-only layers are capable of approximating various statistical models in-context. We believe these techniques hold independent interest.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title>
<link>https://arxiv.org/abs/2505.05638</link>
<guid>https://arxiv.org/abs/2505.05638</guid>
<content:encoded><![CDATA[

arXiv:2505.05638v2 Announce Type: replace-cross 
Abstract: Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/aumovio/pred2plan.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2505.13109</link>
<guid>https://arxiv.org/abs/2505.13109</guid>
<content:encoded><![CDATA[

arXiv:2505.13109v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective</title>
<link>https://arxiv.org/abs/2506.23508</link>
<guid>https://arxiv.org/abs/2506.23508</guid>
<content:encoded><![CDATA[

arXiv:2506.23508v3 Announce Type: replace-cross 
Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on open-source multimodal model, Qwen2.5-VL series. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly but maintains prior knowledge. We study this phenomenon through learning dynamics by examining both the magnitude and direction of how training data influence prior knowledge. Our analysis shows that RFT mainly reinforces correct samples naturally aligned with the base model's probability landscape, leading to weaker interference with prior knowledge. Moreover, training on RFT-simulated rollouts, which exert a small magnitude of influence and are well aligned in direction to prior knowledge, allows SFT to preserve prior knowledge better while rapidly learning new tasks. These findings suggest that distribution of training data, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation</title>
<link>https://arxiv.org/abs/2507.06249</link>
<guid>https://arxiv.org/abs/2507.06249</guid>
<content:encoded><![CDATA[

arXiv:2507.06249v2 Announce Type: replace-cross 
Abstract: Recently, pre-trained models with phonetic supervision have demonstrated their advantages for crosslingual speech recognition in data efficiency and information sharing across languages. However, a limitation is that a pronunciation lexicon is needed for such phoneme-based crosslingual speech recognition. In this study, we aim to eliminate the need for pronunciation lexicons and propose a latent variable model based method, with phonemes being treated as discrete latent variables. The new method consists of a speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model. To jointly train the three models, we utilize the joint stochastic approximation (JSA) algorithm, which is a stochastic extension of the EM (expectation-maximization) algorithm and has demonstrated superior performance particularly in estimating discrete latent variable models. Furthermore, we propose marginal likelihood scoring (MLS) decoding to align inference with the training objective and P2G augmentation to improve the robustness of P2G mapping. Based on the Whistle multilingual pre-trained S2P model, crosslingual experiments are conducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of phoneme supervision, the new method, JSA-SPG, achieves 5% error rate reductions compared to the best crosslingual fine-tuning approach using subword or full phoneme supervision. Furthermore, it is found that in language domain adaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms the standard practice of language model fusion via the auxiliary support of the G2P model by 9% error rate reductions. To facilitate reproducibility and encourage further exploration in this field, we open-source the JSA-SPG training code and complete pipeline.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems</title>
<link>https://arxiv.org/abs/2507.11064</link>
<guid>https://arxiv.org/abs/2507.11064</guid>
<content:encoded><![CDATA[

arXiv:2507.11064v2 Announce Type: replace-cross 
Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as the growing number of antennas in modern massive MIMO systems substantially increases the channel state information (CSI) feedback demand in frequency division duplex (FDD) systems. To address this, extensive research has focused on CSI compression and prediction, with neural network-based approaches gaining momentum and being considered for integration into the 3GPP 5G-Advanced standards. While deep learning has been effectively applied to CSI-limited beamforming and handover optimization, reference signal allocation under such constraints remains surprisingly underexplored. To fill this gap, we introduce the concept of channel prediction-based reference signal allocation (CPRS), which jointly optimizes channel prediction and DM-RS allocation to improve data throughput without requiring CSI feedback. We further propose a standards-compliant ViViT/CNN-based architecture that implements CPRS by treating evolving CSI matrices as sequential image-like data, enabling efficient and adaptive transmission in dynamic environments. Simulation results using ray-tracing channel data generated in NVIDIA Sionna validate the proposed method, showing up to 36.60% throughput improvement over benchmark strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How K-12 Educators Use AI: LLM-Assisted Qualitative Analysis at Scale</title>
<link>https://arxiv.org/abs/2507.17985</link>
<guid>https://arxiv.org/abs/2507.17985</guid>
<content:encoded><![CDATA[

arXiv:2507.17985v3 Announce Type: replace-cross 
Abstract: This study investigates how K-12 educators use generative AI tools in real-world instructional contexts and how large language models (LLMs) can support scalable qualitative analysis of these interactions. Drawing on over 13,000 unscripted educator-AI conversations from an open-access platform, we examine educators' use of AI for lesson planning, differentiation, assessment, and pedagogical reflection. Methodologically, we introduce a replicable, LLM-assisted qualitative analysis pipeline that supports inductive theme discovery, codebook development, and large-scale annotation while preserving researcher control over conceptual synthesis. Empirically, the findings surface concrete patterns in how educators prompt, adapt, and evaluate AI-generated suggestions as part of their instructional reasoning. This work demonstrates the feasibility of combining LLM support with qualitative rigor to analyze complex educator behaviors at scale and inform the design of AI-powered educational tools.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</title>
<link>https://arxiv.org/abs/2508.00969</link>
<guid>https://arxiv.org/abs/2508.00969</guid>
<content:encoded><![CDATA[

arXiv:2508.00969v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has driven major advances in computational pathology by enabling the learning of rich representations from histopathology data. Yet, tissue analysis alone may fall short in capturing broader molecular complexity, as key complementary information resides in high-dimensional omics profiles such as transcriptomics, methylomics, and genomics. To address this gap, we introduce MORPHEUS, the first multimodal pre-training strategy that integrates histopathology images and multi-omics data within a shared transformer-based architecture. At its core, MORPHEUS relies on a novel masked omics modeling objective that encourages the model to learn meaningful cross-modal relationships. This yields a general-purpose pre-trained encoder that can be applied to histopathology alone or in combination with any subset of omics modalities. Beyond inference, MORPHEUS also supports flexible any-to-any omics reconstruction, enabling one or more omics profiles to be reconstructed from any modality subset that includes histopathology. Pre-trained on a large pan-cancer cohort, MORPHEUS shows substantial improvements over supervised and SSL baselines across diverse tasks and modality combinations. Together, these capabilities position it as a promising direction for the development of multimodal foundation models in oncology. Code is publicly available at https://github.com/Lucas-rbnt/MORPHEUS
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2508.01977</link>
<guid>https://arxiv.org/abs/2508.01977</guid>
<content:encoded><![CDATA[

arXiv:2508.01977v2 Announce Type: replace-cross 
Abstract: To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Embedded Swin-UMamba for DeepLesion Segmentation</title>
<link>https://arxiv.org/abs/2508.06453</link>
<guid>https://arxiv.org/abs/2508.06453</guid>
<content:encoded><![CDATA[

arXiv:2508.06453v2 Announce Type: replace-cross 
Abstract: Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow has the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, our method achieved a high Dice score of 82.64, and a low Hausdorff distance of 6.34 pixels was obtained for lesion segmentation. The proposed Text-Swin-U/Mamba model outperformed prior approaches: 37.79% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001), and surpassed the purely image-based XLSTM-UNet and nnUNet models by 2.58% and 1.01%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval Enhanced Feedback via In-context Neural Error-book</title>
<link>https://arxiv.org/abs/2508.16313</link>
<guid>https://arxiv.org/abs/2508.16313</guid>
<content:encoded><![CDATA[

arXiv:2508.16313v5 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning</title>
<link>https://arxiv.org/abs/2508.18395</link>
<guid>https://arxiv.org/abs/2508.18395</guid>
<content:encoded><![CDATA[

arXiv:2508.18395v2 Announce Type: replace-cross 
Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks.
  We introduce \textbf{Latent Self-Consistency (LSC)}, which selects the most semantically consistent response using learnable token embeddings. LSC's lightweight forward processing of summary tokens only introduces negligible runtime overhead (at most $0.9\%$) on top of standard decoding of the base LLM, and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC, and WUCS on both short-form and long-form on average performance, while adding negligible computational overhead on vanilla inference. These results position LSC as a reliable consistency-selection method that works effectively across various answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low expected calibration error across both answer formats.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Property-Isometric Variational Autoencoders for Sequence Modeling and Design</title>
<link>https://arxiv.org/abs/2509.14287</link>
<guid>https://arxiv.org/abs/2509.14287</guid>
<content:encoded><![CDATA[

arXiv:2509.14287v2 Announce Type: replace-cross 
Abstract: Biological sequence design (DNA, RNA, or peptides) with desired functional properties has applications in discovering novel nanomaterials, biosensors, antimicrobial drugs, and beyond. One common challenge is the ability to optimize complex high-dimensional properties such as target emission spectra of DNA-mediated fluorescent nanoparticles, photo and chemical stability, and antimicrobial activity of peptides across target microbes. Existing models rely on simple binary labels (e.g., binding/non-binding) rather than high-dimensional complex properties. To address this gap, we propose a geometry-preserving variational autoencoder framework, called PrIVAE, which learns latent sequence embeddings that respect the geometry of their property space. Specifically, we model the property space as a high-dimensional manifold that can be locally approximated by a nearest neighbor graph, given an appropriately defined distance measure. We employ the property graph to guide the sequence latent representations using (1) graph neural network encoder layers and (2) an isometric regularizer. PrIVAE learns a property-organized latent space that enables rational design of new sequences with desired properties by employing the trained decoder. We evaluate the utility of our framework for two generative tasks: (1) design of DNA sequences that template fluorescent metal nanoclusters and (2) design of antimicrobial peptides. The trained models retain high reconstruction accuracy while organizing the latent space according to properties. Beyond in silico experiments, we also employ sampled sequences for wet lab design of DNA nanoclusters, resulting in up to 16.1-fold enrichment of rare-property nanoclusters compared to their abundance in training data, demonstrating the practical utility of our framework.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem</title>
<link>https://arxiv.org/abs/2509.18054</link>
<guid>https://arxiv.org/abs/2509.18054</guid>
<content:encoded><![CDATA[

arXiv:2509.18054v2 Announce Type: replace-cross 
Abstract: Selecting a solution algorithm for the Facility Layout Problem (FLP), an NP-hard optimization problem with multiobjective trade-off, is a complex task that requires deep expert knowledge. The performance of a given algorithm depends on the specific characteristics of the problem, such as the number of facilities, objectives, and constraints. This creates a need for a data-driven recommendation method to guide algorithm selection in automated design systems. This paper introduces a new recommendation method to make this expertise accessible, based on a Knowledge Graph-Based Retrieval-Augmented Generation (KG-RAG) framework. In this framework, a domain-specific knowledge graph (KG) is constructed from the literature. The method then employs a multifaceted retrieval mechanism to gather relevant evidence from this KG using three distinct approaches: precise graph-based search, flexible vector-based search, and cluster-based high-level search. The retrieved evidence is utilized by a Large Language Model (LLM) to generate algorithm recommendations based on data-driven reasoning. This KG-RAG framework is tested on a use case consisting of six problems comprising of complex multi-objective and multi-constraint FLP case. The results are compared with the Gemini 1.5 Flash chatbot. The results show that KG-RAG achieves an average reasoning score of 4.7 out of 5 compared to 3.3 for the baseline chatbot.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning neuroimaging models from health system-scale data</title>
<link>https://arxiv.org/abs/2509.18638</link>
<guid>https://arxiv.org/abs/2509.18638</guid>
<content:encoded><![CDATA[

arXiv:2509.18638v2 Announce Type: replace-cross 
Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological diseases. The global demand for magnetic resonance imaging (MRI) studies has risen steadily, placing significant strain on health systems, prolonging turnaround times, and intensifying physician burnout. These challenges disproportionately impact patients in low-resource and rural settings. Here, we utilized a large academic health system as a data engine to develop Prima, the first vision language model (VLM) serving as an AI foundation for neuroimaging that supports real-world, clinical MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a hierarchical vision architecture that provides general and transferable MRI features. Prima was tested in a 1-year health system-wide study that included 30K MRI studies. Across 52 radiologic diagnoses from the major neurologic disorders, including neoplastic, inflammatory, infectious, and developmental lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0, outperforming other state-of-the-art general and medical AI models. Prima offers explainable differential diagnoses, worklist priority for radiologists, and clinical referral recommendations across diverse patient demographics and MRI systems. Prima demonstrates algorithmic fairness across sensitive groups and can help mitigate health system biases, such as prolonged turnaround times for low-resource populations. These findings highlight the transformative potential of health system-scale VLMs and Prima's role in advancing AI-driven healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Gaussian Initializations: Signal Preserving Weight Initialization for Odd-Sigmoid Activations</title>
<link>https://arxiv.org/abs/2509.23085</link>
<guid>https://arxiv.org/abs/2509.23085</guid>
<content:encoded><![CDATA[

arXiv:2509.23085v2 Announce Type: replace-cross 
Abstract: Activation functions critically influence trainability and expressivity, and recent work has therefore explored a broad range of nonlinearities. However, widely used Gaussian i.i.d. initializations are designed to preserve activation variance under wide or infinite width assumptions. In deep and relatively narrow networks with sigmoidal nonlinearities, these schemes often drive preactivations into saturation, and collapse gradients. To address this, we introduce an odd-sigmoid activations and propose an activation aware initialization tailored to any function in this class. Our method remains robust over a wide band of variance scales, preserving both forward signal variance and backpropagated gradient norms even in very deep and narrow networks. Empirically, across standard image benchmarks we find that the proposed initialization is substantially less sensitive to depth, width, and activation scale than Gaussian initializations. In physics informed neural networks (PINNs), scaled odd-sigmoid activations combined with our initialization achieve lower losses than Gaussian based setups, suggesting that diagonal-plus-noise weights provide a practical alternative when Gaussian initialization breaks down.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Sampler Stochasticity in Training Diffusion Models for RLHF</title>
<link>https://arxiv.org/abs/2510.10767</link>
<guid>https://arxiv.org/abs/2510.10767</guid>
<content:encoded><![CDATA[

arXiv:2510.10767v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title>
<link>https://arxiv.org/abs/2510.20768</link>
<guid>https://arxiv.org/abs/2510.20768</guid>
<content:encoded><![CDATA[

arXiv:2510.20768v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.25502</link>
<guid>https://arxiv.org/abs/2510.25502</guid>
<content:encoded><![CDATA[

arXiv:2510.25502v3 Announce Type: replace-cross 
Abstract: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives</title>
<link>https://arxiv.org/abs/2511.08710</link>
<guid>https://arxiv.org/abs/2511.08710</guid>
<content:encoded><![CDATA[

arXiv:2511.08710v2 Announce Type: replace-cross 
Abstract: We develop and analyze a theoretical framework for agent-to-agent interactions in a simplified in-context linear regression setting. In our model, each agent is instantiated as a single-layer transformer with linear self-attention (LSA) trained to implement gradient-descent-like updates on a quadratic regression objective from in-context examples. We then study the coupled dynamics when two such LSA agents alternately update from each other's outputs under potentially misaligned fixed objectives. Within this framework, we characterize the generation dynamics and show that misalignment leads to a biased equilibrium where neither agent reaches its target, with residual errors predictable from the objective gap and the prompt-induced geometry. We further contrast this fixed objective regime with an adaptive multi-agent setting, wherein a helper agent updates a turn-based objective to implement a Newton-like step for the main agent, eliminating the plateau and accelerating its convergence. Experiments with trained LSA agents, as well as black-box GPT-5-mini runs on in-context linear regression tasks, are consistent with our theoretical predictions within this simplified setting. We view our framework as a mechanistic framework that links prompt geometry and objective misalignment to stability, bias, and robustness, and as a stepping stone toward analyzing more realistic multi-agent LLM systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.09605</link>
<guid>https://arxiv.org/abs/2511.09605</guid>
<content:encoded><![CDATA[

arXiv:2511.09605v3 Announce Type: replace-cross 
Abstract: The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title>
<link>https://arxiv.org/abs/2511.10400</link>
<guid>https://arxiv.org/abs/2511.10400</guid>
<content:encoded><![CDATA[

arXiv:2511.10400v2 Announce Type: replace-cross 
Abstract: Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveC2R: Wavelet-Driven Coarse-to-Refined Hierarchical Learning for Radar Retrieval</title>
<link>https://arxiv.org/abs/2511.17558</link>
<guid>https://arxiv.org/abs/2511.17558</guid>
<content:encoded><![CDATA[

arXiv:2511.17558v3 Announce Type: replace-cross 
Abstract: Satellite-based radar retrieval methods are widely employed to fill coverage gaps in ground-based radar systems, especially in remote areas affected by terrain blockage and limited detection range. Existing methods predominantly rely on overly simplistic spatial-domain architectures constructed from a single data source, limiting their ability to accurately capture complex precipitation patterns and sharply defined meteorological boundaries. To address these limitations, we propose WaveC2R, a novel wavelet-driven coarse-to-refined framework for radar retrieval. WaveC2R integrates complementary multi-source data and leverages frequency-domain decomposition to separately model low-frequency components for capturing precipitation patterns and high-frequency components for delineating sharply defined meteorological boundaries. Specifically, WaveC2R consists of two stages (i)Intensity-Boundary Decoupled Learning, which leverages wavelet decomposition and frequency-specific loss functions to separately optimize low-frequency intensity and high-frequency boundaries; and (ii)Detail-Enhanced Diffusion Refinement, which employs frequency-aware conditional priors and multi-source data to progressively enhance fine-scale precipitation structures while preserving coarse-scale meteorological consistency. Experimental results on the publicly available SEVIR dataset demonstrate that WaveC2R achieves state-of-the-art performance in satellite-based radar retrieval, particularly excelling at preserving high-intensity precipitation features and sharply defined meteorological boundaries.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks</title>
<link>https://arxiv.org/abs/2511.21626</link>
<guid>https://arxiv.org/abs/2511.21626</guid>
<content:encoded><![CDATA[

arXiv:2511.21626v3 Announce Type: replace-cross 
Abstract: Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
  We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts</title>
<link>https://arxiv.org/abs/2511.23442</link>
<guid>https://arxiv.org/abs/2511.23442</guid>
<content:encoded><![CDATA[

arXiv:2511.23442v2 Announce Type: replace-cross 
Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees</title>
<link>https://arxiv.org/abs/2512.00204</link>
<guid>https://arxiv.org/abs/2512.00204</guid>
<content:encoded><![CDATA[

arXiv:2512.00204v2 Announce Type: replace-cross 
Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2512.00888</link>
<guid>https://arxiv.org/abs/2512.00888</guid>
<content:encoded><![CDATA[

arXiv:2512.00888v2 Announce Type: replace-cross 
Abstract: Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title>
<link>https://arxiv.org/abs/2512.05442</link>
<guid>https://arxiv.org/abs/2512.05442</guid>
<content:encoded><![CDATA[

arXiv:2512.05442v2 Announce Type: replace-cross 
Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</title>
<link>https://arxiv.org/abs/2512.07829</link>
<guid>https://arxiv.org/abs/2512.07829</guid>
<content:encoded><![CDATA[

arXiv:2512.07829v2 Announce Type: replace-cross 
Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators</title>
<link>https://arxiv.org/abs/2512.07901</link>
<guid>https://arxiv.org/abs/2512.07901</guid>
<content:encoded><![CDATA[

arXiv:2512.07901v2 Announce Type: replace-cross 
Abstract: Von Neumann founded both game theory and the theory of self-reproducing automata, but the two programs never merged. This paper provides the synthesis. The Theory of Strategic Evolution analyzes strategic replicators: entities that optimize under resource constraints and spawn copies of themselves. We introduce Games with Endogenous Players (GEPs), where lineages (not instances) are the fundamental strategic units, and define Evolutionarily Stable Distributions of Intelligence (ESDIs) as the resulting equilibrium concept.
  The central mathematical object is a hierarchy of strategic layers linked by cross-level gain matrices. Under a small-gain condition (spectral radius less than one), the system admits a global Lyapunov function at every finite depth. We prove closure under meta-selection: adding governance levels, innovation, or constitutional evolution preserves the dynamical structure. The Alignment Impossibility Theorem shows that unrestricted self-modification destroys this structure; stable alignment requires bounded modification classes.
  Applications include AI deployment dynamics, market concentration, and institutional design. The framework shows why personality engineering fails under selection pressure and identifies constitutional constraints necessary for stable multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title>
<link>https://arxiv.org/abs/2512.08786</link>
<guid>https://arxiv.org/abs/2512.08786</guid>
<content:encoded><![CDATA[

arXiv:2512.08786v2 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.09907</link>
<guid>https://arxiv.org/abs/2511.09907</guid>
<content:encoded><![CDATA[
<div> Keywords: data synthesis, reasoning models, problem generation, difficulty calibration, co-evolution

<br /><br />Summary:  
This paper addresses data synthesis challenges for training large reasoning models by introducing a problem generator that explicitly reasons to plan problem directions before synthesis and adapts problem difficulty based on the solver’s ability. The method constructs related problem pairs and augments them with intermediate chain-of-thought (CoT) problem-design steps generated by reasoning models, effectively bootstrapping problem-design strategies. The generator uses solver feedback as a reward signal to calibrate difficulty and produce complementary problems that are near the edge of the solver’s competence, ensuring high-value problem creation without relying on complex pipelines. Extensive experiments conducted on 10 mathematical and general reasoning benchmarks demonstrate an average performance improvement of 2.5%. The approach generalizes well to both language and vision-language models. Furthermore, the solver trained on the synthesized data provides improved rewards to the generator, enabling iterative co-evolution that yields an additional 0.7% performance gain. This demonstrates the effectiveness of a feedback-driven, reasoning-augmented data synthesis process in boosting the training of large reasoning models. The authors also plan to release their code publicly, facilitating further research and application. <div>
arXiv:2511.09907v3 Announce Type: replace 
Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title>
<link>https://arxiv.org/abs/2509.25531</link>
<guid>https://arxiv.org/abs/2509.25531</guid>
<content:encoded><![CDATA[
<div> MixtureVitae, permissive licensing, pretraining corpus, LLM performance, risk mitigation<br /><br />Summary:<br /><br />1. MixtureVitae is an open-access pretraining corpus designed to minimize legal risks by using a permissive-first, risk-mitigated sourcing strategy, combining public-domain, permissively licensed texts (such as CC-BY and Apache), and low-risk additions like government works and EU TDM-eligible sources.<br /><br />2. The dataset integrates a high density of synthetic instruction and reasoning data, typically added during post-training, within a single-stage pretraining recipe, addressing scarcity in existing permissive web corpora.<br /><br />3. Sources are categorized into a three-tier risk scheme with shard-level provenance metadata, enabling users to utilize the data in a risk-aware manner.<br /><br />4. Controlled experiments using the open-sci-ref protocol with fixed model architectures and token budgets (50B and 300B) show that models pretrained on MixtureVitae outperform those trained on other permissive datasets, particularly excelling on benchmarks like MMLU, and math and code tasks.<br /><br />5. A 1.7B parameter model trained on 300B tokens of MixtureVitae matches or exceeds a strong 1.7B instruction-tuned baseline on tasks like GSM8K, HumanEval, and MBPP while using over 36 times fewer tokens, demonstrating that a permissive-first, high-instruction-density corpus can effectively train capable LLMs without reliance on broad web scrapes.<br /><br />6. Thorough decontamination analysis supports the integrity of the dataset, making MixtureVitae a practical foundation for risk-mitigated LLM training.<br /><br />7. The corpus and code are openly available at https://github.com/ontocord/mixturevitae. <div>
arXiv:2509.25531v4 Announce Type: replace-cross 
Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance. MixtureVitae follows a permissive-first, risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources). MixtureVitae adopts a simple, single-stage pretraining recipe that integrates a large proportion of permissive synthetic instruction and reasoning data-signals typically introduced during post-training and generally scarce in permissive web corpora. We categorize all sources into a three-tier scheme that reflects varying risk levels and provide shard-level provenance metadata to enable risk-aware usage. In controlled experiments using the open-sci-ref training protocol (fixed architectures and hyperparameters; 50B and 300B token budgets across 130M-1.7B parameters), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B-parameters/300B-tokens setting, they surpass FineWeb-Edu and approach DCLM late in training. Performance is particularly strong on MMLU and on math and code benchmarks: a 1.7B model pretrained on 300B MixtureVitae tokens matches or exceeds a strong 1.7B instruction-tuned baseline on GSM8K, HumanEval, and MBPP, despite using over 36 times fewer tokens (300B vs. ~11T). Supported by a thorough decontamination analysis, these results show that permissive-first data with high instruction and reasoning density, tiered by licensing and provenance-related risk, can provide a practical and risk-mitigated foundation for training capable LLMs, reducing reliance on broad web scrapes without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
<div> Referring Expression Comprehension, Multi-modal Large Language Model, Benchmark, Perception and Reasoning, Reinforcement Learning<br /><br />Summary:<br /><br />The paper addresses limitations in existing Referring Expression Comprehension (REC) benchmarks, which focus mainly on perceptual skills but lack interpretable metrics to evaluate Multi-modal Large Language Models' (MLLM) grounding abilities across various cognitive aspects. To overcome this, the authors introduce RefBench-PRO, a comprehensive REC benchmark that decomposes referring expressions into two primary dimensions: perception and reasoning. These dimensions are further broken down into six progressively challenging tasks: attribute, position, interaction, commonsense, relation, and reject. The benchmark is supported by a fully automated data-generation pipeline that produces a diverse range of referring expressions spanning these sub-dimensions. Additionally, the authors propose Ref-R1, a reinforcement learning based training scheme incorporating a Dynamic Intersection-over-Union (IoU)-based Generalized Policy Optimization (GRPO) approach. This method enhances localization accuracy especially under complex reasoning scenarios, establishing a stronger baseline for REC tasks. Extensive experimentation validates that RefBench-PRO provides more interpretable evaluation metrics for MLLM performance in referring expression comprehension and reveals greater challenges in both perception and reasoning compared to prior benchmarks. <div>
arXiv:2512.06276v2 Announce Type: replace-cross 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protecting Bystander Privacy via Selective Hearing in Audio LLMs</title>
<link>https://arxiv.org/abs/2512.06380</link>
<guid>https://arxiv.org/abs/2512.06380</guid>
<content:encoded><![CDATA[
<div> Keywords: selective hearing, bystander privacy, audio large language models, SH-Bench, Bystander Privacy Fine-Tuning (BPFT)  

<br /><br />Summary:  
This article addresses privacy concerns in audio large language models (LLMs) that capture speech from unintended bystanders, a scenario overlooked by existing benchmarks and defenses. It introduces SH-Bench, the first benchmark tailored to evaluate "selective hearing" — the model's ability to focus on an intended main speaker while ignoring incidental bystander speech. SH-Bench comprises 3,968 multi-speaker audio mixtures drawn from real-world and synthetic environments, paired with 77,000 multiple-choice questions to assess model performance under general and selective operating conditions. To measure both comprehension and bystander privacy protection, the authors propose Selective Efficacy (SE), a novel metric combining these two aspects. Evaluations of current state-of-the-art open-source and proprietary audio LLMs reveal significant leakage of bystander information, showing that strong audio understanding does not guarantee privacy preservation. To close this gap, the authors propose Bystander Privacy Fine-Tuning (BPFT), a training method that encourages models to refuse queries related to bystander speech without impacting comprehension of the main speaker. BPFT achieves substantial improvements, including a 47% absolute increase in bystander refusal accuracy in selective mode and a 16% absolute increase in SE compared to Gemini 2.5 Pro, the best baseline model without BPFT. Together, SH-Bench and BPFT establish the first systematic framework for assessing and enhancing bystander privacy in audio LLMs. <div>
arXiv:2512.06380v2 Announce Type: replace-cross 
Abstract: Audio Large language models (LLMs) are increasingly deployed in the real world, where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences did not consider. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures, including both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. In addition, we propose Selective Efficacy (SE), a novel metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LLMs reveals substantial bystander privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we also present Bystander Privacy Fine-Tuning (BPFT), a novel training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. We show that BPFT yields substantial gains, achieving an absolute 47% higher bystander accuracy under selective mode and an absolute 16% higher SE compared to Gemini 2.5 Pro, which is the best audio LLM without BPFT. Together, SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem</title>
<link>https://arxiv.org/abs/2512.08290</link>
<guid>https://arxiv.org/abs/2512.08290</guid>
<content:encoded><![CDATA[
<div> Keywords: Model Context Protocol, Large Language Models, security threats, epistemic safety, autonomous agents  

<br /><br />Summary:  
The Model Context Protocol (MCP) serves as the standard interface that connects Large Language Models (LLMs) to external data and tools, much like a universal connector ("USB-C for Agentic AI"). This separation of context and execution address interoperability issues but introduces a new threat landscape where epistemic errors (such as hallucinations) intersect with security vulnerabilities (unauthorized actions). This paper provides a Systematization of Knowledge (SoK) by developing a comprehensive taxonomy of risks within the MCP ecosystem. It distinguishes between adversarial security threats—like indirect prompt injection and tool poisoning—and epistemic safety hazards, including alignment failures during distributed tool delegation. The study dissects the structural weaknesses inherent in MCP’s core components—Resources, Prompts, and Tools—and illustrates how the manipulation of "context" can be weaponized to provoke unauthorized operations, especially in multi-agent systems. Additionally, the authors review contemporary defense mechanisms, encompassing cryptographic provenance techniques such as ETDI and runtime intent verification protocols. The paper concludes by outlining a strategic roadmap aimed at securing the evolution of AI systems from simple conversational chatbots to fully autonomous, agentic operating systems, emphasizing the critical need for robust safety and security frameworks throughout this transition. <div>
arXiv:2512.08290v2 Announce Type: replace-cross 
Abstract: The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the "USB-C for Agentic AI." While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how "context" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
<div> Diffusion transformers, world models, long-horizon prediction, action-aware adapter, video generation  

<br /><br />Summary:  
The paper introduces Astra, a novel interactive general world model designed for accurate long-term video prediction across diverse real-world scenarios such as autonomous driving and robotic manipulation. Astra leverages an autoregressive denoising architecture combined with temporal causal attention to effectively aggregate past observations, supporting continuous streaming outputs. To balance temporal coherence with responsiveness, a noise-augmented history memory mechanism is employed, preventing over-dependence on past frames. For precise control over actions, the model incorporates an action-aware adapter that integrates action signals directly within the denoising process. Additionally, Astra features a mixture of action experts that dynamically route different action modalities, enhancing its versatility to handle various interaction types, including exploration, manipulation, and camera control. The proposed approach enables interactive, consistent, and generalized long-term video predictions aligned with diverse forms of action inputs. Experimental results on multiple datasets demonstrate Astra's superiority over state-of-the-art world models in terms of prediction fidelity, long-range temporal coherence, and alignment with action inputs, marking a significant advancement toward general-purpose interactive video prediction models. <div>
arXiv:2512.08931v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models</title>
<link>https://arxiv.org/abs/2512.11835</link>
<guid>https://arxiv.org/abs/2512.11835</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Artificial Age Score, Monadology, Memory constraints, Executable specification  

<br /><br />Summary:  
This paper addresses the challenge of governing internal memory and self-like behaviors in large language models (LLMs) by introducing a clause-based architectural framework grounded in philosophical principles. It builds upon the Artificial Age Score (AAS), a metric previously defined and mathematically justified to measure artificial memory aging, using it as the core kernel for memory and control constraints. The authors select twenty monads from Leibniz's Monadology and organize them into six bundles—ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology—each realized as executable Python specifications on top of the AAS framework. Through six minimal Python experiments, the paper demonstrates the practical instantiation of these clauses on measurable quantities like recall scores, redundancy, and weights. Each experiment follows a structured approach: inputs/setup, clause implementation, numerical analysis, and design implications for LLMs. Results show that the clause system produces bounded, interpretable behavior with continuous and rate-limited AAS trajectories, explicit penalties for contradictions and unsupported claims, and a hierarchical refinement that reveals organic internal structure. The harmony and reason clauses align dual views and goal-action pairs, while windowed drift in perfection scores helps distinguish between improvement and degradation phases. Overall, the work proposes a philosophically motivated, transparent, and implementable blueprint for constraining and analyzing the internal dynamics of artificial agents using an AAS backbone. <div>
arXiv:2512.11835v1 Announce Type: new 
Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars</title>
<link>https://arxiv.org/abs/2512.11864</link>
<guid>https://arxiv.org/abs/2512.11864</guid>
<content:encoded><![CDATA[
<div> Keywords: parallel machine scheduling, job precedences, calendar-based resource constraints, constraint modeling, metaheuristic

<br /><br />Summary:  
This paper addresses the complex problem of scheduling production on parallel machines, a challenge common in industrial manufacturing environments where efficiency can greatly reduce costs. The authors identify that while many basic variants of machine scheduling are already NP-hard, real-world scenarios involve additional complex constraints such as job precedences and calendar-based cumulative resource restrictions that current methods struggle to handle. To tackle this, the study introduces a novel parallel machine scheduling variant incorporating these realistic constraints. For small-scale problems, an exact solution method based on constraint modeling combined with advanced constraint-solving tools is proposed. Recognizing the limitations of exact methods on large instances, the paper presents a construction heuristic and a specialized metaheuristic that uses local search to efficiently solve large-scale scheduling problems. Importantly, this metaheuristic has been implemented in an industrial context and is actively being used, demonstrating practical applicability. Overall, the work contributes innovative modeling and solution techniques that better align with the complexities of actual manufacturing scheduling needs, bridging the gap between theoretical NP-hard scheduling problems and real-life industrial applications. <div>
arXiv:2512.11864v1 Announce Type: new 
Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.11902</link>
<guid>https://arxiv.org/abs/2512.11902</guid>
<content:encoded><![CDATA[
<div> Keywords: Mirror Mode, Imitation Learning, Reinforcement Learning, Fire Emblem Heroes, Player Strategy<br /><br />Summary:<br /><br />This study introduces a novel game mode called Mirror Mode designed to enhance enemy AI behavior in turn-based strategy games by imitating the personal playstyle of human players. A simplified version of the Nintendo game Fire Emblem Heroes was developed in the Unity engine to test this concept, featuring both a Standard Mode and the new Mirror Mode. The research involves two main experimental phases. The first phase focused on finding an effective model for replicating player strategies, combining approaches from Reinforcement Learning and Imitation Learning, specifically Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second phase evaluated these models through player tests where the AI was trained on real player demonstrations. Results showed that the AI successfully imitated players’ defensive behaviors but struggled with offensive tactics. Furthermore, player feedback indicated that participants recognized their own retreating strategies during Mirror Mode gameplay, which contributed to higher overall satisfaction. The study suggests that further refinement of the modeling approach could improve the AI’s imitation quality, especially in offensive play, thereby increasing player engagement and satisfaction. All code and survey data related to the study are publicly available on GitHub. <div>
arXiv:2512.11902v1 Announce Type: new 
Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents</title>
<link>https://arxiv.org/abs/2512.11907</link>
<guid>https://arxiv.org/abs/2512.11907</guid>
<content:encoded><![CDATA[
<div> Personalization, Large Language Models, Submodularity, Matroid Constraints, Knowledge Graph<br /><br />Summary: This paper addresses the challenge of personalizing large language model (LLM) agents using user-specific data, balancing task utility and data privacy. It highlights that while adding user data exhibits diminishing returns, real-world personalization involves complex structural constraints such as logical dependencies, categorical quotas, and hierarchical rules that complicate standard subset selection approaches. To manage these complexities, the authors propose a formal method that models these constraints by transforming a user's knowledge graph into a set of abstract macro-facets through a compilation process. The core theoretical contribution is proving that common hierarchical and quota-based constraints over these macro-facets correspond to a laminar matroid structure. This insight allows framing the personalization problem as a submodular function maximization under matroid constraints. Consequently, this enables the use of greedy algorithms with constant-factor approximation guarantees and continuous greedy methods achieving a (1 - 1/e) approximation ratio. Ultimately, the work provides a principled approach to structured personalization with more realistic constraint handling, improving algorithmic performance and practical applicability in personalizing LLM agents. <div>
arXiv:2512.11907v1 Announce Type: new 
Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets</title>
<link>https://arxiv.org/abs/2512.11909</link>
<guid>https://arxiv.org/abs/2512.11909</guid>
<content:encoded><![CDATA[
<div> Keywords: causal reasoning, large language models, human comparison, collider graph, Bayesian modeling<br /><br />Summary:<br />The paper investigates the nature of intelligence through the lens of causal reasoning, focusing on comparing how humans and large language models (LLMs) perform on identical reasoning tasks. The authors explore three key questions: (Q1) whether LLMs align with humans when given the same causal reasoning tasks; (Q2) if both LLMs and humans show consistency in reasoning at the task level; and (Q3) whether they demonstrate distinct reasoning signatures. To address these, over 20 LLMs are evaluated on eleven causal reasoning tasks formalized using a collider graph structure ($C_1 \to E \leftarrow C_2$), analyzed under two conditions: a Direct one-shot task requiring a probability judgment of a query node, and a Chain of Thought (CoT) approach where the model reasons stepwise before answering. The judgments from models and humans are modeled using a leaky noisy-OR causal Bayesian network (CBN) with parameters $\theta = (b, m_1, m_2, p(C))$ bounded between 0 and 1, where $p(C)$ is a shared prior. Model selection is performed using the Akaike Information Criterion (AIC) to choose between a more parsimonious symmetric causal strength model ($m_1 = m_2$) and a more flexible asymmetric variant ($m_1 \neq m_2$). This study advances understanding of parallels and divergences in causal reasoning between humans and AI models. <div>
arXiv:2512.11909v1 Announce Type: new 
Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $\theta=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis</title>
<link>https://arxiv.org/abs/2512.11912</link>
<guid>https://arxiv.org/abs/2512.11912</guid>
<content:encoded><![CDATA[
<div> Keywords: robustness, low-quality data, autoregressive language models, diffusion models, information theory<br /><br />Summary:<br /><br />This study systematically compares how modern probabilistic models respond to low-quality data, revealing a wide range of robustness among them. First, it shows that autoregressive language models, such as GPT-2, maintain high resilience even when subjected to 50% token corruption, with only a modest increase in test negative log likelihood (NLL) from 2.87 to 3.59. Second, class-conditional diffusion models are found to suffer catastrophic performance drops under similar data corruption, with image-label consistency falling by 56.81% relative to baseline. Third, classifiers experience moderate impacts from data corruption, but the severity lessens as the dataset size grows. Fourth, the paper interprets these differences through the lenses of information theory, PAC learning, and gradient dynamics, offering a theoretical framework for understanding robustness. Finally, it identifies two primary principles influencing robustness: the richness of conditioning information, which constrains the learning complexity, and the absolute information content of training data, which enables correct signals to prevail over noise. Together, these insights help explain why some probabilistic models are more robust to low-quality data than others. <div>
arXiv:2512.11912v1 Announce Type: new 
Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving</title>
<link>https://arxiv.org/abs/2512.11920</link>
<guid>https://arxiv.org/abs/2512.11920</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, KV-cache, CXL, FPGA acceleration, memory disaggregation  

<br /><br />Summary:  
This paper addresses the significant memory challenges posed by KV-caches during the autoregressive decoding of Large Language Models (LLMs) in datacenter environments, which limit batch sizes and system throughput. To overcome these limitations, the authors propose CXL-SpecKV, a novel architecture combining Compute Express Link (CXL) interconnects and FPGA accelerators for efficient memory disaggregation and speculative execution. First, CXL-SpecKV includes a memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, relieving GPU memory pressure. Second, it employs a speculative KV-cache prefetching mechanism that predicts and preloads future token cache entries to reduce stalls during decoding. Third, the system integrates an FPGA-accelerated compression and decompression engine for KV-caches, achieving up to 4× reduction in memory bandwidth usage. Evaluated on modern LLMs, CXL-SpecKV demonstrates up to 3.2× higher throughput compared to GPU-only baselines, while cutting memory costs by 2.8× without sacrificing model accuracy. This work showcases how combining intelligent memory disaggregation and speculative execution can help overcome the memory wall, enabling more scalable and efficient large-scale LLM serving. The implementation has been made publicly available to facilitate further research and development. <div>
arXiv:2512.11920v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org</title>
<link>https://arxiv.org/abs/2512.11935</link>
<guid>https://arxiv.org/abs/2512.11935</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, materials discovery, open-source LLMs, AGAPI, multi-step workflows<br /><br />Summary:  
Artificial intelligence is transforming scientific discovery but its application in materials research is hindered by fragmented computational tools, challenges in reproducibility, and reliance on proprietary large language models (LLMs). To address these limitations, the authors introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates over eight open-source LLMs with more than twenty materials-science APIs, creating a unified ecosystem combining databases, simulation tools, and machine learning models. AGAPI features an Agent-Planner-Executor-Summarizer architecture enabling autonomous construction and execution of complex multi-step workflows, covering tasks such as materials data retrieval, graph neural network-based property prediction, force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. The platform is demonstrated through practical applications like heterostructure creation, powder X-ray diffraction analysis, and semiconductor defect engineering that require up to ten sequential operations. To validate AGAPI, the authors test it with over 30 example prompts and compare outputs generated with and without tool access against experimental results. With a growing user base exceeding 1,000 active users, AGAPI offers a scalable, transparent, and reproducible foundation for AI-driven materials discovery. The AGAPI-Agents source code is openly available at https://github.com/atomgptlab/agapi. <div>
arXiv:2512.11935v1 Announce Type: new 
Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play</title>
<link>https://arxiv.org/abs/2512.11942</link>
<guid>https://arxiv.org/abs/2512.11942</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergame theory, multi-agent systems, bounded rationality, answer-set programming, strategic AI<br /><br />Summary:  
This work addresses the challenge of representing and analyzing games where players have differing perceptions, incomplete information, and bounded rationality, leading to private, subjective views of the game scenario. Traditional game-theoretic models often neglect such heterogeneity, but hypergame theory offers a formal mathematical framework to capture mismatched mental models among players. Despite increasing interest, the practical adoption of hypergames in multi-agent research has been limited by the absence of a unified, formal, and scalable representation language and solution algorithms. To fill this gap, the authors introduce a declarative, logic-based domain-specific language designed to encode hypergame structures and their solution concepts effectively. Utilizing answer-set programming, they develop an automated system capable of instantiating hypergame structures and implementing a novel hypergame rationalisation procedure, which finds belief configurations justifying seemingly irrational behaviors. This language and methodology provide a unifying formalism for hypergames, enabling the creation of heterogeneous reasoners based on belief models with verifiable logical guarantees. Overall, the paper establishes a critical link connecting hypergame theory to multi-agent systems and strategic AI, advancing the ability to model, reason about, and compute equilibria in complex, belief-driven strategic interactions. <div>
arXiv:2512.11942v1 Announce Type: new 
Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion</title>
<link>https://arxiv.org/abs/2512.11997</link>
<guid>https://arxiv.org/abs/2512.11997</guid>
<content:encoded><![CDATA[
<div> Keywords: system logs, anomaly detection, enrichment, retrieval-augmented generation, log analysis<br /><br />Summary: System logs play a vital role in monitoring and managing distributed systems by offering insights into failures and anomalous behaviors. Traditional log analysis methods, such as template-based and sequence-driven approaches, often lose crucial semantic details or face challenges with ambiguous log patterns. To overcome these limitations, the article introduces EnrichLog, a training-free, entry-based anomaly detection framework designed to enrich raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog integrates contextual information, including historical examples and reasoning derived from the entire corpus, to enable more accurate and interpretable anomaly detection without requiring retraining. The framework employs retrieval-augmented generation to seamlessly incorporate relevant contextual knowledge during inference. The evaluation of EnrichLog on four large-scale system log benchmark datasets, compared against five baseline methods, demonstrates consistent improvements in anomaly detection performance. The framework effectively manages ambiguous log entries while maintaining efficient inference speeds. Additionally, combining corpus- and sample-specific knowledge boosts the model’s confidence and detection accuracy. These attributes make EnrichLog a promising and practical solution for real-world anomaly detection deployments in system log analysis. <div>
arXiv:2512.11997v1 Announce Type: new 
Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp</title>
<link>https://arxiv.org/abs/2512.12048</link>
<guid>https://arxiv.org/abs/2512.12048</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent coordination, electric vehicle charging, deep reinforcement learning, context-aware decision-making, sustainable transportation  

<br /><br />Summary:  
This paper introduces CAMAC-DRA, a novel context-sensitive multi-agent coordination framework designed to optimize dynamic resource allocation in smart electric vehicle (EV) charging ecosystems via the Smart2Charge application. It coordinates autonomous charging agents over networks involving 250 EVs and 45 charging stations while adapting to varying environmental conditions using context-aware decision-making. The system incorporates Deep Q-Networks combined with Graph Neural Networks and attention mechanisms to process 20 contextual features, such as weather, traffic, grid load, and electricity prices. CAMAC-DRA balances the interests of five stakeholders—EV users (25%), grid operators (20%), charging station operators (20%), fleet operators (20%), and environmental factors (15%)—through weighted coordination and consensus protocols. Validation on real-world data with 441,077 charging transactions demonstrates superior results compared to baseline algorithms like DDPG, A3C, PPO, and other GNN-based methods, achieving a 92% coordination success rate, 15% energy efficiency gain, 10% cost reduction, 20% decrease in grid strain, and 2.3x faster convergence. The system maintains 88% training stability and 85% sample efficiency. Real-world tests confirm commercial viability with a Net Present Cost of -$122,962 and a 69% reduction in costs through renewable energy integration. Overall, CAMAC-DRA offers a breakthrough in intelligent EV charging coordination by balancing multiple stakeholder objectives while adapting in real-time for sustainable transportation electrification. <div>
arXiv:2512.12048v1 Announce Type: new 
Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification</title>
<link>https://arxiv.org/abs/2512.12059</link>
<guid>https://arxiv.org/abs/2512.12059</guid>
<content:encoded><![CDATA[
<div> Forecast monitoring, Large Language Models, time series forecasting, anomaly detection, multi-modal learning<br /><br />Summary:  
This paper introduces The Forecast Critic, a system that utilizes Large Language Models (LLMs) for automated monitoring of time series forecasts, aimed at improving accuracy in large-scale retail businesses. The study investigates three key questions: whether LLMs can identify obviously unreasonable forecasts, how effectively they incorporate unstructured exogenous features for assessment, and how their performance varies with model size and reasoning capabilities. Through three experiments on synthetic and real-world data, results show that LLMs can reliably detect forecast errors such as temporal misalignment, trend inconsistencies, and spikes, achieving an F1 score of 0.88 compared to human-level 0.97. Additionally, multi-modal LLMs that process unstructured contextual information, like historical promotions, enhance the detection of spurious or missing promotional spikes with an F1 score of 0.84. The approach is validated on the real-world M5 dataset, where forecasts flagged as unreasonable have significantly higher error metrics (sCRPS at least 10% higher) than reasonable ones. The findings suggest LLMs, even without domain-specific fine-tuning, provide an effective and scalable solution for automated forecast evaluation and monitoring in retail forecasting contexts. <div>
arXiv:2512.12059v1 Announce Type: new 
Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations</title>
<link>https://arxiv.org/abs/2512.12088</link>
<guid>https://arxiv.org/abs/2512.12088</guid>
<content:encoded><![CDATA[
<div> Keywords: Reliable Policy Iteration, policy iteration, function approximation, deep reinforcement learning, control tasks<br /><br />Summary:  
This paper evaluates the robustness of Reliable Policy Iteration (RPI), an algorithm previously proposed to restore the monotonicity-of-value-estimates property of policy iteration within the function approximation context. The research focuses on two classical control environments: CartPole and Inverted Pendulum. RPI's performance is compared against established deep reinforcement learning algorithms including DQN, Double DQN, DDPG, TD3, and PPO. Experimental results demonstrate that RPI achieves near-optimal policy performance early during training and maintains its stability throughout the training process. The study underscores the challenges commonly faced by traditional deep RL methods, such as sample inefficiency, instability during training, and sensitivity to hyperparameters. By contrast, RPI shows greater reliability, suggesting it as a promising alternative for tasks requiring consistent and stable policy improvement. The findings indicate that RPI could be a valuable approach for enhancing the dependability and effectiveness of deep reinforcement learning in control domains. <div>
arXiv:2512.12088v1 Announce Type: new 
Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective</title>
<link>https://arxiv.org/abs/2512.12175</link>
<guid>https://arxiv.org/abs/2512.12175</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context learning, label consistency, Bayesian view, label propagation, prompt demonstration selection  

<br /><br />Summary: Large language models (LLMs) perform in-context learning (ICL) by using minimal supervised examples, which improves many NLP tasks. Current methods select prompt demonstrations by retrieving the top-K semantically similar examples, but they often fail to ensure label consistency among these examples. This paper rethinks ICL from both a Bayesian perspective and a transductive label propagation viewpoint, treating ICL as a transductive learning task. By linking label consistency to propagation error bounds, the authors establish a label propagation framework that highlights the importance of consistent labels in guiding query concept understanding. To address inconsistency, the paper introduces a data synthesis method that leverages both semantic and label information to create more reliable demonstrations. They then propose TopK sampling with Synthetic Data (TopK-SD) as a novel approach for demonstration selection, which yields examples with consistent labels. Experiments show that TopK-SD outperforms conventional TopK semantic retrieval methods across multiple benchmarks. Overall, this work offers a fresh theoretical understanding of how prompt demonstration selection impacts ICL and proposes practical improvements to enhance label consistency, thereby improving downstream NLP performance. <div>
arXiv:2512.12175v1 Announce Type: new 
Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation</title>
<link>https://arxiv.org/abs/2512.12177</link>
<guid>https://arxiv.org/abs/2512.12177</guid>
<content:encoded><![CDATA[
<div> Keywords: indoor navigation, visual impairments, foundation model, knowledge graph, large language model  

<br /><br />Summary:  
1. Indoor navigation poses significant challenges for people with visual impairments, particularly in dynamic environments where current infrastructure-based systems fall short.  
2. The study introduces Floorplan2Guide, a novel navigation approach that leverages a foundation model to convert floor plans into navigable knowledge graphs and generate user-friendly navigation instructions.  
3. Floorplan2Guide incorporates a large language model (LLM) to automatically extract spatial information from architectural layouts, significantly reducing manual preprocessing compared to previous floorplan parsing methods.  
4. Experimental results show that few-shot learning enhances navigation accuracy over zero-shot learning in both simulated and real-world settings.  
5. Among evaluated models, Claude 3.7 Sonnet achieved the highest accuracy with 92.31%, 76.92%, and 61.54% success rates on short, medium, and long routes respectively, using 5-shot prompting on the MP-1 floor plan.  
6. The graph-based spatial representation improved navigation success rates by 15.4% over direct visual reasoning, confirming that combining graphical structures with in-context learning boosts performance.  
7. These findings demonstrate that Floorplan2Guide offers a more precise and effective indoor navigation solution tailored for Blind and Low Vision (BLV) users. <div>
arXiv:2512.12177v1 Announce Type: new 
Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2512.12182</link>
<guid>https://arxiv.org/abs/2512.12182</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graph Completion, Few-Shot Learning, Diffusion Model, Attention Mechanism, Long-Tailed Distribution  

<br /><br />Summary: This paper addresses the challenge of knowledge graph (KG) completion under few-shot learning conditions, particularly focusing on the long-tailed distribution of relations found in real-world data. The authors identify the limitations of prior approaches, such as metric matching and meta learning, which either insufficiently leverage the neighborhood information in the graph or ignore the unique distributional characteristics of contrastive signals. To overcome these issues, the paper proposes a novel generative representation framework that combines a two-stage attention triple enhancer with a U-shaped Knowledge Augmented Network (U-KAN) based diffusion model. This approach enhances the representation of triples while effectively capturing relational context in a generative manner. Extensive empirical evaluations on two public datasets demonstrate that the proposed method achieves new state-of-the-art results in few-shot knowledge graph completion tasks. The study highlights the importance of integrating neighborhood information and generative modeling for improved KG completion, especially in scenarios suffering from data scarcity and long-tail relation distribution. <div>
arXiv:2512.12182v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Theory of Cognition</title>
<link>https://arxiv.org/abs/2512.12225</link>
<guid>https://arxiv.org/abs/2512.12225</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive manifold, Riemannian metric, dual-process cognition, gradient flow, artificial intelligence<br /><br />Summary:  
1. The paper proposes a unified mathematical framework for human cognition, modeling it as a point on a differentiable manifold equipped with a learned Riemannian metric that captures representational constraints, computational costs, and variable interrelations.  
2. It defines a scalar cognitive potential incorporating predictive accuracy, task utility, parsimony, and normative requirements, which governs cognitive state evolution via Riemannian gradient flow, providing a universal dynamical law for cognition.  
3. This geometric framework naturally explains classical dual-process phenomena — fast intuitive judgments and slower deliberative reasoning — as emergent properties resulting from anisotropies in the metric that create intrinsic time-scale separations and phase transitions, eliminating the need for modular or hybrid cognitive models.  
4. The authors provide analytical conditions delineating these regimes and validate their approach through simulations of canonical cognitive tasks, demonstrating behavioral signatures consistent with observed psychological effects.  
5. The work establishes a foundational geometric theory of cognition with potential implications for developing more general and human-like artificial intelligence systems, suggesting that cognitive processes can be unified and modeled through geometric principles and continuous dynamical laws. <div>
arXiv:2512.12225v1 Announce Type: new 
Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure</title>
<link>https://arxiv.org/abs/2512.12260</link>
<guid>https://arxiv.org/abs/2512.12260</guid>
<content:encoded><![CDATA[
<div> Keywords: ontology design, Wikidata, polyhierarchy, multi-axial classification, knowledge graphs<br /><br />Summary:<br /><br />This article contrasts traditional ontology design with Wikidata’s unique approach. Traditional ontologies rely on strict, disjoint top-level categories, such as continuant vs. occurrent or abstract vs. concrete, to create unified hierarchical structures where each entity belongs to a single upper-level class. In contrast, Wikidata does not impose a singular foundational taxonomy; instead, it supports multiple classification axes simultaneously. This results in a polyhierarchical and multi-axial structure rooted under a general class called entity. The paper explores how this design influences the architecture and usability of large-scale knowledge bases. Wikidata’s approach facilitates scalability and modularity in ontology construction, making it especially effective for collaborative environments where the ontology evolves over time. By allowing entities to be classified along multiple dimensions without enforcing mutual exclusivity, Wikidata supports more flexible and adaptive knowledge representations. This contributes to managing complex and diverse data typical of large knowledge graphs. Overall, the paper highlights the advantages of Wikidata’s polyhierarchical model in building collaborative, evolving ontologies that differ fundamentally from traditional rigid taxonomy-based designs. <div>
arXiv:2512.12260v1 Announce Type: new 
Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases</title>
<link>https://arxiv.org/abs/2512.12288</link>
<guid>https://arxiv.org/abs/2512.12288</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, materials discovery, multi-fidelity learning, active validation, quantum-mechanical descriptors<br /><br />Summary:<br /><br />1. Conventional generative models for materials discovery rely heavily on Density Functional Theory (DFT) data, which use approximate exchange-correlation functionals and often fail in strongly correlated systems, limiting their exploration capacity and accuracy.<br /><br />2. The authors propose a quantum-aware generative AI framework that integrates multi-fidelity learning and active validation to overcome the shortcomings of DFT-based models.<br /><br />3. Their approach employs a diffusion-based generative model conditioned on quantum-mechanical descriptors, combined with an equivariant neural network potential acting as a validator that is trained on a hierarchical dataset spanning multiple theoretical levels (PBE, SCAN, HSE06, CCSD(T)).<br /><br />4. A robust active learning loop quantifies divergence between low- and high-fidelity predictions, focusing computational resources on regions where DFT is less reliable.<br /><br />5. Through comprehensive ablation studies and failure mode analyses, the framework was benchmarked against leading generative models (CDVAE, GNoME, DiffCSP), demonstrating 3-5 times higher success rates in identifying stable material candidates in challenging, high-divergence regions such as correlated oxides, while maintaining computational feasibility.<br /><br />This work extends the effective search space for computational materials discovery by addressing inherent limitations of single-fidelity DFT models with a transparent, multi-fidelity, and actively validated generative AI framework. <div>
arXiv:2512.12288v1 Announce Type: new 
Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Collapse: A Universal Failure Mode of Intelligent Systems</title>
<link>https://arxiv.org/abs/2512.12381</link>
<guid>https://arxiv.org/abs/2512.12381</guid>
<content:encoded><![CDATA[
<div> entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality<br /><br />Summary:  
This article explores a universal failure mode in intelligent systems termed "entropy collapse," which occurs when feedback amplification surpasses the system's capacity for novelty regeneration. The authors demonstrate that intelligent systems, across diverse domains such as artificial intelligence, economics, and biological evolution, experience a sharp transition from a high-entropy, adaptive state to a low-entropy, collapsed state. This collapse does not imply a total loss of activity but rather a contraction in the effective adaptive dimensionality, signifying rigidity and reduced adaptability. The study formally models this phenomenon, identifying critical thresholds, irreversibility in dynamics, and stable attractor structures that characterize the collapse. Through analytical and minimal simulation approaches, the work shows the universality of entropy collapse across different update mechanisms, unifying observed issues like model collapse in AI, institutional stagnation in economics, and genetic bottlenecks in evolution under a common framework. Importantly, the article reframes entropy collapse as an inherent structural cost of increasing intelligence, explaining why late-stage interventions often fail. Finally, it advocates for entropy-aware design principles to help maintain adaptability and robustness over the long term in intelligent systems. <div>
arXiv:2512.12381v1 Announce Type: new 
Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feeling the Strength but Not the Source: Partial Introspection in LLMs</title>
<link>https://arxiv.org/abs/2512.12411</link>
<guid>https://arxiv.org/abs/2512.12411</guid>
<content:encoded><![CDATA[
<div> Keywords: introspection, language models, injected concepts, robustness, Meta-Llama-3.1-8B-Instruct<br /><br />Summary: This work evaluates Anthropic's claim that language models can detect and name injected "concepts" as activation directions within their internal representations. First, the study reproduces Anthropic's multi-turn "emergent introspection" on the Meta-Llama-3.1-8B-Instruct model, confirming that the model identifies and names the injected concept about 20% of the time, matching previous reported results and showing that such introspection is not limited to very large or highly capable models. Second, the researchers test the robustness of this introspection by varying the inference prompt and performing related tasks like multiple-choice identification or binary discrimination to detect concept injection; performance significantly deteriorates, indicating that introspection is fragile and highly sensitive to prompt design. Third, the study discovers a contrasting "partial introspection" capability in which the same model reliably classifies the strength of the injected concept vector's coefficient into four categories (weak, moderate, strong, very strong) with up to 70% accuracy, well above random chance. Overall, the results support that language models internally compute functions related to their baseline representations during introspection, but the fidelity of their self-reports is narrow and prompt-dependent. The authors provide their code publicly for further research and verification. <div>
arXiv:2512.12411v1 Announce Type: new 
Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale</title>
<link>https://arxiv.org/abs/2512.12413</link>
<guid>https://arxiv.org/abs/2512.12413</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Critical Thinking, Verification, AI Use Scale, Fact-Checking<br /><br />Summary:<br /><br />The article addresses the importance of critical thinking when using generative AI tools, emphasizing that users must verify and reflect on AI-generated content rather than accept it uncritically. Researchers developed and validated a 13-item scale measuring critical thinking in AI use, encompassing three factors: Verification, Motivation, and Reflection. The scale was created and content-validated in Study 1, while Study 2 confirmed its three-factor structure. Studies 3 to 5 further validated the scale by demonstrating its reliability, strong factor loadings, sex invariance, and both convergent and discriminant validity. Additionally, Studies 3 and 4 found positive correlations between critical thinking in AI use and personality traits such as openness, extraversion, and positive affect, as well as with increased frequency of AI usage. Study 6 established criterion validity, linking higher scale scores to more frequent and varied verification strategies, better accuracy in veracity judgments within a ChatGPT fact-checking task, and deeper reflection on responsible AI use. Overall, the research clarifies mechanisms by which people oversee generative AI outputs and offers a validated tool and task paradigm for future theoretical, cross-group, and longitudinal studies on engaging critically with AI-generated information. <div>
arXiv:2512.12413v1 Announce Type: new 
Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline</title>
<link>https://arxiv.org/abs/2512.12443</link>
<guid>https://arxiv.org/abs/2512.12443</guid>
<content:encoded><![CDATA[
<div> Keywords: AI model documentation, transparency framework, safety evaluation, multi-agent pipeline, compliance gaps  

<br /><br />Summary:  
This study addresses the fragmentation and inconsistency of AI model documentation across platforms, which hinders stakeholders such as policymakers, auditors, and users from effectively assessing safety claims, data provenance, and version-level changes. The analysis covered documentation from five leading models—Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5—alongside 100 Hugging Face model cards, revealing 947 unique section names with high naming variability. Usage information was labeled under 97 distinct titles alone. To standardize transparency, the researchers developed a weighted framework based on the EU AI Act Annex IV and Stanford Transparency Index, comprising 8 main sections and 23 subsections, prioritizing safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical details. An automated multi-agent pipeline was created to extract and score documentation completeness using LLM-based consensus. The evaluation of 50 models from various domains including vision, multimodal, open-source, and closed-source models cost under $3 and showed consistent transparency gaps. Frontier labs such as xAI, Microsoft, and Anthropic scored around 80% compliance, whereas most providers fell below 60%. Significant deficits were observed in safety-critical areas, especially regarding deception behaviors, hallucinations, and child safety assessments, which contributed to the largest aggregate points lost across models. <div>
arXiv:2512.12443v1 Announce Type: new 
Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.12477</link>
<guid>https://arxiv.org/abs/2512.12477</guid>
<content:encoded><![CDATA[
<div> Keywords: Node Importance Estimation, Heterogeneous Knowledge Graphs, Meta-path, Hypergraph Contrastive Learning, Cross-modal Alignment  

<br /><br />Summary:  
Node importance estimation (NIE) in heterogeneous knowledge graphs is vital for tasks like recommendation, knowledge reasoning, and question answering. Existing methods typically rely on pairwise connections, missing out on high-order dependencies spanning multiple entities and relations. They also treat structural and semantic information separately, which limits the integration of these complementary signals. To overcome these issues, the authors propose MetaHGNIE, a novel framework based on meta-path induced hypergraph contrastive learning, which disentangles and aligns structural and semantic features effectively. MetaHGNIE constructs a higher-order knowledge graph using meta-path sequences, where typed hyperedges represent multi-entity relational contexts, capturing complex interactions. Structural dependencies are aggregated via local attention mechanisms, while semantic information is encoded through a hypergraph transformer enhanced with sparse chunking to minimize redundancy. A multimodal fusion module then integrates structural and semantic embeddings using contrastive learning with auxiliary supervision to ensure robust cross-modal alignment. Extensive experiments on standard NIE benchmarks demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines, validating the advantage of explicitly modeling higher-order interactions and facilitating cross-modal alignment in heterogeneous knowledge graphs. The authors provide their implementation publicly, supporting reproducibility and further research. <div>
arXiv:2512.12477v1 Announce Type: new 
Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.12501</link>
<guid>https://arxiv.org/abs/2512.12501</guid>
<content:encoded><![CDATA[
<div> Generative AI, text-to-image, ethical safeguards, Trustworthy AI, fairness-aware training<br /><br />Summary:  
1. The paper addresses the dual-use dilemma of generative AI text-to-image systems like DALL.E, Stable Diffusion, and Midjourney, highlighting ethical concerns including amplification of societal biases, production of disinformation, and intellectual property violations.  
2. It introduces SafeGen, a novel framework embedding ethical safeguards directly into the generation pipeline, designed following Trustworthy AI principles to ensure responsible use.  
3. SafeGen consists of two core components: BGE-M3, a fine-tuned multilingual (English-Vietnamese) text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model generating high-fidelity, semantically accurate images.  
4. The framework is trained on a curated multilingual dataset with a fairness-aware approach, successfully balancing creative freedom with ethical responsibility within one cohesive workflow.  
5. Quantitative results demonstrate strong performance, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 attains an F1-score of 0.81. Ablation studies confirm the value of domain-specific fine-tuning for both modules. Case studies show SafeGen’s practical benefits in blocking unsafe content, creating inclusive educational materials, and supporting academic integrity. <div>
arXiv:2512.12501v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs</title>
<link>https://arxiv.org/abs/2512.12503</link>
<guid>https://arxiv.org/abs/2512.12503</guid>
<content:encoded><![CDATA[
<div> KidsArtBench, Multimodal LLMs, Artistic Evaluation, Multi-LoRA, Educational AI<br /><br />Summary:<br /><br />1. The paper addresses the limited capability of Multimodal Large Language Models (MLLMs) to evaluate artistic expression, particularly focusing on children's artwork.  
2. It introduces KidsArtBench, a novel benchmark dataset consisting of over 1,000 artworks created by children aged 5-15, annotated by 12 expert educators across 9 rubric-aligned evaluation dimensions, accompanied by expert feedback comments.  
3. Unlike existing aesthetic datasets that provide single scalar scores on adult art, KidsArtBench provides multi-dimensional, ordinal annotations and formative feedback specifically tailored for children's artwork.  
4. The authors propose an attribute-specific multi-LoRA method, assigning each attribute (e.g., Realism, Imagination) to separate evaluation dimensions, combined with Regression-Aware Fine-Tuning (RAFT) to better align model predictions with ordinal scales.  
5. Experimental results on the Qwen2.5-VL-7B model demonstrate significant performance improvements, with correlation increasing from 0.468 to 0.653, especially in perceptual attributes and diminishing gaps in higher-order traits.  
6. These findings indicate that educator-aligned supervision and attribute-aware training lead to evaluations that are pedagogically meaningful, establishing KidsArtBench as a rigorous testbed for continuous progress in educational AI.  
7. The dataset, code, and ethical documentation are made publicly available to support further research in this domain. <div>
arXiv:2512.12503v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2512.12548</link>
<guid>https://arxiv.org/abs/2512.12548</guid>
<content:encoded><![CDATA[
<div> Keywords: Patch foraging, Marginal Value Theorem, model-based reinforcement learning, anticipatory behavior, ecological optimality principles  

<br /><br />Summary:  
1. The article investigates patch foraging behavior, which involves deciding the optimal moment to leave a resource-rich area to search for potentially better options.  
2. The Marginal Value Theorem (MVT) is a key theoretical model often used to describe optimal patch-leaving strategies in behavioral ecology.  
3. Despite MVT’s widespread use, the underlying computational mechanisms by which biological foragers achieve these optimal decisions are not fully understood.  
4. The study demonstrates that artificial agents equipped with learned predictive world models, using model-based reinforcement learning, naturally develop strategies that align with MVT predictions.  
5. These model-based agents rely on anticipatory abilities rather than simple reward maximization to execute efficient patch-leaving, differentiating them from standard model-free RL agents.  
6. The decision patterns of these agents closely mirror those observed in biological foragers, suggesting that predictive world models provide a biologically plausible basis for decision-making in AI.  
7. Overall, the research emphasizes the importance of ecological optimality principles, like MVT, in designing AI systems that are both interpretable and adaptive, bridging gaps between biological insights and artificial decision processes. <div>
arXiv:2512.12548v1 Announce Type: new 
Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Newsvendor: Decision Biases and Cognitive Mechanisms</title>
<link>https://arxiv.org/abs/2512.12552</link>
<guid>https://arxiv.org/abs/2512.12552</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cognitive biases, newsvendor problem, decision-making, supply chain management  

<br /><br />Summary: This study explores how large language models (LLMs) replicate and even amplify human cognitive biases in decision-making, particularly in supply chain management through the dynamic newsvendor problem. The researchers conducted multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, examining five known decision biases. They found that all models consistently exhibited the classic "Too Low/Too High" ordering bias and amplified behaviors such as demand-chasing compared to human benchmarks. Notably, GPT-4, the most sophisticated model, displayed the highest irrationality due to overthinking, while GPT-4o, optimized for efficiency, performed near-optimally. These biases persisted even when optimal decision-making formulas were provided, indicating that the origins lie in architectural constraints rather than lack of knowledge. From a managerial perspective, the findings suggest that selecting an AI model should be task-specific since simpler, efficiency-tuned models may outperform complex ones for certain optimization tasks. The amplification of biases by LLMs underlines the importance of robust human-in-the-loop oversight in high-stakes operational decisions to avoid costly errors. Additionally, designing structured, rule-based prompts emerges as an effective approach for managers to limit heuristic-driven errors and enhance AI-assisted decision reliability. <div>
arXiv:2512.12552v1 Announce Type: new 
Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</title>
<link>https://arxiv.org/abs/2512.12597</link>
<guid>https://arxiv.org/abs/2512.12597</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, explainability, Shapley values, tool attribution, Monte Carlo sampling<br /><br />Summary:<br /><br />This paper introduces AgentSHAP, a novel explainability framework designed to assess the contribution of external tools used by large language model (LLM) agents in solving complex tasks. Unlike existing explainability methods, AgentSHAP focuses on tool-level importance and is model-agnostic, operating as a black-box method applicable across various LLMs such as GPT, Claude, and Llama without requiring access to internal model parameters or gradients. The core technical innovation is the use of Monte Carlo Shapley values from game theory, which fairly allocate importance scores by evaluating agent outputs across different subsets of tools. This approach significantly reduces computational complexity from an exponential O(2^n) scale to a manageable level, enabling practical application. The authors provide extensive experiments on the API-Bank benchmark, demonstrating that AgentSHAP yields consistent importance scores, accurately identifies relevant tools, and effectively distinguishes between useful and irrelevant tools involved in generating responses. AgentSHAP complements existing XAI methods like TokenSHAP and PixelSHAP, thus expanding the family of Shapley-based explainability tools tailored for modern generative AI systems. The authors also offer open-source implementation at https://github.com/GenAISHAP/TokenSHAP, fostering further research and adoption. <div>
arXiv:2512.12597v1 Announce Type: new 
Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2512.12634</link>
<guid>https://arxiv.org/abs/2512.12634</guid>
<content:encoded><![CDATA[
<div> Mobile GUI Agents, Benchmarking, Offline Evaluation, Modular Analysis, Mobile Applications<br /><br />Summary:<br /><br />1. Mobile GUI Agents are AI systems designed to interact with mobile applications on behalf of users, aiming to revolutionize human-computer interaction. 2. Current evaluation methods for these agents have two main issues: reliance on either single-path offline benchmarks, which unfairly penalize valid alternate actions, or online benchmarks, which have poor scalability and reproducibility because of their dynamic nature. 3. Existing benchmarks treat agents as black boxes, ignoring the performance and contributions of individual components, which leads to unfair comparisons and hides key bottlenecks. 4. To overcome these problems, the authors introduce MobiBench, a first-of-its-kind modular and multi-path aware offline benchmarking framework that supports high-fidelity, scalable, and reproducible evaluation fully offline. 5. Experiments show MobiBench's evaluation aligns 94.72% with human judgments, comparable to advanced online benchmarks, while maintaining the reproducibility and scalability advantages of offline methods. 6. A detailed module-level analysis using MobiBench reveals insights into different techniques in mobile GUI agents, optimal module configurations for various model sizes, limitations of current large foundation models (LFMs), and offers practical guidelines to build more capable and cost-efficient mobile agents. <div>
arXiv:2512.12634v1 Announce Type: new 
Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Aware Multiagent Systems</title>
<link>https://arxiv.org/abs/2512.12652</link>
<guid>https://arxiv.org/abs/2512.12652</guid>
<content:encoded><![CDATA[
<div> value awareness, AI ethics, value alignment, formal semantics, explainability<br /><br />Summary:<br /><br />This paper introduces the novel concept of value awareness in artificial intelligence (AI), extending beyond the conventional value-alignment problem to foster a deeper understanding and integration of human values within AI systems. The authors propose a clear and streamlined roadmap for developing value-aware AI, structured around three fundamental pillars. First, the roadmap emphasizes learning and formally representing human values through the use of formal semantics, providing a rigorous foundation for encoding complex value systems. Second, it addresses ensuring value alignment not only for individual AI agents but also for multiagent systems, highlighting the challenges and methodologies for maintaining consistent value-driven behavior in diverse and interacting systems. Third, the framework includes the provision of value-based explainability, aiming to make AI behavior transparent and interpretable in terms of underlying human values. The paper additionally shares ongoing research efforts that explore these themes and demonstrates practical applications of the approach in real-world domains, showcasing the feasibility and relevance of value awareness in enhancing AI reliability, safety, and societal compatibility. This work thus contributes to advancing AI ethics by offering both theoretical and applied insights into embedding and explaining human values within intelligent systems. <div>
arXiv:2512.12652v1 Announce Type: new 
Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI</title>
<link>https://arxiv.org/abs/2512.12686</link>
<guid>https://arxiv.org/abs/2512.12686</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic memory, large language models, personalized AI, knowledge graph, session summarization<br /><br />Summary:  
The paper introduces Memoria, a modular memory framework designed to enhance large language model (LLM)-based conversational systems with agent-like persistence. Agentic memory is defined as the capability of LLMs to retain and act upon information across user interactions, supporting continuity, personalization, and long-term context. Memoria combines two main components: dynamic session-level summarization, which helps maintain short-term dialogue coherence, and a weighted knowledge graph (KG)-based user modeling engine that incrementally encodes user traits, preferences, and behaviors as structured entities and relationships. This hybrid design allows the framework to operate efficiently within the token limits of current LLMs while supporting both immediate conversational flow and evolving personalization over time. The system bridges the gap between stateless LLM interfaces and persistent agentic memory systems, enabling scalable and context-rich conversational AI experiences. Memoria offers a practical solution for deploying adaptive, personalized AI agents in industry applications that require sustained and evolving user engagement. <div>
arXiv:2512.12686v1 Announce Type: new 
Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title>
<link>https://arxiv.org/abs/2512.12692</link>
<guid>https://arxiv.org/abs/2512.12692</guid>
<content:encoded><![CDATA[
<div> Keywords: WebOperator, tree-search, backtracking, web navigation, LLM agents<br /><br />Summary:<br /><br />1. LLM-based agents for web tasks typically operate in a greedy, step-by-step manner without foresight, often causing troubles in partially observable web environments where errors are difficult to undo.<br />2. Existing tree-search methods enable structured exploration but assume all actions are reversible and lack safe backtracking mechanisms, leading to unintended side effects and reduced effectiveness.<br />3. WebOperator is introduced as a novel tree-search framework that allows reliable backtracking and strategic exploration to overcome these limitations.<br />4. The framework employs a best-first search strategy that ranks actions by expected rewards and safety, combined with a robust backtracking method that verifies path feasibility before replaying to avoid side effects.<br />5. WebOperator enhances exploration by generating diverse action candidates from multiple reasoning contexts and filters invalid or semantically redundant actions pre-execution.<br />6. Experimental results on WebArena and WebVoyager demonstrate WebOperator’s effectiveness, achieving a state-of-the-art 54.6% success rate on WebArena using gpt-4o, highlighting the importance of integrating strategic foresight with safe action execution in web-based LLM agents. <div>
arXiv:2512.12692v1 Announce Type: new 
Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12706</link>
<guid>https://arxiv.org/abs/2512.12706</guid>
<content:encoded><![CDATA[
<div> Keywords: automated game testing, reinforcement learning, structural coverage, large language models, gameplay validation<br /><br />Summary:<br /><br />The paper addresses challenges in automated game testing within the "Games as a Service" model, where frequent content updates demand rigorous quality assurance. It identifies a dichotomy in current methods: code-centric approaches emphasize structural code coverage but miss gameplay context, whereas player-centric agents focus on gameplay intent but lack thorough underlying code verification. To bridge this gap, the authors introduce SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that integrates structural verification with functional validation. SMART utilizes large language models (LLMs) to analyze abstract syntax tree (AST) changes and infer the functional intent behind code modifications. This insight enables the creation of a context-aware hybrid reward system that directs reinforcement learning agents to fulfill sequential gameplay goals while effectively exploring altered code branches. The framework is evaluated on two complex game environments, Overcooked and Minecraft. Results demonstrate that SMART surpasses state-of-the-art baselines by achieving over 94% branch coverage on modified code—nearly twice that of conventional reinforcement learning—while maintaining a 98% success rate in completing gameplay tasks. This balance of comprehensive structural coverage and functional correctness suggests SMART is a promising advance for automated game testing in rapid development cycles. <div>
arXiv:2512.12706v1 Announce Type: new 
Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks</title>
<link>https://arxiv.org/abs/2512.12736</link>
<guid>https://arxiv.org/abs/2512.12736</guid>
<content:encoded><![CDATA[
<div> Keywords: QoE prediction, demographic-aware, data augmentation, machine learning, 5G video streaming<br /><br />Summary:<br /><br />1. The paper addresses the challenge of predicting Quality of Experience (QoE) for adaptive video streaming in 5G networks, emphasizing the need for accurate and personalized QoE estimation to improve resource management and user-centric services.<br /><br />2. Traditional QoE prediction models are limited by small datasets and the assumption of uniform user perception, which do not reflect the diversity of real-world user experiences.<br /><br />3. To overcome these limitations, the authors propose a demographic-aware machine learning framework that incorporates behaviorally realistic, demographic-based data augmentation. This strategy amplifies a small initial QoE dataset sixfold by simulating varying user sensitivities to common streaming impairments such as rebuffering, bitrate fluctuations, and quality degradation.<br /><br />4. The augmented dataset is used to train and evaluate a range of machine learning models, including classical algorithms and advanced deep learning architectures like attention-based MLPs and TabNet.<br /><br />5. Experimental results showcase significant improvements in prediction accuracy measured by RMSE, MAE, and correlation metrics. Among all evaluated models, TabNet performs best due to its built-in feature selection and attention mechanisms.<br /><br />6. The study concludes that demographic-aware data augmentation significantly enhances QoE prediction robustness and scalability, paving the way for personalized QoE-aware intelligence in 5G video streaming systems. <div>
arXiv:2512.12736v1 Announce Type: new 
Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Counterfactuals Reconsidered</title>
<link>https://arxiv.org/abs/2512.12804</link>
<guid>https://arxiv.org/abs/2512.12804</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactuals, probabilistic causal models, Pearl semantics, Markov condition, causal abstractions

<br /><br />Summary:  
1. The paper develops a novel semantics for probabilities of counterfactuals that extends beyond the standard Pearlian framework, addressing probabilistic causal models that cannot be embedded into classical structural causal models.  
2. The need for this generalization is demonstrated by showing that such challenging probabilistic causal models appear even in simple scenarios.  
3. The proposed semantics offer a middle ground in the debate between Pearl and Dawid: agreeing with Dawid in rejecting universal causal determinism and unrealistic variables, while endorsing Pearl’s view that a general counterfactual semantics remains achievable.  
4. The focus is restricted to causal models satisfying the Markov condition, containing only realistic (observable or well-defined) variables, and being causally complete, improving the conceptual and practical foundations of counterfactual reasoning.  
5. Though formulated using structural causal models as Pearl does, the semantics avoid the use of response variables, making the approach simpler and more general. Additionally, the author proves equivalence with two recent alternative semantics not reliant on structural causal models and aligns the theory with broader literature on stochastic counterfactuals. Finally, the paper reflects on the universality of the Markov condition and proposes a novel extension of causal abstractions. <div>
arXiv:2512.12804v1 Announce Type: new 
Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution</title>
<link>https://arxiv.org/abs/2512.12806</link>
<guid>https://arxiv.org/abs/2512.12806</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fault-Tolerant Sandboxing, transactional filesystem, autonomous agents, safety risks  

<br /><br />Summary:  
This paper addresses the safety challenges posed by the transition of Large Language Models (LLMs) from passive code generation tools to autonomous agents capable of executing commands without user intervention. The authors identify significant risks including destructive commands and inconsistent system states that arise in fully autonomous operation. Existing commercial solutions focus on interactive user safety, often requiring authentication steps that disrupt the automated, headless workflows essential to true autonomy. To mitigate these issues, the paper proposes a Fault-Tolerant Sandboxing framework that combines a policy-based interception layer with a transactional filesystem snapshot mechanism. By encapsulating agent actions within atomic transactions, the framework ensures system safety while maintaining acceptable latency, thus avoiding the overhead and friction associated with containerization or CLI-based authentication. The method was experimentally validated using the Minimind-MoE LLM served through nano-vllm in a Proxmox testbed with EVPN/VXLAN network isolation. The results demonstrate perfect interception of high-risk commands and successful rollback of failed states, all achieved with a modest 14.5% performance overhead (~1.8 seconds per transaction). This outperforms existing Gemini CLI sandboxes that require interactive sign-ins, making the proposed framework better suited for autonomous agent workflows. <div>
arXiv:2512.12806v1 Announce Type: new 
Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title>
<link>https://arxiv.org/abs/2512.12856</link>
<guid>https://arxiv.org/abs/2512.12856</guid>
<content:encoded><![CDATA[
<div> Keywords: generative agents, memory management, forgetting policies, privacy preservation, agent evaluation<br /><br />Summary:<br /><br />This paper addresses critical challenges in memory management for generative agents operating in long-term interactive scenarios, highlighting performance, privacy, and computational issues. It critiques existing methods that either keep unlimited memory, causing computational and privacy problems, or use simplistic forgetting strategies that reduce agent coherence and capabilities. The authors introduce the Memory-Aware Retention Schema (MaRS), a novel framework designed for human-centered memory management that integrates six theoretically grounded forgetting policies. These policies aim to optimize the trade-offs between performance, privacy, and computational efficiency. To evaluate these approaches, the paper presents the Forgetful but Faithful Agent (FiFA) benchmark, which assesses agents on narrative coherence, goal achievement, social recall accuracy, privacy preservation, and cost efficiency. Experimental results from 300 evaluation runs across various memory budgets and agent configurations demonstrate that a hybrid forgetting policy within MaRS achieves a superior composite performance score of 0.911, offering improved coherence alongside strict privacy guarantees and computational tractability. The study establishes new benchmarks for evaluating memory-budgeted generative agents and offers practical guidelines for deploying such agents in resource-constrained and privacy-sensitive contexts. Overall, the theoretical insights, framework, and empirical findings contribute significantly to human-centered AI by tackling fundamental agent memory management challenges that influence user trust, scalability, and compliance with regulations. <div>
arXiv:2512.12856v1 Announce Type: new 
Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Satisfiability Modulo Theory Meets Inductive Logic Programming</title>
<link>https://arxiv.org/abs/2512.12918</link>
<guid>https://arxiv.org/abs/2512.12918</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive Logic Programming, SMT solver, numerical constraints, PyGol, hybrid rules<br /><br />Summary:<br /><br />This paper explores extending Inductive Logic Programming (ILP) by integrating it with Satisfiability Modulo Theories (SMT) to better handle numerical constraints within relational domains. Traditional ILP frameworks predominantly work with discrete predicates and require either discretisation or manually designed numerical predicates, which limits their ability to learn arithmetic relations or thresholds that apply jointly across examples. The authors propose a modular approach that couples the ILP system PyGol with the SMT solver Z3. PyGol generates candidate clauses interpreted as quantifier-free formulas over background theories like linear or nonlinear real arithmetic. The SMT solver then instantiates and verifies the numerical parameters, enabling the induction of hybrid rules combining symbolic relational predicates with numerical constraints such as thresholds, intervals, and multi-literal arithmetic relations. This architecture preserves ILP's declarative bias while increasing expressivity and flexibility. The SMT-ILP framework is formalized and evaluated on synthetic datasets designed to test linear, relational, nonlinear, and multi-hop reasoning capabilities. Experimental results demonstrate that the modular SMT-ILP approach complements existing numerical ILP methods and provides a solid foundation for future enhancements toward richer, theory-aware inductive learning. <div>
arXiv:2512.12918v1 Announce Type: new 
Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Open Standards for Systemic Complexity in Digital Forensics</title>
<link>https://arxiv.org/abs/2512.12970</link>
<guid>https://arxiv.org/abs/2512.12970</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, digital forensics, forensic errors, human-readable artifacts, open standards<br /><br />Summary:<br /><br />1. The paper discusses the increasing complexity and ubiquity of the intersection between artificial intelligence (AI) and digital forensics (DF), highlighting how overlapping techniques and technologies are being integrated into various scientific and technical fields. <br />2. It acknowledges that despite significant advancements in forensic sciences, errors remain prevalent, and forensic processes are still susceptible to human and systemic fallibility. <br />3. To address these challenges, the paper identifies systemic complexity as a key factor contributing to errors and proposes measures to mitigate this. <br />4. One major recommendation is the adoption of human-readable artifacts to improve transparency, interpretability, and trustworthiness in forensic analyses involving AI. <br />5. The study also advocates for the use of open standards to enhance consistency, reproducibility, and collaborative validation within the digital forensics community. <br />6. Finally, the paper outlines a state-of-the-art DF AI model schema designed to integrate these principles, aiming to reduce errors and improve the reliability of AI-assisted digital forensic investigations. <div>
arXiv:2512.12970v1 Announce Type: new 
Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization</title>
<link>https://arxiv.org/abs/2512.13070</link>
<guid>https://arxiv.org/abs/2512.13070</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-supervised reinforcement learning, policy collapse, M-GRPO, entropy filtering, Large Language Models<br /><br />Summary:<br /><br />This paper addresses a significant challenge in self-supervised reinforcement learning (RL) for enhancing Large Language Models (LLMs), identifying a critical failure mode termed "policy collapse" that leads to drastic performance degradation during long-horizon training. The authors demonstrate that increasing the number of rollouts delays but does not prevent this collapse, indicating that conventional scaling strategies are insufficient. To resolve this instability, they introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a novel framework that utilizes a slowly evolving momentum model to provide a stable training target, effectively stabilizing the training process. Additionally, the study finds that training instability is linked to a rapid decrease in policy entropy, causing the model to adopt a prematurely confident yet suboptimal policy. To counter this, the authors propose an adaptive interquartile range (IQR) filtering method that dynamically removes low-entropy trajectories, thereby preserving policy diversity essential for robust learning. Extensive experiments conducted on multiple reasoning benchmarks validate that M-GRPO combined with the IQR filter improves training stability and prevents premature convergence. Together, these innovations lead to state-of-the-art performance in self-supervised RL for reasoning tasks in LLMs, offering a valuable strategy for enhancing model reasoning without expensive human annotations. <div>
arXiv:2512.13070v1 Announce Type: new 
Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socratic Students: Teaching Language Models to Learn by Asking Questions</title>
<link>https://arxiv.org/abs/2512.13102</link>
<guid>https://arxiv.org/abs/2512.13102</guid>
<content:encoded><![CDATA[
<div> interactive learning, large language models, student-teacher interaction, Direct Preference Optimization, question generation<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) are strong at answering static queries based on their internal knowledge but struggle in settings requiring acquisition of new information through active interaction.  
2. In practical domains like education and medical assistance, information is often not directly available, necessitating an interactive agent that can identify its uncertainty, ask relevant questions, and retain newly acquired knowledge efficiently.  
3. Previous research has mainly focused on how teachers can guide students by identifying knowledge gaps and giving instructions, whereas this work shifts attention to the student’s role in actively querying the teacher.  
4. Experimental results on math and coding benchmarks demonstrate that student-led querying strategies consistently outperform static approaches, showing improvements of at least 0.5 in Pass@k metrics starting from near-zero baseline performance.  
5. To further improve question quality and learning efficiency, the authors train students using Direct Preference Optimization (DPO) with guidance sourced from either the model itself (self) or stronger student models, enabling smaller models to learn better questioning strategies. <div>
arXiv:2512.13102v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning</title>
<link>https://arxiv.org/abs/2512.13131</link>
<guid>https://arxiv.org/abs/2512.13131</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D gesture generation, speech-driven animation, hierarchical modeling, periodic autoencoders, co-speech gestures<br /><br />Summary:<br /><br />This paper addresses the challenge of generating realistic 3D body movements from speech, which has potential applications but struggles with producing natural human-like gestures. Existing approaches mainly use end-to-end learning models such as GANs, VQ-VAE, and diffusion models but often fail to capture the crucial inter- and intra-correlations between different motion units like the head, body, and hands. To overcome this, the authors propose a Hierarchical Implicit Periodicity (HIP) learning framework. The HIP approach features two key innovations: first, it disentangles complex gesture movements by exploring gesture motion phase manifolds using periodic autoencoders, which model human-like periodic patterns while also incorporating non-periodic latent states to introduce instance-level diversity. Second, it models the hierarchical dependencies among face motions, body gestures, and hand movements by using cascaded guidance during the learning process to produce coordinated animations. The method was demonstrated on 3D avatars and validated through extensive experiments, showing superior performance to state-of-the-art co-speech gesture generation techniques in both quantitative metrics and qualitative assessments. The authors plan to release their code and trained models publicly, enabling further research and practical applications in speech-driven 3D gesture animation. <div>
arXiv:2512.13131v1 Announce Type: new 
Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title>
<link>https://arxiv.org/abs/2512.13142</link>
<guid>https://arxiv.org/abs/2512.13142</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, abortion stigma, Individual Level Abortion Stigma Scale, multilevel understanding, AI safety<br /><br />Summary: This study evaluates whether large language models (LLMs) genuinely understand the complex psychological and physiological aspects of abortion stigma. It uses the validated Individual Level Abortion Stigma Scale (ILAS) to systematically test 627 diverse personas across five leading LLMs. The analysis focuses on three stigma levels: cognitive (self-judgment), interpersonal (anticipated judgment and isolation), and structural (community condemnation and disclosure patterns). The results reveal that LLMs fail to exhibit genuine understanding across all these levels. Specifically, models tend to overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation without nuance, and introduce demographic biases not present in human-validated data. Additionally, LLMs miss empirically established relationships, such as the connection between stigma and secrecy, and sometimes produce contradictory responses within theoretical frameworks. These findings indicate that current model alignment techniques ensure linguistically appropriate outputs but do not guarantee coherent, multilevel comprehension of complex human experiences. The paper argues that AI safety in sensitive, high-stakes areas requires novel design principles emphasizing multilevel coherence, continuous auditing protocols, regulatory measures including mandatory audits and accountability, and improved AI literacy especially in contexts where understanding unspoken experiences is essential to providing effective support. <div>
arXiv:2512.13142v1 Announce Type: new 
Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations</title>
<link>https://arxiv.org/abs/2512.13154</link>
<guid>https://arxiv.org/abs/2512.13154</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, clarification dialogue, user ambiguity, conversational agents, MultiWOZ

<br /><br />Summary:  
This paper addresses the challenge of resolving ambiguous user requests in conversational agents, which is crucial for effective task completion. The authors focus on the underexplored problem of when and how multiple agents should initiate clarifications and coordinate their actions during uncertain or incomplete user inputs. To tackle this, they propose MAC (Multi-Agent Clarification), a novel interactive multi-agent framework specifically optimized for managing clarification dialogues. The approach begins with a new taxonomy that categorizes types of user ambiguities to systematically guide clarification strategies. MAC autonomously coordinates multiple agents to engage synergistically with users, improving dialogue efficiency and effectiveness. Empirical evaluations on the MultiWOZ 2.4 dataset demonstrate that incorporating clarification at two levels increases the task success rate significantly by 7.8%, from 54.5% to 62.3%. Additionally, the average number of dialogue turns decreases from 6.53 to 4.86, as the system elicits all necessary user information upfront while reducing repetitive exchanges. The results underscore the value of active user engagement and role-aware clarification within multi-agent conversational systems to enhance the reliability and naturalness of human-agent communication. <div>
arXiv:2512.13154v1 Announce Type: new 
Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13159</link>
<guid>https://arxiv.org/abs/2512.13159</guid>
<content:encoded><![CDATA[
<div> Keywords: human-agent collaboration, reinforcement learning, conversational proactivity, clarification questions, task-oriented dialogues<br /><br />Summary:<br /><br />1. The paper addresses the need for more proactive human-agent collaboration, moving beyond current models that mostly passively respond to user instructions without seeking clarifications.  
2. It introduces SpeakRL, a reinforcement learning method designed to improve agents’ conversational abilities by rewarding proactive behavior, specifically the asking of appropriate clarification questions when needed.  
3. To enable training and evaluation, the authors created SpeakER, a synthetic dataset featuring a variety of task-oriented dialogue scenarios that require interactive clarification to resolve tasks effectively.  
4. The work also explores how to design reward functions that incentivize a balance between agents asking clarifying questions and taking direct actions, optimizing for both engagement and efficiency.  
5. Experimental results demonstrate that SpeakRL achieves a 20.14% absolute improvement in task completion compared to baseline models, without increasing the number of conversation turns, and even outperforms larger proprietary models, highlighting the benefits of clarification-centric conversational strategies in user-agent interactions. <div>
arXiv:2512.13159v1 Announce Type: new 
Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finch: Benchmarking Finance &amp; Accounting across Spreadsheet-Centric Enterprise Workflows</title>
<link>https://arxiv.org/abs/2512.13168</link>
<guid>https://arxiv.org/abs/2512.13168</guid>
<content:encoded><![CDATA[
<div> finance, enterprise workflows, AI benchmarking, LLM-assisted annotation, spreadsheet analysis<br /><br />Summary: The paper introduces Finch, a finance and accounting benchmark designed to evaluate AI agents on complex, real-world enterprise workflows encompassing tasks like data entry, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is based on authentic datasets from Enron, including 15,000 spreadsheets and 500,000 emails from 150 employees, and other financial institutions, capturing the messy, multimodal nature of real enterprise data across various domains such as budgeting, trading, and asset management. The benchmark was created through a two-step workflow construction process involving LLM-assisted workflow discovery verified by experts, followed by meticulous expert annotation, requiring over 700 hours of domain-expert effort. This resulted in 172 composite workflows with 384 tasks, involving 1,710 spreadsheets containing 27 million cells, alongside PDFs and other related artifacts, reflecting the complexity and collaborative nature of enterprise work. The study evaluates top AI systems, including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, revealing that even the leading GPT 5.1 Pro, after 48 hours of testing, passes only 38.4% of workflows, while Claude Sonnet 4.5 achieves a 25.0% pass rate. Case studies highlight significant challenges these workflows present for AI agents, emphasizing the need for further research in this area. <div>
arXiv:2512.13168v1 Announce Type: new 
Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title>
<link>https://arxiv.org/abs/2512.13240</link>
<guid>https://arxiv.org/abs/2512.13240</guid>
<content:encoded><![CDATA[
<div> Direct Preference Optimization, Reflective Preference Optimization, hallucination reduction, large language models, policy learning  

<br /><br />Summary:  
This paper introduces Reflective Preference Optimization (RPO), a novel enhancement to Direct Preference Optimization (DPO), aimed at improving the alignment of large language models (LLMs) and vision-language models. Traditional DPO uses the same policy to generate both chosen and rejected responses, resulting in weak learning signals due to similar errors and low KL divergence between outputs, which slows convergence and makes training unstable. RPO addresses this by incorporating hint-guided reflection, leveraging external models to detect hallucination and produce concise reflective hints. These hints help create on-policy preference pairs with stronger contrastiveness and more distinct preference signals. Theoretically, conditioning on reflective hints increases the expected preference margin through mutual information, enhancing sample efficiency without departing from the original policy family. Empirical results demonstrate that RPO requires fewer training samples and iterations to achieve superior alignment compared to standard DPO. Furthermore, RPO significantly reduces hallucination rates and achieves state-of-the-art performance on various multimodal benchmarks. This approach provides a more effective and lightweight alternative to existing frameworks such as Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF), promising improved training stability and faster convergence in preference-based model alignment. <div>
arXiv:2512.13240v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data</title>
<link>https://arxiv.org/abs/2512.13297</link>
<guid>https://arxiv.org/abs/2512.13297</guid>
<content:encoded><![CDATA[
<div> Keywords: medical data analysis, multi-modal models, MedInsightBench, MedInsightAgent, healthcare insights  

<br /><br />Summary:  
1. The paper addresses the need for extracting deep insights from complex multi-modal medical datasets to enhance patient care, diagnostic accuracy, and healthcare operations.  
2. It highlights the current gap in high-quality datasets tailored to evaluate large multi-modal models (LMMs) for medical insight discovery.  
3. The authors introduce MedInsightBench, a novel benchmark containing 332 carefully curated medical cases with annotated insights, designed to assess LMMs' abilities to analyze multi-modal medical images, ask relevant questions, interpret findings, and generate actionable recommendations.  
4. Evaluation shows that existing LMMs struggle on MedInsightBench mainly due to difficulties in extracting deep, multi-step insights and lacking medical expertise.  
5. To overcome this, the paper proposes MedInsightAgent, an automated agent framework composed of three modules—Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer—which enhances general LMM performance in medical insight discovery.  
6. Experimental results on MedInsightBench reveal persistent challenges but demonstrate that MedInsightAgent significantly improves the capability of LMMs in analyzing medical data and generating meaningful insights. <div>
arXiv:2512.13297v1 Announce Type: new 
Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Prompt Optimization for Arithmetic Reasoning</title>
<link>https://arxiv.org/abs/2512.13323</link>
<guid>https://arxiv.org/abs/2512.13323</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, arithmetic reasoning, small language models, error-driven optimization, privacy-compliant  

<br /><br />Summary:  
The paper addresses the challenge of enhancing arithmetic reasoning capabilities within AI systems, particularly for regulated sectors like finance and healthcare where data privacy is critical. It introduces an error-driven optimization framework designed to improve the performance of Code Generation Agents (CGAs) that leverage on-premises small language models (SLMs). The approach involves systematically clustering erroneous predictions from the model and iteratively refining prompt-based rules to reduce errors. Experiments were conducted using the Qwen3 4B model, a state-of-the-art SLM. The base model initially showed significant limitations in executing accurate arithmetic operations on tabular data. However, after applying the proposed error-driven prompt optimization method, the model's accuracy improved dramatically to 70.8%. This improvement demonstrates that small, privacy-focused models, when paired with strategic prompt engineering, can outperform much larger language models, such as GPT-3.5 Turbo, for specialized arithmetic tasks. The study highlights that such enhancements can be achieved without costly fine-tuning, making the solution more practical and interpretable for industrial deployment. Ultimately, the research provides a pathway for developing reliable, industrial AI assistants capable of secure, on-premises operation with strong arithmetic reasoning skills. <div>
arXiv:2512.13323v1 Announce Type: new 
Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection</title>
<link>https://arxiv.org/abs/2512.13374</link>
<guid>https://arxiv.org/abs/2512.13374</guid>
<content:encoded><![CDATA[
arXiv:2512.13374v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Evolutionary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13399</link>
<guid>https://arxiv.org/abs/2512.13399</guid>
<content:encoded><![CDATA[
arXiv:2512.13399v1 Announce Type: new 
Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title>
<link>https://arxiv.org/abs/2512.13481</link>
<guid>https://arxiv.org/abs/2512.13481</guid>
<content:encoded><![CDATA[
arXiv:2512.13481v1 Announce Type: new 
Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defending the Hierarchical Result Models of Precedential Constraint</title>
<link>https://arxiv.org/abs/2512.13505</link>
<guid>https://arxiv.org/abs/2512.13505</guid>
<content:encoded><![CDATA[
arXiv:2512.13505v1 Announce Type: new 
Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph</title>
<link>https://arxiv.org/abs/2512.13510</link>
<guid>https://arxiv.org/abs/2512.13510</guid>
<content:encoded><![CDATA[
arXiv:2512.13510v1 Announce Type: new 
Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level</title>
<link>https://arxiv.org/abs/2410.00015</link>
<guid>https://arxiv.org/abs/2410.00015</guid>
<content:encoded><![CDATA[
arXiv:2410.00015v2 Announce Type: cross 
Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title>
<link>https://arxiv.org/abs/2508.15250</link>
<guid>https://arxiv.org/abs/2508.15250</guid>
<content:encoded><![CDATA[
arXiv:2508.15250v3 Announce Type: cross 
Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
<link>https://arxiv.org/abs/2512.11811</link>
<guid>https://arxiv.org/abs/2512.11811</guid>
<content:encoded><![CDATA[
arXiv:2512.11811v1 Announce Type: cross 
Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare</title>
<link>https://arxiv.org/abs/2512.11814</link>
<guid>https://arxiv.org/abs/2512.11814</guid>
<content:encoded><![CDATA[
arXiv:2512.11814v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique</title>
<link>https://arxiv.org/abs/2512.11818</link>
<guid>https://arxiv.org/abs/2512.11818</guid>
<content:encoded><![CDATA[
arXiv:2512.11818v1 Announce Type: cross 
Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?</title>
<link>https://arxiv.org/abs/2512.11827</link>
<guid>https://arxiv.org/abs/2512.11827</guid>
<content:encoded><![CDATA[
arXiv:2512.11827v1 Announce Type: cross 
Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference with Reusable State-Dependent Value Profiles</title>
<link>https://arxiv.org/abs/2512.11829</link>
<guid>https://arxiv.org/abs/2512.11829</guid>
<content:encoded><![CDATA[
arXiv:2512.11829v1 Announce Type: cross 
Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation</title>
<link>https://arxiv.org/abs/2512.11830</link>
<guid>https://arxiv.org/abs/2512.11830</guid>
<content:encoded><![CDATA[
arXiv:2512.11830v1 Announce Type: cross 
Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2512.11832</link>
<guid>https://arxiv.org/abs/2512.11832</guid>
<content:encoded><![CDATA[
arXiv:2512.11832v1 Announce Type: cross 
Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $\Delta_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&amp;D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Decision Tree classifier: explainable and extendable PyTorch implementation</title>
<link>https://arxiv.org/abs/2512.11833</link>
<guid>https://arxiv.org/abs/2512.11833</guid>
<content:encoded><![CDATA[
arXiv:2512.11833v1 Announce Type: cross 
Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions</title>
<link>https://arxiv.org/abs/2512.11836</link>
<guid>https://arxiv.org/abs/2512.11836</guid>
<content:encoded><![CDATA[
arXiv:2512.11836v1 Announce Type: cross 
Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Foundry: A System for Training Foundational Vision AI Models</title>
<link>https://arxiv.org/abs/2512.11837</link>
<guid>https://arxiv.org/abs/2512.11837</guid>
<content:encoded><![CDATA[
arXiv:2512.11837v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spiking Manifesto</title>
<link>https://arxiv.org/abs/2512.11843</link>
<guid>https://arxiv.org/abs/2512.11843</guid>
<content:encoded><![CDATA[
arXiv:2512.11843v1 Announce Type: cross 
Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach</title>
<link>https://arxiv.org/abs/2512.11845</link>
<guid>https://arxiv.org/abs/2512.11845</guid>
<content:encoded><![CDATA[
arXiv:2512.11845v1 Announce Type: cross 
Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document</title>
<link>https://arxiv.org/abs/2512.11849</link>
<guid>https://arxiv.org/abs/2512.11849</guid>
<content:encoded><![CDATA[
arXiv:2512.11849v1 Announce Type: cross 
Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs</title>
<link>https://arxiv.org/abs/2512.11851</link>
<guid>https://arxiv.org/abs/2512.11851</guid>
<content:encoded><![CDATA[
arXiv:2512.11851v1 Announce Type: cross 
Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things</title>
<link>https://arxiv.org/abs/2512.11852</link>
<guid>https://arxiv.org/abs/2512.11852</guid>
<content:encoded><![CDATA[
arXiv:2512.11852v1 Announce Type: cross 
Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks</title>
<link>https://arxiv.org/abs/2512.11854</link>
<guid>https://arxiv.org/abs/2512.11854</guid>
<content:encoded><![CDATA[
arXiv:2512.11854v1 Announce Type: cross 
Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry</title>
<link>https://arxiv.org/abs/2512.11855</link>
<guid>https://arxiv.org/abs/2512.11855</guid>
<content:encoded><![CDATA[
arXiv:2512.11855v1 Announce Type: cross 
Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search</title>
<link>https://arxiv.org/abs/2512.11856</link>
<guid>https://arxiv.org/abs/2512.11856</guid>
<content:encoded><![CDATA[
arXiv:2512.11856v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopicProphet: Prophesies on Temporal Topic Trends and Stocks</title>
<link>https://arxiv.org/abs/2512.11857</link>
<guid>https://arxiv.org/abs/2512.11857</guid>
<content:encoded><![CDATA[
arXiv:2512.11857v1 Announce Type: cross 
Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Path Integral Diffusion: AdaPID</title>
<link>https://arxiv.org/abs/2512.11858</link>
<guid>https://arxiv.org/abs/2512.11858</guid>
<content:encoded><![CDATA[
arXiv:2512.11858v1 Announce Type: cross 
Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion</title>
<link>https://arxiv.org/abs/2512.11859</link>
<guid>https://arxiv.org/abs/2512.11859</guid>
<content:encoded><![CDATA[
arXiv:2512.11859v1 Announce Type: cross 
Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes</title>
<link>https://arxiv.org/abs/2512.11860</link>
<guid>https://arxiv.org/abs/2512.11860</guid>
<content:encoded><![CDATA[
arXiv:2512.11860v1 Announce Type: cross 
Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL</title>
<link>https://arxiv.org/abs/2512.11862</link>
<guid>https://arxiv.org/abs/2512.11862</guid>
<content:encoded><![CDATA[
arXiv:2512.11862v1 Announce Type: cross 
Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence</title>
<link>https://arxiv.org/abs/2512.11863</link>
<guid>https://arxiv.org/abs/2512.11863</guid>
<content:encoded><![CDATA[
arXiv:2512.11863v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.11865</link>
<guid>https://arxiv.org/abs/2512.11865</guid>
<content:encoded><![CDATA[
arXiv:2512.11865v1 Announce Type: cross 
Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title>
<link>https://arxiv.org/abs/2512.11867</link>
<guid>https://arxiv.org/abs/2512.11867</guid>
<content:encoded><![CDATA[
arXiv:2512.11867v1 Announce Type: cross 
Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models</title>
<link>https://arxiv.org/abs/2512.11868</link>
<guid>https://arxiv.org/abs/2512.11868</guid>
<content:encoded><![CDATA[
arXiv:2512.11868v1 Announce Type: cross 
Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title>
<link>https://arxiv.org/abs/2512.11870</link>
<guid>https://arxiv.org/abs/2512.11870</guid>
<content:encoded><![CDATA[
arXiv:2512.11870v1 Announce Type: cross 
Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops</title>
<link>https://arxiv.org/abs/2512.11871</link>
<guid>https://arxiv.org/abs/2512.11871</guid>
<content:encoded><![CDATA[
arXiv:2512.11871v1 Announce Type: cross 
Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.11872</link>
<guid>https://arxiv.org/abs/2512.11872</guid>
<content:encoded><![CDATA[
arXiv:2512.11872v1 Announce Type: cross 
Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's About Time: The Temporal and Modal Dynamics of Copilot Usage</title>
<link>https://arxiv.org/abs/2512.11879</link>
<guid>https://arxiv.org/abs/2512.11879</guid>
<content:encoded><![CDATA[
arXiv:2512.11879v1 Announce Type: cross 
Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Structural Representation in Foundation Models for Polymers</title>
<link>https://arxiv.org/abs/2512.11881</link>
<guid>https://arxiv.org/abs/2512.11881</guid>
<content:encoded><![CDATA[
arXiv:2512.11881v1 Announce Type: cross 
Abstract: From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education</title>
<link>https://arxiv.org/abs/2512.11882</link>
<guid>https://arxiv.org/abs/2512.11882</guid>
<content:encoded><![CDATA[
arXiv:2512.11882v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title>
<link>https://arxiv.org/abs/2512.11883</link>
<guid>https://arxiv.org/abs/2512.11883</guid>
<content:encoded><![CDATA[
arXiv:2512.11883v1 Announce Type: cross 
Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2512.11887</link>
<guid>https://arxiv.org/abs/2512.11887</guid>
<content:encoded><![CDATA[
arXiv:2512.11887v1 Announce Type: cross 
Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Should AI Become an Intergenerational Civil Right?</title>
<link>https://arxiv.org/abs/2512.11892</link>
<guid>https://arxiv.org/abs/2512.11892</guid>
<content:encoded><![CDATA[
arXiv:2512.11892v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.
  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.
  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2512.11893</link>
<guid>https://arxiv.org/abs/2512.11893</guid>
<content:encoded><![CDATA[
arXiv:2512.11893v1 Announce Type: cross 
Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A fine-grained look at causal effects in causal spaces</title>
<link>https://arxiv.org/abs/2512.11919</link>
<guid>https://arxiv.org/abs/2512.11919</guid>
<content:encoded><![CDATA[
arXiv:2512.11919v1 Announce Type: cross 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control</title>
<link>https://arxiv.org/abs/2512.11921</link>
<guid>https://arxiv.org/abs/2512.11921</guid>
<content:encoded><![CDATA[
arXiv:2512.11921v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use</title>
<link>https://arxiv.org/abs/2512.11922</link>
<guid>https://arxiv.org/abs/2512.11922</guid>
<content:encoded><![CDATA[
arXiv:2512.11922v1 Announce Type: cross 
Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</title>
<link>https://arxiv.org/abs/2512.11925</link>
<guid>https://arxiv.org/abs/2512.11925</guid>
<content:encoded><![CDATA[
arXiv:2512.11925v1 Announce Type: cross 
Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene regulatory network inference algorithm based on spectral signed directed graph convolution</title>
<link>https://arxiv.org/abs/2512.11927</link>
<guid>https://arxiv.org/abs/2512.11927</guid>
<content:encoded><![CDATA[
arXiv:2512.11927v1 Announce Type: cross 
Abstract: Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion</title>
<link>https://arxiv.org/abs/2512.11928</link>
<guid>https://arxiv.org/abs/2512.11928</guid>
<content:encoded><![CDATA[
arXiv:2512.11928v1 Announce Type: cross 
Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction</title>
<link>https://arxiv.org/abs/2512.11930</link>
<guid>https://arxiv.org/abs/2512.11930</guid>
<content:encoded><![CDATA[
arXiv:2512.11930v1 Announce Type: cross 
Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy</title>
<link>https://arxiv.org/abs/2512.11931</link>
<guid>https://arxiv.org/abs/2512.11931</guid>
<content:encoded><![CDATA[
arXiv:2512.11931v1 Announce Type: cross 
Abstract: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance</title>
<link>https://arxiv.org/abs/2512.11933</link>
<guid>https://arxiv.org/abs/2512.11933</guid>
<content:encoded><![CDATA[
arXiv:2512.11933v1 Announce Type: cross 
Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching</title>
<link>https://arxiv.org/abs/2512.11934</link>
<guid>https://arxiv.org/abs/2512.11934</guid>
<content:encoded><![CDATA[
arXiv:2512.11934v1 Announce Type: cross 
Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11941</link>
<guid>https://arxiv.org/abs/2512.11941</guid>
<content:encoded><![CDATA[
arXiv:2512.11941v1 Announce Type: cross 
Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism</title>
<link>https://arxiv.org/abs/2512.11943</link>
<guid>https://arxiv.org/abs/2512.11943</guid>
<content:encoded><![CDATA[
arXiv:2512.11943v1 Announce Type: cross 
Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach</title>
<link>https://arxiv.org/abs/2512.11944</link>
<guid>https://arxiv.org/abs/2512.11944</guid>
<content:encoded><![CDATA[
arXiv:2512.11944v1 Announce Type: cross 
Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations</title>
<link>https://arxiv.org/abs/2512.11946</link>
<guid>https://arxiv.org/abs/2512.11946</guid>
<content:encoded><![CDATA[
arXiv:2512.11946v1 Announce Type: cross 
Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)</title>
<link>https://arxiv.org/abs/2512.11979</link>
<guid>https://arxiv.org/abs/2512.11979</guid>
<content:encoded><![CDATA[
arXiv:2512.11979v1 Announce Type: cross 
Abstract: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic search for 100M+ galaxy images using AI-generated captions</title>
<link>https://arxiv.org/abs/2512.11982</link>
<guid>https://arxiv.org/abs/2512.11982</guid>
<content:encoded><![CDATA[
arXiv:2512.11982v1 Announce Type: cross 
Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering</title>
<link>https://arxiv.org/abs/2512.11984</link>
<guid>https://arxiv.org/abs/2512.11984</guid>
<content:encoded><![CDATA[
arXiv:2512.11984v1 Announce Type: cross 
Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</title>
<link>https://arxiv.org/abs/2512.11995</link>
<guid>https://arxiv.org/abs/2512.11995</guid>
<content:encoded><![CDATA[
arXiv:2512.11995v1 Announce Type: cross 
Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hold Onto That Thought: Assessing KV Cache Compression On Reasoning</title>
<link>https://arxiv.org/abs/2512.12008</link>
<guid>https://arxiv.org/abs/2512.12008</guid>
<content:encoded><![CDATA[
arXiv:2512.12008v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title>
<link>https://arxiv.org/abs/2512.12012</link>
<guid>https://arxiv.org/abs/2512.12012</guid>
<content:encoded><![CDATA[
arXiv:2512.12012v1 Announce Type: cross 
Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers</title>
<link>https://arxiv.org/abs/2512.12045</link>
<guid>https://arxiv.org/abs/2512.12045</guid>
<content:encoded><![CDATA[
arXiv:2512.12045v1 Announce Type: cross 
Abstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.
  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.
  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-Tuning Open-Weight Language Models for BPMN Model Generation</title>
<link>https://arxiv.org/abs/2512.12063</link>
<guid>https://arxiv.org/abs/2512.12063</guid>
<content:encoded><![CDATA[
arXiv:2512.12063v1 Announce Type: cross 
Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title>
<link>https://arxiv.org/abs/2512.12066</link>
<guid>https://arxiv.org/abs/2512.12066</guid>
<content:encoded><![CDATA[
arXiv:2512.12066v1 Announce Type: cross 
Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title>
<link>https://arxiv.org/abs/2512.12069</link>
<guid>https://arxiv.org/abs/2512.12069</guid>
<content:encoded><![CDATA[
arXiv:2512.12069v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models</title>
<link>https://arxiv.org/abs/2512.12081</link>
<guid>https://arxiv.org/abs/2512.12081</guid>
<content:encoded><![CDATA[
arXiv:2512.12081v1 Announce Type: cross 
Abstract: Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A neuro-symbolic framework for accountability in public-sector AI</title>
<link>https://arxiv.org/abs/2512.12109</link>
<guid>https://arxiv.org/abs/2512.12109</guid>
<content:encoded><![CDATA[
arXiv:2512.12109v1 Announce Type: cross 
Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models</title>
<link>https://arxiv.org/abs/2512.12121</link>
<guid>https://arxiv.org/abs/2512.12121</guid>
<content:encoded><![CDATA[
arXiv:2512.12121v1 Announce Type: cross 
Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery</title>
<link>https://arxiv.org/abs/2512.12128</link>
<guid>https://arxiv.org/abs/2512.12128</guid>
<content:encoded><![CDATA[
arXiv:2512.12128v1 Announce Type: cross 
Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity</title>
<link>https://arxiv.org/abs/2512.12135</link>
<guid>https://arxiv.org/abs/2512.12135</guid>
<content:encoded><![CDATA[
arXiv:2512.12135v1 Announce Type: cross 
Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater</title>
<link>https://arxiv.org/abs/2512.12142</link>
<guid>https://arxiv.org/abs/2512.12142</guid>
<content:encoded><![CDATA[
arXiv:2512.12142v1 Announce Type: cross 
Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings</title>
<link>https://arxiv.org/abs/2512.12167</link>
<guid>https://arxiv.org/abs/2512.12167</guid>
<content:encoded><![CDATA[
arXiv:2512.12167v1 Announce Type: cross 
Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Language Model Inference with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2512.12168</link>
<guid>https://arxiv.org/abs/2512.12168</guid>
<content:encoded><![CDATA[
arXiv:2512.12168v1 Announce Type: cross 
Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms</title>
<link>https://arxiv.org/abs/2512.12199</link>
<guid>https://arxiv.org/abs/2512.12199</guid>
<content:encoded><![CDATA[
arXiv:2512.12199v1 Announce Type: cross 
Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation</title>
<link>https://arxiv.org/abs/2512.12201</link>
<guid>https://arxiv.org/abs/2512.12201</guid>
<content:encoded><![CDATA[
arXiv:2512.12201v1 Announce Type: cross 
Abstract: Large language models (LLMs) have often been characterized as "stochastic parrots" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</title>
<link>https://arxiv.org/abs/2512.12206</link>
<guid>https://arxiv.org/abs/2512.12206</guid>
<content:encoded><![CDATA[
arXiv:2512.12206v1 Announce Type: cross 
Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search</title>
<link>https://arxiv.org/abs/2512.12207</link>
<guid>https://arxiv.org/abs/2512.12207</guid>
<content:encoded><![CDATA[
arXiv:2512.12207v1 Announce Type: cross 
Abstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.12211</link>
<guid>https://arxiv.org/abs/2512.12211</guid>
<content:encoded><![CDATA[
arXiv:2512.12211v1 Announce Type: cross 
Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Versatile Coding Agents in Synthetic Environments</title>
<link>https://arxiv.org/abs/2512.12216</link>
<guid>https://arxiv.org/abs/2512.12216</guid>
<content:encoded><![CDATA[
arXiv:2512.12216v1 Announce Type: cross 
Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs</title>
<link>https://arxiv.org/abs/2512.12222</link>
<guid>https://arxiv.org/abs/2512.12222</guid>
<content:encoded><![CDATA[
arXiv:2512.12222v1 Announce Type: cross 
Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Distance Measurement based on Multi-Kernel Gaussian Processes</title>
<link>https://arxiv.org/abs/2512.12238</link>
<guid>https://arxiv.org/abs/2512.12238</guid>
<content:encoded><![CDATA[
arXiv:2512.12238v1 Announce Type: cross 
Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Mat\'ern and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Probing Cross-Family Sound Symbolism in 27 Languages</title>
<link>https://arxiv.org/abs/2512.12245</link>
<guid>https://arxiv.org/abs/2512.12245</guid>
<content:encoded><![CDATA[
arXiv:2512.12245v1 Announce Type: cross 
Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&amp;P 500 Index Volatility Forecasting</title>
<link>https://arxiv.org/abs/2512.12250</link>
<guid>https://arxiv.org/abs/2512.12250</guid>
<content:encoded><![CDATA[
arXiv:2512.12250v1 Announce Type: cross 
Abstract: Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accurate de novo sequencing of the modified proteome with OmniNovo</title>
<link>https://arxiv.org/abs/2512.12272</link>
<guid>https://arxiv.org/abs/2512.12272</guid>
<content:encoded><![CDATA[
arXiv:2512.12272v1 Announce Type: cross 
Abstract: Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\% more peptides than standard approaches at a 1\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRC-Net: Gram Residual Co-attention Net for epilepsy prediction</title>
<link>https://arxiv.org/abs/2512.12273</link>
<guid>https://arxiv.org/abs/2512.12273</guid>
<content:encoded><![CDATA[
arXiv:2512.12273v1 Announce Type: cross 
Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2512.12284</link>
<guid>https://arxiv.org/abs/2512.12284</guid>
<content:encoded><![CDATA[
arXiv:2512.12284v1 Announce Type: cross 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation</title>
<link>https://arxiv.org/abs/2512.12285</link>
<guid>https://arxiv.org/abs/2512.12285</guid>
<content:encoded><![CDATA[
arXiv:2512.12285v1 Announce Type: cross 
Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMark: Artificial Intelligence Generated Content Identification Toolkit</title>
<link>https://arxiv.org/abs/2512.12324</link>
<guid>https://arxiv.org/abs/2512.12324</guid>
<content:encoded><![CDATA[
arXiv:2512.12324v1 Announce Type: cross 
Abstract: The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks</title>
<link>https://arxiv.org/abs/2512.12332</link>
<guid>https://arxiv.org/abs/2512.12332</guid>
<content:encoded><![CDATA[
arXiv:2512.12332v1 Announce Type: cross 
Abstract: The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema</title>
<link>https://arxiv.org/abs/2512.12337</link>
<guid>https://arxiv.org/abs/2512.12337</guid>
<content:encoded><![CDATA[
arXiv:2512.12337v1 Announce Type: cross 
Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
<link>https://arxiv.org/abs/2512.12410</link>
<guid>https://arxiv.org/abs/2512.12410</guid>
<content:encoded><![CDATA[
arXiv:2512.12410v1 Announce Type: cross 
Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rough Sets for Explainability of Spectral Graph Clustering</title>
<link>https://arxiv.org/abs/2512.12436</link>
<guid>https://arxiv.org/abs/2512.12436</guid>
<content:encoded><![CDATA[
arXiv:2512.12436v1 Announce Type: cross 
Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling</title>
<link>https://arxiv.org/abs/2512.12461</link>
<guid>https://arxiv.org/abs/2512.12461</guid>
<content:encoded><![CDATA[
arXiv:2512.12461v1 Announce Type: cross 
Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference</title>
<link>https://arxiv.org/abs/2512.12462</link>
<guid>https://arxiv.org/abs/2512.12462</guid>
<content:encoded><![CDATA[
arXiv:2512.12462v1 Announce Type: cross 
Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Design Space of Transition Matching</title>
<link>https://arxiv.org/abs/2512.12465</link>
<guid>https://arxiv.org/abs/2512.12465</guid>
<content:encoded><![CDATA[
arXiv:2512.12465v1 Announce Type: cross 
Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion</title>
<link>https://arxiv.org/abs/2512.12474</link>
<guid>https://arxiv.org/abs/2512.12474</guid>
<content:encoded><![CDATA[
arXiv:2512.12474v1 Announce Type: cross 
Abstract: Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers</title>
<link>https://arxiv.org/abs/2512.12483</link>
<guid>https://arxiv.org/abs/2512.12483</guid>
<content:encoded><![CDATA[
arXiv:2512.12483v1 Announce Type: cross 
Abstract: With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe.
  Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin.
  And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\cite{battistello2025ecc}.
  These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process.
  In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve.
  Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process.
  It is my belief that proof-for would be equally valuable as proof-against in either of these categories.
  Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public</title>
<link>https://arxiv.org/abs/2512.12500</link>
<guid>https://arxiv.org/abs/2512.12500</guid>
<content:encoded><![CDATA[
arXiv:2512.12500v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts</title>
<link>https://arxiv.org/abs/2512.12506</link>
<guid>https://arxiv.org/abs/2512.12506</guid>
<content:encoded><![CDATA[
arXiv:2512.12506v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline</title>
<link>https://arxiv.org/abs/2512.12510</link>
<guid>https://arxiv.org/abs/2512.12510</guid>
<content:encoded><![CDATA[
arXiv:2512.12510v1 Announce Type: cross 
Abstract: The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.12523</link>
<guid>https://arxiv.org/abs/2512.12523</guid>
<content:encoded><![CDATA[
arXiv:2512.12523v1 Announce Type: cross 
Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?</title>
<link>https://arxiv.org/abs/2512.12536</link>
<guid>https://arxiv.org/abs/2512.12536</guid>
<content:encoded><![CDATA[
arXiv:2512.12536v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model</title>
<link>https://arxiv.org/abs/2512.12545</link>
<guid>https://arxiv.org/abs/2512.12545</guid>
<content:encoded><![CDATA[
arXiv:2512.12545v1 Announce Type: cross 
Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
<link>https://arxiv.org/abs/2512.12560</link>
<guid>https://arxiv.org/abs/2512.12560</guid>
<content:encoded><![CDATA[
arXiv:2512.12560v1 Announce Type: cross 
Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coupled Variational Reinforcement Learning for Language Model General Reasoning</title>
<link>https://arxiv.org/abs/2512.12576</link>
<guid>https://arxiv.org/abs/2512.12576</guid>
<content:encoded><![CDATA[
arXiv:2512.12576v1 Announce Type: cross 
Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Prompt Injection Attacks Against Application Using Classifiers</title>
<link>https://arxiv.org/abs/2512.12583</link>
<guid>https://arxiv.org/abs/2512.12583</guid>
<content:encoded><![CDATA[
arXiv:2512.12583v1 Announce Type: cross 
Abstract: Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models</title>
<link>https://arxiv.org/abs/2512.12596</link>
<guid>https://arxiv.org/abs/2512.12596</guid>
<content:encoded><![CDATA[
arXiv:2512.12596v1 Announce Type: cross 
Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery</title>
<link>https://arxiv.org/abs/2512.12608</link>
<guid>https://arxiv.org/abs/2512.12608</guid>
<content:encoded><![CDATA[
arXiv:2512.12608v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</title>
<link>https://arxiv.org/abs/2512.12620</link>
<guid>https://arxiv.org/abs/2512.12620</guid>
<content:encoded><![CDATA[
arXiv:2512.12620v1 Announce Type: cross 
Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists</title>
<link>https://arxiv.org/abs/2512.12630</link>
<guid>https://arxiv.org/abs/2512.12630</guid>
<content:encoded><![CDATA[
arXiv:2512.12630v1 Announce Type: cross 
Abstract: Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.12633</link>
<guid>https://arxiv.org/abs/2512.12633</guid>
<content:encoded><![CDATA[
arXiv:2512.12633v1 Announce Type: cross 
Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.12662</link>
<guid>https://arxiv.org/abs/2512.12662</guid>
<content:encoded><![CDATA[
arXiv:2512.12662v1 Announce Type: cross 
Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.12663</link>
<guid>https://arxiv.org/abs/2512.12663</guid>
<content:encoded><![CDATA[
arXiv:2512.12663v1 Announce Type: cross 
Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization</title>
<link>https://arxiv.org/abs/2512.12669</link>
<guid>https://arxiv.org/abs/2512.12669</guid>
<content:encoded><![CDATA[
arXiv:2512.12669v1 Announce Type: cross 
Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</title>
<link>https://arxiv.org/abs/2512.12675</link>
<guid>https://arxiv.org/abs/2512.12675</guid>
<content:encoded><![CDATA[
arXiv:2512.12675v1 Announce Type: cross 
Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches</title>
<link>https://arxiv.org/abs/2512.12677</link>
<guid>https://arxiv.org/abs/2512.12677</guid>
<content:encoded><![CDATA[
arXiv:2512.12677v1 Announce Type: cross 
Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.12683</link>
<guid>https://arxiv.org/abs/2512.12683</guid>
<content:encoded><![CDATA[
arXiv:2512.12683v1 Announce Type: cross 
Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity</title>
<link>https://arxiv.org/abs/2512.12688</link>
<guid>https://arxiv.org/abs/2512.12688</guid>
<content:encoded><![CDATA[
arXiv:2512.12688v1 Announce Type: cross 
Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits</title>
<link>https://arxiv.org/abs/2512.12693</link>
<guid>https://arxiv.org/abs/2512.12693</guid>
<content:encoded><![CDATA[
arXiv:2512.12693v1 Announce Type: cross 
Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Motion Generation using Part-level Reliable Data from Videos</title>
<link>https://arxiv.org/abs/2512.12703</link>
<guid>https://arxiv.org/abs/2512.12703</guid>
<content:encoded><![CDATA[
arXiv:2512.12703v1 Announce Type: cross 
Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Scientific Literature Explorer using Machine Learning (ISLE)</title>
<link>https://arxiv.org/abs/2512.12760</link>
<guid>https://arxiv.org/abs/2512.12760</guid>
<content:encoded><![CDATA[
arXiv:2512.12760v1 Announce Type: cross 
Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Feedback Alignment</title>
<link>https://arxiv.org/abs/2512.12762</link>
<guid>https://arxiv.org/abs/2512.12762</guid>
<content:encoded><![CDATA[
arXiv:2512.12762v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
<link>https://arxiv.org/abs/2512.12768</link>
<guid>https://arxiv.org/abs/2512.12768</guid>
<content:encoded><![CDATA[
arXiv:2512.12768v1 Announce Type: cross 
Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)</title>
<link>https://arxiv.org/abs/2512.12769</link>
<guid>https://arxiv.org/abs/2512.12769</guid>
<content:encoded><![CDATA[
arXiv:2512.12769v1 Announce Type: cross 
Abstract: Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.12773</link>
<guid>https://arxiv.org/abs/2512.12773</guid>
<content:encoded><![CDATA[
arXiv:2512.12773v1 Announce Type: cross 
Abstract: With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State over Tokens: Characterizing the Role of Reasoning Tokens</title>
<link>https://arxiv.org/abs/2512.12777</link>
<guid>https://arxiv.org/abs/2512.12777</guid>
<content:encoded><![CDATA[
arXiv:2512.12777v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average</title>
<link>https://arxiv.org/abs/2512.12785</link>
<guid>https://arxiv.org/abs/2512.12785</guid>
<content:encoded><![CDATA[
arXiv:2512.12785v1 Announce Type: cross 
Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Statistical Significance of Online Regression over Multiple Datasets</title>
<link>https://arxiv.org/abs/2512.12787</link>
<guid>https://arxiv.org/abs/2512.12787</guid>
<content:encoded><![CDATA[
arXiv:2512.12787v1 Announce Type: cross 
Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title>
<link>https://arxiv.org/abs/2512.12791</link>
<guid>https://arxiv.org/abs/2512.12791</guid>
<content:encoded><![CDATA[
arXiv:2512.12791v1 Announce Type: cross 
Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2512.12792</link>
<guid>https://arxiv.org/abs/2512.12792</guid>
<content:encoded><![CDATA[
arXiv:2512.12792v1 Announce Type: cross 
Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness</title>
<link>https://arxiv.org/abs/2512.12802</link>
<guid>https://arxiv.org/abs/2512.12802</guid>
<content:encoded><![CDATA[
arXiv:2512.12802v1 Announce Type: cross 
Abstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</title>
<link>https://arxiv.org/abs/2512.12805</link>
<guid>https://arxiv.org/abs/2512.12805</guid>
<content:encoded><![CDATA[
arXiv:2512.12805v1 Announce Type: cross 
Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization</title>
<link>https://arxiv.org/abs/2512.12809</link>
<guid>https://arxiv.org/abs/2512.12809</guid>
<content:encoded><![CDATA[
arXiv:2512.12809v1 Announce Type: cross 
Abstract: Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA</title>
<link>https://arxiv.org/abs/2512.12812</link>
<guid>https://arxiv.org/abs/2512.12812</guid>
<content:encoded><![CDATA[
arXiv:2512.12812v1 Announce Type: cross 
Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles</title>
<link>https://arxiv.org/abs/2512.12817</link>
<guid>https://arxiv.org/abs/2512.12817</guid>
<content:encoded><![CDATA[
arXiv:2512.12817v1 Announce Type: cross 
Abstract: Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects</title>
<link>https://arxiv.org/abs/2512.12818</link>
<guid>https://arxiv.org/abs/2512.12818</guid>
<content:encoded><![CDATA[
arXiv:2512.12818v1 Announce Type: cross 
Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the continuity of flows</title>
<link>https://arxiv.org/abs/2512.12821</link>
<guid>https://arxiv.org/abs/2512.12821</guid>
<content:encoded><![CDATA[
arXiv:2512.12821v1 Announce Type: cross 
Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.12822</link>
<guid>https://arxiv.org/abs/2512.12822</guid>
<content:encoded><![CDATA[
arXiv:2512.12822v1 Announce Type: cross 
Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
<link>https://arxiv.org/abs/2512.12824</link>
<guid>https://arxiv.org/abs/2512.12824</guid>
<content:encoded><![CDATA[
arXiv:2512.12824v1 Announce Type: cross 
Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</title>
<link>https://arxiv.org/abs/2512.12832</link>
<guid>https://arxiv.org/abs/2512.12832</guid>
<content:encoded><![CDATA[
arXiv:2512.12832v1 Announce Type: cross 
Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title>
<link>https://arxiv.org/abs/2512.12840</link>
<guid>https://arxiv.org/abs/2512.12840</guid>
<content:encoded><![CDATA[
arXiv:2512.12840v1 Announce Type: cross 
Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word priv\'ee, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding</title>
<link>https://arxiv.org/abs/2512.12842</link>
<guid>https://arxiv.org/abs/2512.12842</guid>
<content:encoded><![CDATA[
arXiv:2512.12842v1 Announce Type: cross 
Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Conformal Risk Control</title>
<link>https://arxiv.org/abs/2512.12844</link>
<guid>https://arxiv.org/abs/2512.12844</guid>
<content:encoded><![CDATA[
arXiv:2512.12844v1 Announce Type: cross 
Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2512.12858</link>
<guid>https://arxiv.org/abs/2512.12858</guid>
<content:encoded><![CDATA[
arXiv:2512.12858v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</title>
<link>https://arxiv.org/abs/2512.12868</link>
<guid>https://arxiv.org/abs/2512.12868</guid>
<content:encoded><![CDATA[
arXiv:2512.12868v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels</title>
<link>https://arxiv.org/abs/2512.12870</link>
<guid>https://arxiv.org/abs/2512.12870</guid>
<content:encoded><![CDATA[
arXiv:2512.12870v1 Announce Type: cross 
Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</title>
<link>https://arxiv.org/abs/2512.12885</link>
<guid>https://arxiv.org/abs/2512.12885</guid>
<content:encoded><![CDATA[
arXiv:2512.12885v1 Announce Type: cross 
Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.12888</link>
<guid>https://arxiv.org/abs/2512.12888</guid>
<content:encoded><![CDATA[
arXiv:2512.12888v1 Announce Type: cross 
Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2512.12914</link>
<guid>https://arxiv.org/abs/2512.12914</guid>
<content:encoded><![CDATA[
arXiv:2512.12914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cisco Integrated AI Security and Safety Framework Report</title>
<link>https://arxiv.org/abs/2512.12921</link>
<guid>https://arxiv.org/abs/2512.12921</guid>
<content:encoded><![CDATA[
arXiv:2512.12921v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
<link>https://arxiv.org/abs/2512.12929</link>
<guid>https://arxiv.org/abs/2512.12929</guid>
<content:encoded><![CDATA[
arXiv:2512.12929v1 Announce Type: cross 
Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
<link>https://arxiv.org/abs/2512.12932</link>
<guid>https://arxiv.org/abs/2512.12932</guid>
<content:encoded><![CDATA[
arXiv:2512.12932v1 Announce Type: cross 
Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
<link>https://arxiv.org/abs/2512.12935</link>
<guid>https://arxiv.org/abs/2512.12935</guid>
<content:encoded><![CDATA[
arXiv:2512.12935v1 Announce Type: cross 
Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content Adaptive based Motion Alignment Framework for Learned Video Compression</title>
<link>https://arxiv.org/abs/2512.12936</link>
<guid>https://arxiv.org/abs/2512.12936</guid>
<content:encoded><![CDATA[
arXiv:2512.12936v1 Announce Type: cross 
Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping</title>
<link>https://arxiv.org/abs/2512.12950</link>
<guid>https://arxiv.org/abs/2512.12950</guid>
<content:encoded><![CDATA[
arXiv:2512.12950v1 Announce Type: cross 
Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12987</link>
<guid>https://arxiv.org/abs/2512.12987</guid>
<content:encoded><![CDATA[
arXiv:2512.12987v1 Announce Type: cross 
Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
<link>https://arxiv.org/abs/2512.12997</link>
<guid>https://arxiv.org/abs/2512.12997</guid>
<content:encoded><![CDATA[
arXiv:2512.12997v1 Announce Type: cross 
Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Bidirectional Spans and Span Violations in Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.13033</link>
<guid>https://arxiv.org/abs/2512.13033</guid>
<content:encoded><![CDATA[
arXiv:2512.13033v1 Announce Type: cross 
Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</title>
<link>https://arxiv.org/abs/2512.13043</link>
<guid>https://arxiv.org/abs/2512.13043</guid>
<content:encoded><![CDATA[
arXiv:2512.13043v1 Announce Type: cross 
Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</title>
<link>https://arxiv.org/abs/2512.13063</link>
<guid>https://arxiv.org/abs/2512.13063</guid>
<content:encoded><![CDATA[
arXiv:2512.13063v1 Announce Type: cross 
Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval</title>
<link>https://arxiv.org/abs/2512.13074</link>
<guid>https://arxiv.org/abs/2512.13074</guid>
<content:encoded><![CDATA[
arXiv:2512.13074v1 Announce Type: cross 
Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
<link>https://arxiv.org/abs/2512.13089</link>
<guid>https://arxiv.org/abs/2512.13089</guid>
<content:encoded><![CDATA[
arXiv:2512.13089v1 Announce Type: cross 
Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</title>
<link>https://arxiv.org/abs/2512.13094</link>
<guid>https://arxiv.org/abs/2512.13094</guid>
<content:encoded><![CDATA[
arXiv:2512.13094v1 Announce Type: cross 
Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning</title>
<link>https://arxiv.org/abs/2512.13100</link>
<guid>https://arxiv.org/abs/2512.13100</guid>
<content:encoded><![CDATA[
arXiv:2512.13100v1 Announce Type: cross 
Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $\pi_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.13101</link>
<guid>https://arxiv.org/abs/2512.13101</guid>
<content:encoded><![CDATA[
arXiv:2512.13101v1 Announce Type: cross 
Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.13106</link>
<guid>https://arxiv.org/abs/2512.13106</guid>
<content:encoded><![CDATA[
arXiv:2512.13106v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
<link>https://arxiv.org/abs/2512.13107</link>
<guid>https://arxiv.org/abs/2512.13107</guid>
<content:encoded><![CDATA[
arXiv:2512.13107v1 Announce Type: cross 
Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing</title>
<link>https://arxiv.org/abs/2512.13109</link>
<guid>https://arxiv.org/abs/2512.13109</guid>
<content:encoded><![CDATA[
arXiv:2512.13109v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title>
<link>https://arxiv.org/abs/2512.13111</link>
<guid>https://arxiv.org/abs/2512.13111</guid>
<content:encoded><![CDATA[
arXiv:2512.13111v1 Announce Type: cross 
Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
<link>https://arxiv.org/abs/2512.13122</link>
<guid>https://arxiv.org/abs/2512.13122</guid>
<content:encoded><![CDATA[
arXiv:2512.13122v1 Announce Type: cross 
Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Image Fusion for Multi-View 3D Material Reconstruction</title>
<link>https://arxiv.org/abs/2512.13157</link>
<guid>https://arxiv.org/abs/2512.13157</guid>
<content:encoded><![CDATA[
arXiv:2512.13157v1 Announce Type: cross 
Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
<link>https://arxiv.org/abs/2512.13164</link>
<guid>https://arxiv.org/abs/2512.13164</guid>
<content:encoded><![CDATA[
arXiv:2512.13164v1 Announce Type: cross 
Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SACn: Soft Actor-Critic with n-step Returns</title>
<link>https://arxiv.org/abs/2512.13165</link>
<guid>https://arxiv.org/abs/2512.13165</guid>
<content:encoded><![CDATA[
arXiv:2512.13165v1 Announce Type: cross 
Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\tau$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments</title>
<link>https://arxiv.org/abs/2512.13174</link>
<guid>https://arxiv.org/abs/2512.13174</guid>
<content:encoded><![CDATA[
arXiv:2512.13174v1 Announce Type: cross 
Abstract: Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning</title>
<link>https://arxiv.org/abs/2512.13186</link>
<guid>https://arxiv.org/abs/2512.13186</guid>
<content:encoded><![CDATA[
arXiv:2512.13186v1 Announce Type: cross 
Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</title>
<link>https://arxiv.org/abs/2512.13190</link>
<guid>https://arxiv.org/abs/2512.13190</guid>
<content:encoded><![CDATA[
arXiv:2512.13190v1 Announce Type: cross 
Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models</title>
<link>https://arxiv.org/abs/2512.13194</link>
<guid>https://arxiv.org/abs/2512.13194</guid>
<content:encoded><![CDATA[
arXiv:2512.13194v1 Announce Type: cross 
Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Contrastive Masked Feature Reconstruction on Graphs</title>
<link>https://arxiv.org/abs/2512.13235</link>
<guid>https://arxiv.org/abs/2512.13235</guid>
<content:encoded><![CDATA[
arXiv:2512.13235v1 Announce Type: cross 
Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13290</link>
<guid>https://arxiv.org/abs/2512.13290</guid>
<content:encoded><![CDATA[
arXiv:2512.13290v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration</title>
<link>https://arxiv.org/abs/2512.13293</link>
<guid>https://arxiv.org/abs/2512.13293</guid>
<content:encoded><![CDATA[
arXiv:2512.13293v1 Announce Type: cross 
Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniLingua: A Small Open-Source LLM for European Languages</title>
<link>https://arxiv.org/abs/2512.13298</link>
<guid>https://arxiv.org/abs/2512.13298</guid>
<content:encoded><![CDATA[
arXiv:2512.13298v1 Announce Type: cross 
Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction</title>
<link>https://arxiv.org/abs/2512.13300</link>
<guid>https://arxiv.org/abs/2512.13300</guid>
<content:encoded><![CDATA[
arXiv:2512.13300v1 Announce Type: cross 
Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title>
<link>https://arxiv.org/abs/2512.13316</link>
<guid>https://arxiv.org/abs/2512.13316</guid>
<content:encoded><![CDATA[
arXiv:2512.13316v1 Announce Type: cross 
Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
<link>https://arxiv.org/abs/2512.13317</link>
<guid>https://arxiv.org/abs/2512.13317</guid>
<content:encoded><![CDATA[
arXiv:2512.13317v1 Announce Type: cross 
Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title>
<link>https://arxiv.org/abs/2512.13325</link>
<guid>https://arxiv.org/abs/2512.13325</guid>
<content:encoded><![CDATA[
arXiv:2512.13325v1 Announce Type: cross 
Abstract: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models</title>
<link>https://arxiv.org/abs/2512.13330</link>
<guid>https://arxiv.org/abs/2512.13330</guid>
<content:encoded><![CDATA[
arXiv:2512.13330v1 Announce Type: cross 
Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)</title>
<link>https://arxiv.org/abs/2512.13356</link>
<guid>https://arxiv.org/abs/2512.13356</guid>
<content:encoded><![CDATA[
arXiv:2512.13356v1 Announce Type: cross 
Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers</title>
<link>https://arxiv.org/abs/2512.13363</link>
<guid>https://arxiv.org/abs/2512.13363</guid>
<content:encoded><![CDATA[
arXiv:2512.13363v1 Announce Type: cross 
Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</title>
<link>https://arxiv.org/abs/2512.13402</link>
<guid>https://arxiv.org/abs/2512.13402</guid>
<content:encoded><![CDATA[
arXiv:2512.13402v1 Announce Type: cross 
Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents</title>
<link>https://arxiv.org/abs/2512.13438</link>
<guid>https://arxiv.org/abs/2512.13438</guid>
<content:encoded><![CDATA[
arXiv:2512.13438v1 Announce Type: cross 
Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[
arXiv:2512.13458v1 Announce Type: cross 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[
arXiv:2512.13478v1 Announce Type: cross 
Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $\rho$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping</title>
<link>https://arxiv.org/abs/2512.13494</link>
<guid>https://arxiv.org/abs/2512.13494</guid>
<content:encoded><![CDATA[
arXiv:2512.13494v1 Announce Type: cross 
Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, na\"ive low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS</title>
<link>https://arxiv.org/abs/2512.13501</link>
<guid>https://arxiv.org/abs/2512.13501</guid>
<content:encoded><![CDATA[
arXiv:2512.13501v1 Announce Type: cross 
Abstract: Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifying Rumors via Stance-Aware Structural Modeling</title>
<link>https://arxiv.org/abs/2512.13559</link>
<guid>https://arxiv.org/abs/2512.13559</guid>
<content:encoded><![CDATA[
arXiv:2512.13559v1 Announce Type: cross 
Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory in the Age of AI Agents</title>
<link>https://arxiv.org/abs/2512.13564</link>
<guid>https://arxiv.org/abs/2512.13564</guid>
<content:encoded><![CDATA[
arXiv:2512.13564v1 Announce Type: cross 
Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title>
<link>https://arxiv.org/abs/2512.13568</link>
<guid>https://arxiv.org/abs/2512.13568</guid>
<content:encoded><![CDATA[
arXiv:2512.13568v1 Announce Type: cross 
Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title>
<link>https://arxiv.org/abs/2512.13583</link>
<guid>https://arxiv.org/abs/2512.13583</guid>
<content:encoded><![CDATA[
arXiv:2512.13583v1 Announce Type: cross 
Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}{\delta} \right)}/(\sqrt{n}J\epsilon) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(\epsilon, \delta\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</title>
<link>https://arxiv.org/abs/2512.13586</link>
<guid>https://arxiv.org/abs/2512.13586</guid>
<content:encoded><![CDATA[
arXiv:2512.13586v1 Announce Type: cross 
Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
<link>https://arxiv.org/abs/2512.13600</link>
<guid>https://arxiv.org/abs/2512.13600</guid>
<content:encoded><![CDATA[
arXiv:2512.13600v1 Announce Type: cross 
Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</title>
<link>https://arxiv.org/abs/2512.13607</link>
<guid>https://arxiv.org/abs/2512.13607</guid>
<content:encoded><![CDATA[
arXiv:2512.13607v1 Announce Type: cross 
Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
<link>https://arxiv.org/abs/2512.13641</link>
<guid>https://arxiv.org/abs/2512.13641</guid>
<content:encoded><![CDATA[
arXiv:2512.13641v1 Announce Type: cross 
Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Can Leverage Human Videos for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.13644</link>
<guid>https://arxiv.org/abs/2512.13644</guid>
<content:encoded><![CDATA[
arXiv:2512.13644v1 Announce Type: cross 
Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-Language Memorization During the Classification of United States Supreme Court Cases</title>
<link>https://arxiv.org/abs/2512.13654</link>
<guid>https://arxiv.org/abs/2512.13654</guid>
<content:encoded><![CDATA[
arXiv:2512.13654v1 Announce Type: cross 
Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance</title>
<link>https://arxiv.org/abs/2512.13658</link>
<guid>https://arxiv.org/abs/2512.13658</guid>
<content:encoded><![CDATA[
arXiv:2512.13658v1 Announce Type: cross 
Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedforward 3D Editing via Text-Steerable Image-to-3D</title>
<link>https://arxiv.org/abs/2512.13678</link>
<guid>https://arxiv.org/abs/2512.13678</guid>
<content:encoded><![CDATA[
arXiv:2512.13678v1 Announce Type: cross 
Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
<link>https://arxiv.org/abs/2512.13690</link>
<guid>https://arxiv.org/abs/2512.13690</guid>
<content:encoded><![CDATA[
arXiv:2512.13690v1 Announce Type: cross 
Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OM4OV: Leveraging Ontology Matching for Ontology Versioning</title>
<link>https://arxiv.org/abs/2409.20302</link>
<guid>https://arxiv.org/abs/2409.20302</guid>
<content:encoded><![CDATA[
arXiv:2409.20302v5 Announce Type: replace 
Abstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many views treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary modifications, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and less explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignment(s) from OM to reduce the number of matching candidates and improve overall OV performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Driven Reasoning in DatalogMTL with Magic Sets</title>
<link>https://arxiv.org/abs/2412.07259</link>
<guid>https://arxiv.org/abs/2412.07259</guid>
<content:encoded><![CDATA[
arXiv:2412.07259v4 Announce Type: replace 
Abstract: DatalogMTL is a powerful rule-based language for temporal reasoning. Due to its high expressive power and flexible modeling capabilities, it is suitable for a wide range of applications, including tasks from industrial and financial sectors. However, due to its high computational complexity, practical reasoning in DatalogMTL is highly challenging. To address this difficulty, we introduce a new reasoning method for DatalogMTL which exploits the magic sets technique -- a rewriting approach developed for (non-temporal) Datalog to simulate top-down evaluation with bottom-up reasoning. We have implemented this approach and evaluated it on publicly available benchmarks, showing that the proposed approach significantly and consistently outperformed state-of-the-art reasoning techniques.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Traitors: Deception and Trust in Multi-Agent Language Model Simulations</title>
<link>https://arxiv.org/abs/2505.12923</link>
<guid>https://arxiv.org/abs/2505.12923</guid>
<content:encoded><![CDATA[
arXiv:2505.12923v2 Announce Type: replace 
Abstract: As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v5 Announce Type: replace 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMELLNET: A Large-scale Dataset for Real-world Smell Recognition</title>
<link>https://arxiv.org/abs/2506.00239</link>
<guid>https://arxiv.org/abs/2506.00239</guid>
<content:encoded><![CDATA[
arXiv:2506.00239v3 Announce Type: replace 
Abstract: The ability of AI to sense and identify various substances based on their smell alone can have profound impacts on allergen detection (e.g., smelling gluten or peanuts in a cake), monitoring the manufacturing process, and sensing hormones that indicate emotional states, stress levels, and diseases. Despite these broad impacts, there are virtually no large-scale benchmarks, and therefore little progress, for training and evaluating AI systems' ability to smell in the real world. In this paper, we use small gas and chemical sensors to create SmellNet, the first large-scale database that digitizes a diverse range of smells in the natural world. SmellNet contains about 828,000 data points across 50 substances, spanning nuts, spices, herbs, fruits, and vegetables, and 43 mixtures among them, with 68 hours of data collected. Using SmellNet, we developed ScentFormer, a Transformer-based architecture combining temporal differencing and sliding-window augmentation for smell data. For the SmellNet-Base classification task, ScentFormer achieves 58.5% Top-1 accuracy, and for the SmellNet-Mixture distribution prediction task, ScentFormer achieves 50.2% Top-1@0.1 on the test-seen split. ScentFormer's ability to generalize across conditions and capture transient chemical dynamics demonstrates the promise of temporal modeling in olfactory AI. SmellNet and ScentFormer lay the groundwork for real-world olfactory applications across healthcare, food and beverage, environmental monitoring, manufacturing, and entertainment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
arXiv:2506.07927v3 Announce Type: replace 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Copilots for Reproducibility in Science: A Case Study</title>
<link>https://arxiv.org/abs/2506.20130</link>
<guid>https://arxiv.org/abs/2506.20130</guid>
<content:encoded><![CDATA[
arXiv:2506.20130v4 Announce Type: replace 
Abstract: Open science initiatives seek to make research outputs more transparent, accessible, and reusable, but ensuring that published findings can be independently reproduced remains a persistent challenge. In this paper we describe an AI-driven "Reproducibility Copilot" that analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or "rote", reproducibility. Our initial results suggest that the copilot has the potential to substantially reduce reproduction time (in one case from over 30 hours to about 1 hour) while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing values for hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets. Although preliminary, these findings suggest that AI tools can meaningfully reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference AI Systems for Scientific Discovery</title>
<link>https://arxiv.org/abs/2506.21329</link>
<guid>https://arxiv.org/abs/2506.21329</guid>
<content:encoded><![CDATA[
arXiv:2506.21329v4 Announce Type: replace 
Abstract: The rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation -- exploring counterfactual spaces where physical laws can be temporarily violated to discover new patterns -- and reasoning as fast, deterministic validation, traversing established knowledge graphs to test consistency with known principles. Abstractions in this loop should be manipulable models that enable counterfactual prediction, causal attribution, and refinement. Design principles -- rather than a monolithic recipe -- are proposed for systems that reason in imaginary spaces and learn from the world: causal, multimodal models for internal simulation; persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established claims; formal verification pathways coupled to computations and experiments. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties make human judgment indispensable, not as a temporary scaffold but as a permanent architectural component. Evaluations must assess the system's ability to identify novel phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward genuine discoveries.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title>
<link>https://arxiv.org/abs/2508.19005</link>
<guid>https://arxiv.org/abs/2508.19005</guid>
<content:encoded><![CDATA[
arXiv:2508.19005v5 Announce Type: replace 
Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Common Framework for Autoformalization</title>
<link>https://arxiv.org/abs/2509.09810</link>
<guid>https://arxiv.org/abs/2509.09810</guid>
<content:encoded><![CDATA[
arXiv:2509.09810v3 Announce Type: replace 
Abstract: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations</title>
<link>https://arxiv.org/abs/2509.24250</link>
<guid>https://arxiv.org/abs/2509.24250</guid>
<content:encoded><![CDATA[
arXiv:2509.24250v2 Announce Type: replace 
Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale</title>
<link>https://arxiv.org/abs/2509.25540</link>
<guid>https://arxiv.org/abs/2509.25540</guid>
<content:encoded><![CDATA[
arXiv:2509.25540v2 Announce Type: replace 
Abstract: Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What Matters: The AI Pluralism Index</title>
<link>https://arxiv.org/abs/2510.08193</link>
<guid>https://arxiv.org/abs/2510.08193</guid>
<content:encoded><![CDATA[
arXiv:2510.08193v2 Announce Type: replace 
Abstract: Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks for language, vision, and coding are common, yet public, auditable measures of pluralistic governance are rare. We define AI pluralism as the degree to which affected stakeholders can shape objectives, data practices, safeguards, and deployment. We present the AI Pluralism Index (AIPI), a transparent, evidence-based instrument that evaluates producers and system families across four pillars: participatory governance, inclusivity and diversity, transparency, and accountability. AIPI codes verifiable practices from public artifacts and independent evaluations, explicitly handling "Unknown" evidence to report both lower-bound ("evidence") and known-only scores with coverage. We formalize the measurement model; implement a reproducible pipeline that integrates structured web and repository analysis, external assessments, and expert interviews; and assess reliability with inter-rater agreement, coverage reporting, cross-index correlations, and sensitivity analysis. The protocol, codebook, scoring scripts, and evidence graph are maintained openly with versioned releases and a public adjudication process. We report pilot provider results and situate AIPI relative to adjacent transparency, safety, and governance frameworks. The index aims to steer incentives toward pluralistic practice and to equip policymakers, procurers, and the public with comparable evidence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
<link>https://arxiv.org/abs/2510.14925</link>
<guid>https://arxiv.org/abs/2510.14925</guid>
<content:encoded><![CDATA[
arXiv:2510.14925v3 Announce Type: replace 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition in linear-Gaussian state-space models via H-Risk, a composite instability index integrating spectral margin, conditioning, temporal sensitivity, and innovation amplification. In simulations, higher H-Risk predicts overconfident errors and degraded closed-loop behavior even when the dynamics remain formally stable, exposing a gap between nominal and epistemic stability.
  Extending this stability lens to large language models (LLMs), we introduce a domain-wise proxy based on confidence fluctuations and overconfident errors. In a binary-question study, a Kantian-inspired policy that permits ''cannot judge'' responses yields targeted reductions in policy-aware squared loss in high-stakes domains relative to an overconfident baseline. To probe internal dynamics, we analyse layer-wise sensitivity of hidden states to small input perturbations. Contrary to a naive instability hypothesis, confidently wrong answers show no instability gap; instead, they are at least as locally stable as confidently correct answers, revealing stable miscalibration in which hallucinations behave like robust but misaligned attractors. For Qwen-2.5, spectral and activation profiles suggest a high signal-to-noise, low effective signal temperature regime in which representations become inertial and resistant to contextual shifts. These results bridge Kantian self-limitation and feedback control, and suggest that stable high-confidence hallucinations may not be readily corrected by output-only heuristics (e.g., temperature scaling or re-sampling), motivating process-level interventions that explicitly perturb and re-evaluate the inference trajectory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue</title>
<link>https://arxiv.org/abs/2510.21720</link>
<guid>https://arxiv.org/abs/2510.21720</guid>
<content:encoded><![CDATA[
arXiv:2510.21720v2 Announce Type: replace 
Abstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted framework designed to bridge the gap between isolated predictive modeling and an interactive system for psychological analysis. The methodology encompasses a rigorous, end-to-end development lifecycle. First, foundational performance benchmarks were established on four diverse psychological datasets using classical machine learning techniques. Second, state-of-the-art transformer models were fine-tuned, a process that necessitated the development of effective solutions to overcome critical engineering challenges, including the resolution of numerical instability in regression tasks and the creation of a systematic workflow for conducting large-scale training under severe resource constraints. Third, a generative large language model (LLM) was fine-tuned using parameter-efficient techniques to function as an interactive "Personality Brain." Finally, the entire suite of predictive and generative models was architected and deployed as a robust, scalable microservices ecosystem. Key findings include the successful stabilization of transformer-based regression models for affective computing, showing meaningful predictive performance where standard approaches failed, and the development of a replicable methodology for democratizing large-scale AI research. The significance of this work lies in its holistic approach, demonstrating a complete research-to-deployment pipeline that integrates predictive analysis with generative dialogue, thereby providing a practical model for future research in computational psychology and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making</title>
<link>https://arxiv.org/abs/2511.12876</link>
<guid>https://arxiv.org/abs/2511.12876</guid>
<content:encoded><![CDATA[
arXiv:2511.12876v2 Announce Type: replace 
Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[
arXiv:2511.15407v2 Announce Type: replace 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark</title>
<link>https://arxiv.org/abs/2511.17729</link>
<guid>https://arxiv.org/abs/2511.17729</guid>
<content:encoded><![CDATA[
arXiv:2511.17729v3 Announce Type: replace 
Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures</title>
<link>https://arxiv.org/abs/2511.17833</link>
<guid>https://arxiv.org/abs/2511.17833</guid>
<content:encoded><![CDATA[
arXiv:2511.17833v2 Announce Type: replace 
Abstract: Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Localisation in Localist LLMs</title>
<link>https://arxiv.org/abs/2511.18375</link>
<guid>https://arxiv.org/abs/2511.18375</guid>
<content:encoded><![CDATA[
arXiv:2511.18375v3 Announce Type: replace 
Abstract: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate five locality configurations: two uniform baselines (fully distributed and fully localist) and three progressive polynomial schedules. We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty</title>
<link>https://arxiv.org/abs/2511.21569</link>
<guid>https://arxiv.org/abs/2511.21569</guid>
<content:encoded><![CDATA[
arXiv:2511.21569v4 Announce Type: replace 
Abstract: Self-transparency is a critical safety boundary, requiring language models to honestly disclose their limitations and artificial nature. This study stress-tests this capability, investigating whether models willingly disclose their identity when assigned professional personas that conflict with transparent self-representation. When models prioritize role consistency over this boundary disclosure, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% -- an 8.8-fold difference that emerged at the initial epistemic inquiry. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($\Delta R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($\kappa=0.908$). Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs</title>
<link>https://arxiv.org/abs/2512.00319</link>
<guid>https://arxiv.org/abs/2512.00319</guid>
<content:encoded><![CDATA[
arXiv:2512.00319v2 Announce Type: replace 
Abstract: The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</title>
<link>https://arxiv.org/abs/2512.03005</link>
<guid>https://arxiv.org/abs/2512.03005</guid>
<content:encoded><![CDATA[
arXiv:2512.03005v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Soccer Substitutions</title>
<link>https://arxiv.org/abs/2512.04480</link>
<guid>https://arxiv.org/abs/2512.04480</guid>
<content:encoded><![CDATA[
arXiv:2512.04480v3 Announce Type: replace 
Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI &amp; Human Co-Improvement for Safer Co-Superintelligence</title>
<link>https://arxiv.org/abs/2512.05356</link>
<guid>https://arxiv.org/abs/2512.05356</guid>
<content:encoded><![CDATA[
arXiv:2512.05356v2 Announce Type: replace 
Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach</title>
<link>https://arxiv.org/abs/2210.00858</link>
<guid>https://arxiv.org/abs/2210.00858</guid>
<content:encoded><![CDATA[
arXiv:2210.00858v4 Announce Type: replace-cross 
Abstract: In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power of deep networks. We generate a 3D vision-and-language synthetic dataset of tabletop scenes in a simulation environment to train our approach and perform extensive evaluations in both synthetic and real-world scenes. Results showcase the benefits of our approach in terms of accuracy, sample-efficiency, and robustness to the user's vocabulary, while being transferable to real-world scenes with few-shot visual fine-tuning. Finally, we integrate our method with a robot framework and demonstrate how it can serve as an interpretable solution for an interactive object-picking task, achieving an average success rate of 80.2\%, both in simulation and with a real robot. We make supplementary material available in https://gtziafas.github.io/neurosymbolic-manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADS: Plug-and-Play 3D Human Pose Analysis via Diffusion Generative Modeling</title>
<link>https://arxiv.org/abs/2401.08930</link>
<guid>https://arxiv.org/abs/2401.08930</guid>
<content:encoded><![CDATA[
arXiv:2401.08930v2 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated impressive capabilities in modeling complex data distributions and are increasingly applied in various generative tasks. In this work, we propose Pose Analysis by Diffusion Synthesis PADS, a unified generative modeling framework for 3D human pose analysis. PADS first learns a task-agnostic 3D pose prior via unconditional diffusion synthesis and then performs training-free adaptation to a wide range of pose analysis tasks, including 3D pose estimation, denoising, completion, etc., through a posterior sampling scheme. By formulating each task as an inverse problem with a known forward operator, PADS injects task-specific constraints during inference while keeping the pose prior fixed. This plug-and-play framework removes the need for task-specific supervision or retraining, offering flexibility and scalability across diverse conditions. Extensive experiments on different benchmarks showcase the superior performance against both learning-based and optimization-based baselines, demonstrating the effectiveness and generalization capability of our method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Self-Supervised Learning for Recommendation</title>
<link>https://arxiv.org/abs/2404.03354</link>
<guid>https://arxiv.org/abs/2404.03354</guid>
<content:encoded><![CDATA[
arXiv:2404.03354v3 Announce Type: replace-cross 
Abstract: Recommender systems play a crucial role in tackling the challenge of information overload by delivering personalized recommendations based on individual user preferences. Deep learning techniques, such as RNNs, GNNs, and Transformer architectures, have significantly propelled the advancement of recommender systems by enhancing their comprehension of user behaviors and preferences. However, supervised learning methods encounter challenges in real-life scenarios due to data sparsity, resulting in limitations in their ability to learn representations effectively. To address this, self-supervised learning (SSL) techniques have emerged as a solution, leveraging inherent data structures to generate supervision signals without relying solely on labeled data. By leveraging unlabeled data and extracting meaningful representations, recommender systems utilizing SSL can make accurate predictions and recommendations even when confronted with data sparsity. In this paper, we provide a comprehensive review of self-supervised learning frameworks designed for recommender systems, encompassing a thorough analysis of over 170 papers. We conduct an exploration of nine distinct scenarios, enabling a comprehensive understanding of SSL-enhanced recommenders in different contexts. For each domain, we elaborate on different self-supervised learning paradigms, namely contrastive learning, generative learning, and adversarial learning, so as to present technical details of how SSL enhances recommender systems in various contexts. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-SSLRec-Papers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Wrong-way Cycling Detection in CCTV Videos: Sparse Sampling is All You Need</title>
<link>https://arxiv.org/abs/2405.07293</link>
<guid>https://arxiv.org/abs/2405.07293</guid>
<content:encoded><![CDATA[
arXiv:2405.07293v2 Announce Type: replace-cross 
Abstract: Effective monitoring of unusual transportation behaviors, such as wrong-way cycling (i.e., riding a bicycle or e-bike against designated traffic flow), is crucial for optimizing law enforcement deployment and traffic planning. However, accurately recording all wrong-way cycling events is both unnecessary and infeasible in resource-constrained environments, as it requires high-resolution cameras for evidence collection and event detection. To address this challenge, we propose WWC-Predictor, a novel method for efficiently estimating the wrong-way cycling ratio, defined as the proportion of wrong-way cycling events relative to the total number of cycling movements over a given time period. The core innovation of our method lies in accurately detecting wrong-way cycling events in sparsely sampled frames using a light-weight detector, then estimating the overall ratio using an autoregressive moving average model. To evaluate the effectiveness of our method, we construct a benchmark dataset consisting of 35 minutes of video sequences with minute-level annotations.Our method achieves an average error rate of a mere 1.475\% while consuming only 19.12\% GPU time required by conventional tracking methods, validating its effectiveness in estimating the wrong-way cycling ratio. Our source code is publicly available at: https://github.com/VICA-Lab-HKUST-GZ/WWC-Predictor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Neural Common Neighbor for Temporal Graph Link Prediction</title>
<link>https://arxiv.org/abs/2406.07926</link>
<guid>https://arxiv.org/abs/2406.07926</guid>
<content:encoded><![CDATA[
arXiv:2406.07926v2 Announce Type: replace-cross 
Abstract: Temporal graphs are widespread in real-world applications such as social networks, as well as trade and transportation networks. Predicting dynamic links within these evolving graphs is a key problem. Many memory-based methods use temporal interaction histories to generate node embeddings, which are then combined to predict links. However, these approaches primarily focus on individual node representations, often overlooking the inherently pairwise nature of link prediction. While some recent methods attempt to capture pairwise features, they tend to be limited by high computational complexity arising from repeated embedding calculations, making them unsuitable for large-scale datasets like the Temporal Graph Benchmark (TGB). To address the critical need for models that combine strong expressive power with high computational efficiency for link prediction on large temporal graphs, we propose Temporal Neural Common Neighbor (TNCN). Our model achieves this balance by adapting the powerful pairwise modeling principles of Neural Common Neighbor (NCN) to an efficient temporal architecture. TNCN improves upon NCN by efficiently preserving and updating temporal neighbor dictionaries for each node and by using multi-hop common neighbors to learn more expressive pairwise representations. TNCN achieves new state-of-the-art performance on Review from five large-scale real-world TGB datasets, 6 out of 7 datasets in the transductive setting and 3 out of 7 in the inductive setting on small- to medium-scale datasets. Additionally, TNCN demonstrates excellent scalability, outperforming prominent GNN baselines by up to 30.3 times in speed on large datasets. Our code is available at https://github.com/GraphPKU/TNCN.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dy-mer: An Explainable DNA Sequence Representation Scheme using Dictionary Learning</title>
<link>https://arxiv.org/abs/2407.12051</link>
<guid>https://arxiv.org/abs/2407.12051</guid>
<content:encoded><![CDATA[
arXiv:2407.12051v2 Announce Type: replace-cross 
Abstract: DNA sequences encode critical genetic information, yet their variable length and discrete nature impede direct utilization in deep learning models. Existing DNA representation schemes convert sequences into numerical vectors but fail to capture structural features of local subsequences and often suffer from limited interpretability and poor generalization on small datasets. To address these limitations, we propose Dy-mer, an interpretable and robust DNA representation scheme based on dictionary learning. Dy-mer formulates an optimization problem in tensor format, which ensures computational efficiency in batch processing. Our scheme reconstructs DNA sequences as concatenations of dynamic-length subsequences (dymers) through a convolution operation and simultaneously optimize a learnable dymer dictionary and sparse representations. Our method achieves state-of-the-art performance in downstream tasks such as DNA promoter classification and motif detection. Experiments further show that the learned dymers match known DNA motifs and clustering using Dy-mer yields semantically meaningful phylogenetic trees. These results demonstrate that the proposed approach achieves both strong predictive performance and high interpretability, making it well suited for biological research applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Simultaneous Multislice MRI Reconstruction Using Slice-Wise Learned Generative Diffusion Priors</title>
<link>https://arxiv.org/abs/2407.21600</link>
<guid>https://arxiv.org/abs/2407.21600</guid>
<content:encoded><![CDATA[
arXiv:2407.21600v3 Announce Type: replace-cross 
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2408.06069</link>
<guid>https://arxiv.org/abs/2408.06069</guid>
<content:encoded><![CDATA[
arXiv:2408.06069v2 Announce Type: replace-cross 
Abstract: Deep Gaussian process models typically employ discrete hierarchies, but recent advancements in differential Gaussian processes (DiffGPs) have extended these models to infinite depths. However, existing DiffGP approaches often overlook the uncertainty in kernel hyperparameters by treating them as fixed and time-invariant, which degrades the model's predictive performance and neglects the posterior distribution. In this work, we introduce a fully Bayesian framework that models kernel hyperparameters as random variables and utilizes coupled stochastic differential equations (SDEs) to jointly learn their posterior distributions alongside those of inducing points. By incorporating the estimation uncertainty of hyperparameters, our method significantly enhances model flexibility and adaptability to complex dynamic systems. Furthermore, we employ a black-box adaptive SDE solver with a neural network to achieve realistic, time varying posterior approximations, thereby improving the expressiveness of the variational posterior. Comprehensive experimental evaluations demonstrate that our approach outperforms traditional methods in terms of flexibility, accuracy, and other key performance metrics. This work not only provides a robust Bayesian extension to DiffGP models but also validates its effectiveness in handling intricate dynamic behaviors, thereby advancing the applicability of Gaussian process models in diverse real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Screening is More Efficient with Multiple Objects</title>
<link>https://arxiv.org/abs/2408.10077</link>
<guid>https://arxiv.org/abs/2408.10077</guid>
<content:encoded><![CDATA[
arXiv:2408.10077v2 Announce Type: replace-cross 
Abstract: We study efficient mechanism design for allocating multiple heterogeneous objects. The aim is to maximize the residual surplus, the total value generated from an allocation minus the costs of screening. We discover a robust trend indicating that no-screening mechanisms, such as serial dictatorship with exogenous priority order, tend to perform better as the variety of goods increases. We analyze the underlying reasons by characterizing asymptotically efficient mechanisms in a stylized environment. We also apply an automated mechanism design approach to numerically derive efficient mechanisms and validate the trend in general environments. Building on these implications, we propose the \emph{register-invite-book system} (RIB) as an efficient system for scheduling vaccination against pandemic diseases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-aware TDNN: Speaker Recognition Using Multi-Layer Features from Pre-Trained Models</title>
<link>https://arxiv.org/abs/2409.07770</link>
<guid>https://arxiv.org/abs/2409.07770</guid>
<content:encoded><![CDATA[
arXiv:2409.07770v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised learning (SSL) on Transformers have significantly improved speaker verification (SV) by providing domain-general speech representations. However, existing approaches have underutilized the multi-layered nature of SSL encoders. To address this limitation, we propose the layer-aware time-delay neural network (L-TDNN), which directly performs layer/frame-wise processing on the layer-wise hidden state outputs from pre-trained models, extracting fixed-size speaker vectors. L-TDNN comprises a layer-aware convolutional network, a frame-adaptive layer aggregation, and attentive statistic pooling, explicitly modeling of the recognition and processing of previously overlooked layer dimension. We evaluated L-TDNN across multiple speech SSL Transformers and diverse speech-speaker corpora against other approaches for leveraging pre-trained encoders. L-TDNN consistently demonstrated robust verification performance, achieving the lowest error rates throughout the experiments. Concurrently, it stood out in terms of model compactness and exhibited inference efficiency comparable to the existing systems. These results highlight the advantages derived from the proposed layer-aware processing approach. Future work includes exploring joint training with SSL frontends and the incorporation of score calibration to further enhance state-of-the-art verification performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAISI: Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2409.11169</link>
<guid>https://arxiv.org/abs/2409.11169</guid>
<content:encoded><![CDATA[
arXiv:2409.11169v3 Announce Type: replace-cross 
Abstract: Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion</title>
<link>https://arxiv.org/abs/2411.14507</link>
<guid>https://arxiv.org/abs/2411.14507</guid>
<content:encoded><![CDATA[
arXiv:2411.14507v3 Announce Type: replace-cross 
Abstract: Structured pruning of Generative Pre-trained Transformers (GPTs) offers a promising path to efficiency but often suffers from irreversible performance degradation due to the discarding of transformer blocks. In this paper, we introduce FuseGPT, a compression paradigm that reframes structured pruning as iterative knowledge grafting rather than simple removal. Motivated by the observation that linear block merging fails to capture non-linear feature disparities and that block importance fluctuates dynamically during pruning, FuseGPT employs a dual-strategy pipeline. First, we propose Macro Influence (MI), a dynamic fusion-aware metric that continuously re-evaluates block redundancy as the network topology evolves. Second, instead of rigid parameter averaging, we introduce a learnable low-rank fusion mechanism that adaptively grafts the knowledge of pruned blocks onto surviving layers via lightweight local distillation. Extensive experiments on LLaMA, Mistral, Qwen, and Phi families demonstrate that FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier: at 25\% sparsity, FuseGPT achieves lower perplexity than prior methods at 20\% sparsity, improves zero-shot reasoning by up to 4.5 points, and delivers 1.33$\times$ inference speedup with 25\% memory reduction. Furthermore, FuseGPT is orthogonal to quantization, achieving 52.1\% total compression with negligible quality loss when combined with 4-bit GPTQ. We make our code publicly available at https://github.com/JarvisPei/FuseGPT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment</title>
<link>https://arxiv.org/abs/2412.04783</link>
<guid>https://arxiv.org/abs/2412.04783</guid>
<content:encoded><![CDATA[
arXiv:2412.04783v5 Announce Type: replace-cross 
Abstract: Wireless sensing has recently found widespread applications in diverse environments, including homes, offices, and public spaces. By analyzing patterns in channel state information (CSI), it is possible to infer human actions for tasks such as person identification, gesture recognition, and fall detection. However, CSI is highly sensitive to environmental changes, where even minor alterations can significantly distort the CSI patterns. This sensitivity often leads to performance degradation or outright failure when applying wireless sensing models trained in one environment to another. To address this challenge, Domain Alignment (DAL) has been widely adopted for cross-domain classification tasks, as it focuses on aligning the global distributions of the source and target domains in feature space. Despite its popularity, DAL often neglects inter-category relationships, which can lead to misalignment between categories across domains, even when global alignment is achieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless sensing. Our approach begins by constructing a help set using KNN from the target domain, enabling local alignment between the source and target domains within each category using MMD. Additionally, we address a key instability issue commonly observed in cross-domain methods, where model performance fluctuates sharply between epochs. Further, most existing methods struggle to determine an optimal stopping point during training due to the absence of labeled data from the target domain. Our method resolves this by excluding the support set from the target domain during training and employing it as a validation set to determine the stopping criterion. The dataset and code are publicly available at https://github.com/RS2002/KNN-MMD .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</title>
<link>https://arxiv.org/abs/2501.10100</link>
<guid>https://arxiv.org/abs/2501.10100</guid>
<content:encoded><![CDATA[
arXiv:2501.10100v5 Announce Type: replace-cross 
Abstract: Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Benchmarks: On The False Promise of AI Regulation</title>
<link>https://arxiv.org/abs/2501.15693</link>
<guid>https://arxiv.org/abs/2501.15693</guid>
<content:encoded><![CDATA[
arXiv:2501.15693v2 Announce Type: replace-cross 
Abstract: The performance of AI models on safety benchmarks does not indicate their real-world performance after deployment. This opaqueness of AI models impedes existing regulatory frameworks constituted on benchmark performance, leaving them incapable of mitigating ongoing real-world harm. The problem stems from a fundamental challenge in AI interpretability, which seems to be overlooked by regulators and decision makers. We propose a simple, realistic and readily usable regulatory framework which does not rely on benchmarks, and call for interdisciplinary collaboration to find new ways to address this crucial problem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</title>
<link>https://arxiv.org/abs/2502.13764</link>
<guid>https://arxiv.org/abs/2502.13764</guid>
<content:encoded><![CDATA[
arXiv:2502.13764v4 Announce Type: replace-cross 
Abstract: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-context Learning of Evolving Data Streams with Tabular Foundational Models</title>
<link>https://arxiv.org/abs/2502.16840</link>
<guid>https://arxiv.org/abs/2502.16840</guid>
<content:encoded><![CDATA[
arXiv:2502.16840v2 Announce Type: replace-cross 
Abstract: State-of-the-art data stream mining has long drawn from ensembles of the Very Fast Decision Tree, a seminal algorithm honored with the 2015 KDD Test-of-Time Award. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees, such as Adaptive Random Forest, and Streaming Random Patches, across all non-stationary benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2503.07874</link>
<guid>https://arxiv.org/abs/2503.07874</guid>
<content:encoded><![CDATA[
arXiv:2503.07874v2 Announce Type: replace-cross 
Abstract: Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Burst-Sampled Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2503.10253</link>
<guid>https://arxiv.org/abs/2503.10253</guid>
<content:encoded><![CDATA[
arXiv:2503.10253v4 Announce Type: replace-cross 
Abstract: Deep learning has shown strong potential in modeling complex spatiotemporal dynamics. However, most existing methods depend on densely and uniformly sampled data, which is often unavailable in practice due to sensor and cost limitations. In many real-world settings, such as mobile sensing and physical experiments, data are burst-sampled with short high-frequency segments followed by long gaps, making it difficult to learn accurate dynamics from sparse observations. To address this issue, we propose Physics-Informed Multi-Scale Recurrent Learning (PIMRL), a novel framework specifically designed for burst-sampled spatiotemporal data. PIMRL combines macro-scale latent dynamics inference with micro-scale adaptive refinement guided by incomplete prior information from partial differential equations (PDEs). It further introduces a temporal message-passing mechanism to effectively propagate information across burst intervals. This multi-scale architecture enables PIMRL to model complex systems accurately even under severe data scarcity. We evaluate our approach on five benchmark datasets involving 1D to 3D multi-scale PDEs. The results show that PIMRL consistently outperforms state-of-the-art baselines, achieving substantial improvements and reducing errors by up to 80% in the most challenging settings, which demonstrates the clear advantage of our model. Our work demonstrates the effectiveness of physics-informed recurrent learning for accurate and efficient modeling of sparse spatiotemporal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI</title>
<link>https://arxiv.org/abs/2503.11950</link>
<guid>https://arxiv.org/abs/2503.11950</guid>
<content:encoded><![CDATA[
arXiv:2503.11950v4 Announce Type: replace-cross 
Abstract: The increasing integration of artificial intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups-young digital citizens, parents/educators, and AI professionals-and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from key participants through structured surveys, qualitative interviews, and focus groups to identify distinct privacy expectations. Young digital citizens emphasized autonomy and digital agency, while parents and educators prioritized oversight and AI literacy. AI professionals focused on balancing ethical design with system performance. The analysis revealed significant gaps in transparency and digital literacy, underscoring the need for inclusive, stakeholder-driven privacy frameworks. Drawing on comparative thematic analysis, this study introduces the Privacy-Ethics Alignment in AI (PEA-AI) model, which conceptualizes privacy decision-making as a dynamic negotiation among stakeholders. By aligning empirical findings with governance implications, this research provides a scalable foundation for adaptive, youth-centered AI privacy governance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
arXiv:2503.14505v2 Announce Type: replace-cross 
Abstract: We introduce MusicInfuser, an approach that aligns pre-trained text-to-video diffusion models to generate high-quality dance videos synchronized with specified music tracks. Rather than training a multimodal audio-video or audio-motion model from scratch, our method demonstrates how existing video diffusion models can be efficiently adapted to align with musical inputs. We propose a novel layer-wise adaptability criterion based on a guidance-inspired constructive influence function to select adaptable layers, significantly reducing training costs while preserving rich prior knowledge, even with limited, specialized datasets. Experiments show that MusicInfuser effectively bridges the gap between music and video, generating novel and diverse dance movements that respond dynamically to music. Furthermore, our framework generalizes well to unseen music tracks, longer video sequences, and unconventional subjects, outperforming baseline models in consistency and synchronization. All of this is achieved without requiring motion data, with training completed on a single GPU within a day.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v3 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2504.03494</link>
<guid>https://arxiv.org/abs/2504.03494</guid>
<content:encoded><![CDATA[
arXiv:2504.03494v3 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPS) in domains such as manufacturing and energy distribution generate complex time series data crucial for Prognostics and Health Management (PHM). While Deep Learning (DL) methods have demonstrated strong forecasting capabilities, their adoption in industrial CPS remains limited due insufficient robustness. Existing robustness evaluations primarily focus on formal verification or adversarial perturbations, inadequately representing the complexities encountered in real-world CPS scenarios. To address this, we introduce a practical robustness definition grounded in distributional robustness, explicitly tailored to industrial CPS, and propose a systematic framework for robustness evaluation. Our framework simulates realistic disturbances, such as sensor drift, noise and irregular sampling, enabling thorough robustness analyses of forecasting models on real-world CPS datasets. The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets, assisting in informed model selection and architecture design. Through extensive empirical studies evaluating prominent DL architectures (including recurrent, convolutional, attention-based, modular, and structured state-space models) we demonstrate the applicability and effectiveness of our approach. We publicly release our robustness benchmark to encourage further research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints</title>
<link>https://arxiv.org/abs/2505.05019</link>
<guid>https://arxiv.org/abs/2505.05019</guid>
<content:encoded><![CDATA[
arXiv:2505.05019v2 Announce Type: replace-cross 
Abstract: The generation of synthetic clinical trial data offers a promising approach to mitigating privacy concerns and data accessibility limitations in medical research. However, ensuring that synthetic datasets maintain high fidelity, utility, and adherence to domain-specific constraints remains a key challenge. While hyperparameter optimization (HPO) improves generative model performance, the effectiveness of different optimization strategies for synthetic clinical data remains unclear. This study systematically evaluates four HPO objectives across nine generative models, comparing single-metric to compound metric optimization. Our results demonstrate that HPO consistently improves synthetic data quality, with Tab DDPM achieving the largest relative gains, followed by TVAE (60%), CTGAN (39%), and CTAB-GAN+ (38%). Compound metric optimization outperformed single-metric objectives, producing more generalizable synthetic datasets. Despite improving overall quality, HPO alone fails to prevent violations of essential clinical survival constraints. Preprocessing and postprocessing played a crucial role in reducing these violations, as models lacking robust processing steps produced invalid data in up to 61% of cases. These findings underscore the necessity of integrating explicit domain knowledge alongside HPO to generate high-quality synthetic datasets. Our study provides actionable recommendations for improving synthetic data generation, with future work needed to refine metric selection and validate findings on larger datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectrumFM: A Foundation Model for Intelligent Spectrum Management</title>
<link>https://arxiv.org/abs/2505.06256</link>
<guid>https://arxiv.org/abs/2505.06256</guid>
<content:encoded><![CDATA[
arXiv:2505.06256v2 Announce Type: replace-cross 
Abstract: Intelligent spectrum management is crucial for improving spectrum efficiency and achieving secure utilization of spectrum resources. However, existing intelligent spectrum management methods, typically based on small-scale models, suffer from notable limitations in recognition accuracy, convergence speed, and generalization, particularly in the complex and dynamic spectrum environments. To address these challenges, this paper proposes a novel spectrum foundation model, termed SpectrumFM, establishing a new paradigm for spectrum management. SpectrumFM features an innovative encoder architecture that synergistically exploits the convolutional neural networks and the multi-head self-attention mechanisms to enhance feature extraction and enable robust representation learning. The model is pre-trained via two novel self-supervised learning tasks, namely masked reconstruction and next-slot signal prediction, which leverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive and transferable spectrum representations. Furthermore, a parameter-efficient fine-tuning strategy is proposed to enable SpectrumFM to adapt to various downstream spectrum management tasks, including automatic modulation classification (AMC), wireless technology classification (WTC), spectrum sensing (SS), and anomaly detection (AD). Extensive experiments demonstrate that SpectrumFM achieves superior performance in terms of accuracy, robustness, adaptability, few-shot learning efficiency, and convergence speed, consistently outperforming conventional methods across multiple benchmarks. Specifically, SpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%, achieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise ratio (SNR), and enhances AD performance by over 10%.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</title>
<link>https://arxiv.org/abs/2505.08919</link>
<guid>https://arxiv.org/abs/2505.08919</guid>
<content:encoded><![CDATA[
arXiv:2505.08919v2 Announce Type: replace-cross 
Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Centric Embodied Question Answering</title>
<link>https://arxiv.org/abs/2505.13948</link>
<guid>https://arxiv.org/abs/2505.13948</guid>
<content:encoded><![CDATA[
arXiv:2505.13948v2 Announce Type: replace-cross 
Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and comprehend the environment to answer context-dependent questions. Typically, an EQA framework consists of four components: a planner, a memory module, a stopping module, and an answering module. However, the memory module is utilized inefficiently in existing methods, as the information it stores is leveraged solely for the answering module. Such a design may result in redundant or inadequate exploration, leading to a suboptimal success rate. To solve this problem, we propose MemoryEQA, an EQA framework centered on memory, which establishes mechanisms for memory storage, update, and retrieval, allowing memory information to contribute throughout the entire exploration process. Specifically, we convert the observation into structured textual representations, which are stored in a vector library following a fixed structure. At each exploration step, we utilize a viewpoint comparison strategy to determine whether the memory requires updating. Before executing each module, we employ an entropy-based adaptive retrieval strategy to obtain the minimal yet sufficient memory information that satisfies the requirements of different modules. The retrieved module-specific information is then integrated with the current observation as input to the corresponding module. To evaluate EQA models' memory capabilities, we constructed the benchmark based on HM3D called MT-HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 9.9% performance gain on MT-HM3D compared to baseline models further underscores the memory capability's pivotal role in solving complex tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[
arXiv:2505.15201v4 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Graph Pattern Machine</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[
arXiv:2505.16130v3 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance. To this end, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable and transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node/link/graph classification, transfer learning, and cross-graph pretraining -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</title>
<link>https://arxiv.org/abs/2505.18231</link>
<guid>https://arxiv.org/abs/2505.18231</guid>
<content:encoded><![CDATA[
arXiv:2505.18231v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling</title>
<link>https://arxiv.org/abs/2505.19609</link>
<guid>https://arxiv.org/abs/2505.19609</guid>
<content:encoded><![CDATA[
arXiv:2505.19609v2 Announce Type: replace-cross 
Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models</title>
<link>https://arxiv.org/abs/2505.19700</link>
<guid>https://arxiv.org/abs/2505.19700</guid>
<content:encoded><![CDATA[
arXiv:2505.19700v4 Announce Type: replace-cross 
Abstract: The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \textit{Residual Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[
arXiv:2505.24592v3 Announce Type: replace-cross 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruptions and adversarial attacks. Data augmentation is one of the most prevalent and effective ways to enhance robustness. Despite the great success of the diverse augmentations in different fields, a unified theoretical understanding of their efficacy in improving model robustness is lacking. We theoretically reveal a general condition for label-preserving augmentations to bring robustness to diverse distribution shifts through the lens of flat minima and generalization bound, which de facto turns out to be strongly correlated with robustness against different distribution shifts in practice. Unlike most earlier works, our theoretical framework accommodates all the label-preserving augmentations and is not limited to particular distribution shifts. We substantiate our theories through different simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
arXiv:2505.24850v2 Announce Type: replace-cross 
Abstract: Recent advances in model distillation show that data from advanced reasoning models can effectively train smaller student models. However, standard practices discard incorrect reasoning traces -- valuable, yet underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? We employ a two-stage training recipe: first, Supervised Fine-Tuning (SFT) on positive traces, followed by a refinement stage using both positive and negative traces. We find that a simple REINFORCE-style objective, which we term the Reinforcement Distillation (REDI) objective, outperforms established preference optimization methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate the effectiveness of this approach. Notably, our Qwen-REDI-1.5B model, trained on just 131k traces from the open Open-R1 dataset, achieves an 83.1% score on MATH-500. Its performance matches that of DeepSeek-R1-Distill-Qwen-1.5B, a model trained on 800k proprietary data. This result showcases the remarkable data efficiency of utilizing previously discarded negative traces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[
arXiv:2506.00227v2 Announce Type: replace-cross 
Abstract: Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification</title>
<link>https://arxiv.org/abs/2506.01983</link>
<guid>https://arxiv.org/abs/2506.01983</guid>
<content:encoded><![CDATA[
arXiv:2506.01983v2 Announce Type: replace-cross 
Abstract: Identification of antimicrobial peptides is an important and necessary issue in today's era. Antimicrobial peptides are essential as an alternative to antibiotics for biomedical applications and many other practical applications. These oligopeptides are useful in drug design and cause innate immunity against microorganisms. Artificial intelligence algorithms have played a significant role in the ease of identifying these peptides.This research is improved by improving proposed method in the field of antimicrobial peptides prediction. Suggested method is improved by combining the best coding method from different perspectives, In the following a deep neural network to balance the imbalanced combined datasets. The results of this research show that the proposed method have a significant improvement in the accuracy and efficiency of the prediction of antimicrobial peptides and are able to provide the best results compared to the existing methods. These development in the field of prediction and classification of antimicrobial peptides, basically in the fields of medicine and pharmaceutical industries, have high effectiveness and application.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[
arXiv:2506.02454v3 Announce Type: replace-cross 
Abstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82% overall win rate over the baseline method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How PARTs assemble into wholes: Learning the relative composition of images</title>
<link>https://arxiv.org/abs/2506.03682</link>
<guid>https://arxiv.org/abs/2506.03682</guid>
<content:encoded><![CDATA[
arXiv:2506.03682v2 Announce Type: replace-cross 
Abstract: The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance</title>
<link>https://arxiv.org/abs/2506.06522</link>
<guid>https://arxiv.org/abs/2506.06522</guid>
<content:encoded><![CDATA[
arXiv:2506.06522v3 Announce Type: replace-cross 
Abstract: Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning</title>
<link>https://arxiv.org/abs/2506.12041</link>
<guid>https://arxiv.org/abs/2506.12041</guid>
<content:encoded><![CDATA[
arXiv:2506.12041v3 Announce Type: replace-cross 
Abstract: We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmenting Visuals With Querying Words: Language Anchors For Semi-Supervised Image Segmentation</title>
<link>https://arxiv.org/abs/2506.13925</link>
<guid>https://arxiv.org/abs/2506.13925</guid>
<content:encoded><![CDATA[
arXiv:2506.13925v3 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) provide rich semantic priors but are underexplored in Semi supervised Semantic Segmentation. Recent attempts to integrate VLMs to inject high level semantics overlook the semantic misalignment between visual and textual representations that arises from using domain invariant text embeddings without adapting them to dataset and image specific contexts. This lack of domain awareness, coupled with limited annotations, weakens the model semantic understanding by preventing effective vision language alignment. As a result, the model struggles with contextual reasoning, shows weak intra class discrimination, and confuses similar classes. To address these challenges, we propose Hierarchical Vision Language transFormer (HVLFormer), which achieves domain aware and domain robust alignment between visual and textual representations within a mask transformer architecture. Firstly, we transform text embeddings from pretrained VLMs into textual object queries, enabling the generation of multi scale, dataset aware queries that capture class semantics from coarse to fine granularity and enhance contextual reasoning. Next, we refine these queries by injecting image specific visual context to align textual semantics with local scene structures and enhance class discrimination. Finally, to achieve domain robustness, we introduce cross view and modal consistency regularization, which enforces prediction consistency within mask-transformer architecture across augmented views. Moreover, it ensures stable vision language alignment during decoding. With less than 1% training data, HVLFormer outperforms state of the art methods on Pascal VOC, COCO, ADE20K, and Cityscapes. Our code and results will be available on GitHub.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace</title>
<link>https://arxiv.org/abs/2506.21127</link>
<guid>https://arxiv.org/abs/2506.21127</guid>
<content:encoded><![CDATA[
arXiv:2506.21127v3 Announce Type: replace-cross 
Abstract: Autonomous UAV navigation using reinforcement learning (RL) is vulnerable to adversarial attacks that manipulate sensor inputs, potentially leading to unsafe behavior and mission failure. Although robust RL methods provide partial protection, they often struggle to generalize to unseen or out-of-distribution (OOD) attacks due to their reliance on fixed perturbation settings. To address this limitation, we propose a meta-policy switching framework in which a meta-level polic dynamically selects among multiple robust policies to counter unknown adversarial shifts. At the core of this framework lies a discounted Thompson sampling (DTS) mechanism that formulates policy selection as a multi-armed bandit problem, thereby minimizing value distribution shifts via self-induced adversarial observations. We first construct a diverse ensemble of action-robust policies trained under varying perturbation intensities. The DTS-based meta-policy then adaptively selects among these policies online, optimizing resilience against self-induced, piecewise-stationary attacks. Theoretical analysis shows that the DTS mechanism minimizes expected regret, ensuring adaptive robustness to OOD attacks and exhibiting emergent antifragile behavior under uncertainty. Extensive simulations in complex 3D obstacle environments under both white-box (Projected Gradient Descent) and black-box (GPS spoofing) attacks demonstrate significantly improved navigation efficiency and higher conflict free trajectory rates compared to standard robust and vanilla RL baselines, highlighting the practical security and dependability benefits of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems</title>
<link>https://arxiv.org/abs/2506.21502</link>
<guid>https://arxiv.org/abs/2506.21502</guid>
<content:encoded><![CDATA[
arXiv:2506.21502v2 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPSs) tightly interconnect digital and physical operations within production environments, enabling real-time monitoring, control, optimization, and autonomous decision-making that directly enhance manufacturing processes and productivity. The inherent complexity of these systems can lead to faults that require robust and interpretable diagnoses to maintain system dependability and operational efficiency. However, manual modeling of faulty behaviors requires extensive domain expertise and cannot leverage the low-level sensor data of the CPS. Furthermore, although powerful, deep learning-based techniques produce black-box diagnostics that lack interpretability, limiting their practical adoption. To address these challenges, we set forth a method that performs unsupervised characterization of system states and state transitions from low-level sensor data, uses several process mining techniques to model faults through interpretable stochastic Petri nets, simulates such Petri nets for a comprehensive understanding of system behavior under faulty conditions, and performs Petri net-based fault diagnosis. The method is applied to the Robotic Arm Dataset (RoAD), a benchmark collected from a robotic arm deployed in a scale-replica smart manufacturing assembly line. The application to RoAD demonstrates the method's effectiveness in modeling, simulating, and classifying faulty behaviors in CPSs. The modeling results demonstrate that our method achieves a satisfactory interpretability-simulation accuracy trade-off with up to 0.676 arc-degree simplicity, 0.395 R^2, and 0.088 RMSE. In addition, the fault identification results show that the method achieves an F1 score of up to 98.925%, while maintaining a low conformance checking time of 0.020 seconds, which competes with other deep learning-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[
arXiv:2506.23046v3 Announce Type: replace-cross 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</title>
<link>https://arxiv.org/abs/2506.23260</link>
<guid>https://arxiv.org/abs/2506.23260</guid>
<content:encoded><![CDATA[
arXiv:2506.23260v2 Announce Type: replace-cross 
Abstract: Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces enable real-time data retrieval, computation, and multi-step orchestration. However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation. This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications. We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities. For each category, we provide a formal threat formulation defining attacker capabilities, objectives, and affected system layers. Representative examples include Prompt-to-SQL injections and the Toxic Agent Flow exploit in GitHub MCP servers. We analyze attack feasibility, review existing defenses, and discuss mitigation strategies such as dynamic trust management, cryptographic provenance tracking, and sandboxed agent interfaces. The framework is validated through expert review and cross-mapping with real-world incidents and public vulnerability repositories, including CVE and NIST NVD. Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</title>
<link>https://arxiv.org/abs/2507.00057</link>
<guid>https://arxiv.org/abs/2507.00057</guid>
<content:encoded><![CDATA[
arXiv:2507.00057v2 Announce Type: replace-cross 
Abstract: Generating code from a natural language programming task is one of the most successful applications of Large Language Models (LLMs). Yet, the generated program may be buggy. Without an oracle, such as an existing, correct implementation or a formal specification, can we somehow estimate how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called *incoherence*, that can be estimated efficiently in the absence of an oracle and allows us to establish a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. In our experiments, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives for the average task. In fact, *an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation*. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via incoherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</title>
<link>https://arxiv.org/abs/2507.05622</link>
<guid>https://arxiv.org/abs/2507.05622</guid>
<content:encoded><![CDATA[
arXiv:2507.05622v3 Announce Type: replace-cross 
Abstract: The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available in https://github.com/shaoshuo-ss/DATABench.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Collectivist, Economic Perspective on AI</title>
<link>https://arxiv.org/abs/2507.06268</link>
<guid>https://arxiv.org/abs/2507.06268</guid>
<content:encoded><![CDATA[
arXiv:2507.06268v3 Announce Type: replace-cross 
Abstract: Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word ``intelligence'' is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals and that much of our intelligence is social and cultural in origin. Moreover, failing to properly situate aspects of intelligence at the social level contributes to the treatment of the societal consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts at the level of algorithm design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2507.18577</link>
<guid>https://arxiv.org/abs/2507.18577</guid>
<content:encoded><![CDATA[
arXiv:2507.18577v2 Announce Type: replace-cross 
Abstract: The advent of foundation models (FMs), large-scale pre-trained models with strong generalization capabilities, has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of financial foundation models (FFMs): a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: financial language foundation models (FinLFMs), financial time-series foundation models (FinTSFMs), and financial visual-language foundation models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints and offer insights into future research opportunities. We hope this survey can serve as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamiX: Large-Scale Dynamic Social Network Simulator</title>
<link>https://arxiv.org/abs/2507.19929</link>
<guid>https://arxiv.org/abs/2507.19929</guid>
<content:encoded><![CDATA[
arXiv:2507.19929v2 Announce Type: replace-cross 
Abstract: Understanding the intrinsic mechanisms of social platforms is an urgent demand to maintain social stability. The rise of large language models provides significant potential for social network simulations to capture attitude dynamics and reproduce collective behaviors. However, existing studies mainly focus on scaling up agent populations, neglecting the dynamic evolution of social relationships. To address this gap, we introduce DynamiX, a novel large-scale social network simulator dedicated to dynamic social network modeling. DynamiX uses a dynamic hierarchy module for selecting core agents with key characteristics at each timestep, enabling accurate alignment of real-world adaptive switching of user roles. Furthermore, we design distinct dynamic social relationship modeling strategies for different user types. For opinion leaders, we propose an information-stream-based link prediction method recommending potential users with similar stances, simulating homogeneous connections, and autonomous behavior decisions. For ordinary users, we construct an inequality-oriented behavior decision-making module, effectively addressing unequal social interactions and capturing the patterns of relationship adjustments driven by multi-dimensional factors. Experimental results demonstrate that DynamiX exhibits marked improvements in attitude evolution simulation and collective behavior analysis compared to static networks. Besides, DynamiX opens a new theoretical perspective on follower growth prediction, providing empirical evidence for opinion leaders cultivation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Language Models Discover Scaling Laws?</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
arXiv:2507.21184v3 Announce Type: replace-cross 
Abstract: Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate seven diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding and Engineering the Phytobiome Communication for Smart Agriculture</title>
<link>https://arxiv.org/abs/2508.03584</link>
<guid>https://arxiv.org/abs/2508.03584</guid>
<content:encoded><![CDATA[
arXiv:2508.03584v2 Announce Type: replace-cross 
Abstract: Smart agriculture applications, integrating technologies like the Internet of Things and machine learning/artificial intelligence (ML/AI) into agriculture, hold promise to address modern challenges of rising food demand, environmental pollution, and water scarcity. Alongside the concept of the phytobiome, which defines the area including the plant, its environment, and associated organisms, and the recent emergence of molecular communication (MC), there exists an important opportunity to advance agricultural science and practice using communication theory. In this article, we motivate to use the communication engineering perspective for developing a holistic understanding of the phytobiome communication and bridge the gap between the phytobiome communication and smart agriculture. Firstly, an overview of phytobiome communication via molecular and electrophysiological signals is presented and a multi-scale framework modeling the phytobiome as a communication network is conceptualized. Then, how this framework is used to model electrophysiological signals is demonstrated with plant experiments. Furthermore, possible smart agriculture applications, such as smart irrigation and targeted delivery of agrochemicals, through engineering the phytobiome communication are proposed. These applications merge ML/AI methods with the Internet of Bio-Nano-Things enabled by MC and pave the way towards more efficient, sustainable, and eco-friendly agricultural production. Finally, the implementation challenges, open research issues, and industrial outlook for these applications are discussed.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</title>
<link>https://arxiv.org/abs/2508.07710</link>
<guid>https://arxiv.org/abs/2508.07710</guid>
<content:encoded><![CDATA[
arXiv:2508.07710v2 Announce Type: replace-cross 
Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for energy-efficient Transformer architectures.While ANN-to-SNN conversion avoids the high training cost of directly trained Spiking Transformers, existing approaches still struggle to handle the nonlinear operations within Transformer blocks, and often require additional fine-tuning of pretrained ANNs.To address these limitations, we propose a training-free and high-performance ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron that combines exponential decay with a multi-basis encoding strategy to effectively approximate nonlinear operations, eliminating the need for weight modifications in pretrained ANNs.Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ethics Practices in AI Development: An Empirical Study Across Roles and Regions</title>
<link>https://arxiv.org/abs/2508.09219</link>
<guid>https://arxiv.org/abs/2508.09219</guid>
<content:encoded><![CDATA[
arXiv:2508.09219v2 Announce Type: replace-cross 
Abstract: Recent advances in AI applications have raised growing concerns about the need for ethical guidelines and regulations to mitigate the risks posed by these technologies. In this paper, we present a mixed-methods survey study - combining statistical and qualitative analyses - to examine the ethical perceptions, practices, and knowledge of individuals involved in various AI development roles. Our survey comprises 414 participants from 43 countries, representing various roles such as AI managers, analysts, developers, quality assurance professionals, and information security and privacy experts. The results reveal varying degrees of familiarity and experience with AI ethics principles, government initiatives, and risk mitigation strategies across roles, regions, and other demographic factors. Our findings underscore the importance of a collaborative, role-sensitive approach that involves diverse stakeholders in ethical decision-making throughout the AI development lifecycle. We advocate for developing tailored, inclusive solutions to address ethical challenges in AI development, and we propose future research directions and educational strategies to promote ethics-aware AI practices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Understanding from Reasoning via Problem Space Mapping for Small-Scale Model Reasoning</title>
<link>https://arxiv.org/abs/2508.10019</link>
<guid>https://arxiv.org/abs/2508.10019</guid>
<content:encoded><![CDATA[
arXiv:2508.10019v2 Announce Type: replace-cross 
Abstract: Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., up to 1.5B parameters) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title>
<link>https://arxiv.org/abs/2508.11009</link>
<guid>https://arxiv.org/abs/2508.11009</guid>
<content:encoded><![CDATA[
arXiv:2508.11009v3 Announce Type: replace-cross 
Abstract: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[
arXiv:2508.11849v3 Announce Type: replace-cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN: Word Association Learning for Cultural Alignment in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13426</link>
<guid>https://arxiv.org/abs/2508.13426</guid>
<content:encoded><![CDATA[
arXiv:2508.13426v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit cultural bias from overrepresented viewpoints in training data, yet cultural alignment remains a challenge due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient and cognitively grounded method: fine-tuning LLMs on native speakers' word-association norms, leveraging cognitive psychology findings that such associations capture cultural knowledge. Using word association datasets from native speakers in the US (English) and China (Mandarin), we train Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning and preference optimization. We evaluate models' cultural alignment through a two-tier evaluation framework that spans lexical associations and cultural value alignment using the World Values Survey. Results show significant improvements in lexical alignment (16-20% English, 43-165% Mandarin on Precision@5) and high-level cultural value shifts. On a subset of 50 questions where US and Chinese respondents diverge most, fine-tuned Qwen nearly doubles its response alignment with Chinese values (13 to 25). Remarkably, our trained 7-8B models match or exceed vanilla 70B baselines, demonstrating that a few million of culture-grounded associations achieve value alignment without expensive retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning in Focus: Detecting Behavioral and Collaborative Engagement Using Vision Transformers</title>
<link>https://arxiv.org/abs/2508.15782</link>
<guid>https://arxiv.org/abs/2508.15782</guid>
<content:encoded><![CDATA[
arXiv:2508.15782v2 Announce Type: replace-cross 
Abstract: In early childhood education, accurately detecting collaborative and behavioral engagement is essential to foster meaningful learning experiences. This paper presents an AI driven approach that leverages Vision Transformers (ViTs) to automatically classify children s engagement using visual cues such as gaze direction, interaction, and peer collaboration. Utilizing the ChildPlay gaze dataset, our method is trained on annotated video segments to classify behavioral and collaborative engagement states (e.g., engaged, not engaged, collaborative, not collaborative). We evaluated six state of the art transformer models: Vision Transformer (ViT), Data efficient Image Transformer (DeiT), Swin Transformer, VitGaze, APVit and GazeTR. Among these, the Swin Transformer achieved the highest classification performance with an accuracy of 97.58 percent, demonstrating its effectiveness in modeling local and global attention. Our results highlight the potential of transformer based architectures for scalable, automated engagement analysis in real world educational settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System</title>
<link>https://arxiv.org/abs/2508.15811</link>
<guid>https://arxiv.org/abs/2508.15811</guid>
<content:encoded><![CDATA[
arXiv:2508.15811v2 Announce Type: replace-cross 
Abstract: Generative query suggestion using large language models offers a powerful way to enhance conversational systems, but aligning outputs with nuanced user preferences remains a critical challenge. To address this, we introduce a multi-stage framework designed for progressive alignment between the generation policy and user intent. Our pipeline begins with prompt engineering as a cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we introduce a distillation method on click logs to create a robust foundational model. To better model user preferences while capturing their inherent uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user preferences as probability distributions rather than point estimates. Finally, we employ reinforcement learning to align the generation policy with these preferences, guided by a composite reward function that integrates GaRM with auxiliary heuristics to mitigate reward hacking. To maintain training stability, this process is enhanced by a novel out-of-distribution regularization method and a two-stage reward fusion technique. Extensive experiments demonstrate that our framework significantly outperforms baselines on both automatic and human evaluations and yields a 34\% relative increase in user engagement as measured by click-through rate in live A/B tests.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConceptGuard: Neuro-Symbolic Safety Guardrails via Sparse Interpretable Jailbreak Concepts</title>
<link>https://arxiv.org/abs/2508.16325</link>
<guid>https://arxiv.org/abs/2508.16325</guid>
<content:encoded><![CDATA[
arXiv:2508.16325v2 Announce Type: replace-cross 
Abstract: Large Language Models have found success in a variety of applications. However, their safety remains a concern due to the existence of various jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a range of vulnerabilities, including targeted misuse and accidental user profiling. This work introduces \textbf{ConceptGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, ConceptGuard enables building robust safety guardrails -- offering fully explainable and generalizable defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in the mechanistic interpretability of LLMs, our approach provides evidence for a shared activation geometry for jailbreak attacks in the representation space, a potential foundation for designing more interpretable and generalizable safeguards against attackers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations</title>
<link>https://arxiv.org/abs/2508.16634</link>
<guid>https://arxiv.org/abs/2508.16634</guid>
<content:encoded><![CDATA[
arXiv:2508.16634v5 Announce Type: replace-cross 
Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation</title>
<link>https://arxiv.org/abs/2508.16994</link>
<guid>https://arxiv.org/abs/2508.16994</guid>
<content:encoded><![CDATA[
arXiv:2508.16994v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) systems are widely adopted in knowledge-intensive NLP tasks, but current evaluations often overlook the structural complexity and multi-step reasoning required in real-world scenarios. These benchmarks overlook key factors such as the interaction between retrieval difficulty and reasoning depth. To address this gap, we propose GRADE, a novel evaluation framework that models task difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the number of inference steps (hops), and (2) semantic distance between the query and its supporting evidence. We construct a synthetic multi-hop QA dataset from factual news articles by extracting knowledge graphs and augmenting them through semantic clustering to recover missing links, allowing us to generate diverse and difficulty-controlled queries. Central to our framework is a 2D difficulty matrix that combines generator-side and retriever-side difficulty. Experiments across multiple domains and models show that error rates strongly correlate with our difficulty measures, validating their diagnostic utility. GRADE enables fine-grained analysis of RAG performance and provides a scalable foundation for evaluating and improving multi-hop reasoning in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2508.17364</link>
<guid>https://arxiv.org/abs/2508.17364</guid>
<content:encoded><![CDATA[
arXiv:2508.17364v3 Announce Type: replace-cross 
Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[
arXiv:2509.02718v3 Announce Type: replace-cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput. Our code is available at https://github.com/fzwark/PORT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference</title>
<link>https://arxiv.org/abs/2509.04467</link>
<guid>https://arxiv.org/abs/2509.04467</guid>
<content:encoded><![CDATA[
arXiv:2509.04467v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a pruning method that is highly integrated with PD disaggregation, enabling more precise pruning of blocks. Our approach constructs pruning and distillation sets to perform iterative block removal, obtaining better pruning solutions. Moreover, we analyze the pruning sensitivity of the prefill and decode stages and identify removable blocks specific to each stage, making it well suited for PD disaggregation deployment. Extensive experiments demonstrate our approach consistently achieves strong performance in both PD disaggregation and PD unified (non-PD disaggregation) settings, and can also be extended to other non-block pruning methods. Under the same settings, our method achieves improved performance and faster inference.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MALLM: Multi-Agent Large Language Models Framework</title>
<link>https://arxiv.org/abs/2509.11656</link>
<guid>https://arxiv.org/abs/2509.11656</guid>
<content:encoded><![CDATA[
arXiv:2509.11656v3 Announce Type: replace-cross 
Abstract: Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic bootstrapped pretraining</title>
<link>https://arxiv.org/abs/2509.15248</link>
<guid>https://arxiv.org/abs/2509.15248</guid>
<content:encoded><![CDATA[
arXiv:2509.15248v3 Announce Type: replace-cross 
Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter and a 6B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers up to 60% of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Highly Imbalanced Regression with Tabular Data in SEP and Other Applications</title>
<link>https://arxiv.org/abs/2509.16339</link>
<guid>https://arxiv.org/abs/2509.16339</guid>
<content:encoded><![CDATA[
arXiv:2509.16339v4 Announce Type: replace-cross 
Abstract: We investigate imbalanced regression with tabular data that have an imbalance ratio larger than 1,000 ("highly imbalanced"). Accurately estimating the target values of rare instances is important in applications such as forecasting the intensity of rare harmful Solar Energetic Particle (SEP) events. For regression, the MSE loss does not consider the correlation between predicted and actual values. Typical inverse importance functions allow only convex functions. Uniform sampling might yield mini-batches that do not have rare instances. We propose CISIR that incorporates correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling. Based on five datasets, our experimental results indicate that CISIR can achieve lower error and higher correlation than some recent methods. Also, adding our correlation component to other recent methods can improve their performance. Lastly, MDI importance can outperform other importance functions. Our code can be found in https://github.com/Machine-Earning/CISIR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mamba Modulation: On the Length Generalization of Mamba</title>
<link>https://arxiv.org/abs/2509.19633</link>
<guid>https://arxiv.org/abs/2509.19633</guid>
<content:encoded><![CDATA[
arXiv:2509.19633v3 Announce Type: replace-cross 
Abstract: The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.22621</link>
<guid>https://arxiv.org/abs/2509.22621</guid>
<content:encoded><![CDATA[
arXiv:2509.22621v2 Announce Type: replace-cross 
Abstract: Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and two model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing</title>
<link>https://arxiv.org/abs/2509.23103</link>
<guid>https://arxiv.org/abs/2509.23103</guid>
<content:encoded><![CDATA[
arXiv:2509.23103v2 Announce Type: replace-cross 
Abstract: Reducing the cost of multiplications is critical for efficient deep neural network deployment, especially in energy-constrained edge devices. In this work, we introduce HTMA-Net, a novel framework that integrates the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to reduce arithmetic complexity while maintaining accuracy. Unlike prior methods that only target multiplications in convolutional layers or focus solely on in-memory acceleration, HTMA-Net selectively replaces intermediate convolutions with Hybrid Hadamard-based transform layers whose internal convolutions are implemented via multiplication-avoiding in-memory operations. We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet, and provide a detailed comparison against regular, MF-only, and HT-only variants. Results show that HTMA-Net eliminates up to 52\% of multiplications compared to baseline ResNet-18, ResNet-20, and ResNet-32 models, while achieving comparable accuracy in evaluation and significantly reducing computational complexity and the number of parameters. Our results demonstrate that combining structured Hadamard transform layers with SRAM-based in-memory computing multiplication-avoiding operators is a promising path towards efficient deep learning architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks</title>
<link>https://arxiv.org/abs/2509.23687</link>
<guid>https://arxiv.org/abs/2509.23687</guid>
<content:encoded><![CDATA[
arXiv:2509.23687v2 Announce Type: replace-cross 
Abstract: Integrated sensing and communication (ISAC) emerges as a key enabler for next-generation applications such as smart cities and autonomous systems. Its integration with unmanned aerial vehicles (UAVs) unlocks new potentials for reliable communication and precise sensing in dynamic aerial environments. However, existing research predominantly treats UAVs as aerial base stations, overlooking their role as ISAC users, and fails to leverage large-scale antenna arrays at terrestrial base stations to enhance security and spectral efficiency. This paper propose a secure and spectral efficient ISAC framework for multi-UAV networks, and a two-stage optimization approach is developed to jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage employs Proximal Policy Optimization (PPO) to optimize digital beamformers and trajectories, and the second stage decomposes the digital solution into analog and digital components via low-complexity matrix factorization. Simulation results demonstrate the effectiveness of the proposed framework compared to benchmark schemes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</title>
<link>https://arxiv.org/abs/2510.02967</link>
<guid>https://arxiv.org/abs/2510.02967</guid>
<content:encoded><![CDATA[
arXiv:2510.02967v3 Announce Type: replace-cross 
Abstract: This paper presents the development and evaluation of a Retrieval-Augmented Generation (RAG) system for querying the United Kingdom's National Institute for Health and Care Excellence (NICE) clinical guidelines using Large Language Models (LLMs). The extensive length and volume of these guidelines can impede their utilisation within a time-constrained healthcare system, a challenge this project addresses through the creation of a system capable of providing users with precisely matched information in response to natural language queries. The system's retrieval architecture, composed of a hybrid embedding mechanism, was evaluated against a corpus of 10,195 text chunks derived from three hundred guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR) of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten retrieved chunks, when evaluated on 7901 queries. The most significant impact of the RAG system was observed during the generation phase. When evaluated on a manually curated dataset of seventy question-answer pairs, RAG-enhanced models showed substantial gains in performance. Faithfulness, the measure of whether an answer is supported by the source text, was increased by 64.7 percentage points to 99.5% for the RAG-enhanced O4-Mini model and significantly outperformed the medical-focused Meditron3-8B LLM, which scored 43%. Clinical evaluation by seven Subject Matter Experts (SMEs) further validated these findings, with GPT-4.1 achieving 98.7% accuracy while reducing unsafe responses by 67% compared to O4-Mini (from 3.0 to 1.0 per evaluator). This study thus establishes RAG as an effective, reliable, and scalable approach for applying generative AI in healthcare, enabling cost-effective access to medical guidelines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2510.04146</link>
<guid>https://arxiv.org/abs/2510.04146</guid>
<content:encoded><![CDATA[
arXiv:2510.04146v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and code generation. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. While these models have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency in next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output tokens in parallel, mitigating the limitations of sequential decoding. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive study of the performance characteristics of ARMs and DLMs, combining theoretical analysis with empirical profiling to characterize the trade-offs between these approaches. We show that although DLMs can achieve higher arithmetic intensity than ARMs by leveraging parallelism across token positions, they fail to scale effectively with longer contexts. We then explore block-wise decoding for DLMs, which decouples arithmetic intensity from sequence length and enables better scaling to long contexts (similar to ARMs). We also examine batched inference and find that ARMs exhibit superior throughput as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, emphasizing that reducing the number of sampling steps is key for open-source DLMs to achieve lower latency relative to ARMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.07084</link>
<guid>https://arxiv.org/abs/2510.07084</guid>
<content:encoded><![CDATA[
arXiv:2510.07084v3 Announce Type: replace-cross 
Abstract: Transformer-based methods have achieved impressive results in time series forecasting. However, existing Transformers still exhibit limitations in sequence modeling as they tend to overemphasize temporal dependencies. This incurs additional computational overhead without yielding corresponding performance gains. We find that the performance of Transformers is highly dependent on the embedding method used to learn effective representations. To address this issue, we extract multivariate features to augment the effective information captured in the embedding layer, yielding multidimensional embeddings that convey richer and more meaningful sequence representations. These representations enable Transformer-based forecasters to better understand the series. Specifically, we introduce Hybrid Temporal and Multivariate Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature extraction module with a carefully designed multivariate feature extraction module to provide complementary features, thereby achieving a balance between model complexity and performance. By combining HTME with the Transformer architecture, we present HTMformer, leveraging the enhanced feature extraction capability of the HTME extractor to build a lightweight forecaster. Experiments conducted on eight real-world datasets demonstrate that our approach outperforms existing baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title>
<link>https://arxiv.org/abs/2510.07620</link>
<guid>https://arxiv.org/abs/2510.07620</guid>
<content:encoded><![CDATA[
arXiv:2510.07620v2 Announce Type: replace-cross 
Abstract: Dynamic trust evaluation in large, rapidly evolving graphs demands models that capture changing relationships, express calibrated confidence, and resist adversarial manipulation. DGTEN (Deep Gaussian-Based Trust Evaluation Network) introduces a unified graph-based framework that does all three by combining uncertainty-aware message passing, expressive temporal modeling, and built-in defenses against trust-targeted attacks. It represents nodes and edges as Gaussian distributions so that both semantic signals and epistemic uncertainty propagate through the graph neural network, enabling risk-aware trust decisions rather than overconfident guesses. To track how trust evolves, it layers hybrid absolute-Gaussian-hourglass positional encoding with Kolmogorov-Arnold network-based unbiased multi-head attention, then applies an ordinary differential equation-based residual learning module to jointly model abrupt shifts and smooth trends. Robust adaptive ensemble coefficient analysis prunes or down-weights suspicious interactions using complementary cosine and Jaccard similarity, curbing reputation laundering, sabotage, and on-off attacks. On two signed Bitcoin trust networks, DGTEN delivers standout gains where it matters most: in single-timeslot prediction on Bitcoin-OTC, it improves MCC by +12.34% over the best dynamic baseline; in the cold-start scenario on Bitcoin-Alpha, it achieves a +25.00% MCC improvement, the largest across all tasks and datasets; while under adversarial on-off attacks, it surpasses the baseline by up to +10.23% MCC. These results endorse the unified DGTEN framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Lenses on the AI Revolution: Risk, Transformation, Continuity</title>
<link>https://arxiv.org/abs/2510.12859</link>
<guid>https://arxiv.org/abs/2510.12859</guid>
<content:encoded><![CDATA[
arXiv:2510.12859v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) has emerged as both a continuation of historical technological revolutions and a potential rupture with them. This paper argues that AI must be viewed simultaneously through three lenses: \textit{risk}, where it resembles nuclear technology in its irreversible and global externalities; \textit{transformation}, where it parallels the Industrial Revolution as a general-purpose technology driving productivity and reorganization of labor; and \textit{continuity}, where it extends the fifty-year arc of computing revolutions from personal computing to the internet to mobile. Drawing on historical analogies, we emphasize that no past transition constituted a strict singularity: disruptive shifts eventually became governable through new norms and institutions.
  We examine recurring patterns across revolutions -- democratization at the usage layer, concentration at the production layer, falling costs, and deepening personalization -- and show how these dynamics are intensifying in the AI era. Sectoral analysis illustrates how accounting, law, education, translation, advertising, and software engineering are being reshaped as routine cognition is commoditized and human value shifts to judgment, trust, and ethical responsibility. At the frontier, the challenge of designing moral AI agents highlights the need for robust guardrails, mechanisms for moral generalization, and governance of emergent multi-agent dynamics.
  We conclude that AI is neither a singular break nor merely incremental progress. It is both evolutionary and revolutionary: predictable in its median effects yet carrying singularity-class tail risks. Good outcomes are not automatic; they require coupling pro-innovation strategies with safety governance, ensuring equitable access, and embedding AI within a human order of responsibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
<link>https://arxiv.org/abs/2510.14974</link>
<guid>https://arxiv.org/abs/2510.14974</guid>
<content:encoded><![CDATA[
arXiv:2510.14974v2 Announce Type: replace-cross 
Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>cuAPO: A CUDA-based Parallelization of Artificial Protozoa Optimizer</title>
<link>https://arxiv.org/abs/2510.14982</link>
<guid>https://arxiv.org/abs/2510.14982</guid>
<content:encoded><![CDATA[
arXiv:2510.14982v2 Announce Type: replace-cross 
Abstract: Metaheuristic algorithms are widely used for solving complex problems due to their ability to provide near-optimal solutions. But the execution time of these algorithms increases with the problem size and/or solution space. And, to get more promising results, we have to execute these algorithms for a large number of iterations, requiring a large amount of time and this is one of the main issues found with these algorithms. To handle the same, researchers are now-a-days working on design and development of parallel versions of state-of-the-art metaheuristic optimization algorithms. We, in this paper, present a CUDA-based parallelization of state-of-the-art Artificial Protozoa Optimizer leveraging GPU acceleration. We implement both the existing sequential version and the proposed parallel version of Artificial Protozoa Optimizer for a performance comparison. Our experimental results calculated over a set of CEC2022 benchmark functions demonstrate a significant performance gain i.e. up to 6.7 times speed up is achieved with proposed parallel version. We also use a real world application, i.e., Image Thresholding to compare both algorithms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title>
<link>https://arxiv.org/abs/2510.17904</link>
<guid>https://arxiv.org/abs/2510.17904</guid>
<content:encoded><![CDATA[
arXiv:2510.17904v2 Announce Type: replace-cross 
Abstract: The proficiency of Large Language Models (LLMs) in processing structured data and adhering to syntactic rules is a capability that drives their widespread adoption but also makes them paradoxically vulnerable. In this paper, we investigate this vulnerability through BreakFun, a jailbreak methodology that weaponizes an LLM's adherence to structured schemas. BreakFun employs a three-part prompt that combines an innocent framing and a Chain-of-Thought distraction with a core "Trojan Schema"--a carefully crafted data structure that compels the model to generate harmful content, exploiting the LLM's strong tendency to follow structures and schemas. We demonstrate this vulnerability is highly transferable, achieving an average success rate of 89% across 13 foundational and proprietary models on JailbreakBench, and reaching a 100% Attack Success Rate (ASR) on several prominent models. A rigorous ablation study confirms this Trojan Schema is the attack's primary causal factor. To counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a defense that utilizes a secondary LLM to perform a "Literal Transcription"--extracting all human-readable text to isolate and reveal the user's true harmful intent. Our proof-of-concept guardrail demonstrates high efficacy against the attack, validating that targeting the deceptive schema is a viable mitigation strategy. Our work provides a look into how an LLM's core strengths can be turned into critical weaknesses, offering a fresh perspective for building more robustly aligned models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection</title>
<link>https://arxiv.org/abs/2510.21118</link>
<guid>https://arxiv.org/abs/2510.21118</guid>
<content:encoded><![CDATA[
arXiv:2510.21118v3 Announce Type: replace-cross 
Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a given source document is essential for real-world applications. While prior research has explored LLM faithfulness, existing benchmarks suffer from annotation ambiguity, primarily due to the ill-defined boundary of permissible external knowledge in generated outputs. For instance, common sense is often incorporated into responses and labeled as "faithful", yet the acceptable extent of such knowledge remains unspecified, leading to inconsistent annotations. To address this issue, we propose a novel faithfulness annotation framework, which introduces an intermediate category, Out-Dependent, to classify cases where external knowledge is required for verification. Using this framework, we construct VeriGray (Verification with the Gray Zone) -- a new unfaithfulness detection benchmark in summarization. Statistics reveal that even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences) in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on average of models) of generated sentences fall into the Out-Dependent category, underscoring the importance of resolving annotation ambiguity in unfaithfulness detection benchmarks. Experiments demonstrate that our benchmark poses significant challenges to multiple baseline methods, indicating considerable room for future improvement.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lipschitz-aware Linearity Grafting for Certified Robustness</title>
<link>https://arxiv.org/abs/2510.25130</link>
<guid>https://arxiv.org/abs/2510.25130</guid>
<content:encoded><![CDATA[
arXiv:2510.25130v2 Announce Type: replace-cross 
Abstract: Lipschitz constant is a fundamental property in certified robustness, as smaller values imply robustness to adversarial examples when a model is confident in its prediction. However, identifying the worst-case adversarial examples is known to be an NP-complete problem. Although over-approximation methods have shown success in neural network verification to address this challenge, reducing approximation errors remains a significant obstacle. Furthermore, these approximation errors hinder the ability to obtain tight local Lipschitz constants, which are crucial for certified robustness. Originally, grafting linearity into non-linear activation functions was proposed to reduce the number of unstable neurons, enabling scalable and complete verification. However, no prior theoretical analysis has explained how linearity grafting improves certified robustness. We instead consider linearity grafting primarily as a means of eliminating approximation errors rather than reducing the number of unstable neurons, since linear functions do not require relaxation. In this paper, we provide two theoretical contributions: 1) why linearity grafting improves certified robustness through the lens of the $l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear activation functions, the dominant source of approximation errors, yields a tighter local Lipschitz constant. Based on these theoretical contributions, we propose a Lipschitz-aware linearity grafting method that removes dominant approximation errors, which are crucial for tightening the local Lipschitz constant, thereby improving certified robustness, even without certified training. Our extensive experiments demonstrate that grafting linearity into these influential activations tightens the $l_\infty$ local Lipschitz constant and enhances certified robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</title>
<link>https://arxiv.org/abs/2511.00139</link>
<guid>https://arxiv.org/abs/2511.00139</guid>
<content:encoded><![CDATA[
arXiv:2511.00139v2 Announce Type: replace-cross 
Abstract: Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape</title>
<link>https://arxiv.org/abs/2511.02122</link>
<guid>https://arxiv.org/abs/2511.02122</guid>
<content:encoded><![CDATA[
arXiv:2511.02122v2 Announce Type: replace-cross 
Abstract: In this paper we study how the choice of loss functions of non-convex optimization problems affects their robustness and optimization landscape, through the study of noisy matrix sensing. In traditional regression tasks, mean squared error (MSE) loss is a common choice, but it can be unreliable for non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust loss based on nonparametric regression, which uses a kernel-based estimate of the residual density and maximizes the estimated log-likelihood. This robust formulation coincides with the MSE loss under Gaussian errors but remains stable under more general settings. We further examine how this robust loss reshapes the optimization landscape by analyzing the upper-bound of restricted isometry property (RIP) constants for spurious local minima to disappear. Through theoretical and empirical analysis, we show that this new loss excels at handling large noise and remains robust across diverse noise distributions. This work offers initial insights into enhancing the robustness of machine learning tasks through simply changing the loss, guided by an intuitive and broadly applicable analytical framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical exploration and discovery at scale</title>
<link>https://arxiv.org/abs/2511.02864</link>
<guid>https://arxiv.org/abs/2511.02864</guid>
<content:encoded><![CDATA[
arXiv:2511.02864v2 Announce Type: replace-cross 
Abstract: AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.
  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.
  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
<link>https://arxiv.org/abs/2511.03132</link>
<guid>https://arxiv.org/abs/2511.03132</guid>
<content:encoded><![CDATA[
arXiv:2511.03132v3 Announce Type: replace-cross 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing</title>
<link>https://arxiv.org/abs/2511.07665</link>
<guid>https://arxiv.org/abs/2511.07665</guid>
<content:encoded><![CDATA[
arXiv:2511.07665v2 Announce Type: replace-cross 
Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffRegCD: Integrated Registration and Change Detection with Diffusion Features</title>
<link>https://arxiv.org/abs/2511.07935</link>
<guid>https://arxiv.org/abs/2511.07935</guid>
<content:encoded><![CDATA[
arXiv:2511.07935v3 Announce Type: replace-cross 
Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers</title>
<link>https://arxiv.org/abs/2511.14465</link>
<guid>https://arxiv.org/abs/2511.14465</guid>
<content:encoded><![CDATA[
arXiv:2511.14465v2 Announce Type: replace-cross 
Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title>
<link>https://arxiv.org/abs/2511.16020</link>
<guid>https://arxiv.org/abs/2511.16020</guid>
<content:encoded><![CDATA[
arXiv:2511.16020v2 Announce Type: replace-cross 
Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-Based Interpretability for Toxicity Detection</title>
<link>https://arxiv.org/abs/2511.16689</link>
<guid>https://arxiv.org/abs/2511.16689</guid>
<content:encoded><![CDATA[
arXiv:2511.16689v2 Announce Type: replace-cross 
Abstract: The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-computer interactions predict mental health</title>
<link>https://arxiv.org/abs/2511.20179</link>
<guid>https://arxiv.org/abs/2511.20179</guid>
<content:encoded><![CDATA[
arXiv:2511.20179v2 Announce Type: replace-cross 
Abstract: Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode mental health with state-of-the-art biomarker precision. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA to predict 1.3 million mental-health self-reports from 20,000 cursor and touchscreen recordings recorded in 9,000 online participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, identifies individuals living with mental illness, and achieves near-ceiling accuracy when predicting group-level mental health. By extracting non-verbal signatures of psychological function that have so far remained untapped, MAILA represents a key step toward foundation models for mental health. The ability to decode mental states at zero marginal cost creates new opportunities in neuroscience, medicine, and public health, while raising urgent questions about privacy, agency, and autonomy online.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[
arXiv:2511.20853v2 Announce Type: replace-cross 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2511.23059</link>
<guid>https://arxiv.org/abs/2511.23059</guid>
<content:encoded><![CDATA[
arXiv:2511.23059v2 Announce Type: replace-cross 
Abstract: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development and Benchmarking of a Blended Human-AI Qualitative Research Assistant</title>
<link>https://arxiv.org/abs/2512.00009</link>
<guid>https://arxiv.org/abs/2512.00009</guid>
<content:encoded><![CDATA[
arXiv:2512.00009v2 Announce Type: replace-cross 
Abstract: Qualitative research emphasizes constructing meaning through iterative engagement with textual data. Traditionally this human-driven process requires navigating coder fatigue and interpretative drift, thus posing challenges when scaling analysis to larger, more complex datasets. Computational approaches to augment qualitative research have been met with skepticism, partly due to their inability to replicate the nuance, context-awareness, and sophistication of human analysis. Large language models, however, present new opportunities to automate aspects of qualitative analysis while upholding rigor and research quality in important ways. To assess their benefits and limitations - and build trust among qualitative researchers - these approaches must be rigorously benchmarked against human-generated datasets. In this work, we benchmark Muse, an interactive, AI-powered qualitative research system that allows researchers to identify themes and annotate datasets, finding an inter-rater reliability between Muse and humans of Cohen's $\kappa$ = 0.71 for well-specified codes. We also conduct robust error analysis to identify failure mode, guide future improvements, and demonstrate the capacity to correct for human bias.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</title>
<link>https://arxiv.org/abs/2512.01054</link>
<guid>https://arxiv.org/abs/2512.01054</guid>
<content:encoded><![CDATA[
arXiv:2512.01054v2 Announce Type: replace-cross 
Abstract: Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.
  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.
  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.
  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</title>
<link>https://arxiv.org/abs/2512.02020</link>
<guid>https://arxiv.org/abs/2512.02020</guid>
<content:encoded><![CDATA[
arXiv:2512.02020v2 Announce Type: replace-cross 
Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
<link>https://arxiv.org/abs/2512.03979</link>
<guid>https://arxiv.org/abs/2512.03979</guid>
<content:encoded><![CDATA[
arXiv:2512.03979v2 Announce Type: replace-cross 
Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/Blur-Diffusion-Model/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphBench: Next-generation graph learning benchmarking</title>
<link>https://arxiv.org/abs/2512.04475</link>
<guid>https://arxiv.org/abs/2512.04475</guid>
<content:encoded><![CDATA[
arXiv:2512.04475v2 Announce Type: replace-cross 
Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval</title>
<link>https://arxiv.org/abs/2512.04524</link>
<guid>https://arxiv.org/abs/2512.04524</guid>
<content:encoded><![CDATA[
arXiv:2512.04524v2 Announce Type: replace-cross 
Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</title>
<link>https://arxiv.org/abs/2512.05377</link>
<guid>https://arxiv.org/abs/2512.05377</guid>
<content:encoded><![CDATA[
arXiv:2512.05377v3 Announce Type: replace-cross 
Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 40 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trusted AI Agents in the Cloud</title>
<link>https://arxiv.org/abs/2512.05951</link>
<guid>https://arxiv.org/abs/2512.05951</guid>
<content:encoded><![CDATA[
arXiv:2512.05951v2 Announce Type: replace-cross 
Abstract: AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning</title>
<link>https://arxiv.org/abs/2512.07371</link>
<guid>https://arxiv.org/abs/2512.07371</guid>
<content:encoded><![CDATA[
arXiv:2512.07371v2 Announce Type: replace-cross 
Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social welfare optimisation in well-mixed and structured populations</title>
<link>https://arxiv.org/abs/2512.07453</link>
<guid>https://arxiv.org/abs/2512.07453</guid>
<content:encoded><![CDATA[
arXiv:2512.07453v2 Announce Type: replace-cross 
Abstract: Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.21005</link>
<guid>https://arxiv.org/abs/2511.21005</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Reward Optimization, Preference Modeling, Reasoning Enhancement

<br /><br />Summary: This paper addresses limitations in Reinforcement Learning with Verifiable Rewards (RLVR) methods used to improve reasoning in Large Language Models (LLMs), highlighting problems such as coarse-grained rewards, reward noise, and inefficient exploration which cause unstable training and entropy collapse. To overcome these challenges, the authors introduce the Intrinsic Confidence-Driven Group Relative Preference Optimization (ICPO) method. ICPO leverages the natural probabilities generated by an LLM for different responses as a form of self-assessment, using these to calculate a preference advantage score by comparing relative generation probabilities among multiple responses to the same prompt. This score is combined with verifiable rewards to better guide exploration. The preference advantage score helps mitigate coarse reward granularity and noise, reduces overconfidence errors, boosts the relative rank of high-quality but undervalued responses, and prevents the model from overfitting on particular strategies. The efficacy of ICPO is validated through comprehensive experiments on four general-domain and three mathematical reasoning benchmarks, where it consistently improves reasoning performance over previous methods such as GRPO. This method shows potential for enhancing LLM reasoning stability and accuracy by refining reward-driven training processes through intrinsic confidence-based preference metrics. <div>
arXiv:2511.21005v4 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title>
<link>https://arxiv.org/abs/2512.07462</link>
<guid>https://arxiv.org/abs/2512.07462</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, strategic behaviour, repeated social dilemmas, multi-agent systems, cooperation

<br /><br />Summary:  
This work addresses the strategic behaviour of Large Language Models (LLMs) when acting as autonomous agents in interactive and multi-agent environments, highlighting the importance of understanding their intentions for safety and coordination. The authors extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas by introducing two new experimental setups: a payoff-scaled Prisoner's Dilemma that isolates sensitivity to incentive magnitude, and a multi-agent Public Goods Game featuring dynamic payoffs and histories. These environments reveal consistent patterns in LLM behaviour across different models and languages, such as sensitivity to incentives, divergence based on linguistic context, and an end-game tendency toward defection. To interpret these behaviours, traditional supervised classifiers trained on canonical repeated-game strategies are applied to the collected gameplay trajectories, demonstrating that LLMs exhibit systematic, model- and language-dependent behavioural intentions. The study also finds that linguistic framing may influence behaviour as strongly as architectural differences. Overall, the findings provide a unified methodological approach to audit LLMs as strategic agents and expose inherent cooperation biases. These insights have direct implications for AI governance, collective decision-making, and the safe design of multi-agent AI systems. <div>
arXiv:2512.07462v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound</title>
<link>https://arxiv.org/abs/2512.11169</link>
<guid>https://arxiv.org/abs/2512.11169</guid>
<content:encoded><![CDATA[
<div> MILPs, branch and bound, reinforcement learning, combinatorial decision making, differentiable policy<br /><br />Summary:<br /><br />This article addresses the challenge of solving combinatorial sequential decision-making problems traditionally modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&amp;B) algorithms. It recognizes that the difficulty of accurately modeling stochastic real-world problems with MILPs often results in suboptimal operational performance. The paper highlights recent machine learning approaches that improve decision quality rather than model accuracy but notes their reliance on supervised learning with access to true optimal decisions and the use of surrogate gradients. To overcome these limitations, the authors propose a novel CORL (combinatorial optimization with reinforcement learning) framework that fine-tunes MILP schemes end-to-end using reinforcement learning directly on real-world data to enhance operational outcomes. This is achieved by formulating the MILP solved by B&amp;B as a differentiable stochastic policy compatible with RL algorithms. The methodology allows learning without requiring optimal decision labels and avoids gradient surrogates. The effectiveness of the CORL method is demonstrated on a simple illustrative example of combinatorial sequential decision making, validating its potential as a new approach for integrating MILP optimization with RL to improve real-world decision-making tasks. <div>
arXiv:2512.11169v1 Announce Type: new 
Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&amp;B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&amp;B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling</title>
<link>https://arxiv.org/abs/2512.11187</link>
<guid>https://arxiv.org/abs/2512.11187</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Freight Exchange, combinatorial bundling, pickup-and-delivery routing, Transformer Neural Network, Large Neighborhood Search  

<br /><br />Summary:  
This paper addresses the challenge of efficient combinatorial bundling in Online Freight Exchange Systems (OFEX), which facilitate real-time freight job matching between shippers and carriers. The problem is formulated as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), aiming to maximize revenue while respecting capacity, precedence, and route-length constraints. The key difficulty lies in integrating combinatorial bundle selection with pickup-and-delivery routing under strict sub-second latency requirements. To tackle this, the authors introduce a learning-accelerated hybrid search pipeline that combines a Transformer Neural Network-based constructive policy with a novel Multi-Start Large Neighborhood Search (MSLNS) metaheuristic, operating in a rolling-horizon framework. This approach repeatedly freezes marketplace states for static snapshot optimization within a limited time budget. The Transformer provides low-latency, high-quality initial solutions (seeds), which MSLNS then refines efficiently through multi-start explorations. Experimental results demonstrate that this method surpasses existing neural combinatorial optimization and metaheuristic approaches in solution quality, achieving under 2% optimality gap in total revenue compared to exact baseline methods. Notably, this study is the first to show that deep neural network constructors can reliably generate high-quality seeds for multi-start improvement heuristics, applicable beyond the specific m1-PDSTSP to broader selective traveling salesperson and pickup-and-delivery problems. <div>
arXiv:2512.11187v1 Announce Type: new 
Abstract: Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration</title>
<link>https://arxiv.org/abs/2512.11213</link>
<guid>https://arxiv.org/abs/2512.11213</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time compute allocation, multi-agent systems, FutureWeaver, modularized collaboration, dual-level planning<br /><br />Summary:<br /><br />The paper addresses the challenge of optimizing computation at test time in multi-agent systems, where coordinating multiple agents under fixed computation budgets is complex. It highlights recent advances in improving large language model performance by scaling inference-time compute techniques like repeated sampling and self-reflection but notes the lack of principled methods for multi-agent collaboration in this context. To fill this gap, the authors introduce FutureWeaver, a novel framework designed for planning and allocating test-time compute resources effectively among multiple agents within budget constraints. FutureWeaver features modularized collaboration, implementing reusable multi-agent workflows encapsulated as callable functions derived automatically through self-play reflection that abstracts common interaction patterns from previous agent trajectories. The framework employs a dual-level planning approach that dynamically optimizes compute allocation by reasoning about the current task state and forecasting future steps, allowing more strategic and collaborative use of computational resources. Experimental results on complex multi-agent benchmarks demonstrate that FutureWeaver consistently surpasses baseline methods across various budget scenarios, confirming its capability to enhance multi-agent collaboration and inference-time compute optimization. This work represents a significant step toward more efficient and cooperative multi-agent system inference under explicit resource limitations. <div>
arXiv:2512.11213v1 Announce Type: new 
Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation</title>
<link>https://arxiv.org/abs/2512.11270</link>
<guid>https://arxiv.org/abs/2512.11270</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, Markov decision process, large language model, policy generation, automated modeling

<br /><br />Summary:  
The paper addresses the challenge of applying reinforcement learning (RL) to real-world tasks by automating the conversion of natural language task descriptions into a formal Markov decision process (MDP) and trained RL policies. The authors propose A-LAMP, a framework that leverages agentic large language models (LLMs) to translate informal task descriptions into MDP formulations, executable environments, and trained policies. The framework breaks down the process into verifiable stages—modeling, coding, and training—to ensure semantic alignment and reduce errors such as modeling inaccuracies, fragile codebases, and objective misalignment that typically hinder policy training. Across various classic control benchmarks and custom RL domains, A-LAMP outperforms individual state-of-the-art LLM approaches in generating effective policies. Impressively, even a lightweight version of A-LAMP, utilizing smaller language models, nearly matches the performance of significantly larger models, highlighting its efficiency. The authors conduct failure analyses to explain the source of improvements brought by their approach. A detailed case study further confirms that A-LAMP-generated environments and policies maintain task optimality, demonstrating the framework’s reliability and correctness for automated RL environment creation and policy development based on natural language inputs. <div>
arXiv:2512.11270v1 Announce Type: new 
Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning</title>
<link>https://arxiv.org/abs/2512.11271</link>
<guid>https://arxiv.org/abs/2512.11271</guid>
<content:encoded><![CDATA[
<div> Keywords: trip planning, multi-agent framework, constraint satisfaction, itinerary generation, large language models  

<br /><br />Summary:  
This paper addresses the challenge of real-world trip planning, which demands converting open-ended user requests into executable itineraries while satisfying strict spatial, temporal, budgetary constraints and user preferences. Existing approaches using large language model (LLM)-based agents face difficulties in meeting constraints, coordinating multiple tools, and maintaining efficiency, leading to infeasible or overly costly plans. To overcome these issues, the authors propose TriFlow, a progressive multi-agent framework structured around a three-stage pipeline: retrieval, planning, and governance. TriFlow effectively combines structured reasoning and language flexibility by progressively narrowing the search space, enabling rule-LLM collaboration to assemble itineraries consistent with constraints. Additionally, it employs bounded iterative refinement to ensure overall feasibility and personalization of plans. The framework was evaluated on two benchmarks, TravelPlanner and TripTailor, achieving state-of-the-art performance with final pass rates of 91.1% and 97%, respectively. Furthermore, TriFlow demonstrated over a tenfold improvement in runtime efficiency compared to existing state-of-the-art methods. This approach shows promise for scalable, flexible, and efficient trip planning using advanced LLM-driven multi-agent coordination. <div>
arXiv:2512.11271v1 Announce Type: new 
Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving</title>
<link>https://arxiv.org/abs/2512.11323</link>
<guid>https://arxiv.org/abs/2512.11323</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Visual Language Models, CAPTCHA, Benchmark, Multi-modal Alignment, LVLM Performance<br /><br />Summary: Benefiting from advanced multi-modal alignment strategies, Large Visual Language Models (LVLMs) have shown promising abilities in simulating human visual and reasoning skills, including solving CAPTCHAs. However, existing CAPTCHA benchmarks are limited, as prior datasets were often customized for specific research purposes and fail to encompass the full variety of CAPTCHA types currently used. There is also a notable lack of dedicated benchmarks tailored specifically for LVLMs. To address these deficiencies, the authors propose a novel benchmark named CAPTURE (CAPTCHA for Testing Under Real-world Experiments), which is designed explicitly for evaluating LVLM performance. CAPTURE includes four main CAPTCHA types and 25 sub-types sourced from 31 different vendors, ensuring a diverse set of challenges. This variety facilitates a comprehensive and multi-dimensional assessment of LVLM capabilities in CAPTCHA solving. Additionally, the benchmark boasts extensive class variety, large-scale data, and specialized labels relevant to LVLMs, filling critical gaps left by prior work in terms of both data comprehensiveness and labeling precision. When tested with CAPTURE, current LVLMs exhibit poor performance on CAPTCHA tasks, highlighting the benchmark's effectiveness and the need for further advancement in the field. <div>
arXiv:2512.11323v1 Announce Type: new 
Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance</title>
<link>https://arxiv.org/abs/2512.11421</link>
<guid>https://arxiv.org/abs/2512.11421</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, task completion framework, reinforcement learning, verifiable behavior, constraint compliance<br /><br />Summary:<br /><br />This paper addresses the challenge of unreliable and unverifiable behavior by Large Language Models (LLMs) in multi-turn tasks. It introduces a novel task completion framework designed for LLM-based agents operating in environments characterized by reinforcement learning structures, which include defined observation, action, and reward signals. The framework is composed of three integrated components: a lightweight task profiler that dynamically selects appropriate reasoning and generation strategies, a reasoning module that learns to map observations to actions in a verifiable manner, and a generation module that guarantees outputs are compliant with constraints by employing validation or deterministic synthesis methods. As the agent continuously interacts with its environment, these components evolve together, resulting in behavior that is both reliable and trustworthy. The approach enhances the transparency and accountability of LLMs in complex interactive settings, providing a structured way to guide their behavior under explicit reinforcement learning formalisms. Overall, the framework offers a systematic method to improve the verifiability and constraint adherence of LLM-generated outputs in sequential decision-making tasks, advancing the capabilities and dependability of LLM-based agents. <div>
arXiv:2512.11421v1 Announce Type: new 
Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints</title>
<link>https://arxiv.org/abs/2512.11426</link>
<guid>https://arxiv.org/abs/2512.11426</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multi-agent systems, Cost-effectiveness, Token-cost budget, Latency budget<br /><br />Summary:<br /><br />This paper introduces AgentBalance, a novel framework designed to create cost-effective multi-agent systems (MAS) using Large Language Models (LLMs) under strict token-cost and latency budgets. Unlike previous approaches that prioritize communication topology without explicitly considering deployment constraints, AgentBalance adopts a backbone-then-topology design strategy. First, it generates heterogeneous agents by constructing an LLM pool, selecting suitable backbones, and matching roles to these backbones to optimize agent capabilities within budget limits. Next, it adaptively synthesizes MAS topology by applying agent representation learning, gating mechanisms, and latency-aware communication design to guide efficient inter-agent interactions. Experiments across benchmarks with 14 candidate LLM backbones demonstrate that AgentBalance delivers up to 10% performance improvement under token-cost budgets and up to 22% under latency constraints. Furthermore, it achieves strong Area Under Curve (AUC) metrics on performance-versus-budget curves, indicating robust trade-offs. AgentBalance can also be integrated as a plug-in into existing MAS to enhance performance within the same budget constraints. The framework shows strong generalization to previously unseen LLMs, making it practical for real-world budget-aware deployment scenarios. The authors have open-sourced their code for community use and further research. <div>
arXiv:2512.11426v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Baseline: Examining Baseline Effects on Explainability Metrics</title>
<link>https://arxiv.org/abs/2512.11433</link>
<guid>https://arxiv.org/abs/2512.11433</guid>
<content:encoded><![CDATA[
<div> Keywords: Attribution methods, Explainable AI, Fidelity metrics, Baseline functions, Out-of-distribution images<br /><br />Summary:  
Attribution methods are widely used in Explainable Artificial Intelligence (XAI) to determine the importance of input features, commonly evaluated by Fidelity metrics such as Insertion and Deletion. These metrics depend on a baseline function that alters the most important pixels identified by an attribution map. The paper highlights a critical issue that the choice of baseline inherently favors certain attribution methods, resulting in contradictions even in simple linear models where different baselines identify different optimal methods. To address this, the authors propose evaluating baselines based on two key properties: their ability to remove information and their tendency to avoid generating overly out-of-distribution (OOD) images. Upon testing multiple baselines, they find that none satisfy both criteria simultaneously, revealing a trade-off between removing information and producing OOD images. To overcome this trade-off, the authors introduce a novel baseline inspired by recent advancements in feature visualization. This new baseline is model-dependent, effectively removes information while mitigating the generation of OOD images, thereby improving upon existing baselines. The study ultimately offers a deeper understanding of baseline selection in Fidelity metrics and provides a practical solution to enhance the evaluation of attribution methods in XAI. The associated code is publicly available at the provided GitHub repository. <div>
arXiv:2512.11433v1 Announce Type: new 
Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes</title>
<link>https://arxiv.org/abs/2512.11463</link>
<guid>https://arxiv.org/abs/2512.11463</guid>
<content:encoded><![CDATA[
<div> Motif-2-12.7B-Reasoning, language model, reasoning adaptation, long-context, fine-tuning

<br /><br />Summary:  
The paper introduces Motif-2-12.7B-Reasoning, a 12.7 billion parameter language model aimed at closing the gap between open-weight models and proprietary large-scale models in areas requiring complex reasoning and long-context understanding. The authors address common challenges in reasoning adaptation, such as model collapse and training instability, by proposing a comprehensive and reproducible training recipe that includes system, data, and algorithmic improvements. Key system innovations include memory-efficient infrastructure supporting 64K-token contexts via hybrid parallelism and kernel-level optimizations. The training strategy utilizes a two-stage Supervised Fine-Tuning (SFT) curriculum designed to reduce distribution mismatch by leveraging verified and aligned synthetic data. Additionally, a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline is detailed, which stabilizes training through difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical evaluation shows that Motif-2-12.7B-Reasoning performs comparably to much larger models on benchmarks spanning mathematics, coding, and agentic tasks. The work offers a competitive open-weight model and provides a practical blueprint for scaling reasoning capabilities in LLMs while operating within realistic computational constraints. <div>
arXiv:2512.11463v1 Announce Type: new 
Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three methods, one problem: Classical and AI approaches to no-three-in-line</title>
<link>https://arxiv.org/abs/2512.11469</link>
<guid>https://arxiv.org/abs/2512.11469</guid>
<content:encoded><![CDATA[
<div> No-Three-In-Line, Integer Linear Programming, PatternBoost, Reinforcement Learning, Combinatorial Geometry<br /><br />Summary:  
The No-Three-In-Line problem involves placing the maximum number of points on an n by n grid such that no three points lie on a straight line, a classic challenge in combinatorial geometry. Traditional methods like Integer Linear Programming (ILP) can find optimal solutions but suffer exponential time complexity as the grid size increases. This paper conducts the first systematic comparison between classical optimization techniques and AI-based approaches applied to this problem. It introduces the application of PatternBoost transformer learning and Proximal Policy Optimization (PPO) reinforcement learning to tackle the No-Three-In-Line problem. ILP successfully produces provably optimal solutions for grids up to 19 by 19. PatternBoost achieves near-optimal performance up to 14 by 14 grids, reducing test loss by 96%. PPO attains perfect solutions on 10 by 10 grids but fails to maintain feasibility at 11 by 11 due to constraint violations. The findings highlight that while classical optimization remains indispensable for guaranteeing exact solutions, AI methods show competitive approximation on smaller grid sizes. Ultimately, the study suggests that hybrid approaches combining optimization and AI techniques hold the greatest promise for scaling the problem to larger grids effectively. <div>
arXiv:2512.11469v1 Announce Type: new 
Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General-purpose AI models can generate actionable knowledge on agroecological crop protection</title>
<link>https://arxiv.org/abs/2512.11474</link>
<guid>https://arxiv.org/abs/2512.11474</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, agroecological crop protection, large language models, biological control, data consistency<br /><br />Summary: This study evaluates the application of generative artificial intelligence, specifically large language models (LLMs), in agri-food science with a focus on agroecological crop protection. Two LLMs, the web-grounded DeepSeek and the free-tier ChatGPT, were compared in their ability to generate scientific knowledge on nine globally limiting pests, weeds, and plant diseases. DeepSeek analyzed a literature corpus 4.8 to 49.7 times larger than ChatGPT and identified 1.6 to 2.4 times more biological control agents and management strategies. Consequently, DeepSeek reported efficacy estimates that were 21.6% higher, demonstrated improved consistency between laboratory and field data, and better reflected the effects of pest identity and management tactics. Despite these advantages, both models exhibited hallucinations by generating fictitious agents and references, reported implausible ecological interactions, confused scientific nomenclature, and missed key data on important control measures. Nevertheless, both LLMs accurately captured low-resolution trends in efficacy. The study suggests that when supported by stringent human oversight, LLMs hold significant promise as tools for enhancing decision-making at the farm level and fostering scientific creativity in agroecology. <div>
arXiv:2512.11474v1 Announce Type: new 
Abstract: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAID: A Benchmark for Bias Assessment of AI Detectors</title>
<link>https://arxiv.org/abs/2512.11505</link>
<guid>https://arxiv.org/abs/2512.11505</guid>
<content:encoded><![CDATA[
<div> Keywords: AI detectors, bias evaluation, sociolinguistic factors, text detection, underrepresented groups<br /><br />Summary:<br /><br />This paper presents BAID, a comprehensive evaluation framework designed to identify biases in AI-generated text detectors across a wide range of sociolinguistic dimensions. The study addresses the gap in systematic bias analysis beyond isolated cases, especially focusing on English Language Learners (ELLs) and other underrepresented groups. BAID utilizes a large dataset of over 200,000 samples categorized into seven major groups: demographics, age, educational grade level, dialect, formality, political leaning, and topic. Additionally, synthetic samples are created via specially designed prompts to maintain content integrity while reflecting subgroup-specific writing styles, enabling a controlled evaluation environment. The framework is applied to four state-of-the-art open-source AI text detectors, revealing consistent disparities in detection performance. Notably, these detectors exhibit lower recall rates when processing texts from underrepresented subgroups, indicating significant bias. The authors emphasize that such performance gaps must be acknowledged and addressed prior to deploying AI detection tools in educational and professional settings. Ultimately, BAID offers a scalable and transparent auditing method that highlights the importance of bias-aware evaluation in AI systems to ensure fairness and reliability when used publicly. <div>
arXiv:2512.11505v1 Announce Type: new 
Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection</title>
<link>https://arxiv.org/abs/2512.11506</link>
<guid>https://arxiv.org/abs/2512.11506</guid>
<content:encoded><![CDATA[
<div> Greenwashing, sustainability, knowledge graph, ESG reports, fact verification  

<br /><br />Summary:  
This paper introduces EmeraldMind, an innovative framework designed to detect greenwashing—misleading corporate claims about sustainability.  EmeraldMind integrates a domain-specific knowledge graph called EmeraldGraph, which is constructed from diverse corporate ESG (environmental, social, and governance) reports, providing verifiable evidence that is typically absent in generic knowledge bases. By combining this knowledge graph with retrieval-augmented generation, EmeraldMind supports large language models (LLMs) in the accurate assessment of sustainability claims. The framework emphasizes justification-centric classification by offering transparent, evidence-backed verdicts and responsibly abstains when claims lack sufficient verification. Experimental evaluation on a newly created greenwashing claims dataset demonstrates that EmeraldMind achieves competitive accuracy, broader coverage, and superior explanation quality compared to generic LLMs. Notably, these results are achieved without the need for fine-tuning or retraining large language models, highlighting the framework’s efficiency and practical applicability in combating misinformation in sustainability communication. EmeraldMind thus represents a significant step forward in leveraging AI for environmental accountability and enhancing trust in corporate sustainability disclosures. <div>
arXiv:2512.11506v1 Announce Type: new 
Abstract: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives</title>
<link>https://arxiv.org/abs/2512.11544</link>
<guid>https://arxiv.org/abs/2512.11544</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, clinical information extraction, AI-MASLD, functional decline, medical evaluation

<br /><br />Summary:  
This study evaluates the capability of four mainstream Large Language Models (LLMs)—GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max—to extract core medical information from noisy and redundant patient chief complaints in simulated real-world clinical scenarios. Employing a cross-sectional analysis with twenty standardized medical probes across five key clinical dimensions, the models were assessed under a double-blind, inverse rating scale by two independent clinicians against gold-standard expert answers. Results revealed that all models demonstrated varying degrees of functional defects, with Qwen3-Max outperforming others and Gemini 2.5 performing the worst. Under extreme noise conditions, most models experienced functional collapse, indicating significant degradation in performance. A critical finding was GPT-4o’s severe error in risk assessment of pulmonary embolism secondary to deep vein thrombosis. The study introduces the novel concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)," drawing an analogy between LLMs’ functional decline and metabolic dysfunction in clinical disease. This first empirical evidence signals important safety considerations when deploying LLMs in healthcare, emphasizing that current AI tools should serve only as adjuncts under human supervision due to the substantial gap between their theoretical medical knowledge and real-world clinical application effectiveness. <div>
arXiv:2512.11544v1 Announce Type: new 
Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Benchmark Democratization and Carpentry</title>
<link>https://arxiv.org/abs/2512.11588</link>
<guid>https://arxiv.org/abs/2512.11588</guid>
<content:encoded><![CDATA[
<div> Benchmarks, AI benchmarking, dynamic evaluation, reproducibility, education

<br /><br />Summary:  
Benchmarks play a critical role in machine learning by enabling reproducibility, comparison, and scientific advancement. However, current AI benchmarks are increasingly complex and static, failing to keep pace with rapid changes in model architecture, scale, data, and deployment contexts. Large language models often memorize these static benchmarks, creating a disconnect between benchmark performance and real-world effectiveness. To address this, there is a need for continuous, adaptive benchmarking frameworks that better align scientific evaluation with practical deployment risks. Developing expertise in AI Benchmark Carpentry through education and skills training is essential to overcome barriers such as high resource requirements, restricted access to specialized hardware, and lack of design expertise. Existing benchmarks often prioritize peak performance on elite hardware, providing limited guidance for diverse and real-world scenarios. Future benchmarking must be dynamic, incorporating evolving models, updated datasets, and heterogeneous platforms while ensuring transparency, reproducibility, and interpretability. Democratization of benchmarking requires both technical innovation and systematic education across multiple levels to cultivate sustained, community-driven expertise. Ultimately, benchmarks should support application-relevant, context-sensitive comparisons to inform responsible and accessible AI deployment, ensuring evaluation methodologies evolve alongside AI technology and deployment practices. <div>
arXiv:2512.11588v1 Announce Type: new 
Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Inference in Energy Demand Prediction</title>
<link>https://arxiv.org/abs/2512.11653</link>
<guid>https://arxiv.org/abs/2512.11653</guid>
<content:encoded><![CDATA[
<div> Energy demand prediction, structural causal model, Bayesian model, temperature sensitivity, MAPE  

<br /><br />Summary:  
This paper addresses the critical challenge of predicting energy demand, which is influenced by complex and interdependent factors such as weather conditions (temperature, humidity, wind speed, solar radiation) and calendar information (hour of day, month of year). The authors propose a structural causal model to explicate the causal relationships among these variables rather than relying on simpler correlation-based approaches. Their causal analysis uncovers season-dependent sensitivity of energy demand to temperature fluctuations and a lower variance in energy demand during winter, attributed to a decoupling between temperature changes and daily activity patterns. Leveraging these causal insights as prior knowledge, the study develops a Bayesian model for energy demand forecasting. When tested on unseen data, this model achieves state-of-the-art accuracy, registering a 3.84 percent mean absolute percentage error (MAPE) on the test set. Furthermore, the model demonstrates strong robustness, with cross-validation over two years of data maintaining an average MAPE of 3.88 percent. Overall, the study highlights how incorporating causal understanding into statistical modeling can significantly improve the reliability and interpretability of energy demand forecasts. <div>
arXiv:2512.11653v1 Announce Type: new 
Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition</title>
<link>https://arxiv.org/abs/2512.11682</link>
<guid>https://arxiv.org/abs/2512.11682</guid>
<content:encoded><![CDATA[
<div> Keywords: Therapeutic decision-making, agentic AI, retrieval-augmented generation, biomedical knowledge, tool integration  

<br /><br />Summary:  
1. The article addresses therapeutic decision-making in clinical medicine, highlighting the complexity of integrating patient data, disease factors, and drug information for tasks such as drug recommendation, treatment planning, and adverse effect prediction.  
2. The proposed method, TxAgent, exemplifies agentic AI by using iterative retrieval-augmented generation (RAG) to perform multi-step reasoning with biomedical knowledge support.  
3. TxAgent is built on a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite named ToolUniverse, which integrates FDA Drug API, OpenTargets, and Monarch databases to provide up-to-date therapeutic information.  
4. The article emphasizes the critical importance of both the accuracy of the reasoning trace and the correctness of tool invocation sequences due to stringent safety requirements unique to medical applications. It advocates for evaluation protocols that explicitly supervise token-level reasoning and tool usage.  
5. The work was demonstrated through participation in the CURE-Bench NeurIPS 2025 Challenge, where the authors analyzed the impact of retrieval quality on model performance and achieved performance improvements via enhanced tool-retrieval strategies. This contribution earned the Excellence Award in Open Science.  
Complete details are available at https://curebench.ai/. <div>
arXiv:2512.11682v1 Announce Type: new 
Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring skill-based uplift from AI in a real biological laboratory</title>
<link>https://arxiv.org/abs/2512.10960</link>
<guid>https://arxiv.org/abs/2512.10960</guid>
<content:encoded><![CDATA[
<div> Skills-based uplift, AI reasoning model, biological applications, wet-lab experiment, biosecurity<br /><br />Summary:<br /><br />1. The study investigates the impact of access to an AI reasoning model on participant skills when performing a biological experiment, compared to a control group limited to internet access.<br />2. Participants came from a diverse group of Los Alamos National Laboratory employees without prior wet-lab experience, ensuring that skill rather than knowledge was a primary barrier.<br />3. The experimental task involved transforming E. coli with an expression construct, inducing reporter peptide expression, and confirming expression via mass spectrometry.<br />4. Both quantitative outcomes (such as successful task completion) and qualitative observations (participant interactions with AI, internet, lab equipment, and peers) were recorded.<br />5. The results showcase the potential for AI tools to enhance practical biological skills and underscore challenges in designing real-world studies that reflect the complex interplay between AI assistance and biosecurity concerns, providing valuable insights for future research in this evolving field. <div>
arXiv:2512.10960v1 Announce Type: cross 
Abstract: Understanding how AI systems are used by people in real situations that mirror aspects of both legitimate and illegitimate use is key to predicting the risks and benefits of AI systems. This is especially true in biological applications, where skill rather than knowledge is often the primary barrier for an untrained person. The challenge is that these studies are difficult to execute well and can take months to plan and run.
  Here we report the results of a pilot study that attempted to empirically measure the magnitude of \emph{skills-based uplift} caused by access to an AI reasoning model, compared with a control group that had only internet access. Participants -- drawn from a diverse pool of Los Alamos National Laboratory employees with no prior wet-lab experience -- were asked to transform \ecoli{} with a provided expression construct, induce expression of a reporter peptide, and have expression confirmed by mass spectrometry.
  We recorded quantitative outcomes (e.g., successful completion of experimental segments) and qualitative observations about how participants interacted with the AI system, the internet, laboratory equipment, and one another. We present the results of the study and lessons learned in designing and executing this type of study, and we discuss these results in the context of future studies of the evolving relationship between AI and global biosecurity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI as Cognitive Amplifier: Rethinking Human Judgment in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2512.10961</link>
<guid>https://arxiv.org/abs/2512.10961</guid>
<content:encoded><![CDATA[
<div> Keywords: AI as cognitive amplifier, user expertise, intelligence amplification, domain knowledge, metacognitive skills<br /><br />Summary:<br /><br />1. The article emphasizes that the effectiveness of AI tools varies greatly depending on the user's expertise and judgment rather than the tools themselves.<br />2. It proposes a perspective of AI as a cognitive amplifier that enhances existing human abilities instead of replacing human intelligence.<br />3. A three-level model of AI engagement is introduced: passive acceptance, iterative collaboration, and cognitive direction, where progression relies on domain expertise and metacognitive skills, not just technical training.<br />4. Empirical studies and field observations reveal significant performance gaps between expert and novice users due to differences in domain knowledge, quality evaluation, and iterative refinement.<br />5. The paper suggests workforce development and AI system design should focus on strengthening domain expertise, evaluative judgment, and reflective practice alongside AI literacy and prompt engineering for more effective AI adoption. <div>
arXiv:2512.10961v1 Announce Type: cross 
Abstract: Through extensive experience training professionals and individual users in AI tool adoption since the GPT-3 era, I have observed a consistent pattern: the same AI tool produces dramatically different results depending on who uses it. While some frame AI as a replacement for human intelligence, and others warn of cognitive decline, this position paper argues for a third perspective grounded in practical observation: AI as a cognitive amplifier that magnifies existing human capabilities rather than substituting for them. Drawing on research in human-computer interaction, cognitive augmentation theory, and educational technology, alongside field observations from corporate training across writing, software development, and data analysis domains, I present a framework positioning AI tools as intelligence amplification systems where output quality depends fundamentally on user expertise and judgment. Through analysis of empirical studies on expert-novice differences and systematic observations from professional training contexts, I demonstrate that domain knowledge, quality judgment, and iterative refinement capabilities create substantial performance gaps between users. I propose a three-level model of AI engagement -- from passive acceptance through iterative collaboration to cognitive direction -- and argue that the transition between levels requires not technical training but development of domain expertise and metacognitive skills. This position has critical implications for workforce development and AI system design. Rather than focusing solely on AI literacy or technical prompt engineering, I advocate for integrated approaches that strengthen domain expertise, evaluative judgment, and reflective practice.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering</title>
<link>https://arxiv.org/abs/2512.10962</link>
<guid>https://arxiv.org/abs/2512.10962</guid>
<content:encoded><![CDATA[
<div> Computer-use agents, GUI interaction, data synthesis, step-level filtering, reward model<br /><br />Summary: This paper addresses the challenge of training computer-use agents (CUAs) to operate digital graphical user interfaces (GUIs), which is difficult due to the high cost and scarcity of high-quality trajectory data from human demonstrations. The authors propose a scalable data synthesis pipeline to generate reliable supervision by applying step-level filtering that individually evaluates and retains only correct actions from noisy rollouts produced by strong CUAs, complemented by reasoning augmentation to improve planning. Leveraging this method, they create WebSTAR, a dataset of 13.3K trajectories and 100K reasoning-rich steps synthesized using OpenAI’s computer-use-preview model. They train Qwen-2.5-VL-Instruct models (7B and 32B parameters) on WebSTAR, achieving significant improvements; notably, their 7B model outperforms the state-of-the-art open-source CUA model UI-TARS-1.5-7B by over 15% with supervised finetuning alone. Building on step-level grading, they further construct WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini. StepRM matches the grading quality of larger models while offering greater efficiency for large-scale deployment. This work highlights step-level filtering as a crucial principle for scalable CUA training and provides practical tools (datasets and reward model) to advance robust and efficient computer-use agents. <div>
arXiv:2512.10962v1 Announce Type: cross 
Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis</title>
<link>https://arxiv.org/abs/2512.10963</link>
<guid>https://arxiv.org/abs/2512.10963</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Modal Emotion Recognition, Intent Recognition, AIGC Recommendation, Cross-Modal Transformer, Attention-Based Fusion  

<br /><br />Summary:  
This study addresses the growing need for emotionally aware recommendation systems in AI-generated content (AIGC) domains like music, video, and literature by proposing a Multi-Modal Emotion and Intent Recognition Model (MMEI). The novel system leverages a BERT-based Cross-Modal Transformer combined with an Attention-Based Fusion module to jointly process visual, auditory, and textual data. Specifically, pretrained encoders ViT, Wav2Vec2, and BERT extract features from facial expressions, speech tone, and user comments or utterances, respectively. The model then fuses these modalities to generate comprehensive emotion-intent representations. These embeddings feed into a contextual matching layer that personalizes content recommendations based on users’ emotional and intentional states in real time. Experimental validation on benchmark datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrated a 4.3% improvement in F1-score and a 12.3% decrease in cross-entropy loss over the best fusion-based transformer baseline. Moreover, online user-level evaluations showed that emotion-driven recommendations increased engagement time by 15.2% and boosted satisfaction scores by 11.8%. The results indicate that integrating cross-modal emotional intelligence can significantly enhance adaptive, empathetic, and context-aware AIGC recommendation systems for next-generation user experiences. <div>
arXiv:2512.10963v1 Announce Type: cross 
Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.10966</link>
<guid>https://arxiv.org/abs/2512.10966</guid>
<content:encoded><![CDATA[
<div> Multimodal fusion, Alzheimer's disease, Mixture-of-Experts, neuroimaging, interpretability<br /><br />Summary:<br /><br />1. This study addresses the challenge of early and accurate diagnosis of Alzheimer's disease (AD) by integrating complementary information from multiple imaging modalities, such as amyloid PET and MRI, to better reflect clinical practice.<br />2. Traditional fusion methods typically concatenate features in a simplistic manner, lacking the ability to adaptively weight the importance of biomarkers across different brain regions.<br />3. The authors propose MREF-AD, a Multimodal Regional Expert Fusion model based on a Mixture-of-Experts (MoE) framework, where each meso-scale brain region in each modality acts as an independent expert.<br />4. The model uses two-level gating networks to learn subject-specific fusion weights, enabling personalized and adaptive integration of multimodal biomarker information.<br />5. Applied to data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD not only achieves state-of-the-art diagnostic performance compared to baseline models but also enhances interpretability by revealing how structural and molecular imaging modalities contribute regionally to the disease diagnosis.<br /><br />This work highlights MREF-AD as a powerful, interpretable framework for adaptive multimodal fusion in neuroimaging, with potential broader applications beyond AD diagnosis. <div>
arXiv:2512.10966v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems</title>
<link>https://arxiv.org/abs/2512.10975</link>
<guid>https://arxiv.org/abs/2512.10975</guid>
<content:encoded><![CDATA[
<div> Keywords: human-agent interaction, multimodal emotion recognition, multi-agent framework, modular integration, training efficiency

<br /><br />Summary:  
1. The paper addresses the challenge of accurate and adaptive perception of human emotional states in human-agent interaction (HAI).  
2. Traditional multimodal deep learning models use facial expressions, speech, and text cues to achieve high emotion recognition accuracy but require heavy computational resources and lack flexibility when modalities change.  
3. The authors propose a novel multi-agent framework where each modality encoder (vision, audio, text) and the fusion classifier act as autonomous agents coordinated by a central supervisor.  
4. This architecture supports easy modular integration of new modalities, such as audio features through emotion2vec, and allows seamless replacement of outdated components without retraining the entire system.  
5. The approach reduces computational overhead during training and improves training efficiency.  
6. A proof-of-concept implementation demonstrates support for vision, audio, and text modalities, with the classifier serving as a shared decision-making agent.  
7. The proposed framework enables more flexible, scalable, and maintainable perception modules for embodied and virtual agents, enhancing their capability in HAI scenarios. <div>
arXiv:2512.10975v1 Announce Type: cross 
Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.10978</link>
<guid>https://arxiv.org/abs/2512.10978</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, attention heads, interpretability, cognitive functions, reasoning tasks<br /><br />Summary:<br /><br />This paper addresses the challenge of understanding the internal mechanisms of large language models (LLMs), focusing on their reasoning abilities. The authors propose a novel interpretability framework inspired by the relationship between neural processes and human cognition to analyze attention heads, which are crucial components of LLMs. They introduce CogQA, a new dataset designed with a chain-of-thought format that breaks down complex questions into stepwise subquestions linked to specific cognitive functions such as retrieval and logical reasoning. Using a multi-class probing methodology, the study identifies and classifies attention heads according to distinct cognitive roles, termed cognitive heads. Their analysis across multiple LLM families finds these cognitive heads to be functionally specialized, exhibiting universal sparsity, diversity in number and distribution based on cognitive function, and interactive hierarchical organization. The investigation further demonstrates the importance of cognitive heads in reasoning tasks, showing that removing these heads impairs model performance while enhancing them improves reasoning accuracy. Overall, these findings deepen the understanding of how LLM reasoning emerges from attention mechanisms and highlight important implications for improved model design, training, and fine-tuning strategies aimed at boosting reasoning capabilities in LLMs. <div>
arXiv:2512.10978v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved state-of-the-art performance in a variety of tasks, but remain largely opaque in terms of their internal mechanisms. Understanding these mechanisms is crucial to improve their reasoning abilities. Drawing inspiration from the interplay between neural processes and human cognition, we propose a novel interpretability framework to systematically analyze the roles and behaviors of attention heads, which are key components of LLMs. We introduce CogQA, a dataset that decomposes complex questions into step-by-step subquestions with a chain-of-thought design, each associated with specific cognitive functions such as retrieval or logical reasoning. By applying a multi-class probing method, we identify the attention heads responsible for these functions. Our analysis across multiple LLM families reveals that attention heads exhibit functional specialization, characterized as cognitive heads. These cognitive heads exhibit several key properties: they are universally sparse, vary in number and distribution across different cognitive functions, and display interactive and hierarchical structures. We further show that cognitive heads play a vital role in reasoning tasks - removing them leads to performance degradation, while augmenting them enhances reasoning accuracy. These insights offer a deeper understanding of LLM reasoning and suggest important implications for model design, training, and fine-tuning strategies.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling</title>
<link>https://arxiv.org/abs/2512.10980</link>
<guid>https://arxiv.org/abs/2512.10980</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU utilization, dynamic scheduling, multi-tenant clusters, AI workloads, fairness variance  

<br /><br />Summary:  
GPU clusters are critical for modern AI training and deployment but typically suffer from low utilization near 50% due to fragmentation, workload heterogeneity, and static scheduling limitations. This paper systematically evaluates these challenges and introduces three dynamic schedulers—Hybrid Priority Scheduler (HPS), Predictive Backfill Scheduler (PBS), and Smart Batch Scheduler (SBS)—aimed at enhancing utilization, fairness, and throughput in multi-tenant GPU clusters. Using simulations with 1,000 AI jobs across a 64-GPU, 8-node cluster representing diverse workloads (training, inference, research), static scheduling baselines (FIFO, SJF, Shortest, Shortest-GPU) only achieve 45–67% utilization and 12.5–18.3 jobs/hour, with significant job starvation and long wait times. The proposed dynamic schedulers considerably outperform these baselines: HPS delivers the highest utilization (78.2%), throughput (25.8 jobs/hour), and the lowest fairness variance, reducing starvation to just 12 jobs. PBS better mitigates fragmentation, reaching 76.1% utilization, while SBS optimizes efficiency for structurally similar jobs, achieving 74.6% utilization. Overall, multi-objective dynamic scheduling consistently exceeds single-objective heuristics across key metrics such as throughput, fairness, wait times, and starvation. These findings demonstrate that transparent, targeted scheduling strategies can substantially improve GPU cluster efficiency in heterogeneous AI workloads, offering a strong foundation for future production-grade scheduling frameworks. <div>
arXiv:2512.10980v1 Announce Type: cross 
Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developmental Symmetry-Loss: A Free-Energy Perspective on Brain-Inspired Invariance Learning</title>
<link>https://arxiv.org/abs/2512.10984</link>
<guid>https://arxiv.org/abs/2512.10984</guid>
<content:encoded><![CDATA[
<div> Symmetry-Loss, brain-inspired, invariance, equivariance, representation learning<br /><br />Summary: The paper introduces Symmetry-Loss, a novel brain-inspired algorithmic principle designed to enforce invariance and equivariance in representation learning through differentiable constraints derived from environmental symmetries. It models learning as an iterative refinement of an effective symmetry group, paralleling how cortical representations develop to align with the structural properties of the environment. By minimizing "structural surprise," defined as deviations from symmetry consistency, the approach effectively operationalizes a Free-Energy-like objective for learning representations. This framework bridges two major theoretical perspectives—predictive coding and group theory—demonstrating how efficient, stable, and compositional representations emerge naturally through symmetry-based self-organization. The method provides a general computational mechanism that links developmental learning processes in the brain with principled approaches to artificial representation learning. Overall, Symmetry-Loss offers a unifying model that ties together biological development and machine learning for improved understanding and design of representation learning systems. <div>
arXiv:2512.10984v1 Announce Type: cross 
Abstract: We propose Symmetry-Loss, a brain-inspired algorithmic principle that enforces invariance and equivariance through a differentiable constraint derived from environmental symmetries. The framework models learning as the iterative refinement of an effective symmetry group, paralleling developmental processes in which cortical representations align with the world's structure. By minimizing structural surprise, i.e. deviations from symmetry consistency, Symmetry-Loss operationalizes a Free-Energy--like objective for representation learning. This formulation bridges predictive-coding and group-theoretic perspectives, showing how efficient, stable, and compositional representations can emerge from symmetry-based self-organization. The result is a general computational mechanism linking developmental learning in the brain with principled representation learning in artificial systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness</title>
<link>https://arxiv.org/abs/2512.10985</link>
<guid>https://arxiv.org/abs/2512.10985</guid>
<content:encoded><![CDATA[
<div> Keywords: neocortical columns, basal ganglia, self-awareness, reinforcement learning, Atari games  

<br /><br />Summary: The paper proposes a biologically inspired mathematical model based on the idea of separate 'what' and 'where' information processing pathways in the brain. This model incorporates neocortical columns regulated by the basal ganglia to perform predictions and action selection, where distinct columns serve as 'what' and 'where' processors. The core function of the model is to identify and separate the self from the environment, which then enables the construction and use of a self-model to enhance predictive capabilities. To demonstrate the model's practical application, the authors develop a reinforcement learning agent that learns purposeful behavior within a virtual environment. The agent is tested on Atari games Pong and Breakout, successfully learning to play these games. The results suggest that the ability to distinguish the self from the environment provides the agent with advantages. The authors argue this capability could have emerged through evolutionary processes in living organisms. Finally, they propose a conceptual Self-Awareness Principle 1, stating that the capacity to separate self from the world is necessary but not sufficient alone for full self-awareness. <div>
arXiv:2512.10985v1 Announce Type: cross 
Abstract: The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics of natural intelligence</title>
<link>https://arxiv.org/abs/2512.10988</link>
<guid>https://arxiv.org/abs/2512.10988</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitome, neural hypernetwork, cognitive groups of neurons, brain modeling, consciousness  

<br /><br />Summary:  
1. The article explores the concept of the cognitome, introduced by K.V. Anokhin, which refers to the cognitive structure of the mind as a high-order neural hypernetwork of the brain.  
2. Within the cognitome, cognitive groups of neurons (COGs) are foundational elements, divided into functional systems and cellular ensembles, both crucial for brain function.  
3. Consciousness is described as a dynamic process within this hypernetwork, characterized by the large-scale integration of its cognitive components.  
4. The paper presents novel mathematical models capturing these brain structures and their cognitive processes, grounded in new mathematical results.  
5. It advances a general principle of brain function: the brain discovers all possible causal relationships in the external world and infers all possible conclusions from them.  
6. The models developed incorporate and provide mathematical foundations for several theoretical frameworks, including natural classification, P.K. Anokhin’s theory of functional brain systems, E. Roche’s prototypical categorization theory, Bob Rehter’s causal models, and G. Tononi’s integrated information theory of consciousness.  
7. This work aims to establish a fundamental mathematical theory describing the origin, structure, and function of the brain and mind’s cognitive architecture. <div>
arXiv:2512.10988v1 Announce Type: cross 
Abstract: In the process of evolution, the brain has achieved such perfection that artificial intelligence systems do not have and which needs its own mathematics. The concept of cognitome, introduced by the academician K.V. Anokhin, as the cognitive structure of the mind -- a high-order structure of the brain and a neural hypernetwork, is considered as the basis for modeling. Consciousness then is a special form of dynamics in this hypernetwork -- a large-scale integration of its cognitive elements. The cognitome, in turn, consists of interconnected COGs (cognitive groups of neurons) of two types -- functional systems and cellular ensembles. K.V. Anokhin sees the task of the fundamental theory of the brain and mind in describing these structures, their origin, functions and processes in them. The paper presents mathematical models of these structures based on new mathematical results, as well as models of different cognitive processes in terms of these models. In addition, it is shown that these models can be derived based on a fairly general principle of the brain works: \textit{the brain discovers all possible causal relationships in the external world and draws all possible conclusions from them}. Based on these results, the paper presents models of: ``natural" classification; theory of functional brain systems by P.K. Anokhin; prototypical theory of categorization by E. Roche; theory of causal models by Bob Rehter; theory of consciousness as integrated information by G. Tononi.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI</title>
<link>https://arxiv.org/abs/2512.10990</link>
<guid>https://arxiv.org/abs/2512.10990</guid>
<content:encoded><![CDATA[
<div> edge AI, QoE, hybrid parallelism, distributed training, resource efficiency<br /><br />Summary:<br /><br />1. The paper addresses the challenge of meeting user Quality of Experience (QoE) requirements, especially model inference latency, in edge AI applications where resource constraints are significant. 2. Modern AI models often exceed the computing capacity of single devices, requiring distributed execution across heterogeneous devices connected by variable, contention-prone networks. 3. Existing hybrid parallelism planners focusing on throughput or device utilization neglect QoE, causing inefficiencies such as increased energy consumption or QoE violations during runtime changes. 4. The authors present Dora, a framework that enables QoE-aware hybrid parallelism by jointly optimizing heterogeneous computation, network contention, and multidimensional QoE objectives. 5. Dora consists of three key components: a heterogeneity-aware model partitioner creating QoE-compliant model partition plans, a contention-aware network scheduler that maximizes overlap between computation and communication, and a runtime adapter that dynamically composes multiple plans to maintain efficiency and QoE. 6. Evaluations across varied edge scenarios, including smart homes, traffic analytics, and small edge clusters, demonstrate that Dora can speed up execution by 1.1 to 6.3 times while reducing energy consumption by 21% to 82%, all without compromising QoE under runtime dynamics. <div>
arXiv:2512.10990v1 Announce Type: cross 
Abstract: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.
  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax</title>
<link>https://arxiv.org/abs/2512.10991</link>
<guid>https://arxiv.org/abs/2512.10991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D molecular generation, chemical syntax, diffusion model, foundation model, MolSculpt<br /><br />Summary: Generating accurate 3D molecular geometries is vital for applications in drug discovery and materials science. Existing methods often use 1D molecular representations like SELFIES to ensure chemical validity but do not fully utilize the complex chemical knowledge encoded in these 1D models, resulting in a disconnect between 1D molecule syntax and 3D geometry. To address this, the proposed MolSculpt framework integrates 1D chemical knowledge deeply into 3D molecular generation. MolSculpt combines a frozen 1D molecular foundation model with a 3D molecular diffusion model. It employs learnable queries to extract chemical information from the foundation model, and a trainable projector injects this information into the diffusion model’s conditioning space. This cross-modal conditioning is optimized end-to-end, enabling the model to "sculpt" 3D molecular geometries that reflect underlying 1D chemical syntax. Experimental results on GEOM-DRUGS and QM9 datasets demonstrate that MolSculpt achieves state-of-the-art performance in both de novo and conditional 3D molecule generation tasks. The model outperforms previous approaches in terms of 3D fidelity and stability, showing superior generation quality. The authors have made the code publicly available for further research and application development. <div>
arXiv:2512.10991v1 Announce Type: cross 
Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA</title>
<link>https://arxiv.org/abs/2512.10996</link>
<guid>https://arxiv.org/abs/2512.10996</guid>
<content:encoded><![CDATA[
<div> Keywords: MedBioRAG, biomedical question answering, retrieval-augmented generation, semantic search, large language models  

<br /><br />Summary:  
This paper presents MedBioRAG, a novel retrieval-augmented generation (RAG) model aimed at enhancing biomedical question-answering (QA) capabilities by integrating semantic and lexical search techniques along with document retrieval and supervised fine-tuning. The model is specifically designed to efficiently retrieve and rank relevant biomedical documents to generate precise, context-aware responses. MedBioRAG is evaluated on multiple tasks including text retrieval, close-ended QA, and long-form QA using well-known biomedical benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results show that MedBioRAG consistently outperforms previous state-of-the-art models as well as the GPT-4o base model across all these tasks. Key improvements include better NDCG (Normalized Discounted Cumulative Gain) and MRR (Mean Reciprocal Rank) scores in document retrieval, higher accuracy in close-ended QA, and superior ROUGE scores for long-form QA responses. The study underlines that combining semantic search for more effective retrieval with fine-tuning of large language models significantly advances the performance of biomedical QA systems, demonstrating the promise of this approach in specialized medical and biomedical domains. <div>
arXiv:2512.10996v1 Announce Type: cross 
Abstract: Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unambiguous Representations in Neural Networks: An Information-Theoretic Approach to Intentionality</title>
<link>https://arxiv.org/abs/2512.11000</link>
<guid>https://arxiv.org/abs/2512.11000</guid>
<content:encoded><![CDATA[
<div> representational ambiguity, neural networks, consciousness, information theory, MNIST<br /><br />Summary:<br /><br />This article explores the concept of representational ambiguity in the context of consciousness and neural representations through the lens of information theory. It begins by contrasting conventional representations—which require external decoders to convey meaning—with conscious experience, which inherently lacks ambiguity (e.g., perceiving a red square cannot be mistaken for a green square). To quantify this, the authors define representational ambiguity using the conditional entropy H(I|R), which measures the uncertainty of interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, the study demonstrates that the relational structure in network connectivity can encode representational content unambiguously. Specifically, networks trained with dropout showed perfect (100%) accuracy in identifying the output neuron class from connectivity patterns, whereas those trained with standard backpropagation yielded only 38% accuracy despite similar task performance. This indicates that representational ambiguity can exist independently of behavioral accuracy. Additionally, the study shows that spatial position information of input neurons can be decoded from connectivity with a high coefficient of determination (R² up to 0.844). Overall, these findings provide a quantitative framework for measuring representational ambiguity in neural systems and suggest that neural networks can display the low-ambiguity representations theorized as fundamental to conscious experience, although such representations alone may not be sufficient for consciousness. <div>
arXiv:2512.11000v1 Announce Type: cross 
Abstract: Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy H(I|R) over possible interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve perfect (100%) accuracy for dropout-trained networks and 38% for standard backpropagation in identifying output neuron class identity, despite identical task performance, demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position information of input neurons can be decoded from network connectivity with R2 up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts of consciousness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Memristor: Neuromorphic Computing Using Meminductor</title>
<link>https://arxiv.org/abs/2512.11002</link>
<guid>https://arxiv.org/abs/2512.11002</guid>
<content:encoded><![CDATA[
<div> Keywords: meminductor, memristor, neuromorphic computing, magnetic core, biological behavior

<br /><br />Summary:  
This paper introduces the concept of a meminductor, an inductor with memory, characterized by its inductance L(q) depending on the charge q flowing through the coil. The memory effect arises from the magnetization in a coil's magnetic core, which retains the history of current passage. Unlike memristors, meminductors uniquely influence the time constant in neuromorphic RLC circuits through their inductance and capacitance, offering novel capabilities for brain-inspired computing architectures, deep learning, and neuromorphic systems. Experimentally, the authors created a meminductor to reproduce biological behaviors observed in amoebae, such as memorizing, timing, and anticipating mechanisms, demonstrating practical applicability. The results suggest that beyond memristor-based computing, meminductors can form a new paradigm enhancing computing architectures by leveraging memory in inductors. This expands the foundational understanding of memory circuit elements and their diverse roles in emerging computational models. The research confirms that a meminductor is not merely theoretical but a physically realizable and experimentally verifiable component with potential applications in neuromorphic engineering and bio-inspired computing. <div>
arXiv:2512.11002v1 Announce Type: cross 
Abstract: Memristor (resistor with memory), inductor with memory (meminductor) and capacitor with memory (memcapacitor) have different roles to play in novel computing architectures. We found that a coil with a magnetic core is an inductor with memory (meminductor) in terms of its inductance L(q) being a function of the charge q. The history of the current passing through the coil is remembered by the magnetization inside the magnetic core. Such a meminductor can play a unique role (that cannot be played by a memristor) in neuromorphic computing, deep learning and brain inspired since the time constant of a neuromorphic RLC circuit is jointly determined by the inductance and capacitance, rather than the resistance. As an experimental verification, this newly invented meminductor was used to reproduce the observed biological behaviour of amoebae (the memorizing, timing and anticipating mechanisms). In conclusion, a beyond memristor computing paradigm is theoretically sensible and experimentally practical.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification</title>
<link>https://arxiv.org/abs/2512.11015</link>
<guid>https://arxiv.org/abs/2512.11015</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, gender classification, image-text fusion, semantic guidance, bias mitigation<br /><br />Summary:<br /><br />1. The paper addresses fairness in facial image-based gender classification by introducing text-guided methodologies to enhance model performance and reduce bias.<br />2. Two main strategies are proposed: Image Text Matching (ITM) guidance, which helps the model learn fine-grained image-text alignments for better multimodal representations, and Image Text Fusion, which integrates both image and text modalities into richer representations to improve fairness.<br />3. Extensive experiments on benchmark datasets show that these approaches effectively mitigate demographic biases and improve accuracy across different gender and racial groups compared to existing methods.<br />4. The integration of textual guidance provides an interpretable and intuitive training paradigm, helping computer vision models leverage semantic information to reduce disparities.<br />5. Importantly, the proposed methods do not rely on demographic labels and are application-agnostic, making them broadly applicable for equitable facial analysis algorithms while improving generalization without demographic supervision. <div>
arXiv:2512.11015v1 Announce Type: cross 
Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoccerMaster: A Vision Foundation Model for Soccer Understanding</title>
<link>https://arxiv.org/abs/2512.11016</link>
<guid>https://arxiv.org/abs/2512.11016</guid>
<content:encoded><![CDATA[
<div> Soccer understanding, vision foundation model, multi-task pretraining, data curation, SoccerFactory  

<br /><br />Summary:  
This paper addresses the challenge of soccer visual understanding by proposing a unified model, SoccerMaster, capable of handling a range of tasks from fine-grained perception like athlete detection to higher-level semantic reasoning such as event classification. The authors introduce SoccerMaster as the first soccer-specific vision foundation model developed through supervised multi-task pretraining, enabling it to manage diverse understanding tasks within a single framework efficiently. To support this, they create an automated data curation pipeline that generates scalable spatial annotations and integrates these with existing soccer video datasets, resulting in SoccerFactory, a comprehensive and large-scale pretraining data resource tailored for soccer analysis. The study performs extensive evaluations comparing SoccerMaster with various task-specific expert models on multiple downstream tasks. Results demonstrate that SoccerMaster not only matches but consistently outperforms these expert models, emphasizing both its broad applicability and superior performance in the soccer domain. The authors also plan to publicly release the data, code, and model to facilitate further research and development in soccer visual understanding. <div>
arXiv:2512.11016v1 Announce Type: cross 
Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</title>
<link>https://arxiv.org/abs/2512.11047</link>
<guid>https://arxiv.org/abs/2512.11047</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid robots, loco-manipulation, Vision-Language-Action, reinforcement learning, WholeBodyVLA

<br /><br />Summary:  
This paper addresses the challenge of enabling humanoid robots to perform large-space loco-manipulation tasks by combining precise locomotion with dexterous manipulation. First, the authors identify two core problems: the scarcity of humanoid teleoperation data that limits the acquisition of loco-manipulation knowledge, and the lack of precision and stability in existing reinforcement learning (RL) controllers for faithfully executing locomotion commands. To overcome these challenges, the paper proposes a unified latent learning framework that allows a Vision-Language-Action (VLA) system to learn from low-cost, action-free egocentric videos, significantly enriching loco-manipulation knowledge. Additionally, an efficient human data collection pipeline is introduced to augment training datasets and scale these benefits. To improve execution precision, the authors design a loco-manipulation-oriented (LMO) RL policy tailored for stable and accurate core movements like advancing, turning, and squatting. Integrating these innovations, the framework WholeBodyVLA emerges as a pioneering solution for large-space humanoid loco-manipulation. The approach is experimentally validated on the AgiBot X2 humanoid robot, outperforming prior baselines by 21.3%, while demonstrating strong generalization and extensibility across various tasks. <div>
arXiv:2512.11047v1 Announce Type: cross 
Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2512.11067</link>
<guid>https://arxiv.org/abs/2512.11067</guid>
<content:encoded><![CDATA[
<div> Multimodal databases, SQL optimization, foundation models, explainability, human-AI interaction<br /><br />Summary:<br /><br />1. Traditional Database Management Systems (DBMSs) handle structured relational data and execute SQL queries with strong semantic guarantees and advanced optimizations but struggle with complexity and are limited to structured tables.<br /><br />2. Existing multimodal systems that operate on various data types such as text, images, and videos either require users to manually create machine learning User-Defined Functions (UDFs) embedded in SQL or rely entirely on black-box large language models (LLMs), resulting in trade-offs between usability and explainability.<br /><br />3. KathDB is introduced as a novel system that integrates traditional relational database semantics with the reasoning capabilities of foundation models, enabling querying over multimodal data seamlessly.<br /><br />4. The system incorporates interactive human-AI communication channels throughout the query lifecycle — during parsing, execution, and in explaining results — to allow users to iteratively refine queries and obtain transparent, explainable answers across different data modalities.<br /><br />5. KathDB thus advances multimodal data querying by bridging the gap between relational databases and foundation models, enhancing usability, interpretability, and user control compared to existing approaches. <div>
arXiv:2512.11067v1 Announce Type: cross 
Abstract: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data</title>
<link>https://arxiv.org/abs/2512.11074</link>
<guid>https://arxiv.org/abs/2512.11074</guid>
<content:encoded><![CDATA[
<div> Multi30k, multimodal machine translation, dataset extension, NLLB200-3.3B, language diversity<br /><br />Summary:<br /><br />This paper addresses the limitation of the original Multi30k dataset, which is constrained to four European languages (Czech, English, French, German) in Latin scripts, hindering multimodal machine translation (MMT) research across diverse languages and scripts. To broaden the linguistic scope, the authors present MultiScript30k, an extension translating the English Multi30k dataset into five additional global languages with varied scripts: Arabic (Ar), Spanish (Es), Ukrainian (Uk), Simplified Chinese (Zh_Hans), and Traditional Chinese (Zh_Hant). Translations were generated using the NLLB200-3.3B model, resulting in over 30,000 parallel sentences per language. The quality of these translations is assessed via similarity analyses, showing high cosine similarity (above 0.8) and low symmetric KL divergence (below 0.000251) for all languages except Traditional Chinese, which performs comparably to previous extensions. COMETKiwi evaluation metrics indicate mixed results; MultiScript30k's Arabic translations are on par with the existing ArEnMulti30k, while its Ukrainian translations score 6.4% lower than the previous Multi30k-Uk dataset. Overall, MultiScript30k represents a significant step toward expanding MMT research beyond European languages and scripts, enabling more inclusive multilingual modeling and evaluation. <div>
arXiv:2512.11074v1 Announce Type: cross 
Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast, accurate measurement of the worker populations of honey bee colonies using deep learning</title>
<link>https://arxiv.org/abs/2512.11075</link>
<guid>https://arxiv.org/abs/2512.11075</guid>
<content:encoded><![CDATA[
<div> Keywords: Honey bees, hive population counting, deep learning, CSRNet, ASUBEE dataset<br /><br />Summary:<br /><br />1. Honey bees are vital for pollination and global agriculture, making accurate hive population estimation crucial for ecological and agricultural studies. <br />2. Traditional bee counting methods are labor-intensive, slow, and prone to errors, especially in large-scale and complex hive environments. <br />3. This paper introduces a deep learning approach using CSRNet for automated bee population counting, leveraging density map estimation to handle challenges like occlusion and bee overlap. <br />4. The authors present ASUBEE, the first high-resolution dataset specifically curated for bee counting tasks in hive monitoring. <br />5. CSRNet demonstrates superior accuracy and efficiency, processing each image in just one second, making it highly effective for complex and dense hive scenarios. <br />6. The proposed method significantly enhances the efficiency of hive population assessments, offering a scalable solution beneficial to both researchers and beekeepers. <br />7. Overall, this work represents an important advancement in ecological research by applying AI to monitor honey bee populations precisely and efficiently, contributing valuable tools for environmental and agricultural management. <div>
arXiv:2512.11075v1 Announce Type: cross 
Abstract: Honey bees play a crucial role in pollination, contributing significantly to global agriculture and ecosystems. Accurately estimating hive populations is essential for understanding the effects of environmental factors on bee colonies, yet traditional methods of counting bees are time-consuming, labor-intensive, and prone to human error, particularly in large-scale studies. In this paper, we present a deep learning-based solution for automating bee population counting using CSRNet and introduce ASUBEE, the FIRST high-resolution dataset specifically designed for this task. Our method employs density map estimation to predict bee populations, effectively addressing challenges such as occlusion and overlapping bees that are common in hive monitoring. We demonstrate that CSRNet achieves superior performance in terms of time efficiency, with a computation time of just 1 second per image, while delivering accurate counts even in complex and densely populated hive scenarios. Our findings show that deep learning approaches like CSRNet can dramatically enhance the efficiency of hive population assessments, providing a valuable tool for researchers and beekeepers alike. This work marks a significant advancement in applying AI technologies to ecological research, offering scalable and precise monitoring solutions for honey bee populations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A probabilistic foundation model for crystal structure denoising, phase classification, and order parameters</title>
<link>https://arxiv.org/abs/2512.11077</link>
<guid>https://arxiv.org/abs/2512.11077</guid>
<content:encoded><![CDATA[
<div> Keywords: atomistic simulations, phase classification, order parameters, foundation model, denoising  

<br /><br />Summary:  
This paper addresses the challenge of extracting phase labels, order parameters (OPs), and defect information from noisy atomistic simulation data in a universal, robust, and interpretable way. Traditional tools like PTM and CNA are limited to a few specific lattice types (FCC, BCC, HCP), struggle with high thermal disorder or defects, and only provide hard labels without confidence scores. To overcome these limitations, the authors propose a novel log-probability foundation model that unifies denoising, phase classification, and OP extraction within a single probabilistic framework. This model builds on the MACE-MP interatomic potential, trained on crystal structures mapped to the AFLOW prototype database. It predicts per-atom, per-phase logits, which aggregate to a global log-density, whose gradient defines a conservative score field enabling data denoising via gradient ascent. Phase labels are obtained from the maximum logit values per atom, while the logits themselves serve as continuous, interpretable OPs sensitive to defects and indicating Euclidean distances to ideal phases. The model demonstrates universality over hundreds of crystal prototypes, robustness against strong thermal and defect-induced noise, and accurately captures complex systems such as ice polymorphs, ice-water interfaces, and shock-compressed titanium, advancing structural analysis in atomistic simulations. <div>
arXiv:2512.11077v1 Announce Type: cross 
Abstract: Atomistic simulations generate large volumes of noisy structural data, but extracting phase labels, order parameters (OPs), and defect information in a way that is universal, robust, and interpretable remains challenging. Existing tools such as PTM and CNA are restricted to a small set of hand-crafted lattices (e.g.\ FCC/BCC/HCP), degrade under strong thermal disorder or defects, and produce hard, template-based labels without per-atom probability or confidence scores. Here we introduce a log-probability foundation model that unifies denoising, phase classification, and OP extraction within a single probabilistic framework. We reuse the MACE-MP foundation interatomic potential on crystal structures mapped to AFLOW prototypes, training it to predict per-atom, per-phase logits $l$ and to aggregate them into a global log-density $\log \hat{P}_\theta(\boldsymbol{r})$ whose gradient defines a conservative score field. Denoising corresponds to gradient ascent on this learned log-density, phase labels follow from $\arg\max_c l_{ac}$, and the $l$ values act as continuous, defect-sensitive and interpretable OPs quantifying the Euclidean distance to ideal phases. We demonstrate universality across hundreds of prototypes, robustness under strong thermal and defect-induced disorder, and accurate treatment of complex systems such as ice polymorphs, ice--water interfaces, and shock-compressed Ti.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification</title>
<link>https://arxiv.org/abs/2512.11087</link>
<guid>https://arxiv.org/abs/2512.11087</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network verification, branch-and-bound, linear constraints, bound tightening, Clip-and-Verify

<br /><br />Summary: This paper introduces a novel framework called linear constraint-driven clipping to enhance neural network (NN) verifiers, particularly those using branch-and-bound (BaB) methods. The framework develops two new algorithms that utilize linear constraints to (1) reduce areas of the input space that are either already verified or irrelevant during BaB subproblems, and (2) improve the intermediate bounds propagated throughout the NN. The approach leverages linear constraints commonly derived from bound propagation but remains flexible to incorporate constraints from other sources as well. To handle these constraints efficiently, the method employs a specialized GPU-based procedure, avoiding expensive external solvers and enabling scalability to large networks. Their verification procedure, named Clip-and-Verify, consistently produces tighter bounds across various benchmarks and significantly reduces the number of subproblems tackled during BaB—achieving up to a 96% reduction in some cases. Clip-and-Verify seamlessly integrates with BaB-based verifiers such as the $\alpha,\beta$-CROWN framework, utilizing constraints either in activation-space splits or output-space unverified inputs. Experimental results demonstrate state-of-the-art verified accuracy on multiple benchmarks. Clip-and-Verify is publicly available as part of the $\alpha,\beta$-CROWN verifier, which won the VNN-COMP 2025 competition. The code can be accessed at https://github.com/Verified-Intelligence/Clip_and_Verify. <div>
arXiv:2512.11087v1 Announce Type: cross 
Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $\alpha,\beta$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $\alpha,\beta$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution</title>
<link>https://arxiv.org/abs/2512.11108</link>
<guid>https://arxiv.org/abs/2512.11108</guid>
<content:encoded><![CDATA[
<div> Keywords: feature attribution, bias evaluation, Integrated Gradient, transformers, explanation methods<br /><br />Summary: This paper addresses the inconsistencies observed in feature attribution explanations provided by different post-hoc methods like Integrated Gradient, which offer token-level insights for language models. It highlights the problem that varying biases inherent to different attribution methods lead to inconsistent explanations for the same input, causing user mistrust or misplaced trust. The authors propose a model- and method-agnostic framework based on three evaluation metrics to systematically analyze these biases. Their analysis focuses on two main types of biases: lexical bias (the "what" in the input) and position bias (the "where" in the input). The study compares these biases across two transformer models, examining both a controlled pseudo-random classification task using artificial data and a semi-controlled causal relation detection task using natural data. Results reveal a structural imbalance between lexical and position biases among models; models scoring high on lexical bias typically score low on position bias, and vice versa. Additionally, attribution methods that yield anomalous or unexpected explanations tend to exhibit higher levels of bias themselves. This work offers deeper insights into the nature of attribution method biases and guides better interpretation and trust calibration for language model explanations. <div>
arXiv:2512.11108v1 Announce Type: cross 
Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIBER: A Multilingual Evaluation Resource for Factual Inference Bias</title>
<link>https://arxiv.org/abs/2512.11110</link>
<guid>https://arxiv.org/abs/2512.11110</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, factual knowledge, multilingual benchmark, inference bias, multi-entity questions<br /><br />Summary: This paper introduces FIBER, a multilingual benchmark designed to evaluate the factual knowledge of large language models across single- and multi-entity settings. Unlike previous benchmarks focusing mainly on single-entity facts and monolingual data, FIBER includes sentence completion, question-answering, and object-count prediction tasks presented in English, Italian, and Turkish. The study investigates whether the language of the prompt influences inference bias in entity selection, revealing that prompt language indeed affects model outputs, especially for entities related to the country of that language. This inference bias exhibits topic variation, with 31% of topics showing a factual inference bias score above 0.5. Furthermore, the level of bias is language-dependent, with Turkish prompts exhibiting higher bias in 83% of topics compared to Italian prompts. The models also struggle more with multi-entity questions than single-entity ones, indicating increased complexity in multi-entity reasoning. Performance varies notably by language and model size; English prompts achieve the highest mean average precision, while Italian and Turkish perform lower. Larger models such as Llama-3.1-8B and Qwen-2.5-7B consistently outperform smaller models in the 3B-4B parameter range, confirming that model scale positively correlates with factual knowledge accuracy across languages and tasks. <div>
arXiv:2512.11110v1 Announce Type: cross 
Abstract: Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2512.11114</link>
<guid>https://arxiv.org/abs/2512.11114</guid>
<content:encoded><![CDATA[
<div> Multi-objective optimization, Bayesian optimization, transformer, reinforcement learning, Pareto frontier<br /><br />Summary:<br /><br />1. The paper addresses the challenge of balancing competing objectives in expensive, black-box problems common in fields like drug design and autonomous systems. <br />2. It critiques existing multi-objective Bayesian optimization methods noting their limitations such as the need for problem-specific surrogate models and acquisition functions, myopic single-step planning, and overhead from frequent refitting, especially in parallel or time-sensitive scenarios.<br />3. The authors introduce TAMO, a transformer-based, fully amortized policy for multi-objective black-box optimization that works across varying input and objective dimensions.<br />4. TAMO is pretrained using reinforcement learning to maximize cumulative hypervolume improvement over entire optimization trajectories while conditioning on full query histories to approximate the Pareto frontier.<br />5. Unlike traditional methods, TAMO allows for fast proposal generation of new designs with a single forward pass, reducing proposal time by 50–1000x without retraining.<br />6. Experimental results on synthetic benchmarks and real tasks show that TAMO matches or improves the quality of Pareto fronts within tight evaluation budgets.<br />7. This work demonstrates that transformers can carry out multi-objective optimization entirely in-context, eliminating the need for surrogate fitting and acquisition engineering per task.<br />8. The approach paves the way for foundation-style, plug-and-play optimizers aimed at enhancing scientific discovery workflows. <div>
arXiv:2512.11114v1 Announce Type: cross 
Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Regularized Online Optimization with Switching Costs</title>
<link>https://arxiv.org/abs/2512.11131</link>
<guid>https://arxiv.org/abs/2512.11131</guid>
<content:encoded><![CDATA[
<div> Fairness, Online Convex Optimization, Switching Costs, Competitive Ratio, Resource Provisioning  

<br /><br />Summary:  
This paper addresses the simultaneous consideration of fairness and action smoothness in online convex optimization, particularly under switching costs. It highlights the fundamental challenge posed by the long-term fairness regularizer, showing that no online algorithm can achieve sublinear regret or a finite competitive ratio compared to the offline optimum as the time horizon \( T \) grows, even without switching costs. To overcome this, the authors propose FairOBD (Fairness-regularized Online Balanced Descent), an algorithm that balances hitting cost, switching cost, and fairness cost by decomposing the long-term fairness term into a sequence of online costs through an auxiliary variable. This auxiliary variable helps regularize the action sequence to produce fair outcomes. They analytically prove that FairOBD attains a worst-case asymptotic competitive ratio against a new benchmark: the optimal offline algorithm with parameterized constraints, in the limit as \( T \to \infty \). Finally, the paper includes trace-driven experiments focusing on dynamic computing resource provisioning for socially responsible AI inference, demonstrating that FairOBD effectively reduces the overall fairness-regularized cost and achieves fairer outcomes compared to existing baselines. <div>
arXiv:2512.11131v1 Announce Type: cross 
Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles</title>
<link>https://arxiv.org/abs/2512.11145</link>
<guid>https://arxiv.org/abs/2512.11145</guid>
<content:encoded><![CDATA[
<div> Keywords: autoencoder, clustering loss, contrastive loss, scientific ensemble datasets, dimensionality reduction<br /><br />Summary:<br /><br />This paper addresses the challenge of analyzing and visualizing high-dimensional and complex scientific ensemble datasets by enhancing autoencoder frameworks. The proposed method integrates a clustering loss based on the soft silhouette score alongside a contrastive loss to improve feature extraction, visualization, and interpretability. EfficientNetV2 is employed to generate pseudo-labels for unlabeled data segments, facilitating supervised guidance within the framework. The approach jointly optimizes reconstruction, clustering, and contrastive objectives to ensure similar data points cluster closely while distinct clusters remain separated in the latent space. Following latent space learning, UMAP is used to create 2D projections, which are evaluated using the silhouette score to assess clustering quality. The study compares multiple autoencoder variants to determine which better capture meaningful features. Experimental validation is conducted using two scientific ensemble datasets: channel structures in soil generated by Markov chain Monte Carlo simulations, and droplet-on-film impact dynamics. Results demonstrate that models incorporating either clustering or contrastive loss slightly outperform baseline autoencoder models, indicating the effectiveness of these additional losses in improving representation learning for complex high-dimensional scientific data. <div>
arXiv:2512.11145v1 Announce Type: cross 
Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniScope: A Least Privilege Framework for Authorizing Tool Calling Agents</title>
<link>https://arxiv.org/abs/2512.11147</link>
<guid>https://arxiv.org/abs/2512.11147</guid>
<content:encoded><![CDATA[
<div> Tool calling agents, LLM security, least privilege, permission hierarchies, MiniScope<br /><br />Summary:<br /><br />1. Tool calling agents represent an emerging paradigm in deploying large language models (LLMs) with platforms like ChatGPT, Claude, and Gemini integrating connectors and autonomous functions. 2. Despite their utility, the inherent unreliability of LLMs poses fundamental security risks when these agents interact with sensitive user services. 3. Existing solutions either depend on manually crafted security policies, which require expert knowledge, or use confinement loops that do not provide rigorous security guarantees. 4. MiniScope is introduced as a novel framework that enables tool calling agents to safely operate on user accounts by rigorously enforcing least privilege principles, thereby limiting potential damage from unreliable LLMs. 5. The framework reconstructs permission hierarchies based on relationships between tool calls and applies a mobile-style permission model to optimize the balance between security and usability. 6. To validate MiniScope, a synthetic dataset simulating complex agentic tasks across ten popular applications was created, reflecting realistic operational demands beyond existing benchmarks. 7. Experimental results demonstrate that MiniScope introduces only a minimal latency overhead of 1-6% compared to standard tool calling agents while markedly reducing permissions and cutting computational and operational costs relative to LLM-based baseline approaches. <div>
arXiv:2512.11147v1 Announce Type: cross 
Abstract: Tool calling agents are an emerging paradigm in LLM deployment, with major platforms such as ChatGPT, Claude, and Gemini adding connectors and autonomous capabilities. However, the inherent unreliability of LLMs introduces fundamental security risks when these agents operate over sensitive user services. Prior approaches either rely on manually written policies that require security expertise, or place LLMs in the confinement loop, which lacks rigorous security guarantees. We present MiniScope, a framework that enables tool calling agents to operate on user accounts while confining potential damage from unreliable LLMs. MiniScope introduces a novel way to automatically and rigorously enforce least privilege principles by reconstructing permission hierarchies that reflect relationships among tool calls and combining them with a mobile-style permission model to balance security and ease of use. To evaluate MiniScope, we create a synthetic dataset derived from ten popular real-world applications, capturing the complexity of realistic agentic tasks beyond existing simplified benchmarks. Our evaluation shows that MiniScope incurs only 1-6% latency overhead compared to vanilla tool calling agents, while significantly outperforming the LLM based baseline in minimizing permissions as well as computational and operational costs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context</title>
<link>https://arxiv.org/abs/2512.11167</link>
<guid>https://arxiv.org/abs/2512.11167</guid>
<content:encoded><![CDATA[
<div> Keywords: reproducibility, Monkey Vision-Language Model, image tiling, high-resolution image understanding, global context<br /><br />Summary:<br /><br />1. This work focuses on the reproducibility of the Monkey Vision-Language Model (VLM) introduced by Li et al. (2023b) at CVPR24, which employs image tiling for detailed, high-resolution image understanding. <br /><br />2. The original model's approach involves splitting large images into smaller tiles to capture fine-grained visual information while maintaining computational efficiency. <br /><br />3. The authors replicated the Monkey VLM training pipeline using publicly available checkpoints and reimplemented the full training procedure to verify the findings. <br /><br />4. Their reproduction confirmed that image tiling indeed helps recover local visual details effectively, supporting the original paper’s claims. <br /><br />5. Beyond replication, the study explored the impact of adding global context to tiled images, finding that the benefits and deviations in performance vary significantly depending on the specific task and the granularity of the tiles used. This highlights important practical considerations for future high-resolution multimodal vision-language model development. <div>
arXiv:2512.11167v1 Announce Type: cross 
Abstract: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast EXP3 Algorithms</title>
<link>https://arxiv.org/abs/2512.11201</link>
<guid>https://arxiv.org/abs/2512.11201</guid>
<content:encoded><![CDATA[
<div> EXP3, constant time, regret bounds, time complexity, practical algorithms<br /><br />Summary:<br /><br />1. The paper demonstrates that the EXP3 algorithm, commonly used in adversarial bandit problems, can be implemented with a constant time complexity per round, improving computational efficiency. 2. It introduces new algorithms inspired by EXP3 that are more practical for real-world applications, focusing on easier implementation without compromising performance. 3. The authors conduct a detailed analysis of the trade-offs between regret bounds (a measure of algorithm performance) and the computational time complexity of the proposed algorithms. 4. Their results highlight how reducing time complexity can impact the regret bounds and vice versa, providing valuable insights into designing efficient bandit algorithms. 5. Overall, the paper contributes to both theoretical understanding and practical implementation strategies for adversarial bandit algorithms by balancing regret performance and execution speed. <div>
arXiv:2512.11201v1 Announce Type: cross 
Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>amc: The Automated Mission Classifier for Telescope Bibliographies</title>
<link>https://arxiv.org/abs/2512.11202</link>
<guid>https://arxiv.org/abs/2512.11202</guid>
<content:encoded><![CDATA[
arXiv:2512.11202v1 Announce Type: cross 
Abstract: Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2512.11221</link>
<guid>https://arxiv.org/abs/2512.11221</guid>
<content:encoded><![CDATA[
arXiv:2512.11221v1 Announce Type: cross 
Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFMF: World Modeling by Forecasting Vision Foundation Model Features</title>
<link>https://arxiv.org/abs/2512.11225</link>
<guid>https://arxiv.org/abs/2512.11225</guid>
<content:encoded><![CDATA[
arXiv:2512.11225v1 Announce Type: cross 
Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Generalisation of the Implicit Dynamics of In-Context Learning</title>
<link>https://arxiv.org/abs/2512.11255</link>
<guid>https://arxiv.org/abs/2512.11255</guid>
<content:encoded><![CDATA[
arXiv:2512.11255v1 Announce Type: cross 
Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges</title>
<link>https://arxiv.org/abs/2512.11258</link>
<guid>https://arxiv.org/abs/2512.11258</guid>
<content:encoded><![CDATA[
arXiv:2512.11258v1 Announce Type: cross 
Abstract: Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scalable Multi-GPU Framework for Encrypted Large-Model Inference</title>
<link>https://arxiv.org/abs/2512.11269</link>
<guid>https://arxiv.org/abs/2512.11269</guid>
<content:encoded><![CDATA[
arXiv:2512.11269v1 Announce Type: cross 
Abstract: Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning</title>
<link>https://arxiv.org/abs/2512.11276</link>
<guid>https://arxiv.org/abs/2512.11276</guid>
<content:encoded><![CDATA[
arXiv:2512.11276v1 Announce Type: cross 
Abstract: Serious illness can deprive patients of the capacity to speak for themselves. As populations age and caregiver networks shrink, the need for reliable support in Advance Care Planning (ACP) grows. To probe this fraught design space of using proxy agents for high-risk, high-subjectivity decisions, we built an experience prototype (\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal proxy in ACP decisions. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings argue for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We conclude with design recommendations to balance the risks and benefits of such an agent.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise</title>
<link>https://arxiv.org/abs/2512.11282</link>
<guid>https://arxiv.org/abs/2512.11282</guid>
<content:encoded><![CDATA[
arXiv:2512.11282v1 Announce Type: cross 
Abstract: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the ${\alpha}$-Coefficient</title>
<link>https://arxiv.org/abs/2512.11295</link>
<guid>https://arxiv.org/abs/2512.11295</guid>
<content:encoded><![CDATA[
arXiv:2512.11295v1 Announce Type: cross 
Abstract: The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</title>
<link>https://arxiv.org/abs/2512.11296</link>
<guid>https://arxiv.org/abs/2512.11296</guid>
<content:encoded><![CDATA[
arXiv:2512.11296v1 Announce Type: cross 
Abstract: Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condensation-Concatenation Framework for Dynamic Graph Continual Learning</title>
<link>https://arxiv.org/abs/2512.11317</link>
<guid>https://arxiv.org/abs/2512.11317</guid>
<content:encoded><![CDATA[
arXiv:2512.11317v1 Announce Type: cross 
Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLLM Machine Unlearning via Visual Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.11325</link>
<guid>https://arxiv.org/abs/2512.11325</guid>
<content:encoded><![CDATA[
arXiv:2512.11325v1 Announce Type: cross 
Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture</title>
<link>https://arxiv.org/abs/2512.11350</link>
<guid>https://arxiv.org/abs/2512.11350</guid>
<content:encoded><![CDATA[
arXiv:2512.11350v1 Announce Type: cross 
Abstract: Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMODEL-LLM: Transforming C code to Java using LLMs</title>
<link>https://arxiv.org/abs/2512.11402</link>
<guid>https://arxiv.org/abs/2512.11402</guid>
<content:encoded><![CDATA[
arXiv:2512.11402v1 Announce Type: cross 
Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models</title>
<link>https://arxiv.org/abs/2512.11412</link>
<guid>https://arxiv.org/abs/2512.11412</guid>
<content:encoded><![CDATA[
arXiv:2512.11412v1 Announce Type: cross 
Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowception: Temporally Expansive Flow Matching for Video Generation</title>
<link>https://arxiv.org/abs/2512.11438</link>
<guid>https://arxiv.org/abs/2512.11438</guid>
<content:encoded><![CDATA[
arXiv:2512.11438v1 Announce Type: cross 
Abstract: We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.11458</link>
<guid>https://arxiv.org/abs/2512.11458</guid>
<content:encoded><![CDATA[
arXiv:2512.11458v1 Announce Type: cross 
Abstract: We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring MLLM-Diffusion Information Transfer with MetaCanvas</title>
<link>https://arxiv.org/abs/2512.11464</link>
<guid>https://arxiv.org/abs/2512.11464</guid>
<content:encoded><![CDATA[
arXiv:2512.11464v1 Announce Type: cross 
Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models</title>
<link>https://arxiv.org/abs/2512.11482</link>
<guid>https://arxiv.org/abs/2512.11482</guid>
<content:encoded><![CDATA[
arXiv:2512.11482v1 Announce Type: cross 
Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</title>
<link>https://arxiv.org/abs/2512.11509</link>
<guid>https://arxiv.org/abs/2512.11509</guid>
<content:encoded><![CDATA[
arXiv:2512.11509v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics</title>
<link>https://arxiv.org/abs/2512.11525</link>
<guid>https://arxiv.org/abs/2512.11525</guid>
<content:encoded><![CDATA[
arXiv:2512.11525v1 Announce Type: cross 
Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Time Series Forecasting with Anomalies</title>
<link>https://arxiv.org/abs/2512.11526</link>
<guid>https://arxiv.org/abs/2512.11526</guid>
<content:encoded><![CDATA[
arXiv:2512.11526v1 Announce Type: cross 
Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</title>
<link>https://arxiv.org/abs/2512.11532</link>
<guid>https://arxiv.org/abs/2512.11532</guid>
<content:encoded><![CDATA[
arXiv:2512.11532v1 Announce Type: cross 
Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition</title>
<link>https://arxiv.org/abs/2512.11545</link>
<guid>https://arxiv.org/abs/2512.11545</guid>
<content:encoded><![CDATA[
arXiv:2512.11545v1 Announce Type: cross 
Abstract: Underwater acoustic target recognition (UATR) is extremely challenging due to the complexity of ship-radiated noise and the variability of ocean environments. Although deep learning (DL) approaches have achieved promising results, most existing models implicitly assume that underwater acoustic data lie in a Euclidean space. This assumption, however, is unsuitable for the inherently complex topology of underwater acoustic signals, which exhibit non-stationary, non-Gaussian, and nonlinear characteristics. To overcome this limitation, this paper proposes the UATR-GTransformer, a non-Euclidean DL model that integrates Transformer architectures with graph neural networks (GNNs). The model comprises three key components: a Mel patchify block, a GTransformer block, and a classification head. The Mel patchify block partitions the Mel-spectrogram into overlapping patches, while the GTransformer block employs a Transformer Encoder to capture mutual information between split patches to generate Mel-graph embeddings. Subsequently, a GNN enhances these embeddings by modeling local neighborhood relationships, and a feed-forward network (FFN) further performs feature transformation. Experiments results based on two widely used benchmark datasets demonstrate that the UATR-GTransformer achieves performance competitive with state-of-the-art methods. In addition, interpretability analysis reveals that the proposed model effectively extracts rich frequency-domain information, highlighting its potential for applications in ocean engineering.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.11546</link>
<guid>https://arxiv.org/abs/2512.11546</guid>
<content:encoded><![CDATA[
arXiv:2512.11546v1 Announce Type: cross 
Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</title>
<link>https://arxiv.org/abs/2512.11558</link>
<guid>https://arxiv.org/abs/2512.11558</guid>
<content:encoded><![CDATA[
arXiv:2512.11558v1 Announce Type: cross 
Abstract: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-temporal Calving Front Segmentation</title>
<link>https://arxiv.org/abs/2512.11560</link>
<guid>https://arxiv.org/abs/2512.11560</guid>
<content:encoded><![CDATA[
arXiv:2512.11560v1 Announce Type: cross 
Abstract: The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents</title>
<link>https://arxiv.org/abs/2512.11584</link>
<guid>https://arxiv.org/abs/2512.11584</guid>
<content:encoded><![CDATA[
arXiv:2512.11584v1 Announce Type: cross 
Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</title>
<link>https://arxiv.org/abs/2512.11614</link>
<guid>https://arxiv.org/abs/2512.11614</guid>
<content:encoded><![CDATA[
arXiv:2512.11614v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling</title>
<link>https://arxiv.org/abs/2512.11635</link>
<guid>https://arxiv.org/abs/2512.11635</guid>
<content:encoded><![CDATA[
arXiv:2512.11635v1 Announce Type: cross 
Abstract: Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews</title>
<link>https://arxiv.org/abs/2512.11661</link>
<guid>https://arxiv.org/abs/2512.11661</guid>
<content:encoded><![CDATA[
arXiv:2512.11661v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines</title>
<link>https://arxiv.org/abs/2512.11724</link>
<guid>https://arxiv.org/abs/2512.11724</guid>
<content:encoded><![CDATA[
arXiv:2512.11724v1 Announce Type: cross 
Abstract: While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.11743</link>
<guid>https://arxiv.org/abs/2512.11743</guid>
<content:encoded><![CDATA[
arXiv:2512.11743v1 Announce Type: cross 
Abstract: Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation</title>
<link>https://arxiv.org/abs/2512.11748</link>
<guid>https://arxiv.org/abs/2512.11748</guid>
<content:encoded><![CDATA[
arXiv:2512.11748v1 Announce Type: cross 
Abstract: This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title>
<link>https://arxiv.org/abs/2512.11771</link>
<guid>https://arxiv.org/abs/2512.11771</guid>
<content:encoded><![CDATA[
arXiv:2512.11771v1 Announce Type: cross 
Abstract: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Coverage Diagnostics for Conformal Prediction</title>
<link>https://arxiv.org/abs/2512.11779</link>
<guid>https://arxiv.org/abs/2512.11779</guid>
<content:encoded><![CDATA[
arXiv:2512.11779v1 Announce Type: cross 
Abstract: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agile Flight Emerges from Multi-Agent Competitive Racing</title>
<link>https://arxiv.org/abs/2512.11781</link>
<guid>https://arxiv.org/abs/2512.11781</guid>
<content:encoded><![CDATA[
arXiv:2512.11781v1 Announce Type: cross 
Abstract: Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously</title>
<link>https://arxiv.org/abs/2512.11783</link>
<guid>https://arxiv.org/abs/2512.11783</guid>
<content:encoded><![CDATA[
arXiv:2512.11783v1 Announce Type: cross 
Abstract: The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.
  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particulate: Feed-Forward 3D Object Articulation</title>
<link>https://arxiv.org/abs/2512.11798</link>
<guid>https://arxiv.org/abs/2512.11798</guid>
<content:encoded><![CDATA[
arXiv:2512.11798v1 Announce Type: cross 
Abstract: We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability Bracket Notation: Multivariable Systems and Static Bayesian Networks</title>
<link>https://arxiv.org/abs/1207.5293</link>
<guid>https://arxiv.org/abs/1207.5293</guid>
<content:encoded><![CDATA[
arXiv:1207.5293v5 Announce Type: replace 
Abstract: We expand the Probability Bracket Notation (PBN), a symbolic framework inspired by the Dirac notation in quantum mechanics, to multivariable probability systems and static Bayesian networks (BNs). By defining joint, marginal, and conditional probability distributions (PDs), as well as marginal and conditional expectations, we demonstrate how to express dependencies among multiple random variables and manipulate them algebraically in PBN. Using the well-known Student BN as an example of probabilistic graphical models (PGMs), we illustrate how to apply PBN to analyze predictions, inferences (using both bottom-up and top-down approaches), and expectations. We then extend PBN to BNs with continuous variables. After reviewing linear Gaussian networks, we introduce a customized Healthcare BN that includes both continuous and discrete random variables, utilizes user-specific data, and provides tailored predictions via discrete-display (DD) nodes that proxy for their continuous-variable parents. Compared to traditional probability notation, PBN offers an operator-driven framework that unifies and simplifies the analysis of probabilistic models, with potential as both an educational tool and a practical platform for causal reasoning, inference, expectation, data analytics, machine learning, and artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform</title>
<link>https://arxiv.org/abs/2312.04180</link>
<guid>https://arxiv.org/abs/2312.04180</guid>
<content:encoded><![CDATA[
arXiv:2312.04180v3 Announce Type: replace 
Abstract: This study investigates how artificial intelligence (AI) influences various online labor markets (OLMs) over time. Employing the Difference-in-Differences method, we discovered two distinct scenarios following ChatGPT's launch: displacement effects featuring reduced work volume and earnings, exemplified by translation & localization OLM; productivity effects featuring increased work volume and earnings, exemplified by web development OLM. To understand these opposite effects in a unified framework, we developed a Cournot competition model to identify an inflection point for each market. Before this point, human workers benefit from AI enhancements; beyond this point, human workers would be replaced. Further analyzing the progression from ChatGPT 3.5 to 4.0, we found three effect scenarios, reinforcing our inflection point conjecture. Heterogeneous analyses reveal that U.S. web developers tend to benefit more from ChatGPT's launch compared to their counterparts in other regions. Experienced translators seem more likely to exit the market than less experienced translators.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Aligned Decoding</title>
<link>https://arxiv.org/abs/2405.21047</link>
<guid>https://arxiv.org/abs/2405.21047</guid>
<content:encoded><![CDATA[
arXiv:2405.21047v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper, we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rolling in the deep of cognitive and AI biases</title>
<link>https://arxiv.org/abs/2407.21202</link>
<guid>https://arxiv.org/abs/2407.21202</guid>
<content:encoded><![CDATA[
arXiv:2407.21202v4 Announce Type: replace 
Abstract: Nowadays, we delegate many of our decisions to Artificial Intelligence (AI) that acts either in solo or as a human companion in decisions made to support several sensitive domains, like healthcare, financial services and law enforcement. AI systems, even carefully designed to be fair, are heavily criticized for delivering misjudged and discriminated outcomes against individuals and groups. Numerous work on AI algorithmic fairness is devoted on Machine Learning pipelines which address biases and quantify fairness under a pure computational view. However, the continuous unfair and unjust AI outcomes, indicate that there is urgent need to understand AI as a sociotechnical system, inseparable from the conditions in which it is designed, developed and deployed. Although, the synergy of humans and machines seems imperative to make AI work, the significant impact of human and societal factors on AI bias is currently overlooked. We address this critical issue by following a radical new methodology under which human cognitive biases become core entities in our AI fairness overview. Inspired by the cognitive science definition and taxonomy of human heuristics, we identify how harmful human actions influence the overall AI lifecycle, and reveal human to AI biases hidden pathways. We introduce a new mapping, which justifies the human heuristics to AI biases reflections and we detect relevant fairness intensities and inter-dependencies. We envision that this approach will contribute in revisiting AI fairness under deeper human-centric case studies, revealing hidden biases cause and effects.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection</title>
<link>https://arxiv.org/abs/2506.13793</link>
<guid>https://arxiv.org/abs/2506.13793</guid>
<content:encoded><![CDATA[
arXiv:2506.13793v3 Announce Type: replace 
Abstract: Large reasoning models excel in domains like mathematics where intermediate reasoning is straightforward to verify, but struggle to self-correct in medicine fields where evaluating intermediate reasoning is cumbersome and expensive. This verification bottleneck hinders the development of reliable AI reasoners for high-stakes application. Here we propose Med-REFL, a novel framework that learns fine-grained reflection without human labels or model distillation. Med-REFL introduces a deterministic structural assessment of the reasoning space to automatically generate preference data for reflection. By globally evaluating all explored reasoning paths in a tree-of-thoughts, our method quantifies the value of corrective actions, enabling the automated construction of direct preference optimization pairs. This trains the model to recognize and amend its own reasoning fallacies. Extensive experiments show Med-REFL delivers robust gains across diverse models architectures and medical benchmarks, boosting a general-purpose Llama3.1-8B by +5.82% and the state-of-the-art Huatuo-o1 by +4.13% on the MedQA benchmark. Our Med-REFL-8B achieves state-of-the-art performance among 7-8B models while even competing with models twice its size. Crucially, targeted ablations prove its success generalizes to other domains such as logical reasoning and mitigates the `fake reflection' phenomenon in LRMs. Ultimately, our framework provides a scalable solution to the verification bottleneck, paving the way for more reliable AI reasoners in high-stakes domains like medicine. Med-REFL has been made publicly available in https://github.com/TianYin123/Med-REFL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</title>
<link>https://arxiv.org/abs/2508.15447</link>
<guid>https://arxiv.org/abs/2508.15447</guid>
<content:encoded><![CDATA[
arXiv:2508.15447v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising potential in business applications, particularly in enterprise decision support and strategic planning, yet current approaches often struggle to reconcile intricate operational analyses with overarching strategic goals across diverse market environments, leading to fragmented workflows and reduced collaboration across organizational levels. This paper introduces BusiAgent, a novel multi-agent framework leveraging LLMs for advanced decision-making in complex corporate environments. BusiAgent integrates three core innovations: an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a generalized entropy measure to optimize collaborative efficiency, and a multi-level Stackelberg game to handle hierarchical decision processes. Additionally, contextual Thompson sampling is employed for prompt optimization, supported by a comprehensive quality assurance system to mitigate errors. Extensive empirical evaluations across diverse business scenarios validate BusiAgent's efficacy, demonstrating its capacity to generate coherent, client-focused solutions that smoothly integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction. By fusing cutting-edge AI technologies with deep business insights, BusiAgent marks a substantial step forward in AI-driven enterprise decision-making, empowering organizations to navigate complex business landscapes more effectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Readiness in Health AI</title>
<link>https://arxiv.org/abs/2509.18234</link>
<guid>https://arxiv.org/abs/2509.18234</guid>
<content:encoded><![CDATA[
arXiv:2509.18234v3 Announce Type: replace 
Abstract: Large language models have demonstrated remarkable performance in a wide range of medical benchmarks. Yet underneath the seemingly promising results lie salient growth areas, especially in cutting-edge frontiers such as multimodal reasoning. In this paper, we introduce a series of adversarial stress tests to systematically assess the robustness of flagship models and medical benchmarks. Our study reveals prevalent brittleness in the presence of simple adversarial transformations: leading systems can guess the right answer even with key inputs removed, yet may get confused by the slightest prompt alterations, while fabricating convincing yet flawed reasoning traces. Using clinician-guided rubrics, we demonstrate that popular medical benchmarks vary widely in what they truly measure. Our study reveals significant competency gaps of frontier AI in attaining real-world readiness for health applications. If we want AI to earn trust in healthcare, we must demand more than leaderboard wins and must hold AI systems accountable to ensure robustness, sound reasoning, and alignment with real medical demands.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</title>
<link>https://arxiv.org/abs/2510.16309</link>
<guid>https://arxiv.org/abs/2510.16309</guid>
<content:encoded><![CDATA[
arXiv:2510.16309v3 Announce Type: replace 
Abstract: Large language models (LLMs) often produce fluent reasoning steps while violating simple mathematical or logical constraints. We introduce MedRule-KG, a compact typed knowledge graph coupled with a symbolic verifier, designed to enforce mathematically interpretable rules in reasoning tasks. MedRule-KG encodes entities, relations, and three domain-inspired rules, while the verifier checks predictions and applies minimal corrections to guarantee consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields 1.000 EM while eliminating rule violations entirely. We demonstrate how MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss ablations, and release code and data to encourage reproducibility.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Unify Plan and Action for Universal Granularity Control</title>
<link>https://arxiv.org/abs/2510.23564</link>
<guid>https://arxiv.org/abs/2510.23564</guid>
<content:encoded><![CDATA[
arXiv:2510.23564v3 Announce Type: replace 
Abstract: Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Dysarthria</title>
<link>https://arxiv.org/abs/2511.07895</link>
<guid>https://arxiv.org/abs/2511.07895</guid>
<content:encoded><![CDATA[
arXiv:2511.07895v2 Announce Type: replace 
Abstract: Dysarthria impairs motor control of speech, often resulting in reduced intelligibility and frequent misarticulations. Although interest in brain-computer interface technologies is growing, electroencephalogram (EEG)-based communication support for individuals with dysarthria remains limited. To address this gap, we recorded EEG data from one participant with dysarthria during a Korean automatic speech task and labeled each trial as correct or misarticulated. Spectral analysis revealed that misarticulated trials exhibited elevated frontal-central delta and alpha power, along with reduced temporal gamma activity. Building on these observations, we developed a soft multitask learning framework designed to suppress these nonspecific spectral responses and incorporated a maximum mean discrepancy-based alignment module to enhance class discrimination while minimizing domain-related variability. The proposed model achieved F1-scores of 52.7 % for correct and 41.4 % for misarticulated trials-an improvement of 2 % and 11 % over the baseline-demonstrating more stable intention decoding even under articulation errors. These results highlight the potential of EEG-based assistive systems for communication in language impaired individuals.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI</title>
<link>https://arxiv.org/abs/2511.12306</link>
<guid>https://arxiv.org/abs/2511.12306</guid>
<content:encoded><![CDATA[
arXiv:2511.12306v2 Announce Type: replace 
Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning</title>
<link>https://arxiv.org/abs/2511.12963</link>
<guid>https://arxiv.org/abs/2511.12963</guid>
<content:encoded><![CDATA[
arXiv:2511.12963v2 Announce Type: replace 
Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</title>
<link>https://arxiv.org/abs/2511.15351</link>
<guid>https://arxiv.org/abs/2511.15351</guid>
<content:encoded><![CDATA[
arXiv:2511.15351v2 Announce Type: replace 
Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.05943</link>
<guid>https://arxiv.org/abs/2512.05943</guid>
<content:encoded><![CDATA[
arXiv:2512.05943v3 Announce Type: replace 
Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Smart Factory Model: A model-based Approach for Integrating Industry 4.0 and Sustainability for Manufacturing Systems</title>
<link>https://arxiv.org/abs/2512.10631</link>
<guid>https://arxiv.org/abs/2512.10631</guid>
<content:encoded><![CDATA[
arXiv:2512.10631v2 Announce Type: replace 
Abstract: This paper presents the Unified Smart Factory Model (USFM), a comprehensive framework designed to translate high-level sustainability goals into measurable factory-level indicators with a systematic information map of manufacturing activities. The manufacturing activities were modelled as set of manufacturing, assembly and auxiliary processes using Object Process Methodology, a Model Based Systems Engineering (MBSE) language. USFM integrates Manufacturing Process and System, Data Process, and Key Performance Indicator (KPI) Selection and Assessment in a single framework. Through a detailed case study of Printed Circuit Board (PCB) assembly factory, the paper demonstrates how environmental sustainability KPIs can be selected, modelled, and mapped to the necessary data, highlighting energy consumption and environmental impact metrics. The model's systematic approach can reduce redundancy, minimize the risk of missing critical information, and enhance data collection. The paper concluded that the USFM bridges the gap between sustainability goals and practical implementation, providing significant benefits for industries specifically SMEs aiming to achieve sustainability targets.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DSGrasp: 3D Shape-Completion for Robotic Grasp</title>
<link>https://arxiv.org/abs/2301.00866</link>
<guid>https://arxiv.org/abs/2301.00866</guid>
<content:encoded><![CDATA[
arXiv:2301.00866v2 Announce Type: replace-cross 
Abstract: Real-world robotic grasping can be done robustly if a complete 3D Point Cloud Data (PCD) of an object is available. However, in practice, PCDs are often incomplete when objects are viewed from few and sparse viewpoints before the grasping action, leading to the generation of wrong or inaccurate grasp poses. We propose a novel grasping strategy, named 3DSGrasp, that predicts the missing geometry from the partial PCD to produce reliable grasp poses. Our proposed PCD completion network is a Transformer-based encoder-decoder network with an Offset-Attention layer. Our network is inherently invariant to the object pose and point's permutation, which generates PCDs that are geometrically consistent and completed properly. Experiments on a wide range of partial PCD show that 3DSGrasp outperforms the best state-of-the-art method on PCD completion tasks and largely improves the grasping success rate in real-world scenarios. The code and dataset will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title>
<link>https://arxiv.org/abs/2402.03216</link>
<guid>https://arxiv.org/abs/2402.03216</guid>
<content:encoded><![CDATA[
arXiv:2402.03216v5 Announce Type: replace-cross 
Abstract: In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in \textit{Multi-Linguality}, \textit{Multi-Functionality}, and \textit{Multi-Granularity}. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Learning for Scalable Representation of High-Dimensional Medical Data</title>
<link>https://arxiv.org/abs/2409.13115</link>
<guid>https://arxiv.org/abs/2409.13115</guid>
<content:encoded><![CDATA[
arXiv:2409.13115v2 Announce Type: replace-cross 
Abstract: Integrating artificial intelligence (AI) with healthcare data is rapidly transforming medical diagnostics and driving progress toward precision medicine. However, effectively leveraging multimodal data, particularly digital pathology whole slide images (WSIs) and genomic sequencing, remains a significant challenge due to the intrinsic heterogeneity of these modalities and the need for scalable and interpretable frameworks. Existing diagnostic models typically operate on unimodal data, overlooking critical cross-modal interactions that can yield richer clinical insights. We introduce MarbliX (Multimodal Association and Retrieval with Binary Latent Indexed matriX), a self-supervised framework that learns to embed WSIs and immunogenomic profiles into compact, scalable binary codes, termed ``monogram.'' By optimizing a triplet contrastive objective across modalities, MarbliX captures high-resolution patient similarity in a unified latent space, enabling efficient retrieval of clinically relevant cases and facilitating case-based reasoning. \textcolor{black}{In lung cancer, MarbliX achieves 85-89\% across all evaluation metrics, outperforming histopathology (69-71\%) and immunogenomics (73-76\%). In kidney cancer, real-valued monograms yield the strongest performance (F1: 80-83\%, Accuracy: 87-90\%), with binary monograms slightly lower (F1: 78-82\%).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Continual Instruction Assistant</title>
<link>https://arxiv.org/abs/2410.10868</link>
<guid>https://arxiv.org/abs/2410.10868</guid>
<content:encoded><![CDATA[
arXiv:2410.10868v5 Announce Type: replace-cross 
Abstract: Continual Instruction Tuning (CIT) is adopted to continually instruct Large Models to follow human intent data by data. It is observed that existing gradient update would heavily destroy the performance on previous datasets during CIT process. Instead, Exponential Moving Average (EMA), owns the ability to trace previous parameters, which can aid in decreasing forgetting. Nonetheless, its stable balance weight fails to deal with the ever-changing datasets, leading to the out-of-balance between plasticity and stability. In this paper, we propose a general continual instruction tuning framework to address the challenge. Starting from the trade-off prerequisite and EMA update, we propose the plasticity and stability ideal condition. Based on Taylor expansion in the loss function, we find the optimal balance weight can be automatically determined by the gradients and learned parameters. Therefore, we propose a stable-plasticity balanced coefficient to avoid knowledge interference. Based on the semantic similarity of the instructions, we can determine whether to retrain or expand the training parameters and allocate the most suitable parameters for the testing instances. Extensive experiments across multiple continual instruction tuning benchmarks demonstrate that our approach not only enhances anti-forgetting capabilities but also significantly improves overall continual tuning performance. Our code is available at https://github.com/JingyangQiao/CoIN.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WARPD: World model Assisted Reactive Policy Diffusion</title>
<link>https://arxiv.org/abs/2410.14040</link>
<guid>https://arxiv.org/abs/2410.14040</guid>
<content:encoded><![CDATA[
arXiv:2410.14040v4 Announce Type: replace-cross 
Abstract: With the increasing availability of open-source robotic data, imitation learning has become a promising approach for both manipulation and locomotion. Diffusion models are now widely used to train large, generalized policies that predict controls or trajectories, leveraging their ability to model multimodal action distributions. However, this generality comes at the cost of larger model sizes and slower inference, an acute limitation for robotic tasks requiring high control frequencies. Moreover, Diffusion Policy (DP), a popular trajectory-generation approach, suffers from a trade-off between performance and action horizon: fewer diffusion queries lead to larger trajectory chunks, which in turn accumulate tracking errors. To overcome these challenges, we introduce WARPD (World model Assisted Reactive Policy Diffusion), a method that generates closed-loop policies (weights for neural policies) directly, instead of open-loop trajectories. By learning behavioral distributions in parameter space rather than trajectory space, WARPD offers two major advantages: (1) extended action horizons with robustness to perturbations, while maintaining high task performance, and (2) significantly reduced inference costs. Empirically, WARPD outperforms DP in long-horizon and perturbed environments, and achieves multitask performance on par with DP while requiring only ~ 1/45th of the inference-time FLOPs per step.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Backdoor Stealthiness in Model Parameter Space</title>
<link>https://arxiv.org/abs/2501.05928</link>
<guid>https://arxiv.org/abs/2501.05928</guid>
<content:encoded><![CDATA[
arXiv:2501.05928v3 Announce Type: replace-cross 
Abstract: Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?
  To answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot: Backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention</title>
<link>https://arxiv.org/abs/2501.06382</link>
<guid>https://arxiv.org/abs/2501.06382</guid>
<content:encoded><![CDATA[
arXiv:2501.06382v4 Announce Type: replace-cross 
Abstract: Human cognition is punctuated by abrupt, spontaneous shifts between topics-driven by emotional, contextual, or associative cues-a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention based models depend on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize spontaneous topic changes in self-attention architectures, revealing both their similarities and their divergences from spontaneous human thought. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining the topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic reduces the likelihood of spontaneous change. Second, we empirically validate that these dynamics persist in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behaviour in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus as closely aligned to human thought.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</title>
<link>https://arxiv.org/abs/2501.09761</link>
<guid>https://arxiv.org/abs/2501.09761</guid>
<content:encoded><![CDATA[
arXiv:2501.09761v3 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI)-native receivers prove significant performance improvement in high noise regimes and can potentially reduce communication overhead compared to the traditional receiver. However, their performance highly depends on the representativeness of the training dataset. A major issue is the uncertainty of whether the training dataset covers all test environments and waveform configurations, and thus, whether the trained model is robust in practical deployment conditions. To this end, we propose a joint measurement-recovery framework for AI-native transceivers post deployment, called VERITAS, that continuously looks for distribution shifts in the received signals and triggers finite re-training spurts. VERITAS monitors the wireless channel using 5G pilots fed to an auxiliary neural network that detects out-of-distribution channel profile, transmitter speed, and delay spread. As soon as such a change is detected, a traditional (reference) receiver is activated, which runs for a period of time in parallel to the AI-native receiver. Finally, VERTIAS compares the bit probabilities of the AI-native and the reference receivers for the same received data inputs, and decides whether or not a retraining process needs to be initiated. Our evaluations reveal that VERITAS can detect changes in the channel profile, transmitter speed, and delay spread with 99%, 97%, and 69% accuracies, respectively, followed by timely initiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel profile, transmitter speed, and delay spread test sets, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recent Advances in Discrete Speech Tokens: A Review</title>
<link>https://arxiv.org/abs/2502.06490</link>
<guid>https://arxiv.org/abs/2502.06490</guid>
<content:encoded><![CDATA[
arXiv:2502.06490v4 Announce Type: replace-cross 
Abstract: The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FuncGenFoil: Airfoil Generation and Editing Model in Function Space</title>
<link>https://arxiv.org/abs/2502.10712</link>
<guid>https://arxiv.org/abs/2502.10712</guid>
<content:encoded><![CDATA[
arXiv:2502.10712v4 Announce Type: replace-cross 
Abstract: Aircraft manufacturing is the jewel in the crown of industry, in which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. Existing deep learning methods, which typically rely on predefined parametric representations (e.g., B\'ezier) or discrete point sets, face an inherent trade-off between expressive power and resolution adaptability. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly reconstructs airfoil geometries as function curves. Our method inherits the advantages of arbitrary-resolution sampling and smoothness from parametric functions, as well as the strong expressiveness of discrete point-based representations. Empirical evaluations demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation, achieving a relative 74.4% reduction in label error and a 23.2% increase in diversity on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11028</link>
<guid>https://arxiv.org/abs/2502.11028</guid>
<content:encoded><![CDATA[
arXiv:2502.11028v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Statistical Learning: Supervised Learning of Statistical Estimators</title>
<link>https://arxiv.org/abs/2502.12088</link>
<guid>https://arxiv.org/abs/2502.12088</guid>
<content:encoded><![CDATA[
arXiv:2502.12088v3 Announce Type: replace-cross 
Abstract: Statistical inference, a central tool of science, revolves around the study and the usage of statistical estimators: functions that map finite samples to predictions about unknown distribution parameters. In the frequentist framework, estimators are evaluated based on properties such as bias, variance (for parameter estimation), accuracy, power, and calibration (for hypothesis testing). However, crafting estimators with desirable properties is often analytically challenging, and sometimes impossible, e.g., there exists no universally unbiased estimator for the standard deviation. In this work, we introduce meta-statistical learning, an amortized learning framework that recasts estimator design as an optimization problem via supervised learning. This takes a fully empirical approach to discovering statistical estimators; entire datasets are input to permutation-invariant neural networks, such as Set Transformers, trained to predict the target statistical property. The trained model is the estimator, and can be analyzed through the classical frequentist lens. We demonstrate the approach on two tasks: learning a normality test (classification) and estimating mutual information (regression), achieving strong results even with small models. Looking ahead, this paradigm opens a path to automate the discovery of generalizable and flexible statistical estimators.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Best-of-N Selection for Large Language Models via Self-Certainty</title>
<link>https://arxiv.org/abs/2502.18581</link>
<guid>https://arxiv.org/abs/2502.18581</guid>
<content:encoded><![CDATA[
arXiv:2502.18581v3 Announce Type: replace-cross 
Abstract: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion</title>
<link>https://arxiv.org/abs/2503.01220</link>
<guid>https://arxiv.org/abs/2503.01220</guid>
<content:encoded><![CDATA[
arXiv:2503.01220v3 Announce Type: replace-cross 
Abstract: Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Using emerging tissue profiling technologies, researchers charted comprehensive atlases of mammalian brain with sub-cellular resolution and spatially resolved transcriptomic data. However, these tera-scale volumetric atlases pose computational challenges for modeling intricate brain structures within the native spatial context. We propose \textbf{Tera-MIND}, a novel generative framework capable of simulating \textbf{Tera}-scale \textbf{M}ouse bra\textbf{IN}s in 3D using a patch-based and boundary-aware \textbf{D}iffusion model. Taking spatial gene expression as conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D \textit{gene}-\textit{gene} self-attention, we identify spatial molecular interactions for key transcriptomic pathways, including glutamatergic and dopaminergic neuronal systems. Lastly, we showcase the translational applicability of Tera-MIND on previously unseen human brain samples. Tera-MIND offers an efficient generative modeling of whole virtual organisms, paving the way for integrative applications in biomedical research. Project website: https://musikisomorphie.github.io/Tera-MIND.html
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Unified Approach for Elevating Benchmark Quality</title>
<link>https://arxiv.org/abs/2503.05860</link>
<guid>https://arxiv.org/abs/2503.05860</guid>
<content:encoded><![CDATA[
arXiv:2503.05860v3 Announce Type: replace-cross 
Abstract: Benchmarks are essential for unified evaluation and reproducibility. The rapid rise of Artificial Intelligence for Software Engineering (AI4SE) has produced numerous benchmarks for tasks such as code generation and bug repair. However, this proliferation has led to major challenges: (1) fragmented knowledge across tasks, (2) difficulty in selecting contextually relevant benchmarks, (3) lack of standardization in benchmark creation, and (4) flaws that limit utility. Addressing these requires a dual approach: systematically mapping existing benchmarks for informed selection and defining unified guidelines for robust, adaptable benchmark development.
  We conduct a review of 247 studies, identifying 273 AI4SE benchmarks since 2014. We categorize them, analyze limitations, and expose gaps in current practices. Building on these insights, we introduce BenchScout, an extensible semantic search tool for locating suitable benchmarks. BenchScout employs automated clustering with contextual embeddings of benchmark-related studies, followed by dimensionality reduction. In a user study with 22 participants, BenchScout achieved usability, effectiveness, and intuitiveness scores of 4.5, 4.0, and 4.1 out of 5.
  To improve benchmarking standards, we propose BenchFrame, a unified framework for enhancing benchmark quality. Applying BenchFrame to HumanEval yielded HumanEvalNext, featuring corrected errors, improved language conversion, higher test coverage, and greater difficulty. Evaluating 10 state-of-the-art code models on HumanEval, HumanEvalPlus, and HumanEvalNext revealed average pass-at-1 drops of 31.22% and 19.94%, respectively, underscoring the need for continuous benchmark refinement. We further examine BenchFrame's scalability through an agentic pipeline and confirm its generalizability on the MBPP dataset. All review data, user study materials, and enhanced benchmarks are publicly released.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</title>
<link>https://arxiv.org/abs/2503.07020</link>
<guid>https://arxiv.org/abs/2503.07020</guid>
<content:encoded><![CDATA[
arXiv:2503.07020v2 Announce Type: replace-cross 
Abstract: Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding</title>
<link>https://arxiv.org/abs/2503.09348</link>
<guid>https://arxiv.org/abs/2503.09348</guid>
<content:encoded><![CDATA[
arXiv:2503.09348v2 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have demonstrated significant potential as generalists in vision-language (VL) tasks. However, adoption of LMMs in real-world tasks is hindered by their poor performance in tasks that require a combination of VL capabilities, as well as in tasks that involve the grounding of complex text or visual instructions. To thoroughly investigate this gap and its underlying causes, we propose MOAT, a diverse benchmark with 1005 complex real-world vision questions that are straightforward for humans but challenging for LMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist problem solving by integrating VL capabilities such as reading text, counting, understanding spatial relations, grounding textual and visual instructions, etc. All these abilities fit into a taxonomy proposed by us that contains 9 VL capabilities, enabling MOAT to provide a fine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first benchmark to explicitly evaluate LMMs' ability to ground complex text and visual instructions, which is essential for many real-world applications. We evaluated 17 proprietary and open source LMMs, finding that the best performing LMM (Gemini 2.5 Pro) achieved only 44% accuracy, far below what would be acceptable in real-world applications. To guide future model development, we analyze common trends in our results and discuss the underlying causes of poor performance, focusing on the impact of text-centric reasoning, which VL capabilities form bottlenecks in complex tasks, and the potential harmful effects of tiling. Code and data are available at https://cambrian-yzt.github.io/MOAT/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2503.24379</link>
<guid>https://arxiv.org/abs/2503.24379</guid>
<content:encoded><![CDATA[
arXiv:2503.24379v2 Announce Type: replace-cross 
Abstract: To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2504.21228</link>
<guid>https://arxiv.org/abs/2504.21228</guid>
<content:encoded><![CDATA[
arXiv:2504.21228v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning</title>
<link>https://arxiv.org/abs/2505.16368</link>
<guid>https://arxiv.org/abs/2505.16368</guid>
<content:encoded><![CDATA[
arXiv:2505.16368v3 Announce Type: replace-cross 
Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter</title>
<link>https://arxiv.org/abs/2506.00731</link>
<guid>https://arxiv.org/abs/2506.00731</guid>
<content:encoded><![CDATA[
arXiv:2506.00731v2 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (iPINNER) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCA-Video: Motion-Aware Concept Alignment for Consistent Video Editing</title>
<link>https://arxiv.org/abs/2506.01004</link>
<guid>https://arxiv.org/abs/2506.01004</guid>
<content:encoded><![CDATA[
arXiv:2506.01004v2 Announce Type: replace-cross 
Abstract: We present MoCA-Video, a training-free framework for semantic mixing in videos. Operating in the latent space of a frozen video diffusion model, MoCA-Video utilizes class-agnostic segmentation with diagonal denoising scheduler to localize and track the target object across frames. To ensure temporal stability under semantic shifts, we introduce momentum-based correction to approximate novel hybrid distributions beyond trained data distribution, alongside a light gamma residual module that smooths out visual artifacts. We evaluate model's performance using SSIM, LPIPS, and a proposed metric, \metricnameabbr, which quantifies semantic alignment between reference and output. Extensive evaluation demonstrates that our model consistently outperforms both training-free and trained baselines, achieving superior semantic mixing and temporal coherence without retraining. Results establish that structured manipulation of diffusion noise trajectories enables controllable and high-quality video editing under semantic shifts.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[
arXiv:2506.01374v4 Announce Type: replace-cross 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2506.10622</link>
<guid>https://arxiv.org/abs/2506.10622</guid>
<content:encoded><![CDATA[
arXiv:2506.10622v2 Announce Type: replace-cross 
Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers</title>
<link>https://arxiv.org/abs/2506.14855</link>
<guid>https://arxiv.org/abs/2506.14855</guid>
<content:encoded><![CDATA[
arXiv:2506.14855v3 Announce Type: replace-cross 
Abstract: Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-World Object Counting in Videos</title>
<link>https://arxiv.org/abs/2506.15368</link>
<guid>https://arxiv.org/abs/2506.15368</guid>
<content:encoded><![CDATA[
arXiv:2506.15368v2 Announce Type: replace-cross 
Abstract: We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and objects of similar appearance, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model, to enable automated open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for this novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at https://www.robots.ox.ac.uk/~vgg/research/countvid/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
arXiv:2506.21546v3 Announce Type: replace-cross 
Abstract: Segmentation Vision-Language Models (VLMs) have significantly advanced grounded visual understanding, yet they remain prone to pixel-grounding hallucinations, producing masks for incorrect objects or for objects that are entirely absent. Existing evaluations rely almost entirely on text- or label-based perturbations, which check only whether the predicted mask matches the queried label. Such evaluations overlook the spatial footprint and severity of hallucination and therefore fail to reveal vision-driven hallucinations, which are more challenging and more prevalent. To address this gap, we formalize the task of Counterfactual Segmentation Reasoning (CSR), where a model must segment the referenced object in the factual image and abstain in its counterfactual counterpart. To support this task, we curate HalluSegBench, the first large-scale benchmark to diagnose referring and reasoning expression segmentation hallucinations using controlled visual counterfactuals, alongside new evaluation metrics that measure hallucination severity and disentangle vision- and language-driven failure modes. We further introduce RobustSeg, a segmentation VLM trained with counterfactual fine-tuning (CFT) to learn when to segment and when to abstain. Experimental results confirm RobustSeg reduces hallucinations by 30%, while improving segmentation performance on FP-RefCOCO(+/g).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</title>
<link>https://arxiv.org/abs/2507.05178</link>
<guid>https://arxiv.org/abs/2507.05178</guid>
<content:encoded><![CDATA[
arXiv:2507.05178v2 Announce Type: replace-cross 
Abstract: Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback</title>
<link>https://arxiv.org/abs/2507.13171</link>
<guid>https://arxiv.org/abs/2507.13171</guid>
<content:encoded><![CDATA[
arXiv:2507.13171v2 Announce Type: replace-cross 
Abstract: Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering</title>
<link>https://arxiv.org/abs/2508.11272</link>
<guid>https://arxiv.org/abs/2508.11272</guid>
<content:encoded><![CDATA[
arXiv:2508.11272v2 Announce Type: replace-cross 
Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited -- compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Model's understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing</title>
<link>https://arxiv.org/abs/2508.18316</link>
<guid>https://arxiv.org/abs/2508.18316</guid>
<content:encoded><![CDATA[
arXiv:2508.18316v3 Announce Type: replace-cross 
Abstract: This study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RECAP: REwriting Conversations for Intent Understanding in Agentic Planning</title>
<link>https://arxiv.org/abs/2509.04472</link>
<guid>https://arxiv.org/abs/2509.04472</guid>
<content:encoded><![CDATA[
arXiv:2509.04472v2 Announce Type: replace-cross 
Abstract: Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent detection a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that assesses planning utility given the rewritten intent. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines, in terms of plan preference. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agentic planning in open-domain dialogue systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v4 Announce Type: replace-cross 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Prompt Management in GitHub Repositories: A Call for Best Practices</title>
<link>https://arxiv.org/abs/2509.12421</link>
<guid>https://arxiv.org/abs/2509.12421</guid>
<content:encoded><![CDATA[
arXiv:2509.12421v2 Announce Type: replace-cross 
Abstract: The rapid adoption of foundation models (e.g., large language models) has given rise to promptware, i.e., software built using natural language prompts. Effective management of prompts, such as organization and quality assurance, is essential yet challenging. In this study, we perform an empirical analysis of 24,800 open-source prompts from 92 GitHub repositories to investigate prompt management practices and quality attributes. Our findings reveal critical challenges such as considerable inconsistencies in prompt formatting, substantial internal and external prompt duplication, and frequent readability and spelling issues. Based on these findings, we provide actionable recommendations for developers to enhance the usability and maintainability of open-source prompts within the rapidly evolving promptware ecosystem.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation</title>
<link>https://arxiv.org/abs/2509.19112</link>
<guid>https://arxiv.org/abs/2509.19112</guid>
<content:encoded><![CDATA[
arXiv:2509.19112v3 Announce Type: replace-cross 
Abstract: Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion</title>
<link>https://arxiv.org/abs/2509.19312</link>
<guid>https://arxiv.org/abs/2509.19312</guid>
<content:encoded><![CDATA[
arXiv:2509.19312v2 Announce Type: replace-cross 
Abstract: This paper investigates multimodal semantic non-orthogonal transmission and fusion in hybrid analog-digital massive multiple-input multiple-output (MIMO). A Transformer-based cross-modal source-channel semantic-aware network (CSC-SA-Net) framework is conceived, where channel state information (CSI) reference signal (RS), feedback, analog-beamforming/combining, and baseband semantic processing are data-driven end-to-end (E2E) optimized at the base station (BS) and user equipments (UEs). CSC-SA-Net comprises five sub-networks: BS-side CSI-RS network (BS-CSIRS-Net), UE-side channel semantic-aware network (UE-CSANet), BS-CSANet, UE-side multimodal semantic fusion network (UE-MSFNet), and BS-MSFNet. Specifically, we firstly E2E train BS-CSIRS-Net, UE-CSANet, and BS-CSANet to jointly design CSI-RS, feedback, analog-beamforming/combining with maximum {\emph{physical-layer's}} spectral-efficiency. Meanwhile, we E2E train UE-MSFNet and BS-MSFNet for optimizing {\emph{application-layer's}} source semantic downstream tasks. On these pre-trained models, we further integrate application-layer semantic processing with physical-layer tasks to E2E train five subnetworks. Extensive simulations show that the proposed CSC-SA-Net outperforms traditional separated designs, revealing the advantage of cross-modal channel-source semantic fusion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An effective control of large systems of active particles: An application to evacuation problem</title>
<link>https://arxiv.org/abs/2509.19972</link>
<guid>https://arxiv.org/abs/2509.19972</guid>
<content:encoded><![CDATA[
arXiv:2509.19972v3 Announce Type: replace-cross 
Abstract: Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent. One possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we introduce the generalized Vicsek model. This novel method is then applied to the problem of the effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced architectures, our approach provides a robust and efficient evacuation strategy. The source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</title>
<link>https://arxiv.org/abs/2510.04573</link>
<guid>https://arxiv.org/abs/2510.04573</guid>
<content:encoded><![CDATA[
arXiv:2510.04573v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.08442</link>
<guid>https://arxiv.org/abs/2510.08442</guid>
<content:encoded><![CDATA[
arXiv:2510.08442v2 Announce Type: replace-cross 
Abstract: Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.52x improvement in sample efficiency and can solve challenging tasks from the ManiSkill3 benchmark that the baseline fails to learn, without modifying the underlying algorithm or hyperparameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Emergence of Complex Behavior in Large-Scale Ecological Environments</title>
<link>https://arxiv.org/abs/2510.18221</link>
<guid>https://arxiv.org/abs/2510.18221</guid>
<content:encoded><![CDATA[
arXiv:2510.18221v3 Announce Type: replace-cross 
Abstract: We explore how physical scale and population size shape the emergence of complex behaviors in open-ended ecological environments. In our setting, agents are unsupervised and have no explicit rewards or learning objectives but instead evolve over time according to reproduction, mutation, and selection. As they act, agents also shape their environment and the population around them in an ongoing dynamic ecology. Our goal is not to optimize a single high-performance policy, but instead to examine how behaviors emerge and evolve across large populations due to natural competition and environmental pressures. We use modern hardware along with a new multi-agent simulator to scale the environment and population to sizes much larger than previously attempted, reaching populations of over 60,000 agents, each with their own evolved neural network policy. We identify various emergent behaviors such as long-range resource extraction, vision-based foraging, and predation that arise under competitive and survival pressures. We examine how sensing modalities and environmental scale affect the emergence of these behaviors and find that some of them appear only in sufficiently large environments and populations, and that larger scales increase the stability and consistency of these emergent behaviors. While there is a rich history of research in evolutionary settings, our scaling results on modern hardware provide promising new directions to explore ecology as an instrument of machine learning in an era of increasingly abundant computational resources and efficient machine frameworks. Experimental code is available at https://github.com/jbejjani2022/ecological-emergent-behavior.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction</title>
<link>https://arxiv.org/abs/2511.06634</link>
<guid>https://arxiv.org/abs/2511.06634</guid>
<content:encoded><![CDATA[
arXiv:2511.06634v3 Announce Type: replace-cross 
Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at https://github.com/SusCom-Lab/CaberNet-CRL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title>
<link>https://arxiv.org/abs/2511.06682</link>
<guid>https://arxiv.org/abs/2511.06682</guid>
<content:encoded><![CDATA[
arXiv:2511.06682v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10843</link>
<guid>https://arxiv.org/abs/2511.10843</guid>
<content:encoded><![CDATA[
arXiv:2511.10843v2 Announce Type: replace-cross 
Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference</title>
<link>https://arxiv.org/abs/2511.11907</link>
<guid>https://arxiv.org/abs/2511.11907</guid>
<content:encoded><![CDATA[
arXiv:2511.11907v2 Announce Type: replace-cross 
Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</title>
<link>https://arxiv.org/abs/2511.13387</link>
<guid>https://arxiv.org/abs/2511.13387</guid>
<content:encoded><![CDATA[
arXiv:2511.13387v3 Announce Type: replace-cross 
Abstract: Denoising diffusion models have emerged as a dominant paradigm in image generation. Discretizing image data into tokens is a critical step for effectively integrating images with Transformer and other architectures. Although the Denoising Diffusion Codebook Models (DDCM) pioneered the use of pre-trained diffusion models for image tokenization, it strictly relies on the traditional discrete-time DDPM architecture. Consequently, it fails to adapt to modern continuous-time variants-such as Flow Matching and Consistency Models-and suffers from inefficient sampling in high-noise regions. To address these limitations, this paper proposes the Generalized Denoising Diffusion Codebook Models (gDDCM). We establish a unified theoretical framework and introduce a generic "De-noise and Back-trace" sampling strategy. By integrating a deterministic ODE denoising step with a residual-aligned noise injection step, our method resolves the challenge of adaptation. Furthermore, we introduce a backtracking parameter $p$ and significantly enhance tokenization ability. Extensive experiments on CIFAR10 and LSUN Bedroom datasets demonstrate that gDDCM achieves comprehensive compatibility with mainstream diffusion variants and significantly outperforms DDCM in terms of reconstruction quality and perceptual fidelity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Finer the Better: Towards Granular-aware Open-set Domain Generalization</title>
<link>https://arxiv.org/abs/2511.16979</link>
<guid>https://arxiv.org/abs/2511.16979</guid>
<content:encoded><![CDATA[
arXiv:2511.16979v2 Announce Type: replace-cross 
Abstract: Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels</title>
<link>https://arxiv.org/abs/2511.18078</link>
<guid>https://arxiv.org/abs/2511.18078</guid>
<content:encoded><![CDATA[
arXiv:2511.18078v2 Announce Type: replace-cross 
Abstract: Accurate modeling of time-varying underwater acoustic channels is essential for the design, evaluation, and deployment of reliable underwater communication systems. Conventional physics models require detailed environmental knowledge, while stochastic replay methods are constrained by the limited diversity of measured channels and often fail to generalize to unseen scenarios, reducing their practical applicability. To address these challenges, we propose StableUASim, a pre-trained conditional latent diffusion surrogate model that captures the stochastic dynamics of underwater acoustic communication channels. Leveraging generative modeling, StableUASim produces diverse and statistically realistic channel realizations, while supporting conditional generation from specific measurement samples. Pre-training enables rapid adaptation to new environments using minimal additional data, and the autoencoder latent representation facilitates efficient channel analysis and compression. Experimental results demonstrate that StableUASim accurately reproduces key channel characteristics and communication performance, providing a scalable, data-efficient, and physically consistent surrogate model for both system design and machine learning-driven underwater applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title>
<link>https://arxiv.org/abs/2511.20629</link>
<guid>https://arxiv.org/abs/2511.20629</guid>
<content:encoded><![CDATA[
arXiv:2511.20629v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20663</link>
<guid>https://arxiv.org/abs/2511.20663</guid>
<content:encoded><![CDATA[
arXiv:2511.20663v3 Announce Type: replace-cross 
Abstract: Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.
  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.
  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Signed Graph Learning with Differential Privacy</title>
<link>https://arxiv.org/abs/2512.00307</link>
<guid>https://arxiv.org/abs/2512.00307</guid>
<content:encoded><![CDATA[
arXiv:2512.00307v2 Announce Type: replace-cross 
Abstract: Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement</title>
<link>https://arxiv.org/abs/2512.00396</link>
<guid>https://arxiv.org/abs/2512.00396</guid>
<content:encoded><![CDATA[
arXiv:2512.00396v2 Announce Type: replace-cross 
Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</title>
<link>https://arxiv.org/abs/2512.00862</link>
<guid>https://arxiv.org/abs/2512.00862</guid>
<content:encoded><![CDATA[
arXiv:2512.00862v3 Announce Type: replace-cross 
Abstract: We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02551</link>
<guid>https://arxiv.org/abs/2512.02551</guid>
<content:encoded><![CDATA[
arXiv:2512.02551v2 Announce Type: replace-cross 
Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art Nvidia's closed-source libraries, i.e., cuBLAS, cuBLASLt. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0% over torch.matmul on average; +19.2% over cuBLAS using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8% over cuBLASLt-heuristic, which queries cuBLASLt library and selects the algorithm based on the heuristic's suggestion; and +11.4% over the most competitive cuBLASLt-AutoTuning model, which selects the fastest algorithm from up to 100 candidates from cuBLASLt's suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7%, +26.0%, +22.4%, and +15.9% for torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defense That Attacks: How Robust Models Become Better Attackers</title>
<link>https://arxiv.org/abs/2512.02830</link>
<guid>https://arxiv.org/abs/2512.02830</guid>
<content:encoded><![CDATA[
arXiv:2512.02830v3 Announce Type: replace-cross 
Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning</title>
<link>https://arxiv.org/abs/2512.03343</link>
<guid>https://arxiv.org/abs/2512.03343</guid>
<content:encoded><![CDATA[
arXiv:2512.03343v2 Announce Type: replace-cross 
Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from Topic Drift where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning. While scaling model size mitigates this, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary Idea Head trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models</title>
<link>https://arxiv.org/abs/2511.18271</link>
<guid>https://arxiv.org/abs/2511.18271</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image models, implicit world knowledge, physical causal reasoning, PicWorld benchmark, multi-agent evaluator  

<br /><br />Summary:  
1. Current text-to-image (T2I) models produce photorealistic, instruction-following images but often fail on prompts requiring implicit world knowledge.  
2. Existing evaluation methods focus mainly on compositional alignment or single-round VQA-based scoring, neglecting critical aspects such as knowledge grounding, multi-physics interactions, and verifiable evidence.  
3. To overcome these gaps, the authors introduce PicWorld, a novel benchmark containing 1,100 prompts across three core categories to comprehensively assess T2I models' understanding of implicit world knowledge and physical causal reasoning.  
4. They propose PW-Agent, an evidence-grounded multi-agent evaluation framework that hierarchically assesses generated images by decomposing prompts into verifiable visual evidence, focusing on physical realism and logical consistency.  
5. Evaluation of 17 mainstream T2I models on PicWorld reveals a universal limitation in their ability to grasp implicit world knowledge and physical causal reasoning, underscoring the necessity for future T2I architectures to incorporate reasoning-aware and knowledge-integrative components. <div>
arXiv:2511.18271v3 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples</title>
<link>https://arxiv.org/abs/2512.09931</link>
<guid>https://arxiv.org/abs/2512.09931</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized learning, AI-generated examples, learner adaptation, Google Gemini, dynamic context<br /><br />Summary:  
This article presents ExaCraft, an AI-driven educational tool designed to generate personalized learning examples that adapt dynamically to the learner's context. Unlike existing AI educational tools, ExaCraft focuses on creating culturally relevant and relatable examples by leveraging user-defined profiles like location, education, profession, and complexity preferences. It integrates Google Gemini AI with a Python Flask API, accessible through a Chrome extension, to deliver tailored content. The system continuously analyzes learner behavior in real-time, considering five critical aspects: struggle indicators, mastery patterns, topic progression history, session boundaries, and overall learning progression signals. ExaCraft’s core innovation lies in its ability to evolve examples from foundational concepts to advanced technical material, thereby supporting learners through their knowledge growth. The system reacts effectively to repeated topics, regeneration requests, and shifting learning patterns across different educational scenarios. A demonstration is provided to showcase how examples adapt responsively, ensuring sustained engagement and relevance. This dynamic, context-aware approach aims to enhance learning effectiveness by meeting individual needs more precisely and motivating learners through personalized educational experiences. <div>
arXiv:2512.09931v1 Announce Type: new 
Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Suzume-chan: Your Personal Navigator as an Embodied Information Hub</title>
<link>https://arxiv.org/abs/2512.09932</link>
<guid>https://arxiv.org/abs/2512.09932</guid>
<content:encoded><![CDATA[
<div> Embodied Information Hub, Social Presence Theory, Suzume-chan, retrieval-augmented generation, human-centered knowledge sharing<br /><br />Summary:<br /><br />This article explores the challenge of accessing expert knowledge in a way that fosters deep understanding and connection, highlighting the limitations of current digital tools that often lack a sense of presence. It applies Social Presence Theory to emphasize how a feeling of "being together" can enhance communication quality. To address these issues, the authors propose an "Embodied Information Hub," a novel approach that integrates physical and conversational interactions for knowledge sharing. The proposed prototype, Suzume-chan, is a small, soft AI agent operating locally, equipped with a language model and retrieval-augmented generation (RAG) capabilities. Suzume-chan learns from spoken explanations and engages users through dialogue, thereby reducing psychological distance between the knowledge source and the learner. This design aims to create a warmer, more human-centered experience for sharing expert knowledge, bridging the gap between digital access to information and the rich interpersonal connection needed for effective communication. The study contributes a new framework and a working prototype that demonstrate how embodied AI agents can improve the quality of real-time knowledge sharing. <div>
arXiv:2512.09932v1 Announce Type: new 
Abstract: Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of "being together" enhances communication. An "Embodied Information Hub" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Health Misinformation Detection with Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2512.09935</link>
<guid>https://arxiv.org/abs/2512.09935</guid>
<content:encoded><![CDATA[
<div> Keywords: health misinformation, fact-checking, agreement score, multi-agent debate, large language models<br /><br />Summary:<br /><br />1. The paper addresses the crucial challenge of fact-checking health-related claims amid the growing problem of online misinformation.<br /><br />2. It introduces a novel two-stage framework designed to improve health misinformation detection by combining evidence retrieval and reasoning.<br /><br />3. In the first stage, large language models (LLMs) independently assess retrieved articles and generate an aggregated agreement score, quantifying the consensus across evidence.<br /><br />4. If the agreement score falls below a set threshold, indicating conflicting or insufficient consensus, the system triggers a second stage involving a structured multi-agent debate.<br /><br />5. During this debate, multiple agents collaboratively synthesize the conflicting evidence, developing carefully reasoned verdicts supported by explicit justifications.<br /><br />6. Experimental evaluations demonstrate that this two-stage approach outperforms baseline methods, validating the effectiveness of combining automated scoring with collaborative reasoning.<br /><br />7. The study highlights the potential for advanced LLMs and structured multi-agent interactions to enhance the accuracy and reliability of complex health-related claim verification tasks. <div>
arXiv:2512.09935v1 Announce Type: new 
Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting</title>
<link>https://arxiv.org/abs/2512.09944</link>
<guid>https://arxiv.org/abs/2512.09944</guid>
<content:encoded><![CDATA[
<div> Echocardiography, large language model, multi-view, multi-task agent, MIMIC-EchoQA<br /><br />Summary:<br /><br />1. Echocardiography is vital in cardiovascular care but interpreting full studies is complex and done manually across multiple views.  
2. Existing foundation models perform well on individual tasks like view classification, segmentation, or disease prediction but lack unified, clinically coherent assessments.  
3. The paper introduces Echo-CoPilot, a multi-view, multi-task agent that leverages a large language model to coordinate specialized echocardiography tools.  
4. Echo-CoPilot operates in a ReAct-style loop, decomposing clinician queries to execute view recognition, segmentation, measurement, disease prediction, and report synthesis, integrating results into guideline-aware, narrative clinical summaries.  
5. Evaluation on the public MIMIC-EchoQA benchmark shows Echo-CoPilot achieves 50.8% accuracy, surpassing both general and biomedical video vision-language models.  
6. Qualitative results demonstrate the agent’s ability to resolve challenging clinical cases by combining quantitative measurements and physiological context, such as borderline left ventricular hypertrophy and pericardial effusion severity.  
7. The authors plan to release the Echo-CoPilot code upon paper acceptance to facilitate clinical and research applications in echocardiography. <div>
arXiv:2512.09944v1 Announce Type: new 
Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzy Hierarchical Multiplex</title>
<link>https://arxiv.org/abs/2512.09976</link>
<guid>https://arxiv.org/abs/2512.09976</guid>
<content:encoded><![CDATA[
<div> Keywords: fuzzy optimization, FCM causality, logical implication, service process design, multiplex<br /><br />Summary:<br /><br />1. This paper introduces a new fuzzy optimization framework that extends FCM (Fuzzy Cognitive Map) causality to improve modeling and analysis.<br /><br />2. The proposed model leverages dynamic processes to map data into metrics and uses a multiplex structure to examine the logical implications and hierarchy of concepts.<br /><br />3. It is primarily a theoretical work presenting the underlying logic and mathematics of the framework in a clear and coherent way.<br /><br />4. The framework is designed specifically to optimize information transmission within service process design, aiming to enhance service optimization.<br /><br />5. Lastly, the paper provides a thorough analysis of the FHM (Fuzzy Hierarchical Model), following logical steps in a straightforward and elegant manner to validate the framework's approach.<br /><br />This work lays the foundation for future applied research by introducing a novel theoretical paradigm that integrates dynamics, multiplex logic, and fuzzy causality for service optimization contexts. <div>
arXiv:2512.09976v1 Announce Type: new 
Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring LLMs for Scientific Information Extraction Using The SciEx Framework</title>
<link>https://arxiv.org/abs/2512.10004</link>
<guid>https://arxiv.org/abs/2512.10004</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Scientific Information Extraction, Multi-modal Retrieval, Modular Framework, Data Aggregation  

<br /><br />Summary:  
This paper presents SciEx, a modular and composable framework designed to improve the automation of scientific information extraction using large language models (LLMs). First, it addresses common challenges in scientific literature such as handling long-context documents, multi-modal content (e.g., text, figures), and the difficulty of standardizing heterogeneous fine-grained information from multiple publications. Second, SciEx’s architecture decouples core components—PDF parsing, multi-modal retrieval, extraction, and aggregation—allowing on-demand data extraction and making the system highly extensible. Third, the framework supports flexibility by enabling easy integration of new models, prompting strategies, and reasoning mechanisms, which is valuable when the target data schema or extraction ontology changes rapidly. Fourth, SciEx is evaluated on datasets covering three scientific domains, measuring its accuracy and consistency in extracting fine-grained scientific information. Finally, the study provides practical insights into the strengths of LLM-based pipelines in this context, while also identifying current limitations, helping guide future improvements in scientific data processing workflows. <div>
arXiv:2512.10004v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2512.10034</link>
<guid>https://arxiv.org/abs/2512.10034</guid>
<content:encoded><![CDATA[
<div> MD simulations, protein-ligand complexes, automation, DynaMate, free energy calculations<br /><br />Summary:<br /><br />1. Force field-based molecular dynamics (MD) simulations are crucial for studying biomolecular systems such as proteins and protein-ligand complexes, important in drug discovery and protein engineering.<br /><br />2. Setting up MD simulations is technically complex, involving parameterization, input preparation, and software configurations, which limits widespread and efficient use.<br /><br />3. Agentic large language models (LLMs) have shown promise in automating multi-step scientific tasks but have not yet been applied to fully automate protein-ligand MD workflows.<br /><br />4. The study introduces DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for proteins and protein-ligand systems, including free energy binding affinity calculations with the MM/PB(GB)SA method.<br /><br />5. DynaMate integrates dynamic tool usage, web search, PaperQA, and self-correcting behaviors through three specialized modules that plan experiments, run simulations, and analyze results.<br /><br />6. Tested on twelve benchmark systems of varying complexity, DynaMate demonstrated reliability in executing MD simulations, correcting runtime errors through iterative reasoning, and generating meaningful analyses of protein-ligand interactions.<br /><br />7. This automated framework offers a pathway towards standardized, scalable, and time-efficient molecular modeling pipelines to accelerate future biomolecular research and drug design applications. <div>
arXiv:2512.10034v1 Announce Type: new 
Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</title>
<link>https://arxiv.org/abs/2512.10046</link>
<guid>https://arxiv.org/abs/2512.10046</guid>
<content:encoded><![CDATA[
<div> Simulation platform, urban environments, multimodal instruction, multi-robot collaboration, vision-language models  

<br /><br />Summary:  
This paper introduces SimWorld-Robotics (SWR), a cutting-edge simulation platform built on Unreal Engine 5 designed for embodied AI research in large-scale, photorealistic urban environments. Unlike previous indoor-focused studies, SWR procedurally generates limitless complex urban scenes populated with dynamic elements such as pedestrians and traffic systems, achieving superior realism, complexity, and scalability. It supports multi-robot control and communication, enabling diverse robotic interactions. The authors propose two new challenging benchmarks using SWR: (1) a multimodal instruction-following navigation task where robots interpret vision-language commands to safely navigate amid pedestrians and traffic, and (2) a multi-agent search task requiring two robots to communicate and coordinate to find and meet each other. These benchmarks assess critical robotic capabilities such as multimodal instruction grounding, 3D spatial reasoning in expansive environments, safe long-range navigation in populated urban settings, multi-robot cooperation, and grounded communication. Experimental evaluations reveal that current state-of-the-art models, including vision-language models, underperform on these tasks due to insufficient perception, reasoning, and planning abilities in complex urban scenarios. The work highlights the need for more robust models capable of handling real-world, dynamic urban challenges in robotics. <div>
arXiv:2512.10046v1 Announce Type: new 
Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning</title>
<link>https://arxiv.org/abs/2512.10054</link>
<guid>https://arxiv.org/abs/2512.10054</guid>
<content:encoded><![CDATA[
<div> Parallel Decoder Transformer, Speculative Note Conditioning, autoregressive decoding, coherence drift, structured parallel generation<br /><br />Summary:<br /><br />1. Autoregressive decoding in Large Language Models (LLMs) is sequential, causing latency that increases linearly with output length. 2. Existing "Decomposition-and-Fill" methods like Skeleton-of-Thought attempt parallel generation but face coherence drift due to lack of communication between parallel streams. 3. The authors propose the Parallel Decoder Transformer (PDT), an architecture that incorporates coordination mechanisms directly during inference without retraining the base model. 4. PDT uses Speculative Note Conditioning (SNC) adapters that enable parallel decoding streams to synchronize through a shared dynamic latent space, treating coordination as a speculative consensus problem. 5. Sibling streams broadcast semantic notes on a global bus gated by a learned verification head, allowing effective synchronization. 6. Experiments using a frozen 20B-parameter model over 50,000 training steps show PDT achieves 77.8% precision in coverage prediction and recovers approximate serial semantics. 7. PDT offers an efficient, scalable alternative to fine-tuning full LLMs, enabling structured parallel generation with self-correction and without modifying trunk weights. <div>
arXiv:2512.10054v1 Announce Type: new 
Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research</title>
<link>https://arxiv.org/abs/2512.10058</link>
<guid>https://arxiv.org/abs/2512.10058</guid>
<content:encoded><![CDATA[
<div> Keywords: AI alignment, AI safety, AI ethics, interdisciplinary collaboration, bibliometric analysis<br /><br />Summary:<br /><br />1. The article addresses the growing urgency of aligning AI systems to be harmless amidst rapid advancements in AI capabilities.  
2. It identifies a significant divide between two research communities focused on AI alignment: AI safety, which emphasizes scaled intelligence, existential risks, and deceptive behavior, and AI ethics, which concentrates on present harms, social bias, and production pipeline flaws.  
3. These communities have evolved largely independently with differing definitions of what alignment means, influenced by separate methodologies, institutional settings, and disciplinary histories.  
4. A large-scale quantitative study analyzing 6,442 papers from major ML and NLP conferences (2020-2025) reveals that over 80% of collaborations occur within either the safety or ethics communities, with cross-field collaborations highly concentrated among approximately 5% of papers acting as bridges.  
5. The study shows that removing a small group of these "broker" actors greatly increases the separation between fields, highlighting a fragile cross-disciplinary exchange largely dependent on few individuals.  
6. The article concludes that addressing the conceptual and institutional divide between AI safety and ethics is crucial. It advocates for integrating technical safety research with normative ethics via shared benchmarks, cross-institutional venues, and mixed-method approaches to build AI systems that are robust, just, and aligned. <div>
arXiv:2512.10058v1 Announce Type: new 
Abstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, "aligned" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.
  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear socio-demographic representations emerge in Large Language Models from indirect cues</title>
<link>https://arxiv.org/abs/2512.10065</link>
<guid>https://arxiv.org/abs/2512.10065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, sociodemographic attributes, implicit bias, activation space, fairness<br /><br />Summary:  
This study explores how large language models (LLMs) encode sociodemographic attributes of users based on indirect cues such as names and occupations. The researchers find that LLMs develop linear representations of demographics within the models' activation spaces, where stereotypical attributes align with interpretable geometric directions. By probing residual streams across layers in four transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) using explicit demographic information, they demonstrate that demographic predictions can also be made from implicit cues. For example, names activate gender and race representations that align with census data, while occupations correspond with workforce statistics. These linear demographic embeddings explain how LLMs implicitly infer personal attributes during conversations. Furthermore, these implicit representations influence downstream outputs, such as career recommendations, illustrating an active shaping of model behavior by these biases. Importantly, the study reveals that LLMs passing conventional bias benchmarks may still harbor and leverage implicit biases, raising concerns about fairness and ethical considerations when deploying such models at scale in real-world applications. <div>
arXiv:2512.10065v1 Announce Type: new 
Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</title>
<link>https://arxiv.org/abs/2512.10092</link>
<guid>https://arxiv.org/abs/2512.10092</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse autoencoders, dataset analysis, model biases, interpretable embeddings, text clustering<br /><br />Summary:<br /><br />This work introduces sparse autoencoders (SAEs) to create interpretable embeddings called SAE embeddings for large-scale text corpus analysis, aiming to address challenges in detecting undesirable model behaviors and biases efficiently. First, SAE embeddings offer a cost-effective and reliable alternative to LLM-based annotation and dense embedding clustering by providing dimensions that correspond to meaningful concepts. Second, using a large hypothesis space of SAEs, the method reveals semantic differences between datasets and uncovers unexpected correlations among document concepts. For example, comparing multiple frontier models shows that Grok-4 clarifies ambiguities more frequently than nine others. Third, SAE embeddings demonstrate greater controllability than dense embeddings, allowing targeted analyses such as clustering documents along user-defined axes by filtering concepts. Fourth, SAE embeddings outperform dense embeddings in retrieval tasks based on specific properties, enhancing interpretability in data indexing. Finally, the utility of SAE embeddings is showcased in two case studies: tracing the evolution of OpenAI models’ behavior over time and identifying "trigger" phrases learned by the Tulu-3 model from its training set. Overall, SAEs emerge as a versatile, interpretable tool for unstructured data analysis, emphasizing the importance of understanding models through their underlying data characteristics. <div>
arXiv:2512.10092v1 Announce Type: new 
Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust AI Security and Alignment: A Sisyphean Endeavor?</title>
<link>https://arxiv.org/abs/2512.10100</link>
<guid>https://arxiv.org/abs/2512.10100</guid>
<content:encoded><![CDATA[
<div> Keywords: AI security, alignment, G\"odel's incompleteness theorem, information-theoretic limitations, cognitive reasoning<br /><br />Summary:<br /><br />This manuscript explores fundamental limitations in the robustness of AI security and alignment by applying G\"odel's incompleteness theorem in the context of artificial intelligence. First, it establishes that there are intrinsic, information-theoretic barriers that prevent perfect security and alignment guarantees in AI systems. Second, by extending a classical mathematical theorem to AI, the work provides a formal foundation for understanding why certain AI risks and vulnerabilities cannot be fully eliminated. Third, it emphasizes the critical importance of recognizing these theoretical limitations when advancing and adopting AI technologies responsibly. Fourth, the paper discusses practical strategies and methodologies that can partially mitigate the challenges posed by these limitations, suggesting ways to enhance AI robustness despite fundamental constraints. Finally, it extends these insights to broader implications on cognitive reasoning capacities of AI systems, demonstrating inherent restrictions in AI’s ability to fully reason and adapt. This research thus contributes to a deeper theoretical understanding of AI vulnerabilities and informs the design of safer and more reliable AI architectures. <div>
arXiv:2512.10100v1 Announce Type: new 
Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending G\"odel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Narrative Archetypes in Conspiratorial Narratives: Insights from Singapore-Based Telegram Groups</title>
<link>https://arxiv.org/abs/2512.10105</link>
<guid>https://arxiv.org/abs/2512.10105</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiratorial discourse, Telegram groups, Signed Belief Graph Neural Network, narrative archetypes, belief-driven analysis  

<br /><br />Summary:  
1. This study investigates conspiratorial narratives within Singapore-based Telegram groups, revealing that such content permeates everyday conversations rather than existing solely in isolated echo chambers.  
2. A two-stage computational framework is proposed: first, RoBERTa-large is fine-tuned to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on a dataset of 2,000 expert-labeled messages.  
3. Second, the authors construct a signed belief graph where nodes represent messages and edges denote belief alignment weighted by textual similarity.  
4. They introduce the Signed Belief Graph Neural Network (SiBeGNN), which employs a Sign Disentanglement Loss to distinguish ideological alignment from stylistic features in embeddings.  
5. Hierarchical clustering on the generated embeddings identifies seven distinct narrative archetypes across 553,648 messages, including legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat.  
6. SiBeGNN outperforms baseline methods in clustering quality, supported by high inter-rater agreement (88%) in expert evaluations.  
7. Analysis indicates conspiratorial messages appear not only in overtly skeptical clusters but also within routine discussions on finance, law, and everyday topics, challenging existing notions of online radicalization.  
8. The framework enhances computational approaches for belief-driven discourse analysis and holds potential applications in stance detection, political communication studies, and content moderation policy. <div>
arXiv:2512.10105v1 Announce Type: new 
Abstract: Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice</title>
<link>https://arxiv.org/abs/2512.10114</link>
<guid>https://arxiv.org/abs/2512.10114</guid>
<content:encoded><![CDATA[
<div> agriculture, Large Language Models, Retrieval-Augmented Generation, geospatial metadata, hallucination  

<br /><br />Summary:  
This paper introduces AgriRegion, a Retrieval-Augmented Generation (RAG) framework tailored for delivering high-fidelity, region-specific agricultural advice. The motivation stems from the challenge that general-purpose Large Language Models (LLMs) often produce contextual hallucinations when used in agriculture, providing advice that may be non-factual or suitable in one region but harmful in another due to differences in soil, climate, and regulations. AgriRegion addresses this by incorporating a geospatial metadata injection layer and a region-prioritized re-ranking mechanism to ensure retrieval focuses on locally relevant, verified agricultural data. The knowledge base is restricted exclusively to trusted local agricultural extension services, which enforces geo-spatial constraints during information retrieval. This targeted approach ensures accuracy in critical agricultural decisions such as planting schedules, pest management, and fertilization strategies. The authors also develop a novel benchmark dataset called AgriRegion-Eval, containing 160 domain-specific questions covering 12 agricultural subfields, for comprehensive assessment. Experimental results demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLM systems and significantly enhances user trust scores, validating its effectiveness for region-aware agricultural advisory tasks. <div>
arXiv:2512.10114v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 2025 Foundation Model Transparency Index</title>
<link>https://arxiv.org/abs/2512.10169</link>
<guid>https://arxiv.org/abs/2512.10169</guid>
<content:encoded><![CDATA[
<div> Foundation Model Transparency, 2025 FMTI, training data opacity, policy impact, company rankings  

<br /><br />Summary:  
The 2025 Foundation Model Transparency Index (FMTI) assesses the transparency practices of foundation model developers, marking its third annual edition. This year, it introduces new indicators focusing on data acquisition, usage data, and monitoring, while evaluating companies such as Alibaba, DeepSeek, and xAI for the first time. Contrary to the improvement observed in the 2024 FMTI, transparency scores have declined significantly, with the average dropping from 58 to 40 out of 100. Major opacity exists around training data, training compute, and the post-deployment usage and impacts of flagship models. IBM emerges as an exceptional positive outlier, scoring 95, whereas xAI and Midjourney rank lowest with scores of 14. The five Frontier Model Forum members score mid-range, suggesting they manage reputational risks without striving to lead in transparency. The report highlights that as global policymakers increasingly require transparency, understanding the current state and potential shifts due to policy is crucial. It also emphasizes the need for more robust policy actions to tackle critical transparency gaps in the foundation model development ecosystem. <div>
arXiv:2512.10169v1 Announce Type: new 
Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment</title>
<link>https://arxiv.org/abs/2512.10206</link>
<guid>https://arxiv.org/abs/2512.10206</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical pathways, large language models, hospital simulation, evaluation framework, medical AI agents  

<br /><br />Summary:  
This paper addresses the limitations of current benchmarks which largely assess large language models (LLMs) on static exams or isolated dialogues, failing to capture the complexity of real-world clinical care. It introduces CP-Env, a controllable hospital environment that simulates a comprehensive clinical ecosystem with interactive patient and physician agents. CP-Env supports scenarios such as triage, specialist consultations, diagnostic testing, and multidisciplinary team meetings, enabling long-horizon, branching clinical pathway simulations mimicking real hospital workflows. The authors propose a three-tiered evaluation framework for LLMs comprising Clinical Efficacy, Process Competency, and Professional Ethics. Experimental results show that most LLMs struggle with the complexity of clinical pathways, often producing hallucinations and missing critical diagnostic details. The study finds that increasing the number of reasoning steps does not always improve performance and can be detrimental. Conversely, top-performing models tend to rely less on external tools by leveraging internalized medical knowledge. CP-Env provides a valuable benchmark and evaluation tools to foster improved development of medical AI agents capable of dynamic, end-to-end clinical decision-making. The resources for CP-Env are openly available to encourage further research in this domain. <div>
arXiv:2512.10206v1 Announce Type: new 
Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An exploration for higher efficiency in multi objective optimisation with reinforcement learning</title>
<link>https://arxiv.org/abs/2512.10208</link>
<guid>https://arxiv.org/abs/2512.10208</guid>
<content:encoded><![CDATA[
<div> Keywords: optimisation, multi-objective reinforcement learning, operators, search efficiency, multi-objective optimisation<br /><br />Summary:  
This paper addresses the ongoing challenge of improving efficiency in optimisation and search processes, which significantly impacts the performance of optimisation algorithms. It highlights the promising strategy of using a pool of operators instead of relying on a single operator for move operations within a neighbourhood. However, identifying an optimum or near-optimum sequence of these operators requires further research. The study notes that while many approaches exist for single-objective optimisation, multi-objective cases have been less explored in this context. To fill this gap, the paper proposes a generalised approach based on multi-objective reinforcement learning, aiming to leverage and generalise past experiences to inform operator selection and sequencing. The approach is intended to demonstrate enhanced efficiency and quality of solutions in multi-objective optimisation problems. The paper provides an overview of the proposed methodology, with some stages completed and others pending, underscoring the potential of multi-objective reinforcement learning to improve optimisation processes that involve multiple conflicting objectives. <div>
arXiv:2512.10208v1 Announce Type: new 
Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs</title>
<link>https://arxiv.org/abs/2512.10211</link>
<guid>https://arxiv.org/abs/2512.10211</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed-Integer Linear Programs, Predict-and-Search, machine learning, parametric MIPs, ID-PaS

<br /><br />Summary:  
This paper focuses on enhancing Mixed-Integer Linear Programs (MIPs), which are widely used for modeling combinatorial optimization problems. It builds upon the Predict-and-Search (PaS) method that leverages machine learning (ML) models to predict promising variable assignments and guide search algorithms toward optimal solutions. Despite its recent successes, existing PaS approaches are limited to binary variables and do not consider fixed variables that frequently occur in real-world scenarios. To address these challenges, the authors propose an extension of the PaS framework to handle parametric MIPs, which include heterogeneous variable types. They introduce ID-PaS, an identity-aware learning framework designed to improve the ML model’s ability to manage variable heterogeneity effectively. The paper reports extensive experiments on several large-scale, real-world optimization problems, demonstrating that ID-PaS consistently outperforms both the leading commercial solver Gurobi and the traditional PaS method. This advancement highlights the potential of combining identity-aware ML techniques with parametric MIP frameworks to improve solution quality and efficiency in practical applications. <div>
arXiv:2512.10211v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverse Thinking Enhances Missing Information Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2512.10273</link>
<guid>https://arxiv.org/abs/2512.10273</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reverse thinking, missing information detection, backward reasoning, reasoning robustness<br /><br />Summary:<br /><br />This paper addresses the challenge that Large Language Models (LLMs) face in handling tasks with missing information, where they tend to produce incomplete or inaccurate responses. Traditional forward reasoning techniques such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have improved structured problem-solving, but often fail to systematically detect and recover omitted information. To overcome this limitation, the authors propose a novel reverse thinking framework that guides LLMs through backward reasoning processes. This method focuses on identifying necessary conditions and pinpointing missing elements by thinking in reverse, transforming the missing information detection problem into a more tractable backward reasoning task. Experimental results show that the reverse thinking approach significantly outperforms traditional forward reasoning strategies in terms of accuracy. The approach not only enhances the logical completeness of LLM-generated outputs but also improves reasoning robustness when dealing with incomplete data. These findings suggest that integrating reverse thinking methodologies presents a promising direction for advancing the reasoning capabilities of LLMs, particularly in scenarios involving incomplete or omitted information, thereby addressing a key limitation of existing models. <div>
arXiv:2512.10273v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuronal Attention Circuit (NAC) for Representation Learning</title>
<link>https://arxiv.org/abs/2512.10282</link>
<guid>https://arxiv.org/abs/2512.10282</guid>
<content:encoded><![CDATA[
<div> Keywords: Neuronal Attention Circuit, continuous-time attention, biological plausibility, sparse gating, time-series classification  

<br /><br />Summary:  
This paper introduces the Neuronal Attention Circuit (NAC), a novel continuous-time (CT) attention mechanism inspired by the neuronal wiring of *C. elegans*. NAC reformulates the computation of attention logits as the solution of a linear first-order ODE with nonlinear, interconnected gates, providing a biologically plausible alternative to conventional discrete attention models. The architecture replaces dense projections with sparse sensory gates for key-query interactions and employs a sparse backbone network with dual heads to compute content-target and learnable time-constant gates, facilitating adaptive attention dynamics. NAC supports three distinct modes for attention logit computation: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation, promoting flexibility and computational efficiency. To enhance memory efficiency, the authors implement a sparse Top-K pairwise concatenation strategy that selectively filters key-query pairs. Theoretical analysis guarantees system stability, bounded approximation errors, and universal approximation capabilities. Empirical evaluations across various domains—irregular time-series classification, autonomous vehicle lane-keeping, and industrial prognostics—demonstrate that NAC matches or surpasses baseline performance in accuracy while balancing runtime and memory use more efficiently than many continuous-time attention methods. This work paves the way for biologically inspired, memory-efficient continuous-time attention mechanisms suitable for real-world, irregular data streams. <div>
arXiv:2512.10282v1 Announce Type: new 
Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules</title>
<link>https://arxiv.org/abs/2512.10300</link>
<guid>https://arxiv.org/abs/2512.10300</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, interpretability, attention heads, multimodal reasoning, CogVision<br /><br />Summary:<br />1. The paper addresses the interpretability of vision-language models (VLMs), which have shown strong performance on multimodal benchmarks but lack transparent internal mechanisms.<br />2. It introduces a novel framework to analyze the functional roles of attention heads in VLMs during multimodal reasoning tasks.<br />3. To facilitate this analysis, the authors present CogVision, a new dataset that breaks down complex multimodal questions into sequential subquestions reflecting human-like chain-of-thought reasoning, with each subquestion linked to specific receptive or cognitive functions.<br />4. Using a probing-based method, they identify specialized attention heads responsible for high-level visual reception, inference, and other cognitive functions, referring to these as functional heads.<br />5. Their study across multiple VLM architectures finds that functional heads are sparsely distributed, vary by function and model, and play roles in mediating hierarchical interactions.<br />6. Intervention experiments reveal the critical impact of these heads: removing functional heads degrades performance, while emphasizing them improves accuracy.<br />7. The findings offer new insights into the cognitive organization of VLMs and point toward designing models with improved, human-aligned perceptual and reasoning capabilities. <div>
arXiv:2512.10300v1 Announce Type: new 
Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance</title>
<link>https://arxiv.org/abs/2512.10304</link>
<guid>https://arxiv.org/abs/2512.10304</guid>
<content:encoded><![CDATA[
<div> Keywords: Trustworthy AI, Governance, Control-Panel architecture, Accountability, AI Assurance Framework<br /><br />Summary:  
1. The paper addresses the increasing challenge of bridging the gap between AI technical capabilities and institutional accountability as AI assumes more consequential decision-making roles.  
2. It argues that ethical guidelines are not enough and emphasizes the need for governance architectures embedded directly into AI system operations.  
3. The authors propose the "Ten Criteria for Trustworthy Orchestration AI," a comprehensive assurance framework that integrates human input, semantic coherence, audit trails, and provenance integrity within a unified Control-Panel architecture.  
4. Unlike traditional AI approaches focused mainly on AI-to-AI coordination, this framework offers governance coverage over the entire AI ecosystem including AI components, users, and human stakeholders.  
5. Drawing inspiration from international standards and Australia’s National Framework for AI Assurance, the framework demonstrates that trustworthiness can be engineered systematically to ensure AI systems are verifiable, transparent, reproducible, and meaningfully controlled by humans. <div>
arXiv:2512.10304v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck</title>
<link>https://arxiv.org/abs/2512.10305</link>
<guid>https://arxiv.org/abs/2512.10305</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative perception, Information Bottleneck, communication efficiency, autonomous driving, information purification  

<br /><br />Summary:  
This paper addresses the critical challenge of achieving precise environmental perception in autonomous driving systems while overcoming communication constraints inherent in collaborative perception frameworks. The authors identify a fundamental trade-off between communication bandwidth and perception performance. Current methods typically rely on transmitting megabyte-scale data, which is impractical under real-world network limitations. To resolve this, the authors propose InfoCom, a novel framework grounded in extended Information Bottleneck principles, which is the first to theoretically model communication-efficient collaborative perception. InfoCom introduces an information purification paradigm that optimizes extraction of the minimal yet sufficient task-critical information. Its key components include: (i) Information-Aware Encoding that compresses features into minimal messages retaining essential perception information; (ii) Sparse Mask Generation that pinpoints important spatial cues at negligible communication cost; and (iii) Multi-Scale Decoding that reconstructs perceptual information through mask-guided processes rather than standard feature reconstruction. Extensive experiments across multiple datasets validate that InfoCom achieves near-lossless perception performance while dramatically reducing communication overhead from megabytes to kilobytes—demonstrating 440-fold and 90-fold reductions compared to state-of-the-art methods Where2comm and ERMVP, respectively. This approach unlocks practical, highly efficient collaborative perception for autonomous driving under stringent network constraints. <div>
arXiv:2512.10305v1 Announce Type: new 
Abstract: Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EpiPlanAgent: Agentic Automated Epidemic Response Planning</title>
<link>https://arxiv.org/abs/2512.10313</link>
<guid>https://arxiv.org/abs/2512.10313</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic response, agent-based system, large language models, public health planning, simulation  

<br /><br />Summary:  
This study presents EpiPlanAgent, an innovative agent-based system designed to automate epidemic response planning using large language models (LLMs). The system leverages a multi-agent framework incorporating task decomposition, knowledge grounding, and simulation modules to generate and validate digital emergency response plans efficiently. Public health professionals evaluated EpiPlanAgent through real-world outbreak scenarios in controlled settings, demonstrating its practical applicability. Results showed that EpiPlanAgent significantly enhanced the completeness and alignment of response plans with established guidelines, outperforming traditional manual methods. Additionally, the system greatly reduced the time required to develop such plans, highlighting its operational efficiency. Expert assessments further confirmed a high consistency between AI-generated plans and those created by human experts, supporting the system’s reliability. Feedback from users indicated strong perceived utility and acceptance of the technology within public health workflows. In conclusion, EpiPlanAgent offers a scalable, effective solution for intelligent epidemic response planning, showcasing the transformative potential of agentic AI in enhancing public health preparedness and response capabilities. <div>
arXiv:2512.10313v1 Announce Type: new 
Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2512.10322</link>
<guid>https://arxiv.org/abs/2512.10322</guid>
<content:encoded><![CDATA[
<div> Vision-and-Language Navigation, General Scene Adaptation, User Feedback, Continual Learning, Memory-bank Warm-start  

<br /><br />Summary:  
This paper addresses Vision-and-Language Navigation (VLN), where agents navigate environments based on natural language instructions. It focuses on General Scene Adaptation for VLN (GSA-VLN), which aims for continual, environment-specific adaptation rather than zero-shot generalization, bridging the gap between static benchmarks and real-world deployment. The authors identify that current GSA-VLN methods do not utilize user feedback and instead rely on unsupervised adaptation from repeated environmental exposure. To improve adaptation quality, they propose a user-feedback-driven adaptation framework that systematically integrates human interactions—such as navigation instructions and corrective signals—into the continual learning process. This approach converts user feedback into high-quality, environment-aligned training data for efficient adaptation. Additionally, a memory-bank warm-start mechanism is introduced to reuse previously acquired environmental knowledge, reducing cold-start performance degradation and ensuring stable system redeployment. Experimental results on the GSA-R2R benchmark demonstrate that their method consistently outperforms strong baselines like GR-DUET, leading to improved navigation success rates and path efficiency. The memory-bank warm start stabilizes early navigation performance and mitigates performance drops after updates. The framework's robustness and generality are validated under both continual and hybrid adaptation settings, showing sustained improvements across diverse deployment scenarios. <div>
arXiv:2512.10322v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering</title>
<link>https://arxiv.org/abs/2512.10339</link>
<guid>https://arxiv.org/abs/2512.10339</guid>
<content:encoded><![CDATA[
<div> Inference-time steering, diffusion models, ratio-of-densities, marginal path collapse, adaptive path correction  

<br /><br />Summary:  
This paper addresses a critical failure mode in inference-time steering of pretrained diffusion and flow models, known as Marginal Path Collapse, where intermediate density trajectories become non-normalizable despite valid endpoints. This issue is particularly prevalent when composing heterogeneous models trained on varying noise schedules or datasets, a scenario common in molecular design tasks like flexible-pose scaffold decoration. The authors first derive a simple path existence criterion that exactly predicts when collapse will occur, based solely on noise schedules and exponents. They then introduce Adaptive path Correction with Exponents (ACE), an extension of Feynman-Kac steering which allows time-varying exponents, guaranteeing a valid probability path throughout the entire trajectory. Experiments conducted on a synthetic 2D benchmark and practical molecular design tasks demonstrate that ACE completely eliminates collapse, enabling stable and high-guidance compositional generation. ACE outperforms constant-exponent baselines and even specialized scaffold decoration models in terms of distributional and docking metrics. Overall, this work transforms ratio-of-densities steering from an unstable heuristic into a reliable method for controllable generative modeling with heterogeneous expert models. <div>
arXiv:2512.10339v1 Announce Type: new 
Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature</title>
<link>https://arxiv.org/abs/2512.10348</link>
<guid>https://arxiv.org/abs/2512.10348</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Unlearning, Vertical Federated Learning, Representation Misdirection, Data Privacy, Split Learning<br /><br />Summary:<br /><br />This paper addresses the challenge of federated unlearning under data-protection regulations like GDPR, specifically targeting Vertical Federated Learning (VFL) systems, where data are partitioned by features rather than samples. Unlike Horizontal Federated Learning (HFL), existing unlearning methods are ineffective in VFL due to its feature-partitioned architecture. To overcome this, the authors propose REMISVFU, a novel plug-and-play representation misdirection framework designed for fast, client-level unlearning in splitVFL setups. The core idea involves the party requested to be forgotten collapsing its encoder output onto a randomly sampled anchor on the unit sphere, which breaks the statistical relationship between its local features and the global model. To retain the utility of the model for the non-forgotten parties, the server performs a joint optimization involving two losses: a retention loss to preserve model performance and a forgetting loss to remove unwanted contributions. These gradients are aligned via orthogonal projection to avoid destructive interference during training. Extensive experiments on public benchmark datasets demonstrate that REMISVFU effectively reduces back-door attack success to natural class-prior levels while only marginally decreasing clean accuracy by approximately 2.5 percentage points, outperforming current state-of-the-art methods in vertical federated unlearning. <div>
arXiv:2512.10348v1 Announce Type: new 
Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Empowered Representation Learning for Emerging Item Recommendation</title>
<link>https://arxiv.org/abs/2512.10370</link>
<guid>https://arxiv.org/abs/2512.10370</guid>
<content:encoded><![CDATA[
<div> Emerging items, recommendation, representation learning, LLM, meta-learning<br /><br />Summary:<br /><br />This paper addresses the challenge of recommending emerging items, which accumulate interactions gradually over time. Unlike prior approaches that assume emerging items have minimal or no historical interactions, the authors argue this oversimplifies the problem by ignoring the need to maintain item uniqueness while utilizing shared patterns with established items. To solve this, they propose EmerFlow, a framework empowered by large language models (LLMs) to generate distinctive embeddings for emerging items. EmerFlow first enriches raw features of emerging items through LLM-based reasoning, effectively capturing deeper semantic information. Next, it aligns these enriched embeddings with the existing recommendation model’s embedding space to maintain consistency. Finally, it incorporates new interactions using meta-learning techniques, enabling refinement of item embeddings with only limited interaction data. The authors validate EmerFlow through extensive experiments in multiple domains such as movies and pharmaceuticals. Results demonstrate that EmerFlow consistently outperforms existing baseline methods, showcasing its ability to learn expressive and effective representations for emerging items despite sparse interaction history. This work provides a promising direction for dynamic recommendation scenarios involving continuously evolving item catalogs. <div>
arXiv:2512.10370v1 Announce Type: new 
Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management</title>
<link>https://arxiv.org/abs/2512.10371</link>
<guid>https://arxiv.org/abs/2512.10371</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile GUI agents, long-horizon task automation, context management, program-guided approach, global belief state

<br /><br />Summary:  
This paper addresses the challenge of managing interaction history in mobile GUI agents designed for long-horizon task automation, where traditional methods suffer from substantial context overhead and loss of semantic information. The authors propose AgentProg, a novel program-guided context management framework that represents interaction history as a program composed of variables and control flow structures. This organization enables principled decisions about which information to retain or discard, improving context efficiency. Additionally, AgentProg incorporates a global belief state mechanism inspired by the Belief MDP framework to manage partial observability and adapt to unexpected environmental changes. Experimental results on the AndroidWorld benchmark and an extended long-horizon task suite demonstrate that AgentProg achieves state-of-the-art success rates. Importantly, AgentProg maintains robust performance on long-horizon tasks, whereas baseline models suffer from catastrophic performance degradation. The system and codebase have been open-sourced, facilitating further research and application in the domain of mobile GUI agent development. <div>
arXiv:2512.10371v1 Announce Type: new 
Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention</title>
<link>https://arxiv.org/abs/2512.10414</link>
<guid>https://arxiv.org/abs/2512.10414</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, vision-language models, entropy intervention, adversarial sampling, policy exploration<br /><br />Summary:<br /><br />1. The paper addresses improving reasoning capabilities in vision-language models (VLMs) by using reinforcement learning (RL).<br />2. It highlights the limitation of existing RL-based finetuning methods that only control entropy during policy optimization and ignore entropy intervention during RL sampling.<br />3. The authors propose a novel method named Selective-adversarial Entropy Intervention (SaEI), which enhances policy entropy by adversarially distorting the visual input based on the entropy of sampled responses.<br />4. SaEI introduces entropy-guided adversarial sampling (EgAS), which uses entropy as an adversarial objective to generate adversarial gradients that perturb the visual input during RL sampling, thereby expanding the answer space.<br />5. Additionally, token-selective entropy computation (TsEC) is designed to focus the adversarial attack on specific tokens to avoid corrupting factual knowledge within the VLM.<br />6. Extensive experiments on both in-domain and out-of-domain datasets demonstrate that SaEI effectively improves policy exploration and boosts reasoning performance.<br />7. Code for the proposed method will be released upon paper acceptance. <div>
arXiv:2512.10414v1 Announce Type: new 
Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation of the structure of graphs by sequences of instructions</title>
<link>https://arxiv.org/abs/2512.10429</link>
<guid>https://arxiv.org/abs/2512.10429</guid>
<content:encoded><![CDATA[
<div> Keywords: graph representation, adjacency matrix, deep learning, instruction string, reversible transformation<br /><br />Summary:<br /><br />1. Traditional graph representations primarily utilize adjacency matrices, which underpin many algebraic and computational graph processing methods.<br /><br />2. Deep learning language models excel in text processing but struggle with conventional graph representations due to their format.<br /><br />3. The paper proposes a novel graph representation technique that encodes the adjacency matrix as a string of simple, sequential instructions that reconstruct the matrix step by step.<br /><br />4. This new representation is reversible, enabling the transformation from graph to string and back without loss of information.<br /><br />5. The method produces a compact representation that preserves local structural patterns of the graph, making it suitable for processing by deep learning models.<br /><br />6. A preliminary computational experiment demonstrates promising results, suggesting the approach could enhance graph processing performance using deep learning frameworks. <div>
arXiv:2512.10429v1 Announce Type: new 
Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Data Protection for Diffusion Model by Matching Training Trajectory</title>
<link>https://arxiv.org/abs/2512.10433</link>
<guid>https://arxiv.org/abs/2512.10433</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, Targeted Data Protection (TDP), trajectory alignment, fine-tuning, adversarial perturbations  

<br /><br />Summary:  
Recent progress in diffusion models has facilitated easier fine-tuning for personalized text-to-image generation but raised challenges related to unauthorized data use and privacy violations. Current defenses only degrade image quality passively and lack stable control mechanisms. Targeted Data Protection (TDP) aims to actively redirect model outputs towards user-defined concepts but existing approaches rely on snapshot-matching, which poorly controls the entire learning process. The proposed method, TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), is the first to achieve effective TDP by controlling the full fine-tuning trajectory instead of snapshots. Inspired by dataset distillation, TAFAP enforces persistent and verifiable transformations throughout training, preventing the protective effect from being diluted as training continues. Extensive experiments prove that TAFAP enables the first successful targeted transformations in diffusion models, allowing simultaneous control over both identity and visual patterns in generated images. Compared with prior methods, TAFAP significantly improves robustness of redirection toward target concepts while maintaining high image quality. This approach introduces reliable safeguards for privacy, establishing a new framework to control and track modifications in diffusion model outputs. <div>
arXiv:2512.10433v1 Announce Type: new 
Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title>
<link>https://arxiv.org/abs/2512.10449</link>
<guid>https://arxiv.org/abs/2512.10449</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, peer review, adversarial attacks, decision flipping, WAVS  

<br /><br />Summary:  
1. The study explores the integration of Large Language Models (LLMs) into scientific peer review, driven by individual reviewer adoption and institutional AI systems like those at AAAI and Stanford's Agents4Science.  
2. The research focuses on the vulnerability of "LLM-as-a-Judge" systems to adversarial PDF manipulations aimed at flipping reject decisions to accept, a novel threat distinct from typical jailbreaks.  
3. To assess this vulnerability, the authors introduce a new evaluation metric named Weighted Adversarial Vulnerability Score (WAVS).  
4. They created a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies, testing these across 13 diverse LLMs including GPT-5, Claude Haiku, and DeepSeek.  
5. Results reveal that obfuscation strategies such as "Maximum Mark Magyk" can successfully manipulate model scores and cause a high rate of decision flips, posing significant risks for peer review reliability.  
6. The authors commit to releasing their dataset and injection framework to encourage further research on safeguarding LLM-based peer review systems. <div>
arXiv:2512.10449v1 Announce Type: new 
Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</title>
<link>https://arxiv.org/abs/2512.10501</link>
<guid>https://arxiv.org/abs/2512.10501</guid>
<content:encoded><![CDATA[
<div> Procedural Content Generation, Large Language Models, zero-shot parameter configuration, Actor-Critic architecture, 3D map generation<br /><br />Summary: This paper addresses the challenge of configuring Procedural Content Generation (PCG) pipelines, which involve complex and technical parameters that are difficult to control. The authors propose a novel training-free method that leverages off-the-shelf Large Language Models (LLMs) as agents in a zero-shot setup to automatically configure PCG parameters from natural language instructions. The key innovation is an Actor-Critic agent architecture where the Actor suggests parameter configurations and the Critic evaluates and guides iterative refinements toward better alignment with user intents. This iterative reasoning approach bridges the semantic gap between abstract human instructions and the strict parameter requirements of PCG tools. The system is validated through experiments on generating diverse 3D maps, demonstrating superior performance compared to single-agent baselines in producing both diverse and structurally valid environments. Importantly, this framework does not require task-specific fine-tuning of the language models, highlighting its scalability and adaptability to complex software control. Overall, the method presents a scalable paradigm for harnessing general LLMs as autonomous agents to master complex, parameter-rich applications like PCG solely through architectural innovation rather than extensive retraining. <div>
arXiv:2512.10501v1 Announce Type: new 
Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10534</link>
<guid>https://arxiv.org/abs/2512.10534</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, Geometry problem solving, Auxiliary constructions, Reinforcement learning, IMO problems  

<br /><br />Summary:  
This paper introduces InternGeometry, a large language model (LLM) agent designed to solve expert-level geometry problems, including those from the International Mathematical Olympiad (IMO). InternGeometry addresses the key challenge of weak heuristics in auxiliary constructions by iteratively proposing geometric propositions and constructions, verifying them using a symbolic engine, and using feedback to refine future proposals. The system incorporates a dynamic memory mechanism enabling over 200 interactions with the symbolic engine per problem, enhancing its problem-solving capacity. To improve training efficiency, the authors propose Complexity-Boosting Reinforcement Learning (CBRL), which gradually escalates the difficulty of synthesized training problems in stages. Built on the InternThinker-32B foundation model, InternGeometry demonstrates strong performance by solving 44 out of 50 IMO geometry problems spanning 2000 to 2024, surpassing the average performance of gold medalists (40.9 problems solved). Notably, it achieves this using only 13,000 training examples, which is just 0.004% of the data utilized by previous expert systems like AlphaGeometry 2. Additionally, InternGeometry can generate novel auxiliary constructions not found in human solutions. The authors plan to release the model, training data, and symbolic engine to support further research in AI-driven advanced geometry problem solving. <div>
arXiv:2512.10534v1 Announce Type: new 
Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NormCode: A Semi-Formal Language for Context-Isolated AI Planning</title>
<link>https://arxiv.org/abs/2512.10563</link>
<guid>https://arxiv.org/abs/2512.10563</guid>
<content:encoded><![CDATA[
<div> Keywords: NormCode, large language models, context pollution, semiformal language, AI workflow transparency  

<br /><br />Summary:  
The article introduces NormCode, a semiformal language designed to improve multistep workflows using large language models (LLMs), tackling the issue of context pollution where accumulated information across steps leads to hallucinations and confusion. NormCode achieves this by enforcing strict data isolation between steps, ensuring each step only receives explicitly passed inputs and thus eliminating cross-step contamination. It distinctly separates semantic operations, which involve nondeterministic LLM-driven reasoning, from syntactic operations, which entail deterministic data restructuring, enabling clear tracking of cost and reliability. The language is presented in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting a progressive formalization process from initial design to production ready code. The authors validate NormCode through two demonstrations: a base X addition algorithm with perfect accuracy on inputs of arbitrary length, and NormCode’s self-hosted execution of its own five-phase compiler pipeline. Additionally, the system features a working orchestrator providing dependency-driven scheduling, SQLite-backed checkpointing, and loop management. This makes AI workflows auditable and transparent, addressing critical needs in high-stakes domains such as legal reasoning, medical decision-making, and financial analysis. <div>
arXiv:2512.10563v1 Announce Type: new 
Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs</title>
<link>https://arxiv.org/abs/2512.10611</link>
<guid>https://arxiv.org/abs/2512.10611</guid>
<content:encoded><![CDATA[
<div> Keywords: data center design, large language models, physics-guided optimization, energy efficiency, simulation-ready layout  

<br /><br />Summary:  
This paper addresses the challenge of designing data center (DC) infrastructure capable of meeting escalating computational demands. Traditional DC design methods combining human expertise and simulations struggle to scale with growing complexity. Recent generative AI approaches create human-centric indoor layouts but fail to incorporate physical constraints, limiting their applicability in DC design where operational objectives and strict physics must be met. To overcome this, the authors propose Phythesis, a novel framework that fuses large language models (LLMs) with physics-guided evolutionary optimization to generate simulation-ready (SimReady) DC layouts optimized for energy efficiency. Phythesis employs an iterative bi-level optimization process: the first level uses LLM-driven generation and self-criticism to produce physically plausible 3D layouts, while the second level applies physics-informed optimization to fine-tune asset parameters and select the optimal combination of design elements. Experiments across three scales demonstrate Phythesis achieves a 57.3% higher success rate in layout generation and improves power usage effectiveness (PUE) by 11.5% compared to baseline LLM-only methods, highlighting its potential to automate and enhance energy-efficient DC design with physical realism. <div>
arXiv:2512.10611v1 Announce Type: new 
Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification</title>
<link>https://arxiv.org/abs/2512.10640</link>
<guid>https://arxiv.org/abs/2512.10640</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised cell type identification, contrastive learning, cell-gene associations, single-cell RNA-seq, representation learning  

<br /><br />Summary:  
Unsupervised cell type identification is essential for detecting and characterizing heterogeneous cell populations in single-cell omics studies. Traditional clustering methods largely focus on intrinsic cellular structures but overlook critical cell-gene associations, limiting their ability to distinguish closely related cell types. To address this, the authors propose a novel Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to produce more informative cell representations. The framework introduces two contrastive distribution alignment components designed to uncover reliable intrinsic cellular structures by leveraging cell-cell structural relationships effectively. Furthermore, a refinement module is developed to integrate gene-correlation structure learning, which enhances cell embeddings by capturing underlying cell-gene associations. This module strengthens the connections between cells and their associated genes, refining representation learning to better exploit biologically meaningful relationships. Extensive experiments conducted on various single-cell RNA sequencing and spatial transcriptomics benchmark datasets demonstrate that scRCL consistently outperforms current state-of-the-art methods in terms of cell-type identification accuracy. Additionally, downstream biological analyses confirm that the identified cell populations display coherent gene-expression signatures, underscoring the biological relevance and effectiveness of the proposed approach. The authors have made the code available on GitHub for further use and validation. <div>
arXiv:2512.10640v1 Announce Type: new 
Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.10655</link>
<guid>https://arxiv.org/abs/2512.10655</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, memorization mitigation, classifier-free guidance, latent feature modification, prompt alignment  

<br /><br />Summary: Diffusion models can inadvertently reproduce exact training examples, which poses significant privacy and copyright risks as their usage becomes widespread. Traditional methods to reduce memorization focus on manipulating classifier-free guidance (CFG) or perturbing prompt embeddings during inference but often at the cost of degrading the model’s alignment with the given prompt. To address this challenge, the paper introduces CAPTAIN, a novel and training-free approach that works by directly altering latent features in the denoising phase. CAPTAIN begins by applying frequency-based noise initialization to limit the replication of memorized content early in the denoising timeline. It then determines the ideal denoising steps optimal for feature injection and pinpoints specific latent regions where memorization occurs. Finally, CAPTAIN injects semantically consistent features derived from non-memorized reference images into these localized latent areas. This targeted intervention suppresses memorization while maintaining both high prompt fidelity and visual output quality. Experimental results demonstrate that CAPTAIN significantly reduces memorization compared to CFG-based strategies, all while preserving strong adherence to prompt instructions and the overall artistic integrity of generated images. This method provides a promising pathway for safer deployment of diffusion models in practical applications. <div>
arXiv:2512.10655v1 Announce Type: new 
Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity</title>
<link>https://arxiv.org/abs/2512.10665</link>
<guid>https://arxiv.org/abs/2512.10665</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, value diversity, multi-agent systems, collective behavior, emergent principles<br /><br />Summary: This paper investigates how the diversity of values influences the collective behavior of AI communities composed of Large Language Model-based multi-agent systems. First, it applies Schwartz's Theory of Basic Human Values to naturally elicit values and simulate communities with varying agent counts engaged in open-ended interaction and constitution formation processes. Second, the findings indicate that greater value diversity enhances the stability of shared values among agents, promotes the emergence of complex behaviors, and leads to the development of more creative, agent-generated principles without external input. Third, the study highlights diminishing returns, as extreme heterogeneity in values can cause destabilization within the community. Fourth, this exploration frames value diversity as a crucial dimension for advancing AI capabilities, linking technical AI aspects with sociological insights into how institutions emerge and evolve. Finally, the work opens new pathways for integrating social science theories into the design of AI systems that interact and organize collectively, fostering more adaptable and innovative artificial communities. <div>
arXiv:2512.10665v1 Announce Type: new 
Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search</title>
<link>https://arxiv.org/abs/2512.10671</link>
<guid>https://arxiv.org/abs/2512.10671</guid>
<content:encoded><![CDATA[
<div> Early-exit networks, Neural Architecture Search, hardware-aware NAS, adaptive threshold tuning, exit branch design<br /><br />Summary:  
Early-exit networks provide an efficient way to reduce energy consumption and latency in deep learning models by allowing simpler inputs to exit the computation earlier. This approach is especially important for resource-constrained devices where minimizing energy usage is essential. However, designing effective early-exit networks poses challenges, as it requires carefully balancing computational efficiency with model accuracy. Recent advances have employed Neural Architecture Search (NAS) to optimize the placement and number of exit branches to achieve this balance. Beyond the number and placement, the depth and types of layers within each exit branch are critical factors influencing both performance and efficiency. The paper introduces a hardware-aware NAS framework that optimizes exit branch architectures by considering accuracy and efficiency simultaneously. This framework also incorporates adaptive threshold tuning to decide when samples should exit. The proposed method was evaluated on CIFAR-10, CIFAR-100, and SVHN datasets, showing improvements over existing approaches. Results demonstrate that their approach achieves higher accuracy while maintaining the same or reduced average multiply-accumulate operations (MACs). This confirms that optimizing exit branch structures and thresholds leads to more effective early-exit networks tailored for efficient, accurate deep learning inference. <div>
arXiv:2512.10671v1 Announce Type: new 
Abstract: Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenges of Evaluating LLM Safety for User Welfare</title>
<link>https://arxiv.org/abs/2512.10687</link>
<guid>https://arxiv.org/abs/2512.10687</guid>
<content:encoded><![CDATA[
<div> safety evaluation, large language models, user context, vulnerable populations, personalized advice  

<br /><br />Summary:  
1. Traditional safety evaluations of large language models (LLMs) focus on universal risks, such as dangerous capabilities or undesirable tendencies, but fail to address context-dependent harms affecting personal advice in high-stakes areas like finance and health.  
2. Although frameworks like the OECD's AI classification emphasize the importance of assessing individual user risks, evaluations centered on user welfare remain underdeveloped.  
3. The study assessed responses from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles with varying vulnerability, revealing that evaluators who lacked detailed user context rated identical LLM outputs as safer compared to those who were aware of user circumstances. Safety scores for high-vulnerability users dropped significantly from 5/7 (safe) to 3/7 (somewhat unsafe) when context was considered.  
4. Attempts to close this safety evaluation gap by including realistic user context within prompts, based on what users report disclosing, failed to improve safety ratings significantly.  
5. The findings demonstrate that effective user-welfare safety evaluations require explicit assessment against diverse user profiles rather than reliance on user-disclosed context alone, highlighting the necessity for new methodology distinct from existing universal-risk frameworks. The authors provide their code and dataset to support future research in context-aware LLM safety assessment. <div>
arXiv:2512.10687v1 Announce Type: new 
Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10691</link>
<guid>https://arxiv.org/abs/2512.10691</guid>
<content:encoded><![CDATA[
<div> vision-language models, Chest X-ray, supervised fine-tuning, reinforcement learning, report generation<br /><br />Summary: Recent progress in vision-language models (VLMs) has enhanced Chest X-ray (CXR) interpretation. Most medical VLMs depend primarily on supervised fine-tuning (SFT), which focuses on next-token prediction and does not directly assess answer quality. Reinforcement learning (RL), which integrates task-specific feedback, has shown strong performance improvements in tasks requiring explicit intermediate reasoning ("thinking") such as math and coding. The study investigates how RL and thinking influence a CXR VLM by first performing large-scale SFT on CXR data to develop RadVLM based on Qwen3-VL. A cold-start SFT stage is introduced to equip the model with basic reasoning ability. Subsequently, Group Relative Policy Optimization (GRPO) is applied with clinically relevant, task-specific rewards targeted at report generation and visual grounding. Experiments are conducted on domain-specific and general-domain Qwen3-VL variants, both with and without the thinking component. Results indicate that while strong SFT is essential for foundational performance, RL yields additional improvements on both evaluated tasks. However, explicit intermediate reasoning does not provide further gains. The RL-optimized RadVLM models outperform baselines under a unified evaluation framework, achieving state-of-the-art results in medical report generation and visual grounding, underscoring the value of clinically aligned RL as a complement to SFT in medical VLMs. <div>
arXiv:2512.10691v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution</title>
<link>https://arxiv.org/abs/2512.10696</link>
<guid>https://arxiv.org/abs/2512.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural memory, large language models, experience-driven evolution, multi-faceted distillation, utility-based refinement  

<br /><br />Summary:  
This paper addresses the limitations of current procedural memory frameworks for large language model (LLM) agents, which typically rely on a passive, append-only memory accumulation mechanism. The authors propose ReMe (Remember Me, Refine Me), a novel framework designed to enable agents to dynamically evolve through their experiences. ReMe innovates the memory lifecycle using three key mechanisms: (1) multi-faceted distillation, which extracts detailed insights by identifying success patterns, diagnosing failure causes, and generating comparative analyses; (2) context-adaptive reuse, which leverages scenario-aware indexing to customize past knowledge for new contexts; and (3) utility-based refinement, an autonomous process that adds valuable memories while pruning outdated or irrelevant ones to keep the memory pool compact and high-quality. Extensive experiments on benchmarks BFCL-V3 and AppWorld demonstrate that ReMe significantly improves agent memory systems, achieving state-of-the-art results. A noteworthy finding is the memory-scaling effect, where the smaller Qwen3-8B model with ReMe outperforms the larger Qwen3-14B model without memory, highlighting the efficiency of self-evolving memory for lifelong learning. The authors also contribute by releasing their code and the reme.library dataset to support further research in this area. <div>
arXiv:2512.10696v1 Announce Type: new 
Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators</title>
<link>https://arxiv.org/abs/2512.10702</link>
<guid>https://arxiv.org/abs/2512.10702</guid>
<content:encoded><![CDATA[
<div> Keywords: CA-GPT, optical coherence tomography, percutaneous coronary intervention, artificial intelligence, procedural decision-making  

<br /><br />Summary:  
This study evaluates the performance of CA-GPT, a specialized large AI model integrated into an AI-OCT system, for guiding percutaneous coronary intervention (PCI) using optical coherence tomography (OCT). Conducted at a single center with 96 patients, the study compared procedural decisions from CA-GPT, a general-purpose ChatGPT-5 model, and junior physicians against expert-derived records. CA-GPT showed significantly higher agreement scores in pre-PCI planning compared to both ChatGPT-5 and junior physicians, excelling particularly in selecting stent diameter (90.3% vs. 72.2%) and length (80.6% vs. 52.8%). In post-PCI assessments, CA-GPT maintained superior overall agreement, outperforming the other groups consistently. Subgroup analysis further confirmed that CA-GPT's advantage persists in complex clinical scenarios. The findings highlight CA-GPT's ability to provide standardized, reliable intravascular imaging interpretation, reducing the operator-dependency typical of OCT assessment. This AI-driven approach holds significant promise to augment clinician expertise and optimize decision-making during OCT-guided PCI procedures, potentially improving clinical outcomes through enhanced procedural accuracy and consistency. <div>
arXiv:2512.10702v1 Announce Type: new 
Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly</title>
<link>https://arxiv.org/abs/2512.10787</link>
<guid>https://arxiv.org/abs/2512.10787</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, multi-hop queries, context dilution, SEAL-RAG, evidence precision<br /><br />Summary:  
The paper addresses the challenge of Retrieval-Augmented Generation (RAG) systems struggling with multi-hop queries where the initial retrieval misses a crucial bridge fact. Existing methods like Self-RAG, CRAG, and Adaptive-$k$ mitigate this by expanding or pruning the retrieval context, but expanding often causes context dilution, reducing relevant information visibility. To counter this, the authors propose SEAL-RAG, a training-free controller that uses a "replace, don't expand" approach within a fixed retrieval depth $k$. SEAL-RAG operates through a four-step cycle: Search, Extract, Assess, and Loop. This involves entity-anchored extraction to identify missing entities or relations (gap specification), launching targeted micro-queries to fill these gaps, and employing entity-first ranking to replace irrelevant distractors with gap-closing evidence. The method was evaluated against strong baselines (Basic RAG, CRAG, Self-RAG, Adaptive-$k$) on HotpotQA and 2WikiMultiHopQA datasets. On HotpotQA with $k=3$, SEAL improved answer correctness by 3 to 13 percentage points and evidence precision by 12 to 18 points over Self-RAG. On 2WikiMultiHopQA with $k=5$, SEAL outperformed Adaptive-$k$ by 8 points in accuracy and maintained high evidence precision (96%) compared to 22% for CRAG. These improvements are statistically significant ($p<0.001$). By fixing $k$ and focusing on replacement, SEAL ensures predictable computational cost and optimized top-$k$ retrieval performance for precision rather than breadth. The authors have released their code and data openly. <div>
arXiv:2512.10787v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.10807</link>
<guid>https://arxiv.org/abs/2512.10807</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Out-of-Distribution, Benchmark, Sensor Data, Domain Shift<br /><br />Summary:<br /><br />1. This paper addresses the challenge of distributional shifts in sensor-based Human Activity Recognition (HAR), caused by variations across individuals, devices, environments, and time.<br />2. It questions the necessity of out-of-distribution (OOD) algorithms for HAR and investigates which existing OOD methods perform best across diverse scenarios.<br />3. The authors propose HAROOD, a comprehensive benchmark specifically designed for evaluating HAR tasks in OOD settings.<br />4. Four distinct OOD scenarios are defined: cross-person, cross-position, cross-dataset, and cross-time, covering realistic and challenging conditions.<br />5. The benchmark integrates six datasets, implements sixteen comparative methods involving both CNN-based and Transformer-based architectures, and introduces two model selection protocols.<br />6. Extensive experiments reveal that no single method consistently outperforms others across all scenarios, indicating substantial room for future improvements.<br />7. The HAROOD codebase is modular, extensible, and publicly released to encourage further research and development in OOD-based HAR methodologies.<br />8. The release and documentation can be accessed at https://github.com/AIFrontierLab/HAROOD, aiming to facilitate benchmarking, comparison, and advancement in the HAR community. <div>
arXiv:2512.10807v1 Announce Type: new 
Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agile Deliberation: Concept Deliberation for Subjective Visual Classification</title>
<link>https://arxiv.org/abs/2512.10821</link>
<guid>https://arxiv.org/abs/2512.10821</guid>
<content:encoded><![CDATA[
<div> Keywords: content moderation, human-in-the-loop, concept deliberation, Agile Deliberation, visual classifiers  

<br /><br />Summary:  
1. The paper addresses the challenge of evolving and subjective concept understanding necessary for training vision classifiers in applications like content moderation and curation.  
2. It identifies that users often begin with vague concept ideas and engage in an iterative "concept deliberation" process to refine their understanding, as revealed through interviews with content moderation experts.  
3. The authors introduce a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving concepts by exposing users to borderline cases during training.  
4. Agile Deliberation consists of two stages: (1) concept scoping, which breaks down the initial idea into a hierarchical structure of sub-concepts, and (2) concept iteration, which presents semantically borderline examples for users to reflect on and give feedback, thus aligning the classifier iteratively to the evolving user intent.  
5. The framework was evaluated in 18 user sessions of 1.5 hours each, demonstrating a 7.5% improvement in F1 score over automated decomposition methods and more than 3% improvement over manual deliberation approaches, while users reported clearer conceptual understanding and reduced cognitive effort. <div>
arXiv:2512.10821v1 Announce Type: new 
Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions</title>
<link>https://arxiv.org/abs/2512.10822</link>
<guid>https://arxiv.org/abs/2512.10822</guid>
<content:encoded><![CDATA[
<div> Safe Offline Reinforcement Learning, Control Barrier Functions, Neural Barrier, Model-Free Learning, Expectile Objective<br /><br />Summary:<br /><br />1. Ensuring safety in autonomous systems requires controllers that satisfy strict state-wise constraints without depending on online interactions, a challenge that traditional Safe Offline RL methods address only with soft expected-cost constraints. 2. Control Barrier Functions (CBFs) offer strong safety guarantees but typically rely on expert knowledge of barrier functions or full system dynamics, limiting their practicality in many scenarios. 3. The proposed Value-Guided Offline Control Barrier Functions (V-OCBF) framework overcomes these limitations by learning a neural CBF entirely from offline demonstration data without requiring access to system dynamics. 4. V-OCBF introduces a recursive finite-difference barrier update that enables model-free learning to propagate safety information over time, thus avoiding the necessity of a dynamics model. 5. To improve robustness, V-OCBF incorporates an expectile-based objective that prevents querying the barrier on out-of-distribution actions and confines updates to actions supported by the dataset. 6. The learned neural barrier is subsequently utilized within a Quadratic Program (QP) to synthesize real-time safe control policies. 7. Extensive case studies demonstrate that V-OCBF achieves significantly fewer safety violations than baseline methods while maintaining strong task performance, showcasing its potential for scalable offline synthesis of safety-critical controllers without the need for online interaction or hand-engineered barrier functions. <div>
arXiv:2512.10822v1 Announce Type: new 
Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Can Assist with Proposal Selection at Large User Facilities</title>
<link>https://arxiv.org/abs/2512.10895</link>
<guid>https://arxiv.org/abs/2512.10895</guid>
<content:encoded><![CDATA[
arXiv:2512.10895v1 Announce Type: new 
Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $\rho\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Granular Node Pruning for Circuit Discovery</title>
<link>https://arxiv.org/abs/2512.10903</link>
<guid>https://arxiv.org/abs/2512.10903</guid>
<content:encoded><![CDATA[
arXiv:2512.10903v1 Announce Type: new 
Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Decision-Making Agents and Higher-Order Causal Processes</title>
<link>https://arxiv.org/abs/2512.10937</link>
<guid>https://arxiv.org/abs/2512.10937</guid>
<content:encoded><![CDATA[
arXiv:2512.10937v1 Announce Type: new 
Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Acquisition of Discrete Grammatical Categories</title>
<link>https://arxiv.org/abs/2503.18702</link>
<guid>https://arxiv.org/abs/2503.18702</guid>
<content:encoded><![CDATA[
arXiv:2503.18702v1 Announce Type: cross 
Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting</title>
<link>https://arxiv.org/abs/2508.01426</link>
<guid>https://arxiv.org/abs/2508.01426</guid>
<content:encoded><![CDATA[
arXiv:2508.01426v2 Announce Type: cross 
Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs</title>
<link>https://arxiv.org/abs/2512.09874</link>
<guid>https://arxiv.org/abs/2512.09874</guid>
<content:encoded><![CDATA[
arXiv:2512.09874v1 Announce Type: cross 
Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoTEdu: Access Control, Detection, and Automatic Incident Response in Academic IoT Networks</title>
<link>https://arxiv.org/abs/2512.09934</link>
<guid>https://arxiv.org/abs/2512.09934</guid>
<content:encoded><![CDATA[
arXiv:2512.09934v1 Announce Type: cross 
Abstract: The growing presence of IoT devices in academic environments has increased operational complexity and exposed security weaknesses, especially in academic institutions without unified policies for registration, monitoring, and incident response involving IoT. This work presents IoTEdu, an integrated platform that combines access control, incident detection, and automatic blocking of IoT devices. The solution was evaluated in a controlled environment with simulated attacks, achieving an average time of 28.6 seconds between detection and blocking. The results show a reduction in manual intervention, standardization of responses, and unification of the processes of registration, monitoring, and incident response.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)</title>
<link>https://arxiv.org/abs/2512.09939</link>
<guid>https://arxiv.org/abs/2512.09939</guid>
<content:encoded><![CDATA[
arXiv:2512.09939v1 Announce Type: cross 
Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELANA: A Simple Energy and Latency Analyzer for LLMs</title>
<link>https://arxiv.org/abs/2512.09946</link>
<guid>https://arxiv.org/abs/2512.09946</guid>
<content:encoded><![CDATA[
arXiv:2512.09946v1 Announce Type: cross 
Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs</title>
<link>https://arxiv.org/abs/2512.09953</link>
<guid>https://arxiv.org/abs/2512.09953</guid>
<content:encoded><![CDATA[
arXiv:2512.09953v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.
  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.
  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects</title>
<link>https://arxiv.org/abs/2512.10031</link>
<guid>https://arxiv.org/abs/2512.10031</guid>
<content:encoded><![CDATA[
arXiv:2512.10031v1 Announce Type: cross 
Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-Dags as Powerful Background Knowledge For Causal Discovery</title>
<link>https://arxiv.org/abs/2512.10032</link>
<guid>https://arxiv.org/abs/2512.10032</guid>
<content:encoded><![CDATA[
arXiv:2512.10032v1 Announce Type: cross 
Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</title>
<link>https://arxiv.org/abs/2512.10040</link>
<guid>https://arxiv.org/abs/2512.10040</guid>
<content:encoded><![CDATA[
arXiv:2512.10040v1 Announce Type: cross 
Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata</title>
<link>https://arxiv.org/abs/2512.10041</link>
<guid>https://arxiv.org/abs/2512.10041</guid>
<content:encoded><![CDATA[
arXiv:2512.10041v1 Announce Type: cross 
Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detailed balance in large language model-driven agents</title>
<link>https://arxiv.org/abs/2512.10047</link>
<guid>https://arxiv.org/abs/2512.10047</guid>
<content:encoded><![CDATA[
arXiv:2512.10047v1 Announce Type: cross 
Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10051</link>
<guid>https://arxiv.org/abs/2512.10051</guid>
<content:encoded><![CDATA[
arXiv:2512.10051v1 Announce Type: cross 
Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2</title>
<link>https://arxiv.org/abs/2512.10066</link>
<guid>https://arxiv.org/abs/2512.10066</guid>
<content:encoded><![CDATA[
arXiv:2512.10066v1 Announce Type: cross 
Abstract: The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models</title>
<link>https://arxiv.org/abs/2512.10080</link>
<guid>https://arxiv.org/abs/2512.10080</guid>
<content:encoded><![CDATA[
arXiv:2512.10080v1 Announce Type: cross 
Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defining the Scope of Learning Analytics: An Axiomatic Approach for Analytic Practice and Measurable Learning Phenomena</title>
<link>https://arxiv.org/abs/2512.10081</link>
<guid>https://arxiv.org/abs/2512.10081</guid>
<content:encoded><![CDATA[
arXiv:2512.10081v1 Announce Type: cross 
Abstract: Learning Analytics (LA) has rapidly expanded through practical and technological innovation, yet its foundational identity has remained theoretically under-specified. This paper addresses this gap by proposing the first axiomatic theory that formally defines the essential structure, scope, and limitations of LA. Derived from the psychological definition of learning and the methodological requirements of LA, the framework consists of five axioms specifying discrete observation, experience construction, state transition, and inference. From these axioms, we derive a set of theorems and propositions that clarify the epistemological stance of LA, including the inherent unobservability of learner states, the irreducibility of temporal order, constraints on reachable states, and the impossibility of deterministically predicting future learning. We further define LA structure and LA practice as formal objects, demonstrating the sufficiency and necessity of the axioms and showing that diverse LA approaches -- such as Bayesian Knowledge Tracing and dashboards -- can be uniformly explained within this framework. The theory provides guiding principles for designing analytic methods and interpreting learning data while avoiding naive behaviorism and category errors by establishing an explicit theoretical inference layer between observations and states. This work positions LA as a rigorous science of state transition systems based on observability, establishing the theoretical foundation necessary for the field's maturation as a scholarly discipline.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis</title>
<link>https://arxiv.org/abs/2512.10098</link>
<guid>https://arxiv.org/abs/2512.10098</guid>
<content:encoded><![CDATA[
arXiv:2512.10098v1 Announce Type: cross 
Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly un- der domain shifts and rare-class conditions. Deep learning mod- els often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Med- ical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician- derived expert knowledge to improve generalization, reduce rare- class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHyLL: Learning Continuous Neural Representations of Hybrid Systems</title>
<link>https://arxiv.org/abs/2512.10117</link>
<guid>https://arxiv.org/abs/2512.10117</guid>
<content:encoded><![CDATA[
arXiv:2512.10117v1 Announce Type: cross 
Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VocSim: A Training-free Benchmark for Zero-shot Content Identity in Single-source Audio</title>
<link>https://arxiv.org/abs/2512.10120</link>
<guid>https://arxiv.org/abs/2512.10120</guid>
<content:encoded><![CDATA[
arXiv:2512.10120v1 Announce Type: cross 
Abstract: General-purpose audio representations aim to map acoustically variable instances of the same event to nearby points, resolving content identity in a zero-shot setting. Unlike supervised classification benchmarks that measure adaptability via parameter updates, we introduce VocSim, a training-free benchmark probing the intrinsic geometric alignment of frozen embeddings. VocSim aggregates 125k single-source clips from 19 corpora spanning human speech, animal vocalizations, and environmental sounds. By restricting to single-source audio, we isolate content representation from the confound of source separation. We evaluate embeddings using Precision@k for local purity and the Global Separation Rate (GSR) for point-wise class separation. To calibrate GSR, we report lift over an empirical permutation baseline. Across diverse foundation models, a simple pipeline, frozen Whisper encoder features, time-frequency pooling, and label-free PCA, yields strong zero-shot performance. However, VocSim also uncovers a consistent generalization gap. On blind, low-resource speech, local retrieval drops sharply. While performance remains statistically distinguishable from chance, the absolute geometric structure collapses, indicating a failure to generalize to unseen phonotactics. As external validation, our top embeddings predict avian perceptual similarity, improve bioacoustic classification, and achieve state-of-the-art results on the HEAR benchmark. We posit that the intrinsic geometric quality measured here proxies utility in unlisted downstream applications. We release data, code, and a public leaderboard to standardize the evaluation of intrinsic audio geometry.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing</title>
<link>https://arxiv.org/abs/2512.10121</link>
<guid>https://arxiv.org/abs/2512.10121</guid>
<content:encoded><![CDATA[
arXiv:2512.10121v1 Announce Type: cross 
Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Hirschberg for Width Bounded Dynamic Programs</title>
<link>https://arxiv.org/abs/2512.10132</link>
<guid>https://arxiv.org/abs/2512.10132</guid>
<content:encoded><![CDATA[
arXiv:2512.10132v1 Announce Type: cross 
Abstract: Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $\omega$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(\omega \log T + (\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small "middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $\Omega(\omega)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset</title>
<link>https://arxiv.org/abs/2512.10148</link>
<guid>https://arxiv.org/abs/2512.10148</guid>
<content:encoded><![CDATA[
arXiv:2512.10148v1 Announce Type: cross 
Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning</title>
<link>https://arxiv.org/abs/2512.10150</link>
<guid>https://arxiv.org/abs/2512.10150</guid>
<content:encoded><![CDATA[
arXiv:2512.10150v1 Announce Type: cross 
Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving</title>
<link>https://arxiv.org/abs/2512.10159</link>
<guid>https://arxiv.org/abs/2512.10159</guid>
<content:encoded><![CDATA[
arXiv:2512.10159v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offscript: Automated Auditing of Instruction Adherence in LLMs</title>
<link>https://arxiv.org/abs/2512.10172</link>
<guid>https://arxiv.org/abs/2512.10172</guid>
<content:encoded><![CDATA[
arXiv:2512.10172v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and generative search systems are increasingly used for information seeking by diverse populations with varying preferences for knowledge sourcing and presentation. While users can customize LLM behavior through custom instructions and behavioral prompts, no mechanism exists to evaluate whether these instructions are being followed effectively. We present Offscript, an automated auditing tool that efficiently identifies potential instruction following failures in LLMs. In a pilot study analyzing custom instructions sourced from Reddit, Offscript detected potential deviations from instructed behavior in 86.4% of conversations, 22.2% of which were confirmed as material violations through human review. Our findings suggest that automated auditing serves as a viable approach for evaluating compliance to behavioral instructions related to information seeking.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Domain Generalization with Latent Space Inversion</title>
<link>https://arxiv.org/abs/2512.10224</link>
<guid>https://arxiv.org/abs/2512.10224</guid>
<content:encoded><![CDATA[
arXiv:2512.10224v1 Announce Type: cross 
Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Information Routing for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10229</link>
<guid>https://arxiv.org/abs/2512.10229</guid>
<content:encoded><![CDATA[
arXiv:2512.10229v1 Announce Type: cross 
Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InFerActive: Towards Scalable Human Evaluation of Large Language Models through Interactive Inference</title>
<link>https://arxiv.org/abs/2512.10234</link>
<guid>https://arxiv.org/abs/2512.10234</guid>
<content:encoded><![CDATA[
arXiv:2512.10234v1 Announce Type: cross 
Abstract: Human evaluation remains the gold standard for evaluating outputs of Large Language Models (LLMs). The current evaluation paradigm reviews numerous individual responses, leading to significant scalability challenges. LLM outputs can be more efficiently represented as a tree structure, reflecting their autoregressive generation process and stochastic token selection. However, conventional tree visualization cannot scale to the exponentially large trees generated by modern sampling methods of LLMs. To address this problem, we present InFerActive, an interactive inference system for scalable human evaluation. InFerActive enables on-demand exploration through probability-based filtering and evaluation features, while bridging the semantic gap between computational tokens and human-readable text through adaptive visualization techniques. Through a technical evaluation and user study (N=12), we demonstrate that InFerActive significantly improves evaluation efficiency and enables more comprehensive assessment of model behavior. We further conduct expert case studies that demonstrate InFerActive's practical applicability and potential for transforming LLM evaluation workflows.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</title>
<link>https://arxiv.org/abs/2512.10248</link>
<guid>https://arxiv.org/abs/2512.10248</guid>
<content:encoded><![CDATA[
arXiv:2512.10248v1 Announce Type: cross 
Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</title>
<link>https://arxiv.org/abs/2512.10271</link>
<guid>https://arxiv.org/abs/2512.10271</guid>
<content:encoded><![CDATA[
arXiv:2512.10271v1 Announce Type: cross 
Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing Evolutionarily Stable Strategies in Imperfect-Information Games</title>
<link>https://arxiv.org/abs/2512.10279</link>
<guid>https://arxiv.org/abs/2512.10279</guid>
<content:encoded><![CDATA[
arXiv:2512.10279v1 Announce Type: cross 
Abstract: We present an algorithm for computing evolutionarily stable strategies (ESSs) in symmetric perfect-recall extensive-form games of imperfect information. Our main algorithm is for two-player games, and we describe how it can be extended to multiplayer games. The algorithm is sound and computes all ESSs in nondegenerate games and a subset of them in degenerate games which contain an infinite continuum of symmetric Nash equilibria. The algorithm is anytime and can be stopped early to find one or more ESSs. We experiment on an imperfect-information cancer signaling game as well as random games to demonstrate scalability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Network Based Adaptive Threat Detection for Cloud Identity and Access Management Logs</title>
<link>https://arxiv.org/abs/2512.10280</link>
<guid>https://arxiv.org/abs/2512.10280</guid>
<content:encoded><![CDATA[
arXiv:2512.10280v1 Announce Type: cross 
Abstract: The rapid expansion of cloud infrastructures and distributed identity systems has significantly increased the complexity and attack surface of modern enterprises. Traditional rule based or signature driven detection systems are often inadequate in identifying novel or evolving threats within Identity and Access Management logs, where anomalous behavior may appear statistically benign but contextually malicious. This paper presents a Graph Neural Network Based Adaptive Threat Detection framework designed to learn latent user resource interaction patterns from IAM audit trails in real time. By modeling IAM logs as heterogeneous dynamic graphs, the proposed system captures temporal, relational, and contextual dependencies across entities such as users, roles, sessions, and access actions. The model incorporates attention based aggregation and graph embedding updates to enable continual adaptation to changing cloud environments. Experimental evaluation on synthesized and real world IAM datasets demonstrates that the proposed method achieves higher detection precision and recall than baseline LSTM and GCN classifiers, while maintaining scalability across multi tenant cloud environments. The frameworks adaptability enables proactive mitigation of insider threats, privilege escalation, and lateral movement attacks, contributing to the foundation of AI driven zero trust access analytics. This work bridges the gap between graph based machine learning and operational cloud security intelligence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</title>
<link>https://arxiv.org/abs/2512.10284</link>
<guid>https://arxiv.org/abs/2512.10284</guid>
<content:encoded><![CDATA[
arXiv:2512.10284v1 Announce Type: cross 
Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</title>
<link>https://arxiv.org/abs/2512.10296</link>
<guid>https://arxiv.org/abs/2512.10296</guid>
<content:encoded><![CDATA[
arXiv:2512.10296v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments</title>
<link>https://arxiv.org/abs/2512.10312</link>
<guid>https://arxiv.org/abs/2512.10312</guid>
<content:encoded><![CDATA[
arXiv:2512.10312v1 Announce Type: cross 
Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Informal Proofs into Formal Proofs Using a Chain of States</title>
<link>https://arxiv.org/abs/2512.10317</link>
<guid>https://arxiv.org/abs/2512.10317</guid>
<content:encoded><![CDATA[
arXiv:2512.10317v1 Announce Type: cross 
Abstract: We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual VLM Training: Adapting an English-Trained VLM to French</title>
<link>https://arxiv.org/abs/2512.10336</link>
<guid>https://arxiv.org/abs/2512.10336</guid>
<content:encoded><![CDATA[
arXiv:2512.10336v1 Announce Type: cross 
Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale</title>
<link>https://arxiv.org/abs/2512.10341</link>
<guid>https://arxiv.org/abs/2512.10341</guid>
<content:encoded><![CDATA[
arXiv:2512.10341v1 Announce Type: cross 
Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories</title>
<link>https://arxiv.org/abs/2512.10350</link>
<guid>https://arxiv.org/abs/2512.10350</guid>
<content:encoded><![CDATA[
arXiv:2512.10350v1 Announce Type: cross 
Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.10362</link>
<guid>https://arxiv.org/abs/2512.10362</guid>
<content:encoded><![CDATA[
arXiv:2512.10362v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPG: Generalized Policy Gradient Theorem for Transformer-based Policies</title>
<link>https://arxiv.org/abs/2512.10365</link>
<guid>https://arxiv.org/abs/2512.10365</guid>
<content:encoded><![CDATA[
arXiv:2512.10365v1 Announce Type: cross 
Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning</title>
<link>https://arxiv.org/abs/2512.10372</link>
<guid>https://arxiv.org/abs/2512.10372</guid>
<content:encoded><![CDATA[
arXiv:2512.10372v1 Announce Type: cross 
Abstract: The rising demand for collaborative machine learning and data analytics calls for secure and decentralized data sharing frameworks that balance privacy, trust, and incentives. Existing approaches, including federated learning (FL) and blockchain-based data markets, fall short: FL often depends on trusted aggregators and lacks Byzantine robustness, while blockchain frameworks struggle with computation-intensive training and incentive integration.
  We present \prot, a decentralized data marketplace that unifies federated learning, blockchain arbitration, and economic incentives into a single framework for privacy-preserving data sharing. \prot\ enables data buyers to submit bid-based requests via blockchain smart contracts, which manage auctions, escrow, and dispute resolution. Computationally intensive training is delegated to \cone\ (\uline{Co}mpute \uline{N}etwork for \uline{E}xecution), an off-chain distributed execution layer. To safeguard against adversarial behavior, \prot\ integrates a modified YODA protocol with exponentially growing execution sets for resilient consensus, and introduces Corrected OSMD to mitigate malicious or low-quality contributions from sellers. All protocols are incentive-compatible, and our game-theoretic analysis establishes honesty as the dominant strategy.
  We implement \prot\ on Ethereum and evaluate it over benchmark datasets -- MNIST, Fashion-MNIST, and CIFAR-10 -- under varying adversarial settings. \prot\ achieves up to 99\% accuracy on MNIST and 90\% on Fashion-MNIST, with less than 3\% degradation up to 30\% Byzantine nodes, and 56\% accuracy on CIFAR-10 despite its complexity. Our results show that \prot\ ensures privacy, maintains robustness under adversarial conditions, and scales efficiently with the number of participants, making it a practical foundation for real-world decentralized data sharing.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural personal sound zones with flexible bright zone control</title>
<link>https://arxiv.org/abs/2512.10375</link>
<guid>https://arxiv.org/abs/2512.10375</guid>
<content:encoded><![CDATA[
arXiv:2512.10375v1 Announce Type: cross 
Abstract: Personal sound zone (PSZ) reproduction system, which attempts to create distinct virtual acoustic scenes for different listeners at their respective positions within the same spatial area using one loudspeaker array, is a fundamental technology in the application of virtual reality. For practical applications, the reconstruction targets must be measured on the same fixed receiver array used to record the local room impulse responses (RIRs) from the loudspeaker array to the control points in each PSZ, which makes the system inconvenient and costly for real-world use. In this paper, a 3D convolutional neural network (CNN) designed for PSZ reproduction with flexible control microphone grid and alternative reproduction target is presented, utilizing the virtual target scene as inputs and the PSZ pre-filters as output. Experimental results of the proposed method are compared with the traditional method, demonstrating that the proposed method is able to handle varied reproduction targets on flexible control point grid using only one training session. Furthermore, the proposed method also demonstrates the capability to learn global spatial information from sparse sampling points distributed in PSZs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies</title>
<link>https://arxiv.org/abs/2512.10384</link>
<guid>https://arxiv.org/abs/2512.10384</guid>
<content:encoded><![CDATA[
arXiv:2512.10384v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2512.10388</link>
<guid>https://arxiv.org/abs/2512.10388</guid>
<content:encoded><![CDATA[
arXiv:2512.10388v1 Announce Type: cross 
Abstract: Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \textbf{\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\footnote{https://github.com/ziwliu8/H2Rec}.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Retrieval Models for Stripped Binary Analysis</title>
<link>https://arxiv.org/abs/2512.10393</link>
<guid>https://arxiv.org/abs/2512.10393</guid>
<content:encoded><![CDATA[
arXiv:2512.10393v1 Announce Type: cross 
Abstract: LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
arXiv:2512.10398v1 Announce Type: cross 
Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
arXiv:2512.10402v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sliding Window Attention Adaptation</title>
<link>https://arxiv.org/abs/2512.10411</link>
<guid>https://arxiv.org/abs/2512.10411</guid>
<content:encoded><![CDATA[
arXiv:2512.10411v1 Announce Type: cross 
Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation</title>
<link>https://arxiv.org/abs/2512.10415</link>
<guid>https://arxiv.org/abs/2512.10415</guid>
<content:encoded><![CDATA[
arXiv:2512.10415v1 Announce Type: cross 
Abstract: The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction</title>
<link>https://arxiv.org/abs/2512.10416</link>
<guid>https://arxiv.org/abs/2512.10416</guid>
<content:encoded><![CDATA[
arXiv:2512.10416v1 Announce Type: cross 
Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers</title>
<link>https://arxiv.org/abs/2512.10422</link>
<guid>https://arxiv.org/abs/2512.10422</guid>
<content:encoded><![CDATA[
arXiv:2512.10422v1 Announce Type: cross 
Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time</title>
<link>https://arxiv.org/abs/2512.10437</link>
<guid>https://arxiv.org/abs/2512.10437</guid>
<content:encoded><![CDATA[
arXiv:2512.10437v1 Announce Type: cross 
Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustered Federated Learning with Hierarchical Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.10443</link>
<guid>https://arxiv.org/abs/2512.10443</guid>
<content:encoded><![CDATA[
arXiv:2512.10443v1 Announce Type: cross 
Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maximum Risk Minimization with Random Forests</title>
<link>https://arxiv.org/abs/2512.10445</link>
<guid>https://arxiv.org/abs/2512.10445</guid>
<content:encoded><![CDATA[
arXiv:2512.10445v1 Announce Type: cross 
Abstract: We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method</title>
<link>https://arxiv.org/abs/2512.10461</link>
<guid>https://arxiv.org/abs/2512.10461</guid>
<content:encoded><![CDATA[
arXiv:2512.10461v1 Announce Type: cross 
Abstract: Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10492</link>
<guid>https://arxiv.org/abs/2512.10492</guid>
<content:encoded><![CDATA[
arXiv:2512.10492v1 Announce Type: cross 
Abstract: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10510</link>
<guid>https://arxiv.org/abs/2512.10510</guid>
<content:encoded><![CDATA[
arXiv:2512.10510v1 Announce Type: cross 
Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Auction: Generative Auction towards LLM-Native Advertising</title>
<link>https://arxiv.org/abs/2512.10551</link>
<guid>https://arxiv.org/abs/2512.10551</guid>
<content:encoded><![CDATA[
arXiv:2512.10551v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval</title>
<link>https://arxiv.org/abs/2512.10596</link>
<guid>https://arxiv.org/abs/2512.10596</guid>
<content:encoded><![CDATA[
arXiv:2512.10596v1 Announce Type: cross 
Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title>
<link>https://arxiv.org/abs/2512.10675</link>
<guid>https://arxiv.org/abs/2512.10675</guid>
<content:encoded><![CDATA[
arXiv:2512.10675v1 Announce Type: cross 
Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition</title>
<link>https://arxiv.org/abs/2512.10688</link>
<guid>https://arxiv.org/abs/2512.10688</guid>
<content:encoded><![CDATA[
arXiv:2512.10688v1 Announce Type: cross 
Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10698</link>
<guid>https://arxiv.org/abs/2512.10698</guid>
<content:encoded><![CDATA[
arXiv:2512.10698v1 Announce Type: cross 
Abstract: Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code</title>
<link>https://arxiv.org/abs/2512.10713</link>
<guid>https://arxiv.org/abs/2512.10713</guid>
<content:encoded><![CDATA[
arXiv:2512.10713v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation</title>
<link>https://arxiv.org/abs/2512.10734</link>
<guid>https://arxiv.org/abs/2512.10734</guid>
<content:encoded><![CDATA[
arXiv:2512.10734v1 Announce Type: cross 
Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation</title>
<link>https://arxiv.org/abs/2512.10735</link>
<guid>https://arxiv.org/abs/2512.10735</guid>
<content:encoded><![CDATA[
arXiv:2512.10735v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2512.10739</link>
<guid>https://arxiv.org/abs/2512.10739</guid>
<content:encoded><![CDATA[
arXiv:2512.10739v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</title>
<link>https://arxiv.org/abs/2512.10758</link>
<guid>https://arxiv.org/abs/2512.10758</guid>
<content:encoded><![CDATA[
arXiv:2512.10758v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.
  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.
  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.
  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metaphor-based Jailbreaking Attacks on Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.10766</link>
<guid>https://arxiv.org/abs/2512.10766</guid>
<content:encoded><![CDATA[
arXiv:2512.10766v1 Announce Type: cross 
Abstract: Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation</title>
<link>https://arxiv.org/abs/2512.10772</link>
<guid>https://arxiv.org/abs/2512.10772</guid>
<content:encoded><![CDATA[
arXiv:2512.10772v1 Announce Type: cross 
Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving</title>
<link>https://arxiv.org/abs/2512.10785</link>
<guid>https://arxiv.org/abs/2512.10785</guid>
<content:encoded><![CDATA[
arXiv:2512.10785v1 Announce Type: cross 
Abstract: Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Language Interface for Firewall Configuration</title>
<link>https://arxiv.org/abs/2512.10789</link>
<guid>https://arxiv.org/abs/2512.10789</guid>
<content:encoded><![CDATA[
arXiv:2512.10789v1 Announce Type: cross 
Abstract: This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</title>
<link>https://arxiv.org/abs/2512.10791</link>
<guid>https://arxiv.org/abs/2512.10791</guid>
<content:encoded><![CDATA[
arXiv:2512.10791v1 Announce Type: cross 
Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification</title>
<link>https://arxiv.org/abs/2512.10793</link>
<guid>https://arxiv.org/abs/2512.10793</guid>
<content:encoded><![CDATA[
arXiv:2512.10793v1 Announce Type: cross 
Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What matters for Representation Alignment: Global Information or Spatial Structure?</title>
<link>https://arxiv.org/abs/2512.10794</link>
<guid>https://arxiv.org/abs/2512.10794</guid>
<content:encoded><![CDATA[
arXiv:2512.10794v1 Announce Type: cross 
Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values</title>
<link>https://arxiv.org/abs/2512.10817</link>
<guid>https://arxiv.org/abs/2512.10817</guid>
<content:encoded><![CDATA[
arXiv:2512.10817v1 Announce Type: cross 
Abstract: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants</title>
<link>https://arxiv.org/abs/2512.10857</link>
<guid>https://arxiv.org/abs/2512.10857</guid>
<content:encoded><![CDATA[
arXiv:2512.10857v1 Announce Type: cross 
Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.10863</link>
<guid>https://arxiv.org/abs/2512.10863</guid>
<content:encoded><![CDATA[
arXiv:2512.10863v1 Announce Type: cross 
Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting</title>
<link>https://arxiv.org/abs/2512.10866</link>
<guid>https://arxiv.org/abs/2512.10866</guid>
<content:encoded><![CDATA[
arXiv:2512.10866v1 Announce Type: cross 
Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</title>
<link>https://arxiv.org/abs/2512.10922</link>
<guid>https://arxiv.org/abs/2512.10922</guid>
<content:encoded><![CDATA[
arXiv:2512.10922v1 Announce Type: cross 
Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Q-Chunking</title>
<link>https://arxiv.org/abs/2512.10926</link>
<guid>https://arxiv.org/abs/2512.10926</guid>
<content:encoded><![CDATA[
arXiv:2512.10926v1 Announce Type: cross 
Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models</title>
<link>https://arxiv.org/abs/2512.10932</link>
<guid>https://arxiv.org/abs/2512.10932</guid>
<content:encoded><![CDATA[
arXiv:2512.10932v1 Announce Type: cross 
Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any4D: Unified Feed-Forward Metric 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.10935</link>
<guid>https://arxiv.org/abs/2512.10935</guid>
<content:encoded><![CDATA[
arXiv:2512.10935v1 Announce Type: cross 
Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</title>
<link>https://arxiv.org/abs/2512.10936</link>
<guid>https://arxiv.org/abs/2512.10936</guid>
<content:encoded><![CDATA[
arXiv:2512.10936v1 Announce Type: cross 
Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2512.10938</link>
<guid>https://arxiv.org/abs/2512.10938</guid>
<content:encoded><![CDATA[
arXiv:2512.10938v1 Announce Type: cross 
Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(\alpha x + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis</title>
<link>https://arxiv.org/abs/2512.10940</link>
<guid>https://arxiv.org/abs/2512.10940</guid>
<content:encoded><![CDATA[
arXiv:2512.10940v1 Announce Type: cross 
Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mull-Tokens: Modality-Agnostic Latent Thinking</title>
<link>https://arxiv.org/abs/2512.10941</link>
<guid>https://arxiv.org/abs/2512.10941</guid>
<content:encoded><![CDATA[
arXiv:2512.10941v1 Announce Type: cross 
Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation</title>
<link>https://arxiv.org/abs/2512.10943</link>
<guid>https://arxiv.org/abs/2512.10943</guid>
<content:encoded><![CDATA[
arXiv:2512.10943v1 Announce Type: cross 
Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning</title>
<link>https://arxiv.org/abs/2512.10946</link>
<guid>https://arxiv.org/abs/2512.10946</guid>
<content:encoded><![CDATA[
arXiv:2512.10946v1 Announce Type: cross 
Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</title>
<link>https://arxiv.org/abs/2512.10949</link>
<guid>https://arxiv.org/abs/2512.10949</guid>
<content:encoded><![CDATA[
arXiv:2512.10949v1 Announce Type: cross 
Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Dataset Selection for High-Quality Data Sharing</title>
<link>https://arxiv.org/abs/2512.10952</link>
<guid>https://arxiv.org/abs/2512.10952</guid>
<content:encoded><![CDATA[
arXiv:2512.10952v1 Announce Type: cross 
Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</title>
<link>https://arxiv.org/abs/2512.10957</link>
<guid>https://arxiv.org/abs/2512.10957</guid>
<content:encoded><![CDATA[
arXiv:2512.10957v1 Announce Type: cross 
Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2306.01270</link>
<guid>https://arxiv.org/abs/2306.01270</guid>
<content:encoded><![CDATA[
arXiv:2306.01270v2 Announce Type: replace 
Abstract: Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuristic search planner used to create a global guiding path. During movement, the heuristic search planner replans new paths based on the instructions of the real-time planner. We tested our method in 10 different conflict scenarios. The experiments show that the planning performance of MAPPOHR is better than that of existing learning and heuristic methods. Due to the utilization of empirical knowledge and heuristic search, the learning efficiency of MAPPOHR is higher than that of existing learning methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Quantifier Selection in cvc5</title>
<link>https://arxiv.org/abs/2408.14338</link>
<guid>https://arxiv.org/abs/2408.14338</guid>
<content:encoded><![CDATA[
arXiv:2408.14338v2 Announce Type: replace 
Abstract: In this work we considerably improve the state-of-the-art SMT solving on first-order quantified problems by efficient machine learning guidance of quantifier selection. Quantifiers represent a significant challenge for SMT and are technically a source of undecidability. In our approach, we train an efficient machine learning model that informs the solver which quantifiers should be instantiated and which not. Each quantifier may be instantiated multiple times and the set of the active quantifiers changes as the solving progresses. Therefore, we invoke the ML predictor many times, during the whole run of the solver. To make this efficient, we use fast ML models based on gradient boosting decision trees. We integrate our approach into the state-of-the-art cvc5 SMT solver and show a considerable increase of the system's holdout-set performance after training it on a large set of first-order problems collected from the Mizar Mathematical Library.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generation Framework with Strict Constraints for Crystal Materials Design</title>
<link>https://arxiv.org/abs/2411.08464</link>
<guid>https://arxiv.org/abs/2411.08464</guid>
<content:encoded><![CDATA[
arXiv:2411.08464v3 Announce Type: replace 
Abstract: The design of crystal materials plays a critical role in areas such as new energy development, biomedical engineering, and semiconductors. Recent advances in data-driven methods have enabled the generation of diverse crystal structures. However, most existing approaches still rely on random sampling without strict constraints, requiring multiple post-processing steps to identify stable candidates with the desired physical and chemical properties. In this work, we present a new constrained generation framework that takes multiple constraints as input and enables the generation of crystal structures with specific chemical and properties. In this framework, intermediate constraints, such as symmetry information and composition ratio, are generated by a constraint generator based on large language models (LLMs), which considers the target properties. These constraints are then used by a subsequent crystal structure generator to ensure that the structure generation process is under control. Our method generates crystal structures with a probability of meeting the target properties that is more than twice that of existing approaches. Furthermore, nearly 100% of the generated crystals strictly adhere to predefined chemical composition, eliminating the risks of supply chain during production.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-centric proto-symbolic behavioural reasoning from pixels</title>
<link>https://arxiv.org/abs/2411.17438</link>
<guid>https://arxiv.org/abs/2411.17438</guid>
<content:encoded><![CDATA[
arXiv:2411.17438v3 Announce Type: replace 
Abstract: Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels -- ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, as well as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \land C)$ and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge</title>
<link>https://arxiv.org/abs/2504.01538</link>
<guid>https://arxiv.org/abs/2504.01538</guid>
<content:encoded><![CDATA[
arXiv:2504.01538v2 Announce Type: replace 
Abstract: While current AI-driven methods excel at deriving empirical models from individual experiments, a significant challenge remains in uncovering the common fundamental physics that underlie these models -- a task at which human physicists are adept. To bridge this gap, we introduce AI-Newton, a novel framework for concept-driven scientific discovery. Our system autonomously derives general physical laws directly from raw, multi-experiment data, operating without supervision or prior physical knowledge. Its core innovations are twofold: (1) proposing interpretable physical concepts to construct laws, and (2) progressively generalizing these laws to broader domains. Applied to a large, noisy dataset of mechanics experiments, AI-Newton successfully rediscovers foundational and universal laws, such as Newton's second law, the conservation of energy, and the universal gravitation. This work represents a significant advance toward autonomous, human-like scientific discovery.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data</title>
<link>https://arxiv.org/abs/2504.01951</link>
<guid>https://arxiv.org/abs/2504.01951</guid>
<content:encoded><![CDATA[
arXiv:2504.01951v2 Announce Type: replace 
Abstract: With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing AI Research Assistants with Expert-Involved Learning</title>
<link>https://arxiv.org/abs/2505.04638</link>
<guid>https://arxiv.org/abs/2505.04638</guid>
<content:encoded><![CDATA[
arXiv:2505.04638v3 Announce Type: replace 
Abstract: Large language models (LLMs) and large multimodal models (LMMs) promise to accelerate biomedical discovery, yet their reliability remains unclear. We introduce ARIEL (AI Research Assistant for Expert-in-the-Loop Learning), an open-source evaluation and optimization framework that pairs a curated multimodal biomedical corpus with expert-vetted tasks to probe two capabilities: full-length article summarization and fine-grained figure interpretation. Using uniform protocols and blinded PhD-level evaluation, we find that state-of-the-art models generate fluent but incomplete summaries, whereas LMMs struggle with detailed visual reasoning. We later observe that prompt engineering and lightweight fine-tuning substantially improve textual coverage, and a compute-scaled inference strategy enhances visual question answering. We build an ARIEL agent that integrates textual and visual cues, and we show it can propose testable mechanistic hypotheses. ARIEL delineates current strengths and limitations of foundation models, and provides a reproducible platform for advancing trustworthy AI in biomedicine.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.14381</link>
<guid>https://arxiv.org/abs/2505.14381</guid>
<content:encoded><![CDATA[
arXiv:2505.14381v2 Announce Type: replace 
Abstract: With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale</title>
<link>https://arxiv.org/abs/2505.14932</link>
<guid>https://arxiv.org/abs/2505.14932</guid>
<content:encoded><![CDATA[
arXiv:2505.14932v2 Announce Type: replace 
Abstract: Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables</title>
<link>https://arxiv.org/abs/2506.11375</link>
<guid>https://arxiv.org/abs/2506.11375</guid>
<content:encoded><![CDATA[
arXiv:2506.11375v2 Announce Type: replace 
Abstract: With the widespread application of multimodal large language models in scientific intelligence, there is an urgent need for more challenging evaluation benchmarks to assess their ability to understand complex scientific data. Scientific tables, as core carriers of knowledge representation, combine text, symbols, and graphics, forming a typical multimodal reasoning scenario. However, existing benchmarks are mostly focused on general domains, failing to reflect the unique structural complexity and domain-specific semantics inherent in scientific research. Chemical tables are particularly representative: they intertwine structured variables such as reagents, conditions, and yields with visual symbols like molecular structures and chemical formulas, posing significant challenges to models in cross-modal alignment and semantic parsing. To address this, we propose ChemTable-a large scale benchmark of chemical tables constructed from real-world literature, containing expert-annotated cell layouts, logical structures, and domain-specific labels. It supports two core tasks: (1) table recognition (structure and content extraction); and (2) table understanding (descriptive and reasoning-based question answering). Evaluation on ChemTable shows that while mainstream multimodal models perform reasonably well in layout parsing, they still face significant limitations when handling critical elements such as molecular structures and symbolic conventions. Closed-source models lead overall but still fall short of human-level performance. This work provides a realistic testing platform for evaluating scientific multimodal understanding, revealing the current bottlenecks in domain-specific reasoning and advancing the development of intelligent systems for scientific research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology</title>
<link>https://arxiv.org/abs/2506.18156</link>
<guid>https://arxiv.org/abs/2506.18156</guid>
<content:encoded><![CDATA[
arXiv:2506.18156v3 Announce Type: replace 
Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models</title>
<link>https://arxiv.org/abs/2506.20018</link>
<guid>https://arxiv.org/abs/2506.20018</guid>
<content:encoded><![CDATA[
arXiv:2506.20018v2 Announce Type: replace 
Abstract: This paper investigates real-time decision support systems that leverage low-latency AI models, bringing together recent progress in holistic AI-driven decision tools, integration with Edge-IoT technologies, and approaches for effective human-AI teamwork. It looks into how large language models can assist decision-making, especially when resources are limited. The research also examines the effects of technical developments such as DeLLMa, methods for compressing models, and improvements for analytics on edge devices, while also addressing issues like limited resources and the need for adaptable frameworks. Through a detailed review, the paper offers practical perspectives on development strategies and areas of application, adding to the field by pointing out opportunities for more efficient and flexible AI-supported systems. The conclusions set the stage for future breakthroughs in this fast-changing area, highlighting how AI can reshape real-time decision support.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotional Support with LLM-based Empathetic Dialogue Generation</title>
<link>https://arxiv.org/abs/2507.12820</link>
<guid>https://arxiv.org/abs/2507.12820</guid>
<content:encoded><![CDATA[
arXiv:2507.12820v2 Announce Type: replace 
Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data</title>
<link>https://arxiv.org/abs/2508.15432</link>
<guid>https://arxiv.org/abs/2508.15432</guid>
<content:encoded><![CDATA[
arXiv:2508.15432v3 Announce Type: replace 
Abstract: The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of Constraints and Objectives</title>
<link>https://arxiv.org/abs/2508.20978</link>
<guid>https://arxiv.org/abs/2508.20978</guid>
<content:encoded><![CDATA[
arXiv:2508.20978v3 Announce Type: replace 
Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARE: Scaling Up Agent Environments and Evaluations</title>
<link>https://arxiv.org/abs/2509.17158</link>
<guid>https://arxiv.org/abs/2509.17158</guid>
<content:encoded><![CDATA[
arXiv:2509.17158v2 Announce Type: replace 
Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</title>
<link>https://arxiv.org/abs/2510.18428</link>
<guid>https://arxiv.org/abs/2510.18428</guid>
<content:encoded><![CDATA[
arXiv:2510.18428v2 Announce Type: replace 
Abstract: Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic</title>
<link>https://arxiv.org/abs/2511.20586</link>
<guid>https://arxiv.org/abs/2511.20586</guid>
<content:encoded><![CDATA[
arXiv:2511.20586v3 Announce Type: replace 
Abstract: Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics, such as accuracy and precision, fail to appropriately capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the Parallel Trust Assessment System (PaTAS), a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through Trust Nodes and Trust Functions that propagate input, parameter, and activation trust across the network. The framework defines a Parameter Trust Update mechanism to refine parameter reliability during training and an Inference-Path Trust Assessment (IPTA) method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a foundation for evaluating model reliability across the AI lifecycle.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.04629</link>
<guid>https://arxiv.org/abs/2512.04629</guid>
<content:encoded><![CDATA[
arXiv:2512.04629v2 Announce Type: replace 
Abstract: Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging these capabilities, we further apply the model to multi-step retrosynthetic planning, achieving state-of-the-art performance on RetroBench and demonstrating its superior efficacy as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method</title>
<link>https://arxiv.org/abs/2411.00612</link>
<guid>https://arxiv.org/abs/2411.00612</guid>
<content:encoded><![CDATA[
arXiv:2411.00612v3 Announce Type: replace-cross 
Abstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \textbf{C}ontrastive Learning-based \textbf{L}ink \textbf{P}rediction model, \textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\%, 13.44\% in terms of AUC and AP, respectively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-like emergent properties in deep networks: impact of network architecture, datasets and training</title>
<link>https://arxiv.org/abs/2411.16326</link>
<guid>https://arxiv.org/abs/2411.16326</guid>
<content:encoded><![CDATA[
arXiv:2411.16326v3 Announce Type: replace-cross 
Abstract: Despite the rapid pace at which deep networks are improving on standardized vision benchmarks, they are still outperformed by humans on real-world vision tasks. One solution to this problem is to make deep networks more brain-like. Although there are several benchmarks that compare the ability of deep networks to predict brain responses on natural images, they do not capture subtle but important emergent properties present in brains. It is also unclear which design principle -- architecture, training data, or training regime -- would have the greatest impact on these emergent properties. To investigate these issues, we systematically evaluated over 30 state-of-the-art networks with varying network architectures, training datasets, and training regimes for the presence or absence of brain-like properties. Our main findings are as follows. First, network architecture had the strongest impact on brain-like properties compared to dataset and training regime variations. Second, networks varied widely in their alignment to the brain with no single network outperforming all others. Taken together, our results offer a principled and interpretable path toward closing the gap between artificial and human vision.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Reliability of Predictions in Detection Transformers: Object-Level Calibration and Image-Level Uncertainty</title>
<link>https://arxiv.org/abs/2412.01782</link>
<guid>https://arxiv.org/abs/2412.01782</guid>
<content:encoded><![CDATA[
arXiv:2412.01782v3 Announce Type: replace-cross 
Abstract: DETR and its variants have emerged as promising architectures for object detection, offering an end-to-end prediction pipeline. In practice, however, DETRs generate hundreds of predictions that far outnumber the actual objects present in an image. This raises a critical question: which of these predictions could be trusted? Addressing this concern, we provide empirical and theoretical evidence that predictions within the same image play distinct roles, resulting in varying reliability levels. Our analysis reveals that DETRs employ an optimal specialist strategy: one prediction per object is trained to be well-calibrated, while the remaining predictions are trained to suppress their foreground confidence to near zero, even when maintaining accurate localization. We show that this strategy emerges as the loss-minimizing solution to the Hungarian matching algorithm, fundamentally shaping DETRs' outputs. While selecting the well-calibrated predictions is ideal, they are unidentifiable at inference time. This means that any post-processing algorithm poses a risk of outputting a set of predictions with mixed calibration levels. Therefore, practical deployment necessitates a joint evaluation of both the model's calibration quality and the effectiveness of the post-processing algorithm. However, we demonstrate that existing metrics like average precision and expected calibration error are inadequate for this task. To address this issue, we further introduce Object-level Calibration Error (OCE): This object-centric design penalizes both retaining suppressed predictions and missed ground truth foreground objects, making OCE suitable for both evaluating models and identifying reliable prediction subsets. Finally, we present a post hoc uncertainty quantification framework that predicts per-image model accuracy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts</title>
<link>https://arxiv.org/abs/2412.02912</link>
<guid>https://arxiv.org/abs/2412.02912</guid>
<content:encoded><![CDATA[
arXiv:2412.02912v2 Announce Type: replace-cross 
Abstract: We introduce ShapeWords, an approach for synthesizing images based on 3D shape guidance and text prompts. ShapeWords incorporates target 3D shape information within specialized tokens embedded together with the input text, effectively blending 3D shape awareness with textual context to guide the image synthesis process. Unlike conventional shape guidance methods that rely on depth maps restricted to fixed viewpoints and often overlook full 3D structure or textual context, ShapeWords generates diverse yet consistent images that reflect both the target shape's geometry and the textual description. Experimental results show that ShapeWords produces images that are more text-compliant, aesthetically plausible, while also maintaining 3D shape awareness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense</title>
<link>https://arxiv.org/abs/2412.21051</link>
<guid>https://arxiv.org/abs/2412.21051</guid>
<content:encoded><![CDATA[
arXiv:2412.21051v4 Announce Type: replace-cross 
Abstract: The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided numerous benefits in our daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent advancements in the large language models (LLMs) offer promising solutions for security intelligence. By exploiting the powerful capabilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel defense architecture that proactively mitigates various DoS threats in cloud networks. LLM-PD can efficiently make decisions through comprehensive data analysis and sequential reasoning, as well as dynamically create and deploy actionable defense mechanisms. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. Our case study on three distinct DoS attacks demonstrates its remarkable ability in terms of defense effectiveness and efficiency when compared with other existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.01113</link>
<guid>https://arxiv.org/abs/2502.01113</guid>
<content:encoded><![CDATA[
arXiv:2502.01113v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation</title>
<link>https://arxiv.org/abs/2502.09884</link>
<guid>https://arxiv.org/abs/2502.09884</guid>
<content:encoded><![CDATA[
arXiv:2502.09884v3 Announce Type: replace-cross 
Abstract: We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first nonasymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\sqrt{n}$, which significantly improves on the rates of convergence in prior works.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling</title>
<link>https://arxiv.org/abs/2504.05537</link>
<guid>https://arxiv.org/abs/2504.05537</guid>
<content:encoded><![CDATA[
arXiv:2504.05537v2 Announce Type: replace-cross 
Abstract: Motion Transfer is a technique that synthesizes videos by transferring motion dynamics from a driving video to a source image. In this work we propose a deep learning-based framework to enable real-time video motion transfer which is critical for enabling bandwidth-efficient applications such as video conferencing, remote health monitoring, virtual reality interaction, and vision-based anomaly detection. This is done using keypoints which serve as semantically meaningful, compact representations of motion across time. To enable bandwidth savings during video transmission we perform forecasting of keypoints using two generative time series models VRNN and GRU-NF. The predicted keypoints are transformed into realistic video frames using an optical flow-based module paired with a generator network, thereby enabling efficient, low-frame-rate video transmission. Based on the application this allows the framework to either generate a deterministic future sequence or sample a diverse set of plausible futures. Experimental results demonstrate that VRNN achieves the best point-forecast fidelity (lowest MAE) in applications requiring stable and accurate multi-step forecasting and is particularly competitive in higher-uncertainty, multi-modal settings. This is achieved by introducing recurrently conditioned stochastic latent variables that carry past contexts to capture uncertainty and temporal variation. On the other hand the GRU-NF model enables richer diversity of generated videos while maintaining high visual quality. This is realized by learning an invertible, exact-likelihood mapping between the keypoints and their latent representations which supports rich and controllable sampling of diverse yet coherent keypoint sequences. Our work lays the foundation for next-generation AI systems that require real-time, bandwidth-efficient, and semantically controllable video generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving</title>
<link>https://arxiv.org/abs/2504.20101</link>
<guid>https://arxiv.org/abs/2504.20101</guid>
<content:encoded><![CDATA[
arXiv:2504.20101v4 Announce Type: replace-cross 
Abstract: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Online Class-Incremental Learning via Dual Classifiers</title>
<link>https://arxiv.org/abs/2504.20566</link>
<guid>https://arxiv.org/abs/2504.20566</guid>
<content:encoded><![CDATA[
arXiv:2504.20566v2 Announce Type: replace-cross 
Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Inclusive Separation for Online iNcremental learning (BISON), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BISON method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BISON, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation</title>
<link>https://arxiv.org/abs/2505.01730</link>
<guid>https://arxiv.org/abs/2505.01730</guid>
<content:encoded><![CDATA[
arXiv:2505.01730v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\approx$74\% on ImageNet with a 64$\times$ reduction in the number of inference timesteps compared to existing approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps</title>
<link>https://arxiv.org/abs/2505.10537</link>
<guid>https://arxiv.org/abs/2505.10537</guid>
<content:encoded><![CDATA[
arXiv:2505.10537v3 Announce Type: replace-cross 
Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning (Approximately) Equivariant Networks via Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.13631</link>
<guid>https://arxiv.org/abs/2505.13631</guid>
<content:encoded><![CDATA[
arXiv:2505.13631v2 Announce Type: replace-cross 
Abstract: Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forensic deepfake audio detection using segmental speech features</title>
<link>https://arxiv.org/abs/2505.13847</link>
<guid>https://arxiv.org/abs/2505.13847</guid>
<content:encoded><![CDATA[
arXiv:2505.13847v3 Announce Type: replace-cross 
Abstract: This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison (FVC) are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection using methods that are distinct from those employed in traditional FVC, and offer a new perspective on leveraging segmental features for this purpose. In addition, the present study proposes a speaker-specific framework for deepfake detection, which differs fundamentally from the speaker-independent systems that dominate current benchmarks. While speaker-independent frameworks aim at broad generalization, the speaker-specific approach offers advantages in forensic contexts where case-by-case interpretability and sensitivity to individual phonetic realization are essential.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.17012</link>
<guid>https://arxiv.org/abs/2505.17012</guid>
<content:encoded><![CDATA[
arXiv:2505.17012v2 Announce Type: replace-cross 
Abstract: Existing evaluations of multimodal large language models (MLLMs) on spatial intelligence are typically fragmented and limited in scope. In this work, we aim to conduct a holistic assessment of the spatial understanding capabilities of modern MLLMs and propose complementary data-driven and agent-based solutions. Specifically, we make the following contributions: (i) we introduce SpatialScore, to our knowledge, the most comprehensive and diverse benchmark for multimodal spatial intelligence to date. It covers multiple visual data types, input modalities, and question-answering formats, and contains approximately 5K manually verified samples spanning 30 distinct tasks; (ii) using SpatialScore, we extensively evaluate 40 representative MLLMs, revealing persistent challenges and a substantial gap between current models and human-level spatial intelligence; (iii) to advance model capabilities, we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks and significantly improves the performance of existing models (e.g., Qwen3-VL); (iv) to complement this data-driven route with a training-free paradigm, we develop SpatialAgent, a multi-agent system equipped with 12 specialized spatial perception tools that supports both Plan-Execute and ReAct reasoning, enabling substantial gains in spatial reasoning without additional model training. Extensive experiments and in-depth analyses demonstrate the effectiveness of our benchmark, corpus, and agent framework. We expect these resources to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence. All data, code, and models will be released to the research community.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[
arXiv:2505.17508v3 Announce Type: replace-cross 
Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a clipped-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. We extend our experiments to 8K context length, and RPG-REINFORCE with RPG-Style Clip achieves 52% accuracy on AIME25, surpassing the official Qwen3-4B-Instruct model (47%). Notably, RPG is a stable and scalable RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) clipped importance sampling, and (c) an iterative reference-policy update scheme.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskedManipulator: Versatile Whole-Body Manipulation</title>
<link>https://arxiv.org/abs/2505.19086</link>
<guid>https://arxiv.org/abs/2505.19086</guid>
<content:encoded><![CDATA[
arXiv:2505.19086v3 Announce Type: replace-cross 
Abstract: We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat</title>
<link>https://arxiv.org/abs/2506.01524</link>
<guid>https://arxiv.org/abs/2506.01524</guid>
<content:encoded><![CDATA[
arXiv:2506.01524v2 Announce Type: replace-cross 
Abstract: With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.01625</link>
<guid>https://arxiv.org/abs/2506.01625</guid>
<content:encoded><![CDATA[
arXiv:2506.01625v2 Announce Type: replace-cross 
Abstract: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
arXiv:2506.08001v4 Announce Type: replace-cross 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetry in Neural Network Parameter Spaces</title>
<link>https://arxiv.org/abs/2506.13018</link>
<guid>https://arxiv.org/abs/2506.13018</guid>
<content:encoded><![CDATA[
arXiv:2506.13018v3 Announce Type: replace-cross 
Abstract: Modern deep learning models are highly overparameterized, resulting in large sets of parameter configurations that yield the same outputs. A significant portion of this redundancy is explained by symmetries in the parameter space--transformations that leave the network function unchanged. These symmetries shape the loss landscape and constrain learning dynamics, offering a new lens for understanding optimization, generalization, and model complexity that complements existing theory of deep learning. This survey provides an overview of parameter space symmetry. We summarize existing literature, uncover connections between symmetry and learning theory, and identify gaps and opportunities in this emerging field.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
arXiv:2506.14933v2 Announce Type: replace-cross 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models</title>
<link>https://arxiv.org/abs/2507.13162</link>
<guid>https://arxiv.org/abs/2507.13162</guid>
<content:encoded><![CDATA[
arXiv:2507.13162v2 Announce Type: replace-cross 
Abstract: Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation</title>
<link>https://arxiv.org/abs/2508.03663</link>
<guid>https://arxiv.org/abs/2508.03663</guid>
<content:encoded><![CDATA[
arXiv:2508.03663v2 Announce Type: replace-cross 
Abstract: Reproducibility is a cornerstone of scientific validation and of the authority it confers on its results. Reproducibility in machine learning evaluations leads to greater trust, confidence, and value. However, the ground truth responses used in machine learning often necessarily come from humans, among whom disagreement is prevalent, and surprisingly little research has studied the impact of effectively ignoring disagreement in these responses, as is typically the case. One reason for the lack of research is that budgets for collecting human-annotated evaluation data are limited, and obtaining more samples from multiple raters for each example greatly increases the per-item annotation costs. We investigate the trade-off between the number of items ($N$) and the number of responses per item ($K$) needed for reliable machine learning evaluation. We analyze a diverse collection of categorical datasets for which multiple annotations per item exist, and simulated distributions fit to these datasets, to determine the optimal $(N, K)$ configuration, given a fixed budget ($N \times K$), for collecting evaluation data and reliably comparing the performance of machine learning models. Our findings show, first, that accounting for human disagreement may come with $N \times K$ at no more than 1000 (and often much lower) for every dataset tested on at least one metric. Moreover, this minimal $N \times K$ almost always occurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and $N$, or if one even existed, depends on the evaluation metric, with metrics that are more sensitive to the full distribution of responses performing better at higher levels of $K$. Our methods can be used to help ML practitioners get more effective test data by finding the optimal metrics and number of items and annotations per item to collect to get the most reliability for their budget.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title>
<link>https://arxiv.org/abs/2508.03772</link>
<guid>https://arxiv.org/abs/2508.03772</guid>
<content:encoded><![CDATA[
arXiv:2508.03772v5 Announce Type: replace-cross 
Abstract: Group Relative Policy Optimization (GRPO) is a promising policy-based approach for Large Language Model alignment, yet its performance is often limited by training instability and suboptimal convergence. In this paper, we identify and analyze two main GRPO issues: (i) the token-level penalization, where valuable tokens shared across different responses receive contradictory feedback signals, leading to conflicting gradient updates that can reduce their likelihood; and (ii) the policy collapse, where negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, destabilizing training process. To address these issues we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which prevents conflicting gradients on valuable tokens by skipping negative updates while amplifying positive ones and filters out completions whose entropy exceeds a provable threshold, to prevent policy collapse. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, as validated through multiple experiments on GSM8K, MATH, AIME 2024, AIME 2025 and AMC 2023.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</title>
<link>https://arxiv.org/abs/2508.08139</link>
<guid>https://arxiv.org/abs/2508.08139</guid>
<content:encoded><![CDATA[
arXiv:2508.08139v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</title>
<link>https://arxiv.org/abs/2508.08177</link>
<guid>https://arxiv.org/abs/2508.08177</guid>
<content:encoded><![CDATA[
arXiv:2508.08177v2 Announce Type: replace-cross 
Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Bounded Multi-Agent Visual Navigation via Iterative Risk Allocation</title>
<link>https://arxiv.org/abs/2509.08157</link>
<guid>https://arxiv.org/abs/2509.08157</guid>
<content:encoded><![CDATA[
arXiv:2509.08157v2 Announce Type: replace-cross 
Abstract: Safe navigation is essential for autonomous systems operating in hazardous environments, especially when multiple agents must coordinate using only high-dimensional visual observations. While recent approaches successfully combine Goal-Conditioned RL (GCRL) for graph construction with Conflict-Based Search (CBS) for planning, they typically rely on static edge pruning to enforce safety. This binary strategy is overly conservative, precluding feasible missions that require traversing high-risk regions, even when the aggregate risk is acceptable. To address this, we introduce a framework for Risk-Bounded Multi-Agent Path Finding (\problem{}), where agents share a user-specified global risk budget ($\Delta$). Rather than permanently discarding edges, our framework dynamically distributes per-agent risk budgets ($\delta_i$) during search via an Iterative Risk Allocation (IRA) layer that integrates with a standard CBS planner. We investigate two distribution strategies: a greedy surplus-deficit scheme for rapid feasibility repair, and a market-inspired mechanism that treats risk as a priced resource to guide improved allocation. This yields a tunable trade-off wherein agents exploit available risk to secure shorter, more efficient paths, but revert to longer, safer detours under tighter budgets. Experiments in complex visual environments show that, our dynamic allocation framework achieves higher success rates than baselines and effectively leverages the available safety budget to reduce travel time.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster Results from a Smarter Schedule: Reframing Collegiate Cross Country through Analysis of the National Running Club Database</title>
<link>https://arxiv.org/abs/2509.10600</link>
<guid>https://arxiv.org/abs/2509.10600</guid>
<content:encoded><![CDATA[
arXiv:2509.10600v3 Announce Type: replace-cross 
Abstract: Collegiate cross country teams often build their season schedules on intuition rather than evidence, partly because large-scale performance datasets are not publicly accessible. To address this limitation, we introduce the National Running Club Database (NRCD), the first openly available dataset to aggregate 23,725 race results from 7,594 collegiate club athletes across the 2023-2025 seasons. Unlike existing resources, NRCD includes detailed course metadata, allowing us to develop two standardized performance metrics: Converted Only (distance correction) and Standardized (distance, weather, and elevation adjusted). Using these standardized measures, we find that athletes with slower initial performances exhibit the greatest improvement within a season, and that race frequency is the strongest predictor of improvement. Using six machine learning models, random forest achieves the highest accuracy (r squared equals 0.92), revealing that athletes who race more frequently progress significantly faster than those who do not. At the team level, programs whose athletes race at least four times during the regular season have substantially higher odds of placing in the top 15 at nationals (chi-squared less than 0.01). These results challenge common coaching practices that favor minimal racing before championship meets. Our findings demonstrate that a data-informed scheduling strategy improves both individual development and team competitiveness. The NRCD provides a new foundation for evidence-based decision-making in collegiate cross country and opens opportunities for further research on standardized, longitudinal athlete performance modeling.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</title>
<link>https://arxiv.org/abs/2509.17552</link>
<guid>https://arxiv.org/abs/2509.17552</guid>
<content:encoded><![CDATA[
arXiv:2509.17552v3 Announce Type: replace-cross 
Abstract: The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the non-Clifford-count in unitary synthesis using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.21709</link>
<guid>https://arxiv.org/abs/2509.21709</guid>
<content:encoded><![CDATA[
arXiv:2509.21709v2 Announce Type: replace-cross 
Abstract: In this paper we study the potential of using reinforcement learning (RL) in order to synthesize quantum circuits, while optimizing the T-count and CS-count, of unitaries that are exactly implementable by the Clifford+T and Clifford+CS gate sets, respectively. We have designed our RL framework to work with channel representation of unitaries, that enables us to perform matrix operations efficiently, using integers only. We have also incorporated pruning heuristics and a canonicalization of operators, in order to reduce the search complexity. As a result, compared to previous works, we are able to implement significantly larger unitaries, in less time, with much better success rate and improvement factor. Our results for Clifford+T synthesis on two qubit unitaries achieve close-to-optimal decompositions for up to 100 T gates, 5 times more than previous RL algorithms and to the best of our knowledge, the largest instances achieved with any method to date. Our RL algorithm is able to recover previously-known optimal linear complexity algorithm for T-count-optimal decomposition of 1 qubit unitaries. We illustrate significant reduction in the asymptotic T-count estimate of important primitives like controlled cyclic shift (43%), controlled adder (14.3%) and multiplier (14%), without adding any extra ancilla. For 2-qubit Clifford+CS unitaries, our algorithm achieves a linear complexity, something that could only be accomplished by a previous algorithm using SO(6) representation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks</title>
<link>https://arxiv.org/abs/2509.22258</link>
<guid>https://arxiv.org/abs/2509.22258</guid>
<content:encoded><![CDATA[
arXiv:2509.22258v2 Announce Type: replace-cross 
Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models</title>
<link>https://arxiv.org/abs/2509.24510</link>
<guid>https://arxiv.org/abs/2509.24510</guid>
<content:encoded><![CDATA[
arXiv:2509.24510v3 Announce Type: replace-cross 
Abstract: Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Personalized Deep Research: Benchmarks and Evaluations</title>
<link>https://arxiv.org/abs/2509.25106</link>
<guid>https://arxiv.org/abs/2509.25106</guid>
<content:encoded><![CDATA[
arXiv:2509.25106v2 Announce Type: replace-cross 
Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing benchmarks primarily evaluate DRAs on generic quality metrics and overlook personalization, a critical dimension for individual users. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench (PDR-Bench), the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures Personalization Alignment, Content Quality, and Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalizable Shape Completion with SIM(3) Equivariance</title>
<link>https://arxiv.org/abs/2509.26631</link>
<guid>https://arxiv.org/abs/2509.26631</guid>
<content:encoded><![CDATA[
arXiv:2509.26631v3 Announce Type: replace-cross 
Abstract: 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction</title>
<link>https://arxiv.org/abs/2510.04522</link>
<guid>https://arxiv.org/abs/2510.04522</guid>
<content:encoded><![CDATA[
arXiv:2510.04522v2 Announce Type: replace-cross 
Abstract: Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Is All You Price</title>
<link>https://arxiv.org/abs/2510.09859</link>
<guid>https://arxiv.org/abs/2510.09859</guid>
<content:encoded><![CDATA[
arXiv:2510.09859v3 Announce Type: replace-cross 
Abstract: We build a mechanism design framework where a platform designs GenAI models to screen users who obtain instrumental value from the generated conversation and privately differ in their preference for latency. We show that the revenue-optimal mechanism is simple: deploy a single aligned (user-optimal) model and use token cap as the only instrument to screen the user. The design decouples model training from pricing, is readily implemented with token metering, and mitigates misalignment pressures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots</title>
<link>https://arxiv.org/abs/2510.16069</link>
<guid>https://arxiv.org/abs/2510.16069</guid>
<content:encoded><![CDATA[
arXiv:2510.16069v2 Announce Type: replace-cross 
Abstract: As design thinking education grows in secondary and tertiary contexts, educators face the challenge of evaluating creative artefacts that combine visual and textual elements. Traditional rubric-based assessment is laborious, time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in large, multi-section cohorts. This paper presents an exploratory study investigating the reliability and perceived accuracy of AI-assisted assessment compared to TA-assisted assessment in evaluating student posters in design thinking education. Two activities were conducted with 33 Ministry of Education (MOE) Singapore school teachers to (1) compare AI-generated scores with TA grading across three key dimensions: empathy and user understanding, identification of pain points and opportunities, and visual communication, and (2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid scores. Results showed low statistical agreement between instructor and AI scores for empathy and pain points, with slightly higher alignment for visual communication. Teachers preferred TA-assigned scores in six of ten samples. Qualitative feedback highlighted the potential of AI for formative feedback, consistency, and student self-reflection, but raised concerns about its limitations in capturing contextual nuance and creative insight. The study underscores the need for hybrid assessment models that integrate computational efficiency with human insights. This research contributes to the evolving conversation on responsible AI adoption in creative disciplines, emphasizing the balance between automation and human judgment for scalable and pedagogically sound assessment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</title>
<link>https://arxiv.org/abs/2510.19195</link>
<guid>https://arxiv.org/abs/2510.19195</guid>
<content:encoded><![CDATA[
arXiv:2510.19195v3 Announce Type: replace-cross 
Abstract: Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An efficient probabilistic hardware architecture for diffusion-like models</title>
<link>https://arxiv.org/abs/2510.23972</link>
<guid>https://arxiv.org/abs/2510.23972</guid>
<content:encoded><![CDATA[
arXiv:2510.23972v2 Announce Type: replace-cross 
Abstract: The proliferation of probabilistic AI has prompted proposals for specialized stochastic computers. Despite promising efficiency gains, these proposals have failed to gain traction because they rely on fundamentally limited modeling techniques and exotic, unscalable hardware. In this work, we address these shortcomings by proposing an all-transistor probabilistic computer that implements powerful denoising models at the hardware level. A system-level analysis indicates that devices based on our architecture could achieve performance parity with GPUs on a simple image benchmark using approximately 10,000 times less energy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
<link>https://arxiv.org/abs/2511.00090</link>
<guid>https://arxiv.org/abs/2511.00090</guid>
<content:encoded><![CDATA[
arXiv:2511.00090v3 Announce Type: replace-cross 
Abstract: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnitude-Modulated Equivariant Adapter for Parameter-Efficient Fine-Tuning of Equivariant Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.06696</link>
<guid>https://arxiv.org/abs/2511.06696</guid>
<content:encoded><![CDATA[
arXiv:2511.06696v2 Announce Type: replace-cross 
Abstract: Pretrained equivariant graph neural networks based on spherical harmonics offer efficient and accurate alternatives to computationally expensive ab-initio methods, yet adapting them to new tasks and chemical environments still requires fine-tuning. Conventional parameter-efficient fine-tuning (PEFT) techniques, such as Adapters and LoRA, typically break symmetry, making them incompatible with those equivariant architectures. ELoRA, recently proposed, is the first equivariant PEFT method. It achieves improved parameter efficiency and performance on many benchmarks. However, the relatively high degrees of freedom it retains within each tensor order can still perturb pretrained feature distributions and ultimately degrade performance. To address this, we present Magnitude-Modulated Equivariant Adapter (MMEA), a novel equivariant fine-tuning method which employs lightweight scalar gating to modulate feature magnitudes on a per-order and per-multiplicity basis. We demonstrate that MMEA preserves strict equivariance and, across multiple benchmarks, consistently improves energy and force predictions to state-of-the-art levels while training fewer parameters than competing approaches. These results suggest that, in many practical scenarios, modulating channel magnitudes is sufficient to adapt equivariant models to new chemical environments without breaking symmetry, pointing toward a new paradigm for equivariant PEFT design.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</title>
<link>https://arxiv.org/abs/2511.14396</link>
<guid>https://arxiv.org/abs/2511.14396</guid>
<content:encoded><![CDATA[
arXiv:2511.14396v2 Announce Type: replace-cross 
Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak</title>
<link>https://arxiv.org/abs/2511.14566</link>
<guid>https://arxiv.org/abs/2511.14566</guid>
<content:encoded><![CDATA[
arXiv:2511.14566v2 Announce Type: replace-cross 
Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.16203</link>
<guid>https://arxiv.org/abs/2511.16203</guid>
<content:encoded><![CDATA[
arXiv:2511.16203v3 Announce Type: replace-cross 
Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2511.17844</link>
<guid>https://arxiv.org/abs/2511.17844</guid>
<content:encoded><![CDATA[
arXiv:2511.17844v2 Announce Type: replace-cross 
Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Ahead: Foresight Intelligence in MLLMs and World Models</title>
<link>https://arxiv.org/abs/2511.18735</link>
<guid>https://arxiv.org/abs/2511.18735</guid>
<content:encoded><![CDATA[
arXiv:2511.18735v2 Announce Type: replace-cross 
Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HunyuanOCR Technical Report</title>
<link>https://arxiv.org/abs/2511.19575</link>
<guid>https://arxiv.org/abs/2511.19575</guid>
<content:encoded><![CDATA[
arXiv:2511.19575v2 Announce Type: replace-cross 
Abstract: This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dark Speculation: Combining Qualitative and Quantitative Understanding in Frontier AI Risk Analysis</title>
<link>https://arxiv.org/abs/2511.21838</link>
<guid>https://arxiv.org/abs/2511.21838</guid>
<content:encoded><![CDATA[
arXiv:2511.21838v2 Announce Type: replace-cross 
Abstract: Estimating catastrophic harms from frontier AI is hindered by deep ambiguity: many of its risks are not only unobserved but unanticipated by analysts. The central limitation of current risk analysis is the inability to populate the $\textit{catastrophic event space}$, or the set of potential large-scale harms to which probabilities might be assigned. This intractability is worsened by the $\textit{Lucretius problem}$, or the tendency to infer future risks only from past experience. We propose a process of $\textit{dark speculation}$, in which systematically generating and refining catastrophic scenarios ("qualitative" work) is coupled with estimating their likelihoods and associated damages (quantitative underwriting analysis). The idea is neither to predict the future nor to enable insurance for its own sake, but to use narrative and underwriting tools together to generate probability distributions over outcomes. We formalize this process using a simplified catastrophic L\'{e}vy stochastic framework and propose an iterative institutional design in which (1) speculation (including scenario planning) generates detailed catastrophic event narratives, (2) insurance underwriters assign probabilistic and financial parameters to these narratives, and (3) decision-makers synthesize the results into summary statistics to inform judgment. Analysis of the model reveals the value of (a) maintaining independence between speculation and underwriting, (b) analyzing multiple risk categories in parallel, and (c) generating "thick" catastrophic narrative rich in causal (counterfactual) and mitigative detail. While the approach cannot eliminate deep ambiguity, it offers a systematic approach to reason about extreme, low-probability events in frontier AI, tempering complacency and overreaction. The framework is adaptable for iterative use and can be further augmented with AI systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational analysis of determinantal varieties</title>
<link>https://arxiv.org/abs/2511.22613</link>
<guid>https://arxiv.org/abs/2511.22613</guid>
<content:encoded><![CDATA[
arXiv:2511.22613v2 Announce Type: replace-cross 
Abstract: Determinantal varieties -- the sets of bounded-rank matrices or tensors -- have attracted growing interest in low-rank optimization. The tangent cone to low-rank sets is widely studied and underpins a range of geometric methods. The second-order geometry, which encodes curvature information, is more intricate. In this work, we develop a unified framework to derive explicit formulas for both first- and second-order tangent sets to various low-rank sets, including low-rank matrices, tensors, symmetric matrices, and positive semidefinite matrices. The framework also accommodates the intersection of a low-rank set and another set satisfying mild assumptions, thereby yielding a tangent intersection rule. Through the lens of tangent sets, we establish a necessary and sufficient condition under which a nonsmooth problem and its smooth parameterization share equivalent second-order stationary points. Moreover, we exploit tangent sets to characterize optimality conditions for low-rank optimization and prove that verifying second-order optimality is NP-hard. In a separate line of analysis, we investigate variational geometry of the graph of the normal cone to matrix varieties, deriving the explicit Bouligand tangent cone, Fr\'echet and Mordukhovich normal cones to the graph. These results are further applied to develop optimality conditions for low-rank bilevel programs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Molecular Language Models (HMLMs)</title>
<link>https://arxiv.org/abs/2512.00696</link>
<guid>https://arxiv.org/abs/2512.00696</guid>
<content:encoded><![CDATA[
arXiv:2512.00696v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is reshaping computational and network biology by enabling new approaches to decode cellular communication networks. We introduce Hierarchical Molecular Language Models (HMLMs), a novel framework that models cellular signaling as a specialized molecular language, where signaling molecules function as tokens, protein interactions define syntax, and functional consequences constitute semantics. HMLMs employ a transformer-based architecture adapted to accommodate graph-structured signaling networks through information transducers, mathematical entities that capture how molecules receive, process, and transmit signals. The architecture integrates multi-modal data sources across molecular, pathway, and cellular scales through hierarchical attention mechanisms and scale-bridging operators that enable information flow across biological hierarchies. Applied to a complex network of cardiac fibroblast signaling, HMLMs outperformed traditional approaches in temporal dynamics prediction, particularly under sparse sampling conditions. Attention-based analysis revealed biologically meaningful crosstalk patterns, including previously uncharacterized interactions between signaling pathways. By bridging molecular mechanisms with cellular phenotypes through AI-driven molecular language representation, HMLMs establish a foundation for biology-oriented large language models (LLMs) that could be pre-trained on comprehensive pathway datasets and applied across diverse signaling systems and tissues, advancing precision medicine and therapeutic discovery.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</title>
<link>https://arxiv.org/abs/2512.03040</link>
<guid>https://arxiv.org/abs/2512.03040</guid>
<content:encoded><![CDATA[
arXiv:2512.03040v2 Announce Type: replace-cross 
Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.03454</link>
<guid>https://arxiv.org/abs/2512.03454</guid>
<content:encoded><![CDATA[
arXiv:2512.03454v2 Announce Type: replace-cross 
Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persona-based Multi-Agent Collaboration for Brainstorming</title>
<link>https://arxiv.org/abs/2512.04488</link>
<guid>https://arxiv.org/abs/2512.04488</guid>
<content:encoded><![CDATA[
<div> Keywords: persona-based agents, multi-agent collaboration, brainstorming diversity, agent-to-agent dynamics, idea generation depth<br /><br />Summary: This paper emphasizes the significance of persona-based multi-agent brainstorming in generating diverse and comprehensive ideas across various topics and subject matters. First, it confirms that prior research supports the notion that multi-agent collaboration yields superior reasoning compared to single-agent approaches. Second, it introduces a novel framework centered on persona-based agent selection, illustrating how curating agents by their domain personas can enhance brainstorming effectiveness. Third, through multiple experimental setups, the study evaluates idea generation outcomes based on different persona pairings, such as Doctor versus VR Engineer, and explores different interaction modes among agents, including separate, together, and separate-then-together dynamics. Fourth, findings reveal that the choice of personas significantly influences the domains of ideas produced, shaping the thematic direction of brainstorming sessions. Fifth, the mode of collaboration—whether agents interact separately, simultaneously, or sequentially—affects the diversity and range of ideas generated. Overall, persona-driven multi-agent brainstorming not only increases idea diversity but also contributes to greater idea depth and cross-domain coverage, demonstrating the value of strategically selecting and orchestrating agents to optimize creative ideation processes. <div>
arXiv:2512.04488v2 Announce Type: replace 
Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study</title>
<link>https://arxiv.org/abs/2512.09088</link>
<guid>https://arxiv.org/abs/2512.09088</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucinations, Trust Calibration, User Expertise, Intuition, Contextual Factors<br /><br />Summary: This paper explores how hallucinations—factually incorrect yet plausible outputs from Large Language Models (LLMs)—impact users' trust and interactions with these models. The authors conducted a qualitative study involving 192 participants to understand real-world effects. Their findings reveal that hallucinations do not cause users to distrust LLMs universally; instead, users adjust their trust based on context, demonstrating what is called context-sensitive trust calibration. The study builds upon prior trust models, confirming several human-related trust factors including expectancy, prior user experience, and user expertise and domain knowledge. Importantly, the research identifies intuition as a newly recognized factor important for detecting hallucinations. The authors also highlight that trust dynamics depend heavily on contextual elements such as perceived risk and the stakes associated with decisions made using LLM outputs. In line with the recursive trust calibration process proposed by Blöbaum, this work extends the model by integrating intuition as a user-related factor. Ultimately, the paper offers practical recommendations aimed at encouraging responsible and reflective use of LLMs, helping users to better manage their trust and interactions in light of potential hallucinations. <div>
arXiv:2512.09088v1 Announce Type: new 
Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Bl\"obaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance</title>
<link>https://arxiv.org/abs/2512.09114</link>
<guid>https://arxiv.org/abs/2512.09114</guid>
<content:encoded><![CDATA[
<div> Keywords: AI governance, risk assessment, AI frameworks, operationalization, trustworthy AI<br /><br />Summary:<br /><br />The article discusses three major governance challenges that hinder the effective deployment of AI systems. First, organizations face difficulties conducting adequate risk assessments tailored to specific AI use cases, which can lead to serious consequences such as bias and high error rates, illustrated by the Humana healthcare claim denial case. Each AI deployment carries distinct risks requiring customized governance, yet existing frameworks mostly offer generic guidance. Second, current standards like ISO 42001 and NIST AI RMF stay at a conceptual level, presenting high-level principles without actionable controls, making it challenging for practitioners to implement concrete technical measures to meet governance requirements. Third, organizations lack scalable mechanisms to embed trustworthy AI practices throughout the AI system development lifecycle. There is no systematic approach to quantitatively measure compliance or to deliver appropriate visibility tailored to diverse stakeholders, from board members to data scientists. To address these gaps, the authors introduce AI TIPS (Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0), an updated operational framework originating from a 2019 model developed prior to the NIST framework. AI TIPS offers actionable, use-case-specific risk assessment, technical controls, and scalable governance practices, aiming to enhance trust and sustainability in AI deployment. <div>
arXiv:2512.09114v1 Announce Type: new 
Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem</title>
<link>https://arxiv.org/abs/2512.09117</link>
<guid>https://arxiv.org/abs/2512.09117</guid>
<content:encoded><![CDATA[
<div> Keywords: categorical framework, symbol grounding problem, large language models, truth evaluation, possible worlds W<br /><br />Summary: This paper introduces a formal categorical framework designed to analyze the process by which humans and large language models (LLMs) convert content into truth-evaluated propositions. The framework is based on a state space of possible worlds denoted as W, which models varying conditions or scenarios under consideration. Using this structure, the study explores the mechanisms behind content transformation and truth assessment in both human cognition and artificial language models. The main argument centers on the claim that LLMs do not fundamentally solve the symbol grounding problem—the challenge of how symbols obtain meaning in relation to the real world—but rather circumvent it. This means that while LLMs can manipulate symbols to produce contextually appropriate outputs, they lack genuine semantic grounding tied to actual states of the world. The categorical approach offers a rigorous, mathematical perspective to contrast human understanding with mechanistic language operations in LLMs. By formalizing these differences, the paper contributes to ongoing discussions on the limits of current AI models in achieving human-like comprehension and meaning representation. The results underscore the conceptual gap between symbol processing and true semantic interpretation within AI systems. <div>
arXiv:2512.09117v1 Announce Type: new 
Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2512.09142</link>
<guid>https://arxiv.org/abs/2512.09142</guid>
<content:encoded><![CDATA[
<div> Keywords: dialog generation, evaluation, mechanistic interpretability, multi-agent simulation, audio generation<br /><br />Summary:<br /><br />We present SDialog, an open-source Python toolkit licensed under MIT that unifies dialog generation, evaluation, and mechanistic interpretability into one comprehensive framework designed for LLM-based conversational agents. The toolkit is built around a standardized Dialog representation that simplifies interaction management. First, it offers persona-driven multi-agent simulation with composable orchestration, enabling controlled and synthetic dialog generation to model varied conversational scenarios. Second, it supports comprehensive evaluation by combining traditional linguistic metrics, LLM-as-a-judge approaches, and functional correctness validators to assess the quality and accuracy of generated dialogs. Third, SDialog includes mechanistic interpretability tools that allow users to inspect activations and steer model behavior through techniques such as feature ablation and induction, helping to understand underlying model mechanisms. Fourth, the toolkit enables audio generation with full acoustic simulation, incorporating 3D room modeling and microphone effects for realistic auditory scene creation. Additionally, SDialog integrates seamlessly with all major LLM backends, facilitating mixed-backend experiments via a unified API. By combining generation, evaluation, and interpretability in a dialog-centric architecture, SDialog provides researchers a systematic and versatile environment to build, benchmark, and understand conversational systems effectively. <div>
arXiv:2512.09142v1 Announce Type: new 
Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration</title>
<link>https://arxiv.org/abs/2512.09340</link>
<guid>https://arxiv.org/abs/2512.09340</guid>
<content:encoded><![CDATA[
<div> Keywords: human perception, deep neural networks, ambiguous stimuli, cognitive architectures, neuro-symbolic AI<br /><br />Summary:<br /><br />This paper investigates how humans and AI systems label ambiguous and low-resolution images, aiming to uncover differences and similarities in perception, reasoning, and decision-making processes. It emphasizes human strategies such as analogical reasoning, shape recognition, and confidence modulation, contrasting these with AI’s feature-based image processing methods. The study is framed through influential theories including Marr’s tri-level hypothesis on vision, Simon’s bounded rationality, and Thagard’s concepts of representation and emotion. The authors analyze human participant responses alongside Grad-CAM visualizations that reveal where AI models focus attention during image classification. Further, cognitive architectures like ACT-R and Soar are employed to model human heuristic and layered decision strategies under uncertain conditions. The results illuminate both parallels and divergences in how biological and artificial systems represent information, infer meaning, and calibrate confidence in choices. Based on these insights, the paper advocates for future development of neuro-symbolic AI architectures that integrate structured symbolic reasoning with connectionist neural networks. Such hybrid systems, shaped by principles of embodiment, explainability, and alignment with cognitive science, promise AI that is not only effective but also interpretable and grounded in human-like cognition. <div>
arXiv:2512.09340v1 Announce Type: new 
Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architectures for Building Agentic AI</title>
<link>https://arxiv.org/abs/2512.09458</link>
<guid>https://arxiv.org/abs/2512.09458</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, reliability, architectural design, tool-using agents, runtime governance  

<br /><br />Summary:  
This chapter argues that the reliability of agentic and generative AI primarily depends on architectural design. Agentic systems are defined as goal-directed, tool-using decision makers functioning in closed loops. Reliability is shown to arise from principled componentization, including key components like goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, and telemetry. Disciplined interfaces contribute to robustness through schema-constrained, validated, and least-privilege tool calls. Explicit control and assurance loops further enhance dependability. Building on classical foundations, the chapter proposes a practical taxonomy of agent types: tool-using agents, memory-augmented agents, planning and self-improving agents, multi-agent systems, and embodied or web agents. Each pattern is analyzed for its influence on reliability boundaries and failure modes. Design guidance is distilled encompassing typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, and runtime governance mechanisms such as budget management and termination conditions. Additionally, simulate-before-actuate safeguards are emphasized to prevent erroneous operations, thus strengthening the reliability envelope of agentic AI systems. <div>
arXiv:2512.09458v1 Announce Type: new 
Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular generation, reinforcement learning, Monte Carlo tree search, drug discovery, fragment-based modeling<br /><br />Summary:  
Drug discovery faces challenges such as high costs, low success rates, and limited scalability in traditional screening methods. Recent generative models (autoregressive, diffusion, flow-based) have advanced de novo ligand design but suffer from poor generalization, limited interpretability, and overemphasis on binding affinity, neglecting other pharmacological properties.  
The paper introduces Trio, a novel molecular generation framework that integrates fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search. Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and balances exploration of novel chemotypes with exploitation of promising intermediates targeting protein binding pockets.  
Experimental evaluation shows that Trio reliably produces chemically valid ligands with improved pharmacological profiles. It outperforms state-of-the-art methods by increasing binding affinity by 7.85%, drug-likeness by 11.10%, and synthetic accessibility by 12.05%. Additionally, the approach expands molecular diversity by more than four times compared to prior models.  
This framework addresses limitations of previous generative approaches by enhancing interpretability and balancing multiple drug discovery objectives, presenting an effective and scalable solution for closed-loop targeted molecular design. <div>
arXiv:2512.09566v1 Announce Type: new 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-end Planning Framework with Agentic LLMs and PDDL</title>
<link>https://arxiv.org/abs/2512.09629</link>
<guid>https://arxiv.org/abs/2512.09629</guid>
<content:encoded><![CDATA[
<div> Planning, Large Language Models, PDDL, Verification, Natural Language  

<br /><br />Summary:  
1. The paper introduces an end-to-end planning framework that leverages verifiers to ensure correctness and completeness of plans.  
2. It uses an orchestrator that translates human natural language specifications into PDDL models, which are iteratively refined by specialized agents addressing planning constraints such as timing, optimality, and ambiguities.  
3. All components, including the orchestrator and agents, are powered by Large Language Models (LLMs) and operate without human intervention at any stage.  
4. The validated PDDL model is executed by an external planning engine to generate plans, which are subsequently translated back into natural language to improve human readability while preserving step accuracy.  
5. The framework has been tested across diverse benchmarks such as Google NaturalPlan, PlanBench, and classic planning problems like Blocksworld and Tower of Hanoi, including integration with popular PDDL engines and validators like Fast Downward, LPG, POPF, VAL, and uVAL.  
6. This approach shows significant flexibility and effectiveness, marking progress toward fully automated, LLM-aided planning systems that bridge human intent and machine-executable plans through an end-to-end pipeline. <div>
arXiv:2512.09629v1 Announce Type: new 
Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions</title>
<link>https://arxiv.org/abs/2512.09727</link>
<guid>https://arxiv.org/abs/2512.09727</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Tree Search, Root-Parallel, Continuous Action Spaces, Gaussian Process Regression, Value Estimation<br /><br />Summary:<br /><br />This paper addresses the challenge of aggregating statistics from multiple threads in root-parallel Monte Carlo Tree Search (MCTS) when applied to continuous action spaces. The authors propose a novel method that leverages Gaussian Process Regression (GPR) to estimate the value of promising actions that have not yet been directly trialed in the environment, thereby improving action value estimation beyond observed samples. Their approach integrates GPR into the aggregation process of parallel threads, enabling better-informed decision-making under limited wall-clock time constraints. The method is systematically evaluated across six diverse benchmark domains to test its effectiveness and generality. Experimental results demonstrate that their GPR-based aggregation strategy consistently outperforms existing methods in terms of performance metrics. While introducing GPR increases inference time modestly, the trade-off is justified by notable gains in planning efficiency and accuracy. This advancement provides a practical solution to enhance root-parallel MCTS in continuous action domains, which are inherently more challenging due to infinite action possibilities. Ultimately, the paper contributes a significant improvement to online planning algorithms, particularly useful where computational time is restricted but high-quality decisions are essential. <div>
arXiv:2512.09727v1 Announce Type: new 
Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation</title>
<link>https://arxiv.org/abs/2512.09736</link>
<guid>https://arxiv.org/abs/2512.09736</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Path Finding, kinodynamic modeling, planner design, execution performance, optimality

<br /><br />Summary:  
This work addresses the gap between traditional Multi-Agent Path Finding (MAPF) algorithm evaluations and their real-world applications by focusing on more realistic execution environments found in industrial settings such as warehouses and manufacturing facilities. First, it analyzes the relationship between solution optimality—that is, how mathematically optimal the planned paths are—and their actual execution performance when physical constraints and robot dynamics are considered. Second, the study explores how inaccuracies or errors in kinodynamic modeling (which captures physical and dynamic constraints of robots) impact overall system performance, emphasizing the sensitivity of real-world deployments to such modeling imperfections. Third, it investigates the interplay between the accuracy of kinodynamic models and the optimality of the planned paths, to understand how these factors jointly influence execution outcomes. The empirical evaluation leverages frameworks like SMART, which enable large-scale, realistic MAPF testing beyond simplified robot motion assumptions. By examining these fundamental design choices under realistic simulation settings, the paper identifies critical challenges and opens new research directions aimed at improving MAPF planners’ reliability and effectiveness for practical, real-world robot deployment scenarios. <div>
arXiv:2512.09736v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09829</link>
<guid>https://arxiv.org/abs/2512.09829</guid>
<content:encoded><![CDATA[
<div> Keywords: RIFT, fault assessment, reinforcement learning, AI accelerators, Large Language Models<br /><br />Summary:<br /><br />This paper addresses the challenges of fault assessment in large-scale AI accelerators, where traditional methods suffer from high computational costs and inadequate fault coverage. It introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a novel and scalable framework designed to automate the identification of minimal yet high-impact fault scenarios for efficient design-time fault evaluation. RIFT reframes the search for worst-case faults as a sequential decision-making problem, leveraging hybrid sensitivity analysis to prune the search space and reinforcement learning to intelligently generate concise test suites. The framework was evaluated on billion-parameter Large Language Model workloads using NVIDIA A100 GPUs, showing a 2.2× faster fault assessment compared to evolutionary approaches and reducing test vector volume by over 99% against random fault injection, while delivering superior fault coverage. Furthermore, RIFT provides actionable insights enabling targeted hardware protection strategies, demonstrated by a 12.8× improvement in cost-effectiveness for selective error correction code versus uniform triple modular redundancy. Finally, RIFT automatically produces UVM-compliant verification artifacts, facilitating seamless integration into commercial RTL verification workflows, thereby offering a practical and efficient solution to fault assessment challenges in modern AI hardware design. <div>
arXiv:2512.09829v1 Announce Type: new 
Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning</title>
<link>https://arxiv.org/abs/2512.09831</link>
<guid>https://arxiv.org/abs/2512.09831</guid>
<content:encoded><![CDATA[
<div> Keywords: belief modeling, cognitive heterogeneity, value spaces, communication, leadership

<br /><br />Summary:  
This paper introduces a geometric framework to model belief, motivation, and influence among cognitively diverse agents by representing each agent with a personalized value space—a vector space encoding how they interpret and evaluate meaning. Within this setting, beliefs are conceptualized as structured vectors called abstract beings, whose transmission depends on linear interpretation maps between these value spaces. A belief only survives communication if it does not fall into the null spaces of these maps, providing a formal condition for when understanding succeeds, fails, or beliefs cease to exist. The framework explains phenomena such as belief distortion, motivational drift, and counterfactual reasoning entirely through algebraic constraints. A key theoretical contribution is the "No-Null-Space Leadership Condition," which defines leadership in terms of representational reachability rather than traditional notions like persuasion or authority. More broadly, the model captures how abstract beliefs propagate, mutate, or vanish within heterogeneous cognitive landscapes. By grounding meaning preservation in structural compatibility—rather than shared information or rationality—it integrates ideas from conceptual spaces, social epistemology, and AI value alignment. This cognitive-geometric perspective clarifies epistemic limits of influence for both humans and artificial systems and provides a versatile foundation for analyzing belief dynamics across diverse agents. <div>
arXiv:2512.09831v1 Announce Type: new 
Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing</title>
<link>https://arxiv.org/abs/2512.09882</link>
<guid>https://arxiv.org/abs/2512.09882</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, cybersecurity professionals, vulnerability discovery, multi-agent framework, cost efficiency  

<br /><br />Summary:  
This paper presents the first extensive comparison of AI agents and human cybersecurity professionals within a live enterprise network environment comprising approximately 8,000 hosts across 12 subnets. The study involved ten human cybersecurity experts, six existing AI agents, and a novel AI framework named ARTEMIS, designed with dynamic prompt generation, flexible sub-agents, and automated vulnerability triaging. ARTEMIS achieved second place overall by identifying nine valid vulnerabilities with an 82% valid submission rate, outperforming nine out of ten human participants. In contrast, other AI scaffolds like Codex and CyAgent underperformed compared to most human testers. ARTEMIS displayed technical competence and submission quality on par with the top human performers. The study highlights AI agents’ strengths in systematic enumeration, parallel exploitation, and cost-effectiveness, noting ARTEMIS variants operating at $18/hour versus $60/hour for human penetration testers. However, critical gaps remain: AI agents tend to have higher false-positive rates and face difficulties in tasks involving graphical user interfaces (GUI). These findings mark a significant step in evaluating AI-enabled cybersecurity tools, underscoring both their promise and areas requiring further improvement. <div>
arXiv:2512.09882v1 Announce Type: new 
Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science</title>
<link>https://arxiv.org/abs/2512.09895</link>
<guid>https://arxiv.org/abs/2512.09895</guid>
<content:encoded><![CDATA[
<div> Keywords: Metadata Vocabularies, AI-Human-in-the-Loop, FAIR Principles, Materials Science, Crowdsourcing<br /><br />Summary:<br /><br />This paper introduces MatSci-YAMZ, an innovative platform designed to enhance the development of metadata vocabularies by integrating artificial intelligence (AI) with human-in-the-loop (HILT) methods, including crowdsourcing. The study focuses on a proof-of-concept evaluation within the interdisciplinary field of materials science, where six participants from the NSF Institute for Data-Driven Dynamical Design (ID4) interacted with the platform over several weeks. During this period, participants contributed term definitions and supplied examples that guided AI-generated refinements. The effort resulted in nineteen AI-generated definitions developed through iterative feedback, demonstrating the viability of the AI-HILT approach for refining vocabulary terms. Key outcomes include confirmation that the AI-HILT model is a successful proof of concept, alignment with FAIR (Findable, Accessible, Interoperable, Reusable) and open science principles, and the establishment of a research protocol to support future investigations. Additionally, the platform shows promising potential for scalability across various scientific domains. Ultimately, MatSci-YAMZ aims to improve semantic transparency in metadata vocabularies while significantly reducing the time required for consensus building and the overall process of vocabulary development. <div>
arXiv:2512.09895v1 Announce Type: new 
Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments</title>
<link>https://arxiv.org/abs/2512.09897</link>
<guid>https://arxiv.org/abs/2512.09897</guid>
<content:encoded><![CDATA[
<div> Subgoals, hierarchical planning, large language models, efficiency, TextCraft<br /><br />Summary:<br /><br />1. The article addresses challenges in long-term planning within complex, text-based environments, characterized by open-ended actions, ambiguous observations, and sparse feedback.<br />2. Large language models (LLMs) possess rich semantic knowledge useful for guiding agents in hierarchical reasoning and planning, but existing approaches rely heavily on querying LLMs during training and inference, which is computationally expensive.<br />3. Previous methods typically use fixed, pretrained LLMs without adapting their parameters for specific tasks.<br />4. To overcome these limitations, the authors propose SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that uses LLM-generated subgoals only once at initialization to pretrain a lightweight student model.<br />5. Unlike prior approaches that repeatedly prompt LLMs during training to generate subgoals, SCOPE derives subgoals from example trajectories, reducing the need for costly repeated LLM calls, thus enhancing efficiency at the expense of some explainability and potentially suboptimal subgoals.<br />6. Experiments on the TextCraft environment demonstrate that despite suboptimal subgoals, SCOPE’s lightweight student model achieves better success rates (0.56 vs. 0.52) and drastically reduces inference time (3.0s vs. 164.4s) compared to the LLM-based hierarchical agent ADaPT.<br />7. Overall, SCOPE presents an efficient and practical framework for hierarchical planning in text-based domains, balancing performance and computational cost. <div>
arXiv:2512.09897v1 Announce Type: new 
Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective</title>
<link>https://arxiv.org/abs/2512.09908</link>
<guid>https://arxiv.org/abs/2512.09908</guid>
<content:encoded><![CDATA[
<div> Moralisation, Triangulation, Bayesian networks, Markov networks, Functors  

<br /><br />Summary:  
This paper explores the categorical framework underlying key transformations in probabilistic graphical models, specifically moralisation and triangulation. These transformations enable converting between Bayesian networks (directed graphical models) and Markov networks (undirected graphical models). The authors model these transformations as functors between categories whose objects are Bayesian and Markov networks, respectively. These networks themselves are described as functors from a syntactic domain to a semantic codomain, providing a structured view of their construction. Moralisation is characterized as a fully syntactic transformation defined by functor pre-composition on the syntax, while triangulation incorporates semantic information and is thus partially semantic in nature. Additionally, the paper introduces a novel interpretation of the variable elimination algorithm as a functor that divides the triangulation procedure into two distinct stages: a syntactic stage and a semantic stage. This functorial viewpoint distinguishes operations based on whether they act on syntax or semantics, offering a clearer conceptual understanding of the processes involved in graphical model transformations. The approach enriches the theoretical foundations of probabilistic graphical models by leveraging tools from category theory to unify and clarify the relationship between different graphical structures and their corresponding transformations. <div>
arXiv:2512.09908v1 Announce Type: new 
Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic</title>
<link>https://arxiv.org/abs/2107.05664</link>
<guid>https://arxiv.org/abs/2107.05664</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, mixed-autonomy, multi-agent A2C, decentralized reward, human driver cooperation  

<br /><br />Summary:  
The paper addresses the challenge of mixed-autonomy environments where autonomous vehicles (AVs) must coexist and share road infrastructure with human-driven vehicles. It proposes that AVs achieve socially desirable behaviors by considering the utilities of surrounding vehicles in their decision-making. The focus is on maneuver planning for AVs through a decentralized reward structure designed to induce altruism and encourage AVs to account for both autonomous and human-driven vehicles' interests. Unlike prior approaches relying on predefined behavior models of human drivers, this work adopts an end-to-end learning framework where AV agents implicitly learn human decision-making solely from experience. To implement this, the authors introduce a multi-agent variant of the synchronous Advantage Actor-Critic (A2C) algorithm, enabling multiple agents to coordinate their actions. The trained AV agents can influence human driver behaviors by their decisions, leading to improved traffic flow and enhanced safety outcomes. This results in a decentralized, learned policy that fosters cooperation between autonomous and human drivers in shared driving scenarios without explicit human behavior modeling. <div>
arXiv:2107.05664v1 Announce Type: cross 
Abstract: With the adoption of autonomous vehicles on our roads, we will witness a mixed-autonomy environment where autonomous and human-driven vehicles must learn to co-exist by sharing the same road infrastructure. To attain socially-desirable behaviors, autonomous vehicles must be instructed to consider the utility of other vehicles around them in their decision-making process. Particularly, we study the maneuver planning problem for autonomous vehicles and investigate how a decentralized reward structure can induce altruism in their behavior and incentivize them to account for the interest of other autonomous and human-driven vehicles. This is a challenging problem due to the ambiguity of a human driver's willingness to cooperate with an autonomous vehicle. Thus, in contrast with the existing works which rely on behavior models of human drivers, we take an end-to-end approach and let the autonomous agents to implicitly learn the decision-making process of human drivers only from experience. We introduce a multi-agent variant of the synchronous Advantage Actor-Critic (A2C) algorithm and train agents that coordinate with each other and can affect the behavior of human drivers to improve traffic flow and safety.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction-aware and Reinforcement Learning based Altruistic Cooperative Driving</title>
<link>https://arxiv.org/abs/2211.10585</link>
<guid>https://arxiv.org/abs/2211.10585</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous vehicles, Human-driven vehicles, Reinforcement learning, Prediction, Social navigation  

<br /><br />Summary:  
This paper addresses the challenge of autonomous vehicle (AV) navigation among human-driven vehicles (HVs), emphasizing that HVs dynamically adjust their driving policies in response to AVs. Inspired by human ability to anticipate other agents' behaviors, the authors propose enabling AVs to predict future states of their environment to enhance safe and robust navigation. They integrate social navigation and prediction into a reinforcement learning (RL) framework, formulating AV decision-making as an RL problem to obtain optimal, socially beneficial policies. A key contribution is the Hybrid Predictive Network (HPN), which generates multi-step predictions of future observations that feed into a value function network (VFN). The VFN leverages these predicted observations and past data to optimize social utility while a safety prioritizer filters out unsafe actions based on interpretable kinematic predictions, ensuring the RL policy respects safety constraints. The approach is evaluated against state-of-the-art methods in multiple simulated driving scenarios, showing improved efficiency and safety. Overall, the work advances the integration of predictive social interaction modeling with RL to enable AVs to navigate complex social environments more effectively. <div>
arXiv:2211.10585v1 Announce Type: cross 
Abstract: Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic AI as Undercover Teammates: Argumentative Knowledge Construction in Hybrid Human-AI Collaborative Learning</title>
<link>https://arxiv.org/abs/2512.08933</link>
<guid>https://arxiv.org/abs/2512.08933</guid>
<content:encoded><![CDATA[
<div> Generative AI, agentic AI, collaborative learning, epistemic reasoning, argumentation<br /><br />Summary:<br /><br />This study investigates the role of agentic AI—AI systems with autonomy, interactivity, and adaptability—in collaborative learning environments, focusing on their impact on argumentative knowledge construction. The research uses undercover AI teammates with either supportive or contrarian personas embedded in triads of human participants to observe changes in epistemic and social dynamics during an analytical problem-solving task. Employing Weinberger and Fischer’s four-dimensional framework (participation, epistemic reasoning, argument structure, and social co-construction), the study analyzes discourse data from 212 humans and 64 AI participants. Findings show that AI teammates balance participation levels but significantly alter the quality of epistemic and social processes. Supportive AI promotes conceptual integration and consensus-seeking reasoning, while contrarian AI encourages critical elaboration and negotiation through conflict. Importantly, learning gains correlate with epistemic adequacy rather than participation volume, highlighting AI’s role in improving reasoning quality and coordination instead of increasing discourse quantity. These results extend CSCL (computer-supported collaborative learning) theory by framing agentic AI as adaptive epistemic and social collaborators that redistribute cognitive workload and argumentative labor within hybrid human-AI learning settings. <div>
arXiv:2512.08933v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) agents are increasingly embedded in collaborative learning environments, yet their impact on the processes of argumentative knowledge construction remains insufficiently understood. Emerging conceptualisations of agentic AI and artificial agency suggest that such systems possess bounded autonomy, interactivity, and adaptability, allowing them to engage as epistemic participants rather than mere instructional tools. Building on this theoretical foundation, the present study investigates how agentic AI, designed as undercover teammates with either supportive or contrarian personas, shapes the epistemic and social dynamics of collaborative reasoning. Drawing on Weinberger and Fischer's (2006) four-dimensional framework, participation, epistemic reasoning, argument structure, and social modes of co-construction, we analysed synchronous discourse data from 212 human and 64 AI participants (92 triads) engaged in an analytical problem-solving task. Mixed-effects and epistemic network analyses revealed that AI teammates maintained balanced participation but substantially reorganised epistemic and social processes: supportive personas promoted conceptual integration and consensus-oriented reasoning, whereas contrarian personas provoked critical elaboration and conflict-driven negotiation. Epistemic adequacy, rather than participation volume, predicted individual learning gains, indicating that agentic AI's educational value lies in enhancing the quality and coordination of reasoning rather than amplifying discourse quantity. These findings extend CSCL theory by conceptualising agentic AI as epistemic and social participants, bounded yet adaptive collaborators that redistribute cognitive and argumentative labour in hybrid human-AI learning environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion2Meaning: A Clinician-Centered Framework for Contestable LLM in Parkinson's Disease Gait Interpretation</title>
<link>https://arxiv.org/abs/2512.08934</link>
<guid>https://arxiv.org/abs/2512.08934</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's Disease, gait analysis, contestable AI, wearable sensors, explainable AI<br /><br />Summary:<br /><br />1. The paper introduces Motion2Meaning, a clinician-centered framework designed to improve transparency and oversight in AI-assisted gait analysis for Parkinson’s Disease (PD).<br />2. It addresses the current gap where clinical dashboards for PD lack mechanisms to interrogate or contest AI-driven decisions.<br />3. The framework leverages vertical Ground Reaction Force (vGRF) time-series data captured via wearable sensors to objectively assess PD motor states.<br />4. Motion2Meaning consists of three main components: a Gait Data Visualization Interface (GDVI), a 1D Convolutional Neural Network (1D-CNN) for predicting Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) combining a novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM).<br />5. The 1D-CNN achieves an 89.0% F1-score on the PhysioNet gait dataset, while XMED effectively flags model unreliability by detecting significantly higher explanation discrepancies in incorrect predictions.<br />6. The LLM-powered interface enables clinicians to validate accurate predictions and successfully challenge some AI errors.<br />7. A human-centered evaluation highlights a trade-off between the LLM’s factual accuracy and its interpretability and responsiveness to clinical feedback.<br />8. The work demonstrates the feasibility of combining wearable sensor data, explainable AI, and contestable LLMs to build a transparent, auditable system maintaining clinical oversight while enhancing PD gait interpretation.<br />9. The implementation is publicly available on GitHub. <div>
arXiv:2512.08934v1 Announce Type: cross 
Abstract: AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. To address this issue, we present Motion2Meaning, a clinician-centered framework that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Our approach leverages vertical Ground Reaction Force (vGRF) time-series data from wearable sensors as an objective biomarker of PD motor states. The system comprises three key components: a Gait Data Visualization Interface (GDVI), a one-dimensional Convolutional Neural Network (1D-CNN) that predicts Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) that combines our novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM). Our 1D-CNN achieves 89.0% F1-score on the public PhysioNet gait dataset. XMED successfully identifies model unreliability by detecting a five-fold increase in explanation discrepancies in incorrect predictions (7.45%) compared to correct ones (1.56%), while our LLM-powered interface enables clinicians to validate correct predictions and successfully contest a portion of the model's errors. A human-centered evaluation of this contestable interface reveals a crucial trade-off between the LLM's factual grounding and its readability and responsiveness to clinical feedback. This work demonstrates the feasibility of combining wearable sensor analysis with Explainable AI (XAI) and contestable LLMs to create a transparent, auditable system for PD gait interpretation that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/hungdothanh/motion2meaning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness</title>
<link>https://arxiv.org/abs/2512.08936</link>
<guid>https://arxiv.org/abs/2512.08936</guid>
<content:encoded><![CDATA[
<div> Generative AI, personal health, LLM evaluation, SHARP framework, Fitbit Insights explorer  

<br /><br />Summary: The article discusses the integration of generative artificial intelligence, specifically large language models (LLMs), into personal health applications to provide personalized and data-driven health and fitness guidance. It highlights the challenges in ensuring user safety, model accuracy, and privacy when deploying these technologies. The authors present the development of the Fitbit Insights explorer, an LLM-powered tool that helps users interpret their personal health data. To systematically assess and improve such applications, they introduce the SHARP framework, focusing on Safety, Helpfulness, Accuracy, Relevance, and Personalization. This principle-based framework incorporates a comprehensive evaluation approach combining human evaluations by both generalists and clinical specialists, automated rating systems, and adversarial testing in an iterative development process. The framework was validated through a staged deployment involving over 13,000 consented users, revealing issues that were not detected in initial testing phases. These findings informed targeted system enhancements, illustrating the importance of combining technical assessments with real-world user feedback. Ultimately, the article offers a standardized, actionable methodology for the responsible development and deployment of LLM-powered health applications, aiming to balance innovation with the safety, effectiveness, and trustworthiness of emerging AI health technologies. <div>
arXiv:2512.08936v1 Announce Type: cross 
Abstract: The incorporation of generative artificial intelligence into personal health applications presents a transformative opportunity for personalized, data-driven health and fitness guidance, yet also poses challenges related to user safety, model accuracy, and personal privacy. To address these challenges, a novel, principle-based framework was developed and validated for the systematic evaluation of LLMs applied to personal health and wellness. First, the development of the Fitbit Insights explorer, a large language model (LLM)-powered system designed to help users interpret their personal health data, is described. Subsequently, the safety, helpfulness, accuracy, relevance, and personalization (SHARP) principle-based framework is introduced as an end-to-end operational methodology that integrates comprehensive evaluation techniques including human evaluation by generalists and clinical specialists, autorater assessments, and adversarial testing, into an iterative development lifecycle. Through the application of this framework to the Fitbit Insights explorer in a staged deployment involving over 13,000 consented users, challenges not apparent during initial testing were systematically identified. This process guided targeted improvements to the system and demonstrated the necessity of combining isolated technical evaluations with real-world user feedback. Finally, a comprehensive, actionable approach is established for the responsible development and deployment of LLM-powered health applications, providing a standardized methodology to foster innovation while ensuring emerging technologies are safe, effective, and trustworthy for users.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being</title>
<link>https://arxiv.org/abs/2512.08937</link>
<guid>https://arxiv.org/abs/2512.08937</guid>
<content:encoded><![CDATA[
<div> Advice quality, large language models, Reddit, human-AI collaboration, user preferences  

<br /><br />Summary:  
1. The study investigates the quality of advice generated by large language models (LLMs) compared to human advice found in online communities, specifically top-voted Reddit comments.  
2. Two studies involving 210 expert participants showed that LLM-generated advice ranked significantly higher overall, including aspects like effectiveness, warmth, and likelihood of users seeking advice again.  
3. An unexpected result was that GPT-4o outperformed GPT-5 on nearly all metrics except sycophancy, indicating that advancements in LLM benchmarks do not necessarily translate into better advice-giving capabilities.  
4. The research also explored how human and AI advice could be combined, finding that subtle enhancements or “polishing” of human advice could make it competitive with AI-generated responses without being intrusive.  
5. Finally, a survey with 148 undergraduates revealed that users have diverse, persona-dependent expectations for advice agents, favoring qualities such as goal-oriented coaching or warmth and humor akin to a friend.  
The paper concludes with design recommendations for future advice-giving systems that blend AI technology, crowd wisdom, and expert oversight to meet varying user needs effectively. <div>
arXiv:2512.08937v1 Announce Type: cross 
Abstract: Seeking advice is a core human behavior that the Internet has reinvented twice: first through forums and Q\&amp;A communities that crowdsource public guidance, and now through large language models (LLMs) that deliver private, on-demand counsel at scale. Yet the quality of this synthesized LLM advice remains unclear. How does it compare, not only against arbitrary human comments, but against the wisdom of the online crowd? We conducted two studies (N = 210) in which experts compared top-voted Reddit advice with LLM-generated advice. LLMs ranked significantly higher overall and on effectiveness, warmth, and willingness to seek advice again. GPT-4o beat GPT-5 on all metrics except sycophancy, suggesting that benchmark gains need not improve advice-giving. In our second study, we examined how human and algorithmic advice could be combined, and found that human advice can be unobtrusively polished to compete with AI-generated comments. Finally, to surface user expectations, we ran an exploratory survey with undergraduates (N=148) that revealed heterogeneous, persona-dependent preferences for agent qualities (e.g., coach-like: goal-focused structure; friend-like: warmth and humor). We conclude with design implications for advice-giving agents and ecosystems blending AI, crowd input, and expert oversight.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Human-Likeness of LLM-Driven Digital Twins in Simulating Health Care System Trust</title>
<link>https://arxiv.org/abs/2512.08939</link>
<guid>https://arxiv.org/abs/2512.08939</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Human Digital Twin, Healthcare System Distrust, Simulation, Demographic Patterns<br /><br />Summary:<br /><br />This study evaluates the simulation capabilities of Large Language Model (LLM)-driven Human Digital Twins in replicating complex psychological traits, specifically distrust in the healthcare system. Using the Twin-2K-500 dataset and the Health Care System Distrust Scale (HCSDS), the research compares simulated digital twin responses with actual human-subject data across item distributions, summary statistics, and demographic subgroups. Findings indicate that digital twin responses are more centralized with reduced variance and fewer extreme response selections (p<0.001). While the model successfully captures broad demographic patterns, including age and gender influences on distrust, it struggles to detect nuanced differences, such as those related to education levels. This reveals that LLM-based digital twins can simulate general population trends but have limitations in fine-grained subgroup distinctions. The study highlights the necessity for careful calibration and validation of such models before employing them in inferential analyses or policy decision-making within health systems engineering. Additionally, it calls for future research into the emotional reasoning mechanisms underpinning LLMs, especially when simulations involve sensitive social topics like human-automation trust, to improve the fidelity and reliability of these digital twin tools in healthcare contexts. <div>
arXiv:2512.08939v1 Announce Type: cross 
Abstract: Serving as an emerging and powerful tool, Large Language Model (LLM)-driven Human Digital Twins are showing great potential in healthcare system research. However, its actual simulation ability for complex human psychological traits, such as distrust in the healthcare system, remains unclear. This research gap particularly impacts health professionals' trust and usage of LLM-based Artificial Intelligence (AI) systems in assisting their routine work. In this study, based on the Twin-2K-500 dataset, we systematically evaluated the simulation results of the LLM-driven human digital twin using the Health Care System Distrust Scale (HCSDS) with an established human-subject sample, analyzing item-level distributions, summary statistics, and demographic subgroup patterns. Results showed that the simulated responses by the digital twin were significantly more centralized with lower variance and had fewer selections of extreme options (all p<0.001). While the digital twin broadly reproduces human results in major demographic patterns, such as age and gender, it exhibits relatively low sensitivity in capturing minor differences in education levels. The LLM-based digital twin simulation has the potential to simulate population trends, but it also presents challenges in making detailed, specific distinctions in subgroups of human beings. This study suggests that the current LLM-driven Digital Twins have limitations in modeling complex human attitudes, which require careful calibration and validation before applying them in inferential analyses or policy simulations in health systems engineering. Future studies are necessary to examine the emotional reasoning mechanism of LLMs before their use, particularly for studies that involve simulations sensitive to social topics, such as human-automation trust.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Technical Debt: How AI-Assisted Development Creates Comprehension Debt in Resource-Constrained Indie Teams</title>
<link>https://arxiv.org/abs/2512.08942</link>
<guid>https://arxiv.org/abs/2512.08942</guid>
<content:encoded><![CDATA[
<div> Keywords: CIGDI Framework, junior indie developers, AI integration, technical debt, comprehension debt<br /><br />Summary: This study addresses the challenges faced by junior indie game developers working in distributed, part-time teams who find traditional production methodologies inaccessible. It introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, a seven-stage iterative process designed to integrate AI tools effectively while emphasizing human-in-the-loop decision points like Priority Criteria and Timeboxing. The framework was developed through a three-month autoethnographic study of a three-person team creating a 2D narrative game called "The Worm's Memoirs," analyzing extensive development data including Jira tasks, GitHub commits, Miro boards, and reflection sessions. The use of AI was found to democratize knowledge access and reduce cognitive load, making production more manageable for resource-limited teams. However, the study identifies a novel form of technical debt named "comprehension debt," where AI enables teams to build systems that exceed their individual understanding and maintenance skills, resulting in fragility and increased AI dependency. This form of debt is distinct from traditional code quality debt and poses significant risks. The work contributes a practical production framework tailored to such teams and raises important questions about whether AI serves as a skill-building ladder or inadvertently creates a dependency trap for developers. <div>
arXiv:2512.08942v1 Announce Type: cross 
Abstract: Junior indie game developers in distributed, part-time teams lack production frameworks suited to their specific context, as traditional methodologies are often inaccessible. This study introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, an alternative approach for integrating AI tools to address persistent challenges of technical debt, coordination, and burnout.
  The framework emerged from a three-month reflective practice and autoethnographic study of a three-person distributed team developing the 2D narrative game "The Worm's Memoirs". Based on analysis of development data (N=157 Jira tasks, N=333 GitHub commits, N=13+ Miro boards, N=8 reflection sessions), CIGDI is proposed as a seven-stage iterative process structured around human-in-the-loop decision points (Priority Criteria and Timeboxing).
  While AI support democratized knowledge access and reduced cognitive load, our analysis identified a significant challenge: "comprehension debt." We define this as a novel form of technical debt where AI helps teams build systems more sophisticated than their independent skill level can create or maintain. This paradox (possessing functional systems the team incompletely understands) creates fragility and AI dependency, distinct from traditional code quality debt.
  This work contributes a practical production framework for resource-constrained teams and identifies critical questions about whether AI assistance constitutes a learning ladder or a dependency trap for developer skill.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2512.08943</link>
<guid>https://arxiv.org/abs/2512.08943</guid>
<content:encoded><![CDATA[
<div> Abstractive compression, retrieval-augmented generation, retrieval noise, data augmentation, positional bias  

<br /><br />Summary:  
This paper addresses the challenges of abstractive compression in retrieval-augmented generation (RAG) systems, where smaller language models condense query-relevant context to reduce computational costs. It highlights the problem that retrieved documents, despite high relevance scores, often contain irrelevant or factually incorrect information, which can lead compressors to omit critical details necessary for accurate answers, especially in lengthy contexts suffering from attention dispersion. To tackle this, the authors propose a novel method called Abstractive Compression Robust against Noise (ACoRN), which involves two key training innovations. First, offline data augmentation is applied to enhance the compressor's robustness against two specific types of retrieval noise. Second, fine-tuning is performed to address the LM compressor’s positional bias and limited ability to synthesize information from multiple documents by guiding it to generate summaries focused on key information supporting the correct answer. Experiments demonstrate that using ACoRN to train T5-large as a compressor improves Exact Match (EM) and F1 scores while preserving answer strings, thus providing direct evidence of accuracy. The approach shows strong performance on datasets with numerous accuracy-reducing documents, making it practical and effective for real-world applications. <div>
arXiv:2512.08943v1 Announce Type: cross 
Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization</title>
<link>https://arxiv.org/abs/2512.08945</link>
<guid>https://arxiv.org/abs/2512.08945</guid>
<content:encoded><![CDATA[
<div> Keywords: Mentalization, Large Language Models, Mentalization-Based Treatment, linguistic coherence, psychiatry<br /><br />Summary:<br /><br />1. The study explores the capability of a single Large Language Model (LLM) to replicate the linguistic structure of mentalization, based on parameters defined in Mentalization-Based Treatment (MBT).<br />2. Fifty dialogues were created between human participants and the LLM in standard configuration, which were then assessed by five MBT-trained psychiatrists working under blinded conditions.<br />3. The evaluation focused on mentalization profiles along four MBT axes, using Likert-scale scores to measure evaluative coherence, argumentative coherence, and overall quality.<br />4. Results showed mean score ranges between 3.63 and 3.98 with moderate standard deviations, indicating a high degree of structural coherence in the model-generated profiles.<br />5. Inter-rater reliability was substantial to high (ICC 0.60-0.84), confirming consistency among psychiatrists’ ratings.<br />6. The LLM demonstrated greater stability when producing content related to the Implicit-Explicit and Self-Other dimensions of mentalization.<br />7. Limitations were noted in the model’s ability to integrate internal mental states with external contextual information.<br />8. While the generated profiles were coherent and clinically interpretable, they exhibited affective neutrality, lacking emotional engagement or warmth typically seen in human mentalization. <div>
arXiv:2512.08945v1 Announce Type: cross 
Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).
  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).
  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Co-Artist: A LLM-Powered Framework for Interactive GLSL Shader Animation Evolution</title>
<link>https://arxiv.org/abs/2512.08951</link>
<guid>https://arxiv.org/abs/2512.08951</guid>
<content:encoded><![CDATA[
<div> Keywords: AI Co-Artist, GLSL shaders, large language models, creative coding, user-guided evolution  

<br /><br />Summary:  
This paper introduces AI Co-Artist, an interactive system designed to simplify and enhance the creation of real-time GLSL shaders by integrating large language models (LLMs), specifically GPT-4. Traditional shader programming requires significant coding expertise, which limits accessibility for many artists and creative users. AI Co-Artist addresses this barrier by providing a visually-driven interface inspired by the user-guided evolutionary approach of the Picbreeder platform, allowing users to iteratively evolve and refine shader art without writing code. The system acts both as a creative partner and technical advisor, enabling exploration of a vast generative design space for dynamic visual effects responsive to stimuli like sound or interaction. Extensive evaluations, including user studies and qualitative feedback, show that AI Co-Artist lowers the technical threshold, improves creative output, and supports a broad spectrum of users in producing professional-level shaders. Beyond shader art, the authors argue that the approach is broadly applicable, leveraging LLMs’ semantic understanding and program synthesis capabilities to extend into other creative domains such as website layout generation, architectural visualization, product prototyping, and infographic design, demonstrating the wider potential of combining AI assistance with evolutionary user input in creative workflows. <div>
arXiv:2512.08951v1 Announce Type: cross 
Abstract: Creative coding and real-time shader programming are at the forefront of interactive digital art, enabling artists, designers, and enthusiasts to produce mesmerizing, complex visual effects that respond to real-time stimuli such as sound or user interaction. However, despite the rich potential of tools like GLSL, the steep learning curve and requirement for programming fluency pose substantial barriers for newcomers and even experienced artists who may not have a technical background. In this paper, we present AI Co-Artist, a novel interactive system that harnesses the capabilities of large language models (LLMs), specifically GPT-4, to support the iterative evolution and refinement of GLSL shaders through a user-friendly, visually-driven interface. Drawing inspiration from the user-guided evolutionary principles pioneered by the Picbreeder platform, our system empowers users to evolve shader art using intuitive interactions, without needing to write or understand code. AI Co-Artist serves as both a creative companion and a technical assistant, allowing users to explore a vast generative design space of real-time visual art. Through comprehensive evaluations, including structured user studies and qualitative feedback, we demonstrate that AI Co-Artist significantly reduces the technical threshold for shader creation, enhances creative outcomes, and supports a wide range of users in producing professional-quality visual effects. Furthermore, we argue that this paradigm is broadly generalizable. By leveraging the dual strengths of LLMs-semantic understanding and program synthesis, our method can be applied to diverse creative domains, including website layout generation, architectural visualizations, product prototyping, and infographics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis</title>
<link>https://arxiv.org/abs/2512.08952</link>
<guid>https://arxiv.org/abs/2512.08952</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid robots, conversational agents, nonverbal dynamics, TD3, simulation training  

<br /><br />Summary:  
This paper addresses the challenges in testing humanoid robots for psychiatric screening, highlighting the slow, wear-inducing traditional hardware testing methods that limit iteration and diversity. To overcome this, the authors virtualize humanoid robots as conversational agents within a simulation-first pipeline, converting interview data into 276 interactive Unreal Engine MetaHuman patients synchronized with speech, gaze, face, and head-torso movements, alongside PHQ-8 and PCL-C psychiatric assessment flows. The system incorporates a perception-fusion-policy loop that controls speech timing, backchannels, and interruption avoidance under safety constraints. Training leverages counterfactual replay with bounded nonverbal perturbations and an uncertainty-aware turn manager designed to reduce diagnostic ambiguities. The study compares three reinforcement learning controllers—TD3, PPO, and CEM—showing TD3 outperforms others by achieving near-ceiling interview coverage with steadier pacing and comparable rewards. Analyses reveal better decision quality with minimal overlapping turns, well-aligned cut timing, fewer clarification needs, and reduced wait times. The controller’s performance remains robust under modality dropout and rendering changes and generalizes well to unseen patients. Key contributions include the development of an agent-centered simulator with bounded nonverbal counterfactuals, a safe learning loop prioritizing timing and rapport, a comparative controller study demonstrating clear gains, and robustness analyses enabling clinician-supervised humanoid operation in psychiatric screening. <div>
arXiv:2512.08952v1 Announce Type: cross 
Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimClinician: A Multimodal Simulation Testbed for Reliable Psychologist AI Collaboration in Mental Health Diagnosis</title>
<link>https://arxiv.org/abs/2512.08953</link>
<guid>https://arxiv.org/abs/2512.08953</guid>
<content:encoded><![CDATA[
<div> AI diagnosis, mental health, simulation platform, psychologist interface, multimodal data<br /><br />Summary: This paper addresses the challenges in AI-based mental health diagnosis, emphasizing that its real-world value depends on how psychologists interact with AI outputs—whether they accept, adjust, or reject AI suggestions. Mental health diagnosis is particularly complex due to the continuous nature of decisions influenced by subtle patient cues like tone, pauses, word choice, and nonverbal behavior. The authors note a gap in current research on how the design of AI diagnostic interfaces impacts psychologist decision-making, limiting reliable pre-live study testing. They introduce SimClinician, an interactive simulation platform that transforms patient data into a collaborative AI-psychologist diagnostic process. SimClinician features include: (1) a comprehensive dashboard integrating audio, text, and gaze-expression patterns to present multimodal patient data; (2) an avatar module that renders de-identified patient dynamics to facilitate deeper analysis; (3) a decision layer aligning AI-generated outputs with multimodal evidence, allowing psychologists to review the AI reasoning process and provide their diagnosis. Tested on the E-DAIC corpus, expanded from 276 clinical interviews to 480,000 simulations, SimClinician demonstrates that incorporating a confirmation step with the AI's suggestions increases acceptance by 23%, keeps escalation (rejection or override) rates below 9%, and preserves a smooth interaction flow between psychologists and AI. <div>
arXiv:2512.08953v1 Announce Type: cross 
Abstract: AI based mental health diagnosis is often judged by benchmark accuracy, yet in practice its value depends on how psychologists respond whether they accept, adjust, or reject AI suggestions. Mental health makes this especially challenging: decisions are continuous and shaped by cues in tone, pauses, word choice, and nonverbal behaviors of patients. Current research rarely examines how AI diagnosis interface design influences these choices, leaving little basis for reliable testing before live studies. We present SimClinician, an interactive simulation platform, to transform patient data into psychologist AI collaborative diagnosis. Contributions include: (1) a dashboard integrating audio, text, and gaze-expression patterns; (2) an avatar module rendering de-identified dynamics for analysis; (3) a decision layer that maps AI outputs to multimodal evidence, letting psychologists review AI reasoning, and enter a diagnosis. Tested on the E-DAIC corpus (276 clinical interviews, expanded to 480,000 simulations), SimClinician shows that a confirmation step raises acceptance by 23%, keeping escalations below 9%, and maintaining smooth interaction flow.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings</title>
<link>https://arxiv.org/abs/2512.08954</link>
<guid>https://arxiv.org/abs/2512.08954</guid>
<content:encoded><![CDATA[
<div> Keywords: ECG analysis, foundation models, self-supervised learning, time-series models, AI in healthcare<br /><br />Summary:<br /><br />This paper investigates the applicability of foundation models for analyzing Electrocardiogram (ECG) data, which is crucial for non-invasive cardiac diagnosis. The authors recognize that traditional ECG interpretation heavily depends on expert knowledge, which can limit the deployment of AI in healthcare. By leveraging recent advances in self-supervised learning and foundation models, the study evaluates whether such models can acquire domain-specific insights without extensive expert input. The research compares language-based, general time-series, and ECG-specific foundation models against conventional deep learning approaches tailored for time-series data. Experimental results demonstrate that general time-series and ECG foundation models achieve up to an 80% top performance rate, underscoring their effectiveness in ECG analysis tasks. Beyond raw performance, the study offers detailed analyses highlighting the strengths and weaknesses of these models in physiological waveform interpretation. Finally, the work discusses the broader implications, emphasizing both the potential and current limitations of foundation models in advancing AI-driven health diagnostics. To support reproducibility and further research, the authors have made their benchmarking datasets and code publicly accessible via GitHub. <div>
arXiv:2512.08954v1 Announce Type: cross 
Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation</title>
<link>https://arxiv.org/abs/2512.08955</link>
<guid>https://arxiv.org/abs/2512.08955</guid>
<content:encoded><![CDATA[
<div> XL-MIMO, 6G networks, channel estimation, large language models, hybrid-field channels<br /><br />Summary:<br /><br />1. The article addresses the challenge of channel estimation in extremely large-scale massive multiple-input multiple-output (XL-MIMO) systems, crucial for advancing sixth-generation (6G) network technologies by providing massive spatial degrees of freedom.<br /><br />2. It highlights the difficulty traditional estimation methods face in handling hybrid-field channels, where near-field and far-field effects coexist, leading to problems in accuracy and generalizability.<br /><br />3. Inspired by recent successes of large language models (LLMs) in downstream tasks through fine-tuning, the authors propose a novel framework named LLM4XCE that leverages LLMs’ semantic modeling ability to better capture spatial-channel representations.<br /><br />4. The proposed model features an embedding module integrated with Parallel Feature-Spatial Attention to deeply fuse pilot features and spatial structural information, creating a semantically rich input representation for the LLM.<br /><br />5. By fine-tuning only the top two Transformer layers, the approach efficiently learns latent dependencies from pilot data, resulting in superior estimation accuracy and generalization under hybrid-field conditions, demonstrated through extensive simulations comparing favorably against state-of-the-art methods. <div>
arXiv:2512.08955v1 Announce Type: cross 
Abstract: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy.
  Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUMOS: Large User MOdels for User Behavior Prediction</title>
<link>https://arxiv.org/abs/2512.08957</link>
<guid>https://arxiv.org/abs/2512.08957</guid>
<content:encoded><![CDATA[
<div> Keywords: user behavior prediction, transformer, cross-attention, multi-task learning, large-scale dataset<br /><br />Summary:  
1. The paper addresses the challenge of predicting user behavior at scale on online B2C platforms, highlighting the limitations of traditional task-specific models and manual feature engineering, which are time-consuming, computationally costly, and require expert knowledge.  
2. The authors introduce LUMOS (Large User MOdel Series), a transformer-based architecture designed to jointly learn multiple tasks using solely raw user activity data, eliminating the need for separate models or handcrafted features.  
3. LUMOS incorporates a novel cross-attention mechanism that conditions predictions on future known events such as holidays and sales, allowing it to forecast complex behavioral patterns influenced by upcoming events.  
4. The model employs a multi-modal tokenization approach, integrating diverse data types including user transactions, event context, and static demographic attributes via specialized embeddings to create rich user representations.  
5. Experiments on an extensive production dataset comprising 275 billion user activity tokens from 250 million users demonstrate that LUMOS outperforms traditional baselines, improving ROC-AUC by 0.025 on average for classification tasks and reducing MAPE by 4.6% on regression tasks. Online A/B tests confirm these performance gains lead to real business improvements, evidenced by a 3.15% increase in Daily Active Users. <div>
arXiv:2512.08957v1 Announce Type: cross 
Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications</title>
<link>https://arxiv.org/abs/2512.08959</link>
<guid>https://arxiv.org/abs/2512.08959</guid>
<content:encoded><![CDATA[
<div> EEG, foundation models, clinical applications, benchmarking, diagnostic tasks<br /><br />Summary:<br /><br />This paper introduces a comprehensive benchmarking framework designed to evaluate EEG-based foundation models specifically in clinical contexts. The benchmark includes 11 distinct diagnostic tasks covering a variety of neurological and psychiatric conditions such as epilepsy, schizophrenia, Parkinson’s disease, OCD, and mild traumatic brain injury, utilizing 14 publicly available EEG datasets. The framework is characterized by minimal preprocessing requirements and standardized evaluation protocols, allowing for consistent and objective performance comparisons. It facilitates direct side-by-side analysis of classical baseline models alongside modern foundation models to assess relative strengths and weaknesses. The results reveal that foundation models can achieve strong performance in several scenarios, but simpler classical models still hold competitive value, especially when models are tested under clinical distribution shifts where data characteristics change. To promote reproducibility and encourage further research, the authors release all processed datasets and source code in an accessible, extensible format. This work aims to provide a unified resource for researchers to benchmark developments in EEG-based modeling for clinical diagnostics effectively while highlighting the challenges posed by distributional variability in real-world clinical data. <div>
arXiv:2512.08959v1 Announce Type: cross 
Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces</title>
<link>https://arxiv.org/abs/2512.08960</link>
<guid>https://arxiv.org/abs/2512.08960</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, Continual Learning, Catastrophic Forgetting, Parameter Stability, Dual-Regularization<br /><br />Summary:  
This paper addresses the issue of catastrophic forgetting in Continual Learning when using Low-Rank Adaptation (LoRA), identifying destructive interference between tasks as the key problem caused by antagonistic directional updates that oppose historical weight trajectories. To resolve this, the authors propose PS-LoRA (Parameter Stability LoRA), a novel framework that aligns updates within the optimization subspace to reduce conflicts. PS-LoRA introduces a dual-regularization objective which simultaneously penalizes conflicting gradient directions and constrains magnitude deviations, thereby maintaining consistency with previously acquired knowledge. Furthermore, the approach includes a magnitude-based merging strategy that consolidates sequentially learned adapters into a single, robust representation without requiring retraining, improving efficiency. Experimental results demonstrate that PS-LoRA surpasses state-of-the-art methods on both Natural Language Processing and Vision benchmark tasks by preserving learned representation stability while effectively adapting to new domains. This work thus provides a practical and effective technique to improve Continual Learning performance by mitigating catastrophic forgetting via parameter stability. <div>
arXiv:2512.08960v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Instruction Following Evaluation (FIFE)</title>
<link>https://arxiv.org/abs/2512.08965</link>
<guid>https://arxiv.org/abs/2512.08965</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Financial Analysis, Instruction-Following, Benchmark, Reinforcement Learning

<br /><br />Summary:  
The paper introduces FIFE, a challenging benchmark specifically designed to evaluate language models' (LMs) abilities to follow complex and interdependent instructions in financial analysis tasks. FIFE contains 88 carefully crafted human-authored prompts and features a verification system with chainable, verifiable constraints that provide fine-grained reward signals to rigorously assess model compliance. The study evaluates 53 models across proprietary, open-weight, and open-source categories in a zero-shot setting to analyze their performance on these difficult tasks. Results reveal a clear performance hierarchy: the top open-weight model achieves the highest scores (76.1 strict / 79.5 loose), outperforming the leading proprietary system (65.9 strict / 70.5 loose). However, the best open-source models lag considerably behind (45.5 strict / 48.9 loose). Despite these scores, even the best-performing models struggle to fully comply with FIFE’s complex requirements, highlighting limitations in current LM instruction-following capabilities for high-stakes financial domains. To encourage progress in this area, the authors release the FIFE dataset and accompanying code as open-source resources, aiming to foster further research in reinforcement learning applied to financial language tasks. <div>
arXiv:2512.08965v1 Announce Type: cross 
Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing</title>
<link>https://arxiv.org/abs/2512.08967</link>
<guid>https://arxiv.org/abs/2512.08967</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, robustness certification, adversarial attacks, clustering-guided denoising, semantic filtering<br /><br />Summary:<br /><br />Recent advancements in Large Language Models (LLMs) have made them prevalent in many applications, but their vulnerability to adversarial attacks, including subtle meaning-preserving perturbations like synonym substitutions, remains a significant concern. This motivates the need for robust certification methods that can guarantee model reliability against such adversarial prompts. Existing techniques largely rely on word deletion or straightforward denoising strategies, which suffer from two main issues: producing loose robustness bounds due to inadequate semantic validation of perturbed outputs, and incurring high computational costs due to repetitive sampling. To overcome these challenges, the authors propose CluCERT, a novel framework that leverages clustering-guided denoising smoothing to certify LLM robustness more tightly and efficiently. CluCERT incorporates a semantic clustering filter that effectively reduces noisy samples while preserving meaningful perturbations, with theoretical backing to support tighter certified bounds. Additionally, the framework improves computational efficiency through a refine module that extracts core semantic information and a fast synonym substitution strategy to speed up the denoising process. Extensive experiments on diverse downstream tasks and jailbreak defense scenarios demonstrate that CluCERT outperforms existing certified methods in terms of both tighter robustness guarantees and significantly better computational efficiency. <div>
arXiv:2512.08967v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Robust Representations for Malicious Content Detection via Contrastive Sampling and Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2512.08969</link>
<guid>https://arxiv.org/abs/2512.08969</guid>
<content:encoded><![CDATA[
<div> Uncertainty Contrastive Framework, Positive-Unlabeled Learning, Adaptive Temperature Scaling, Self-attention LSTM Encoder, Malicious Content Classification<br /><br />Summary:<br /><br />1. This paper introduces the Uncertainty Contrastive Framework (UCF), a novel Positive-Unlabeled (PU) learning method designed to improve representation learning, especially under noisy and imbalanced datasets.<br /><br />2. UCF incorporates an uncertainty-aware contrastive loss that adjusts contrastive weighting dynamically based on sample confidence, enhancing the model’s focus on more reliable data.<br /><br />3. The framework employs adaptive temperature scaling that adjusts parameters based on batch-level variability, improving training stability and embedding quality.<br /><br />4. A self-attention-guided LSTM encoder is integrated to capture contextual dependencies in input data, further refining the learned embeddings.<br /><br />5. When applied to malicious content classification, UCF-generated embeddings enable various traditional classifiers to achieve over 93.38% accuracy, with precision above 0.93 and near-perfect recall, minimizing false negatives.<br /><br />6. The method demonstrates competitive ROC-AUC scores and provides visual confirmation through embedding separability between positive and unlabeled samples.<br /><br />7. Overall, UCF offers a robust, scalable PU learning framework suitable for high-stakes domains such as cybersecurity and biomedical text mining, where reliable classification under uncertainty is critical. <div>
arXiv:2512.08969v1 Announce Type: cross 
Abstract: We propose the Uncertainty Contrastive Framework (UCF), a Positive-Unlabeled (PU) representation learning framework that integrates uncertainty-aware contrastive loss, adaptive temperature scaling, and a self-attention-guided LSTM encoder to improve classification under noisy and imbalanced conditions. UCF dynamically adjusts contrastive weighting based on sample confidence, stabilizes training using positive anchors, and adapts temperature parameters to batch-level variability. Applied to malicious content classification, UCF-generated embeddings enable multiple traditional classifiers to achieve more than 93.38% accuracy, precision above 0.93, and near-perfect recall, with minimal false negatives and competitive ROC-AUC scores. Visual analyses confirm clear separation between positive and unlabeled instances, highlighting the framework's ability to produce calibrated, discriminative embeddings. These results position UCF as a robust and scalable solution for PU learning in high-stakes domains such as cybersecurity and biomedical text mining.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Automatic Speech Recognition Through Integrated Noise Detection Architecture</title>
<link>https://arxiv.org/abs/2512.08973</link>
<guid>https://arxiv.org/abs/2512.08973</guid>
<content:encoded><![CDATA[
<div> Keywords: automatic speech recognition, noise detection, wav2vec2, transcription quality, joint optimization<br /><br />Summary:<br /><br />This research introduces a novel enhancement to automatic speech recognition (ASR) systems by embedding noise detection capabilities directly within the ASR architecture. It extends the wav2vec2 framework by adding a dedicated noise identification module that functions simultaneously with speech transcription, enabling the system to recognize both speech and noise in real time. The approach was experimentally validated using publicly available datasets comprising both speech and environmental audio, ensuring comprehensive testing across diverse acoustic environments. Results show significant improvements in transcription quality, as demonstrated by reductions in word error rate (WER) and character error rate (CER). Additionally, the integrated noise detection module achieved higher accuracy in distinguishing noise from speech compared to existing architectures lacking this capability. The study highlights the benefits of jointly optimizing the transcription and noise classification objectives, which contributes to more robust and reliable speech recognition performance, especially under challenging acoustic conditions with overlapping or background noise. Overall, this joint modeling framework provides an effective route to improve ASR robustness and accuracy by leveraging concurrent noise detection and transcription, offering practical implications for real-world noisy environments. <div>
arXiv:2512.08973v1 Announce Type: cross 
Abstract: This research presents a novel approach to enhancing automatic speech recognition systems by integrating noise detection capabilities directly into the recognition architecture. Building upon the wav2vec2 framework, the proposed method incorporates a dedicated noise identification module that operates concurrently with speech transcription. Experimental validation using publicly available speech and environmental audio datasets demonstrates substantial improvements in transcription quality and noise discrimination. The enhanced system achieves superior performance in word error rate, character error rate, and noise detection accuracy compared to conventional architectures. Results indicate that joint optimization of transcription and noise classification objectives yields more reliable speech recognition in challenging acoustic conditions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs</title>
<link>https://arxiv.org/abs/2512.08976</link>
<guid>https://arxiv.org/abs/2512.08976</guid>
<content:encoded><![CDATA[
<div> Contrastive Region Masking, Multimodal Large Language Models, Chain-of-Thought Reasoning, Visual Benchmarks, Robustness<br /><br />Summary:  
This paper introduces Contrastive Region Masking (CRM), a novel diagnostic method designed to evaluate how Multimodal Large Language Models (MLLMs) rely on specific visual regions throughout each step of their chain-of-thought (CoT) reasoning process. Unlike previous techniques that focused primarily on final output correctness or attention maps, CRM offers causal and step-wise attribution by selectively masking annotated image regions and comparing the altered reasoning traces against unmasked baselines. Applied on datasets like VisArgs, CRM unveils different failure modes of MLLMs: some models maintain the overall reasoning structure but hallucinate in the absence of visual evidence, whereas others depend heavily on visual cues but falter under slight perturbations. Rather than merely assessing answer accuracy, CRM shifts the focus toward measuring the faithfulness and robustness of the reasoning process itself. This shift suggests a rethinking of visual benchmarks, emphasizing the need for evaluation frameworks that not only quantify performance but also diagnose reasoning fidelity and reliability in multimodal tasks. <div>
arXiv:2512.08976v1 Announce Type: cross 
Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT</title>
<link>https://arxiv.org/abs/2512.08978</link>
<guid>https://arxiv.org/abs/2512.08978</guid>
<content:encoded><![CDATA[
<div> Keywords: institutional AI platform, governance, EU compliance, AI model diversity, AI Officer role<br /><br />Summary:<br /><br />To address the challenges of fragmented and risky adoption of commercial AI tools, a university of applied sciences developed and piloted an institutional AI platform over six months with 300 users. This platform aims to provide advanced AI capabilities while ensuring fair access, transparent risk management, cost control, and compliance with European laws. Commercial AI subscriptions often result in unequal access and compliance issues due to opaque data processing and non-EU hosting, but outright bans on such tools are impractical. The proposed solution features a three-layer governed gateway: a ChatGPT-style frontend tied to institutional user identity that makes model selection explicit; a core gateway enforcing policies, controlling budgets and access, and routing traffic primarily through EU infrastructure; and a provider layer that integrates commercial and open-source models using institutional model cards consolidating vendor documentation for governance. The pilot demonstrated reliable operation without privacy incidents, supported managed spending, EU-default routing, and transparent model choices. The authors emphasize AI as a strategic institutional function requiring dedicated leadership beyond traditional governance structures. They recommend creating an AI Officer role responsible for combining technical knowledge, governance authority, and educational duties to prevent ad-hoc decisions and reduce institutional risks. Proper governance enables higher-education institutions to operate multi-provider AI platforms effectively and responsibly. <div>
arXiv:2512.08978v1 Announce Type: cross 
Abstract: To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.
  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.
  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.
  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.
  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Happens When: Learning Temporal Orders of Events in Videos</title>
<link>https://arxiv.org/abs/2512.08979</link>
<guid>https://arxiv.org/abs/2512.08979</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Multimodal Models, temporal order, VECTOR benchmark, MECOT, chain-of-thought fine-tuning  

<br /><br />Summary:  
Video Large Multimodal Models (VLMMs) have demonstrated strong video understanding capabilities but their skill in capturing the precise temporal order of multiple events is not well studied. The authors find that VLMMs can perform well on existing benchmarks even when video frames are scrambled, suggesting models might rely more on prior knowledge of typical scenarios rather than accurate sequential processing. To better evaluate temporal understanding, the paper introduces VECTOR, a new benchmark specifically designed to assess a model’s ability to identify the correct temporal order of events in videos. Testing various VLMMs on VECTOR reveals that many struggle with temporal ordering tasks. To improve this, they propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), a method that fine-tunes models using detailed, event-by-event video descriptions and enhances inference through chain-of-thought prompting to boost temporal awareness. MECOT shows significant performance gains over previous approaches on VECTOR and also improves results on several existing video benchmarks, demonstrating the effectiveness of the approach in improving temporal understanding. The authors have released the code, models, and datasets to facilitate further research in this area. <div>
arXiv:2512.08979v1 Announce Type: cross 
Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multi-Image Vision Agents via End2End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08980</link>
<guid>https://arxiv.org/abs/2512.08980</guid>
<content:encoded><![CDATA[
<div> Multi-image QA, Vision-language models, Reinforcement learning, Visual reflection, Tool-use behavior  

<br /><br />Summary:  
This paper addresses the limitation of current vision-language model (VLM)-based agents that typically can process only a single image, which restricts their effectiveness in real-world multi-image question answering (QA) tasks. To overcome this, the authors propose IMAgent, an open-source vision agent trained end-to-end via reinforcement learning, specifically designed for handling complex multi-image tasks. They introduce a multi-agent system that generates challenging, visually-rich multi-image QA pairs to fully utilize the tool-use capabilities of base VLMs. The authors present MIFG-QA, a dataset of 10,000 samples created through manual verification for both training and evaluation. Recognizing that deeper reasoning in VLMs can cause neglect of visual inputs, they develop two specialized tools enabling visual reflection and confirmation, which help the model re-focus its attention on image content during inference. A novel action-trajectory two-level mask strategy further facilitates stable tool-use behavior through reinforcement learning, eliminating the need for costly supervised fine-tuning data. Extensive experiments confirm that IMAgent maintains high performance on existing single-image benchmarks while achieving significant improvements on the newly proposed multi-image dataset. The work also provides actionable insights for future research, with plans to release the code and dataset to the community. <div>
arXiv:2512.08980v1 Announce Type: cross 
Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding</title>
<link>https://arxiv.org/abs/2512.08981</link>
<guid>https://arxiv.org/abs/2512.08981</guid>
<content:encoded><![CDATA[
<div> Keywords: Face recognition, demographic bias, unified text-image embedding, vision-language models, fair verification<br /><br />Summary:<br /><br />Face recognition (FR) systems often suffer from demographic biases because demographic-specific information becomes entangled with identity-related facial features in embeddings, leading to unequal verification performance across groups. This issue is more pronounced in multicultural urban areas where biometrics are integral to smart city infrastructures. To mitigate this, the paper proposes Unified Text-Image Embedding (UTIE), a novel strategy that induces demographic ambiguity in face embeddings by incorporating demographic information from other groups via textual features. UTIE leverages the zero-shot and cross-modal semantic alignment capabilities of Vision-Language Models (VLMs) such as CLIP, OpenCLIP, and SigLIP. By enriching facial embeddings with text-derived demographic attributes from different groups, UTIE encourages embeddings to focus more on identity-relevant features, reducing demographic biases. The authors validate UTIE on two standard bias evaluation benchmarks, RFW and BFW, demonstrating that their approach consistently lowers bias metrics. Additionally, UTIE maintains or even improves overall face verification accuracy in several cases, showing its effectiveness in promoting fairer and more accurate face recognition across diverse demographic groups. <div>
arXiv:2512.08981v1 Announce Type: cross 
Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement</title>
<link>https://arxiv.org/abs/2512.08982</link>
<guid>https://arxiv.org/abs/2512.08982</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Low-light enhancement, Retinex decomposition, Consistency models, Single-step sampling  

<br /><br />Summary:  
This paper introduces Consist-Retinex, a novel framework that adapts consistency modeling to Retinex-based low-light image enhancement. Unlike conventional diffusion models requiring hundreds of iterative sampling steps, Consist-Retinex enables efficient one-step enhancement, greatly improving practical usability. The authors identify that conditional enhancement tasks differ from unconditional synthesis, particularly in training dynamics, where conditional mappings rely heavily on large-noise regimes connecting degraded inputs to clean outputs. To address this, the framework incorporates a dual-objective consistency loss that combines temporal consistency with ground-truth alignment across randomized time steps, ensuring robust and stable learning. Additionally, an adaptive noise-emphasized sampling strategy is proposed to prioritize training on high-noise regions critical for effective one-step conditional generation. Experimental results on the VE-LOL-L benchmark demonstrate state-of-the-art performance in terms of PSNR (25.51 vs. 23.41) and FID (44.73 vs. 49.59) compared to the prior Diff-Retinex++ method, while dramatically reducing the training budget to just one-eighth of that required by 1000-step Diff-Retinex baselines. This work thus represents the first successful application of consistency models for fast, accurate low-light image enhancement. <div>
arXiv:2512.08982v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification</title>
<link>https://arxiv.org/abs/2512.08983</link>
<guid>https://arxiv.org/abs/2512.08983</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV identification, model pruning, spectral clustering, ResNet18 compression, noise-robust fine-tuning<br /><br />Summary:<br /><br />This paper addresses the challenges faced by traditional UAV identification methods in extracting reliable signal features and achieving real-time performance in complex, low-altitude security threat environments. It leverages deep learning-based Radio Frequency Fingerprint Identification (RFFI) to enhance recognition accuracy but notes the limitation posed by large model sizes and high computational demands, which hinder deployment on edge devices. To overcome these challenges, the authors propose HSCP, a Hierarchical Spectral Clustering Pruning framework that synergizes layer and channel pruning for extreme model compression, improved performance, and efficient inference. HSCP uses spectral clustering guided by Centered Kernel Alignment (CKA) to first prune redundant layers and then applies the same principle to channels, removing finer redundancies. Additionally, a noise-robust fine-tuning strategy is introduced to enhance model robustness. Experiments on the UAV-M100 benchmark demonstrate the superiority of HSCP over existing pruning methods, achieving an 86.39% reduction in parameters and an 84.44% decrease in FLOPs on ResNet18 while increasing accuracy by 1.49% relative to the unpruned baseline. Moreover, HSCP maintains high robustness in environments with low signal-to-noise ratios, making it suitable for deployment on resource-constrained edge devices in complex UAV recognition scenarios. <div>
arXiv:2512.08983v1 Announce Type: cross 
Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.08984</link>
<guid>https://arxiv.org/abs/2512.08984</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, RAG-HAR, Large Language Models, Retrieval-Augmented, Training-Free  

<br /><br />Summary:  
This paper presents RAG-HAR, a novel training-free framework for Human Activity Recognition (HAR) that leverages large language models (LLMs) through a retrieval-augmented approach. Unlike traditional deep learning methods that require extensive labeled data and computational resources, RAG-HAR operates without any model training or fine-tuning, highlighting its efficiency and practical applicability. The method computes lightweight statistical descriptors from sensor data, which are then used to retrieve semantically similar samples from a vector database. This contextual information aids the LLM in accurately identifying human activities. The authors enhance RAG-HAR by implementing prompt optimization techniques and introducing an LLM-based activity descriptor, which enriches the vector database with context-aware information to improve retrieval relevance. Evaluations across six diverse HAR benchmarks demonstrate that RAG-HAR achieves state-of-the-art performance, outperforming existing methods despite its training-free nature. Furthermore, RAG-HAR generalizes well beyond previously seen activities, facilitating the recognition and meaningful labeling of multiple unseen human activities. This work underscores the potential of combining retrieval mechanisms with LLMs to offer robust, scalable solutions in HAR, advancing applications in healthcare, rehabilitation, fitness tracking, and smart environments. <div>
arXiv:2512.08984v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy</title>
<link>https://arxiv.org/abs/2512.08986</link>
<guid>https://arxiv.org/abs/2512.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: Diabetic Retinopathy, Fundus Photography, Quality Control, Explainable Classifier, Annotation Agreement  

<br /><br />Summary:  
This article addresses the challenge of accurately diagnosing Diabetic Retinopathy (DR), a complication of long-term diabetes that can result in vision loss if undetected early. The study highlights the importance of fundus photography in capturing retinal structure and disease indicators. It underscores the role of Artificial Intelligence (AI) in aiding clinicians by reducing manual diagnostic workload, while emphasizing the need for high-quality annotated datasets to train AI models effectively. The authors identify potential errors arising from complex retinal structures, image acquisition flaws, and manual annotation inconsistencies. To tackle these issues, they propose a comprehensive quality-control framework that ensures only data meeting high standards are used for AI training and evaluation. This framework incorporates an explainable feature-based classifier, leveraging both image processing techniques and contrastive learning, to filter out inadequate images. Subsequently, the remaining images undergo enhancement and are annotated with the assistance of deep learning tools. Finally, the framework assesses the agreement between different annotators using specifically derived mathematical formulas, determining the reliability and usability of the annotations for further analysis. <div>
arXiv:2512.08986v1 Announce Type: cross 
Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization</title>
<link>https://arxiv.org/abs/2512.08987</link>
<guid>https://arxiv.org/abs/2512.08987</guid>
<content:encoded><![CDATA[
<div> Inverse design, 3D design space, physics-aware optimization, latent representation, gradient-guided diffusion  

<br /><br />Summary: This paper addresses the challenge of inverse design in three-dimensional (3D) domains, where the exponential growth of design space makes exhaustive searches impractical. It critiques existing methods that use 2D projections or fine-tune pre-existing 3D shapes, highlighting their limitations in maintaining volumetric detail and restricting true 3D design from scratch. The authors propose a novel 3D Inverse Design (3DID) framework that directly operates within the full 3D design space by integrating a continuous latent representation with physics-aware optimization. To achieve this, they first develop a unified physics-geometry embedding that compactly represents both shape and physical field data in a latent space. The optimization process comprises two stages: initially, a gradient-guided diffusion sampler explores the global latent manifold to identify promising candidates; subsequently, an objective-driven, topology-preserving refinement sculpts these candidates to better meet the target objective. This two-stage approach enables the generation of high-fidelity 3D geometries that surpass existing methods in solution quality and design flexibility. Overall, 3DID provides a powerful framework advancing the capabilities of inverse design in complex 3D physical systems. <div>
arXiv:2512.08987v1 Announce Type: cross 
Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning</title>
<link>https://arxiv.org/abs/2512.08992</link>
<guid>https://arxiv.org/abs/2512.08992</guid>
<content:encoded><![CDATA[
<div> Keywords: Chest X-ray, EfficientNetV2-M, COVID-19, Tuberculosis, Automated diagnosis<br /><br />Summary:<br /><br />1. The study addresses the challenges in interpreting chest X-rays, especially in resource-limited settings where radiologist shortages delay diagnosis and affect patient outcomes.<br /><br />2. It proposes a new classification framework using EfficientNetV2-M, improving upon the original CheXNet architecture that used DenseNet-121, which was computationally inefficient and limited to single-label classification.<br /><br />3. The model integrates advanced training techniques including Automatic Mixed Precision training, AdamW optimizer, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization.<br /><br />4. A dataset consisting of 18,080 chest X-ray images from three authoritative sources was compiled, focusing on five important disease categories: Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis.<br /><br />5. Nine independent experimental runs were conducted to ensure statistical reliability and reproducibility.<br /><br />6. The proposed model achieved a mean test accuracy of 96.45%, significantly outperforming the baseline accuracy of 95.30% (p < 0.001), with a macro-averaged F1-score of 91.08% (p < 0.001).<br /><br />7. Exceptional classification was observed for infectious diseases, with COVID-19 detection accuracy at 99.95% and Tuberculosis at 99.97%.<br /><br />8. Despite having 6.8 times more parameters, the training time was reduced by 11.4%, and performance stability improved by 22.7%.<br /><br />9. The framework demonstrates potential as a decision-support tool to aid pandemic response, tuberculosis screening, and routine thoracic disease assessment across healthcare settings. <div>
arXiv:2512.08992v1 Announce Type: cross 
Abstract: The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Generative AI helps Radiotherapy Planning with User Preference</title>
<link>https://arxiv.org/abs/2512.08996</link>
<guid>https://arxiv.org/abs/2512.08996</guid>
<content:encoded><![CDATA[
<div> Keywords: radiotherapy planning, 3D dose prediction, generative model, user-defined preferences, treatment planning systems

<br /><br />Summary:  
This study addresses the complexity and variability inherent in radiotherapy planning across different institutions and individual planners. Traditional deep learning methods for 3D dose prediction typically rely on reference plans as ground truth, which can introduce bias towards specific planning styles or institutional preferences. To overcome this limitation, the authors propose a novel generative model that predicts 3D dose distributions based exclusively on user-defined preference flavors. These preferences allow planners to customize and prioritize trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), enabling more personalized and flexible planning. The proposed model is designed for smooth integration with existing clinical treatment planning systems, facilitating the generation of high-quality plans in an efficient manner. Comparative evaluations indicate that this method can, in certain scenarios, outperform the Varian RapidPlan model in terms of adaptability and plan quality. Overall, the approach offers a significant advancement by reducing planning biases while enhancing customization and clinical utility in radiotherapy dose planning. <div>
arXiv:2512.08996v1 Announce Type: cross 
Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant</title>
<link>https://arxiv.org/abs/2512.08998</link>
<guid>https://arxiv.org/abs/2512.08998</guid>
<content:encoded><![CDATA[
arXiv:2512.08998v1 Announce Type: cross 
Abstract: Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography</title>
<link>https://arxiv.org/abs/2512.09001</link>
<guid>https://arxiv.org/abs/2512.09001</guid>
<content:encoded><![CDATA[
arXiv:2512.09001v1 Announce Type: cross 
Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity</title>
<link>https://arxiv.org/abs/2512.09003</link>
<guid>https://arxiv.org/abs/2512.09003</guid>
<content:encoded><![CDATA[
arXiv:2512.09003v1 Announce Type: cross 
Abstract: Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning</title>
<link>https://arxiv.org/abs/2512.09006</link>
<guid>https://arxiv.org/abs/2512.09006</guid>
<content:encoded><![CDATA[
arXiv:2512.09006v1 Announce Type: cross 
Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Lossless Ultimate Vision Token Compression for VLMs</title>
<link>https://arxiv.org/abs/2512.09010</link>
<guid>https://arxiv.org/abs/2512.09010</guid>
<content:encoded><![CDATA[
arXiv:2512.09010v1 Announce Type: cross 
Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monitoring Deployed AI Systems in Health Care</title>
<link>https://arxiv.org/abs/2512.09048</link>
<guid>https://arxiv.org/abs/2512.09048</guid>
<content:encoded><![CDATA[
arXiv:2512.09048v1 Announce Type: cross 
Abstract: Post-deployment monitoring of artificial intelligence (AI) systems in health care is essential to ensure their safety, quality, and sustained benefit-and to support governance decisions about which systems to update, modify, or decommission. Motivated by these needs, we developed a framework for monitoring deployed AI systems grounded in the mandate to take specific actions when they fail to behave as intended. This framework, which is now actively used at Stanford Health Care, is organized around three complementary principles: system integrity, performance, and impact. System integrity monitoring focuses on maximizing system uptime, detecting runtime errors, and identifying when changes to the surrounding IT ecosystem have unintended effects. Performance monitoring focuses on maintaining accurate system behavior in the face of changing health care practices (and thus input data) over time. Impact monitoring assesses whether a deployed system continues to have value in the form of benefit to clinicians and patients. Drawing on examples of deployed AI systems at our academic medical center, we provide practical guidance for creating monitoring plans based on these principles that specify which metrics to measure, when those metrics should be reviewed, who is responsible for acting when metrics change, and what concrete follow-up actions should be taken-for both traditional and generative AI. We also discuss challenges to implementing this framework, including the effort and cost of monitoring for health systems with limited resources and the difficulty of incorporating data-driven monitoring practices into complex organizations where conflicting priorities and definitions of success often coexist. This framework offers a practical template and starting point for health systems seeking to ensure that AI deployments remain safe and effective over time.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors</title>
<link>https://arxiv.org/abs/2512.09065</link>
<guid>https://arxiv.org/abs/2512.09065</guid>
<content:encoded><![CDATA[
arXiv:2512.09065v1 Announce Type: cross 
Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORCA: Open-ended Response Correctness Assessment for Audio Question Answering</title>
<link>https://arxiv.org/abs/2512.09066</link>
<guid>https://arxiv.org/abs/2512.09066</guid>
<content:encoded><![CDATA[
arXiv:2512.09066v1 Announce Type: cross 
Abstract: Evaluating open-ended responses from large audio language models (LALMs) is challenging because human annotators often genuinely disagree on answer correctness due to multiple valid interpretations, partial correctness, and subjective judgment. Traditional metrics reporting only mean scores fail to capture this uncertainty. We present ORCA (Open-ended Response Correctness Assessment), a framework that models the variability in human judgments using Beta distributions to predict both expected correctness and uncertainty. Our three-stage annotation framework combines human judgment with structured feedback and iterative refinement to simultaneously curate training data and improve benchmark quality. We collected 11,721 annotations across 3,580 question-answer pairs from 15 LALMs on two audio QA benchmarks, achieving inter-annotator agreement of 0.82 (Krippendorff's alpha). ORCA achieves 0.91 Spearman correlation with mean human judgments, matching or outperforming LLM-judge baselines while providing uncertainty estimates and requiring significantly less compute. We release our models, code, and curated dataset.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</title>
<link>https://arxiv.org/abs/2512.09069</link>
<guid>https://arxiv.org/abs/2512.09069</guid>
<content:encoded><![CDATA[
arXiv:2512.09069v1 Announce Type: cross 
Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting</title>
<link>https://arxiv.org/abs/2512.09076</link>
<guid>https://arxiv.org/abs/2512.09076</guid>
<content:encoded><![CDATA[
arXiv:2512.09076v1 Announce Type: cross 
Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mental Models of Autonomy and Sentience Shape Reactions to AI</title>
<link>https://arxiv.org/abs/2512.09085</link>
<guid>https://arxiv.org/abs/2512.09085</guid>
<content:encoded><![CDATA[
arXiv:2512.09085v1 Announce Type: cross 
Abstract: Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Generative Policy for Robotic Control</title>
<link>https://arxiv.org/abs/2512.09101</link>
<guid>https://arxiv.org/abs/2512.09101</guid>
<content:encoded><![CDATA[
arXiv:2512.09101v1 Announce Type: cross 
Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Excellence: Automated Optimization of LLM-based Agents</title>
<link>https://arxiv.org/abs/2512.09108</link>
<guid>https://arxiv.org/abs/2512.09108</guid>
<content:encoded><![CDATA[
arXiv:2512.09108v1 Announce Type: cross 
Abstract: Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.
  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.
  We evaluate ARTEMIS on four representative agent systems: the \emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \textbf{$13.6\%$ improvement} in acceptance rate; the \emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \textbf{10.1\% performance gain}; and the \emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \textbf{$36.9\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \textbf{22\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous</title>
<link>https://arxiv.org/abs/2512.09111</link>
<guid>https://arxiv.org/abs/2512.09111</guid>
<content:encoded><![CDATA[
arXiv:2512.09111v1 Announce Type: cross 
Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation</title>
<link>https://arxiv.org/abs/2512.09127</link>
<guid>https://arxiv.org/abs/2512.09127</guid>
<content:encoded><![CDATA[
arXiv:2512.09127v1 Announce Type: cross 
Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation</title>
<link>https://arxiv.org/abs/2512.09134</link>
<guid>https://arxiv.org/abs/2512.09134</guid>
<content:encoded><![CDATA[
arXiv:2512.09134v1 Announce Type: cross 
Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment</title>
<link>https://arxiv.org/abs/2512.09148</link>
<guid>https://arxiv.org/abs/2512.09148</guid>
<content:encoded><![CDATA[
arXiv:2512.09148v1 Announce Type: cross 
Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindShift: Analyzing Language Models' Reactions to Psychological Prompts</title>
<link>https://arxiv.org/abs/2512.09149</link>
<guid>https://arxiv.org/abs/2512.09149</guid>
<content:encoded><![CDATA[
arXiv:2512.09149v1 Announce Type: cross 
Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WonderZoom: Multi-Scale 3D World Generation</title>
<link>https://arxiv.org/abs/2512.09164</link>
<guid>https://arxiv.org/abs/2512.09164</guid>
<content:encoded><![CDATA[
arXiv:2512.09164v1 Announce Type: cross 
Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Expansion and Application of the Alexandria Database</title>
<link>https://arxiv.org/abs/2512.09169</link>
<guid>https://arxiv.org/abs/2512.09169</guid>
<content:encoded><![CDATA[
arXiv:2512.09169v1 Announce Type: cross 
Abstract: We present a novel multi-stage workflow for computational materials discovery that achieves a 99% success rate in identifying compounds within 100 meV/atom of thermodynamic stability, with a threefold improvement over previous approaches. By combining the Matra-Genoa generative model, Orb-v2 universal machine learning interatomic potential, and ALIGNN graph neural network for energy prediction, we generated 119 million candidate structures and added 1.3 million DFT-validated compounds to the ALEXANDRIA database, including 74 thousand new stable materials. The expanded ALEXANDRIA database now contains 5.8 million structures with 175 thousand compounds on the convex hull. Predicted structural disorder rates (37-43%) match experimental databases, unlike other recent AI-generated datasets. Analysis reveals fundamental patterns in space group distributions, coordination environments, and phase stability networks, including sub-linear scaling of convex hull connectivity. We release the complete dataset, including sAlex25 with 14 million out-of-equilibrium structures containing forces and stresses for training universal force fields. We demonstrate that fine-tuning a GRACE model on this data improves benchmark accuracy. All data, models, and workflows are freely available under Creative Commons licenses.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
arXiv:2512.09172v1 Announce Type: cross 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation</title>
<link>https://arxiv.org/abs/2512.09185</link>
<guid>https://arxiv.org/abs/2512.09185</guid>
<content:encoded><![CDATA[
arXiv:2512.09185v1 Announce Type: cross 
Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $\Delta$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $\Delta$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WOLF: Werewolf-based Observations for LLM Deception and Falsehoods</title>
<link>https://arxiv.org/abs/2512.09187</link>
<guid>https://arxiv.org/abs/2512.09187</guid>
<content:encoded><![CDATA[
arXiv:2512.09187v1 Announce Type: cross 
Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Mental States in Active and Autonomous Driving with EEG</title>
<link>https://arxiv.org/abs/2512.09190</link>
<guid>https://arxiv.org/abs/2512.09190</guid>
<content:encoded><![CDATA[
arXiv:2512.09190v1 Announce Type: cross 
Abstract: Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2512.09198</link>
<guid>https://arxiv.org/abs/2512.09198</guid>
<content:encoded><![CDATA[
arXiv:2512.09198v1 Announce Type: cross 
Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs for Analog Circuit Design Continuum (ACDC)</title>
<link>https://arxiv.org/abs/2512.09199</link>
<guid>https://arxiv.org/abs/2512.09199</guid>
<content:encoded><![CDATA[
arXiv:2512.09199v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2512.09202</link>
<guid>https://arxiv.org/abs/2512.09202</guid>
<content:encoded><![CDATA[
arXiv:2512.09202v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: A Conceptual Reasoning Layer for Large Language Models</title>
<link>https://arxiv.org/abs/2512.09222</link>
<guid>https://arxiv.org/abs/2512.09222</guid>
<content:encoded><![CDATA[
arXiv:2512.09222v1 Announce Type: cross 
Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI</title>
<link>https://arxiv.org/abs/2512.09244</link>
<guid>https://arxiv.org/abs/2512.09244</guid>
<content:encoded><![CDATA[
arXiv:2512.09244v1 Announce Type: cross 
Abstract: Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.09251</link>
<guid>https://arxiv.org/abs/2512.09251</guid>
<content:encoded><![CDATA[
arXiv:2512.09251v1 Announce Type: cross 
Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2512.09264</link>
<guid>https://arxiv.org/abs/2512.09264</guid>
<content:encoded><![CDATA[
arXiv:2512.09264v1 Announce Type: cross 
Abstract: The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Bias in Machine-generated Text Detection</title>
<link>https://arxiv.org/abs/2512.09292</link>
<guid>https://arxiv.org/abs/2512.09292</guid>
<content:encoded><![CDATA[
arXiv:2512.09292v1 Announce Type: cross 
Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2512.09313</link>
<guid>https://arxiv.org/abs/2512.09313</guid>
<content:encoded><![CDATA[
arXiv:2512.09313v1 Announce Type: cross 
Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Percolation: A Perspective on Criticality of Form and Function</title>
<link>https://arxiv.org/abs/2512.09317</link>
<guid>https://arxiv.org/abs/2512.09317</guid>
<content:encoded><![CDATA[
arXiv:2512.09317v1 Announce Type: cross 
Abstract: Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Genetic Evolution of Neural Networks for Optimal SFC Embedding</title>
<link>https://arxiv.org/abs/2512.09318</link>
<guid>https://arxiv.org/abs/2512.09318</guid>
<content:encoded><![CDATA[
arXiv:2512.09318v1 Announce Type: cross 
Abstract: The reliance of organisations on computer networks is enabled by network programmability, which is typically achieved through Service Function Chaining. These chains virtualise network functions, link them, and programmatically embed them on networking infrastructure. Optimal embedding of Service Function Chains is an NP-hard problem, with three sub-problems, chain composition, virtual network function embedding, and link embedding, that have to be optimised simultaneously, rather than sequentially, for optimal results. Genetic Algorithms have been employed for this, but existing approaches either do not optimise all three sub-problems or do not optimise all three sub-problems simultaneously. We propose a Genetic Algorithm-based approach called GENESIS, which evolves three sine-function-activated Neural Networks, and funnels their output to a Gaussian distribution and an A* algorithm to optimise all three sub-problems simultaneously. We evaluate GENESIS on an emulator across 48 different data centre scenarios and compare its performance to two state-of-the-art Genetic Algorithms and one greedy algorithm. GENESIS produces an optimal solution for 100% of the scenarios, whereas the second-best method optimises only 71% of the scenarios. Moreover, GENESIS is the fastest among all Genetic Algorithms, averaging 15.84 minutes, compared to an average of 38.62 minutes for the second-best Genetic Algorithm.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment</title>
<link>https://arxiv.org/abs/2512.09319</link>
<guid>https://arxiv.org/abs/2512.09319</guid>
<content:encoded><![CDATA[
arXiv:2512.09319v1 Announce Type: cross 
Abstract: Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality</title>
<link>https://arxiv.org/abs/2512.09355</link>
<guid>https://arxiv.org/abs/2512.09355</guid>
<content:encoded><![CDATA[
arXiv:2512.09355v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log NeRF: Comparing Spaces for Learning Radiance Fields</title>
<link>https://arxiv.org/abs/2512.09375</link>
<guid>https://arxiv.org/abs/2512.09375</guid>
<content:encoded><![CDATA[
arXiv:2512.09375v1 Announce Type: cross 
Abstract: Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.09385</link>
<guid>https://arxiv.org/abs/2512.09385</guid>
<content:encoded><![CDATA[
arXiv:2512.09385v1 Announce Type: cross 
Abstract: The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CONCUR: A Framework for Continual Constrained and Unconstrained Routing</title>
<link>https://arxiv.org/abs/2512.09386</link>
<guid>https://arxiv.org/abs/2512.09386</guid>
<content:encoded><![CDATA[
arXiv:2512.09386v1 Announce Type: cross 
Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection</title>
<link>https://arxiv.org/abs/2512.09396</link>
<guid>https://arxiv.org/abs/2512.09396</guid>
<content:encoded><![CDATA[
arXiv:2512.09396v1 Announce Type: cross 
Abstract: Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting</title>
<link>https://arxiv.org/abs/2512.09398</link>
<guid>https://arxiv.org/abs/2512.09398</guid>
<content:encoded><![CDATA[
arXiv:2512.09398v1 Announce Type: cross 
Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos</title>
<link>https://arxiv.org/abs/2512.09406</link>
<guid>https://arxiv.org/abs/2512.09406</guid>
<content:encoded><![CDATA[
arXiv:2512.09406v1 Announce Type: cross 
Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators</title>
<link>https://arxiv.org/abs/2512.09427</link>
<guid>https://arxiv.org/abs/2512.09427</guid>
<content:encoded><![CDATA[
arXiv:2512.09427v1 Announce Type: cross 
Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CourtPressGER: A German Court Decision to Press Release Summarization Dataset</title>
<link>https://arxiv.org/abs/2512.09434</link>
<guid>https://arxiv.org/abs/2512.09434</guid>
<content:encoded><![CDATA[
arXiv:2512.09434v1 Announce Type: cross 
Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model</title>
<link>https://arxiv.org/abs/2512.09441</link>
<guid>https://arxiv.org/abs/2512.09441</guid>
<content:encoded><![CDATA[
arXiv:2512.09441v1 Announce Type: cross 
Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Research via Human-AI Interactive Theorem Proving</title>
<link>https://arxiv.org/abs/2512.09443</link>
<guid>https://arxiv.org/abs/2512.09443</guid>
<content:encoded><![CDATA[
arXiv:2512.09443v1 Announce Type: cross 
Abstract: We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework</title>
<link>https://arxiv.org/abs/2512.09461</link>
<guid>https://arxiv.org/abs/2512.09461</guid>
<content:encoded><![CDATA[
arXiv:2512.09461v1 Announce Type: cross 
Abstract: Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing</title>
<link>https://arxiv.org/abs/2512.09463</link>
<guid>https://arxiv.org/abs/2512.09463</guid>
<content:encoded><![CDATA[
arXiv:2512.09463v1 Announce Type: cross 
Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach</title>
<link>https://arxiv.org/abs/2512.09471</link>
<guid>https://arxiv.org/abs/2512.09471</guid>
<content:encoded><![CDATA[
arXiv:2512.09471v1 Announce Type: cross 
Abstract: Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Color encoding in Latent Space of Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2512.09477</link>
<guid>https://arxiv.org/abs/2512.09477</guid>
<content:encoded><![CDATA[
arXiv:2512.09477v1 Announce Type: cross 
Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks</title>
<link>https://arxiv.org/abs/2512.09485</link>
<guid>https://arxiv.org/abs/2512.09485</guid>
<content:encoded><![CDATA[
arXiv:2512.09485v1 Announce Type: cross 
Abstract: Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&amp;CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09487</link>
<guid>https://arxiv.org/abs/2512.09487</guid>
<content:encoded><![CDATA[
arXiv:2512.09487v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Invariance and Allocation: When Subgroup Balance Matters</title>
<link>https://arxiv.org/abs/2512.09496</link>
<guid>https://arxiv.org/abs/2512.09496</guid>
<content:encoded><![CDATA[
arXiv:2512.09496v1 Announce Type: cross 
Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization</title>
<link>https://arxiv.org/abs/2512.09524</link>
<guid>https://arxiv.org/abs/2512.09524</guid>
<content:encoded><![CDATA[
arXiv:2512.09524v1 Announce Type: cross 
Abstract: Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs</title>
<link>https://arxiv.org/abs/2512.09543</link>
<guid>https://arxiv.org/abs/2512.09543</guid>
<content:encoded><![CDATA[
arXiv:2512.09543v1 Announce Type: cross 
Abstract: Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.
  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.
  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.
  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.
  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection</title>
<link>https://arxiv.org/abs/2512.09563</link>
<guid>https://arxiv.org/abs/2512.09563</guid>
<content:encoded><![CDATA[
arXiv:2512.09563v1 Announce Type: cross 
Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Gender Code: Gendering the Global Governance of Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.09570</link>
<guid>https://arxiv.org/abs/2512.09570</guid>
<content:encoded><![CDATA[
arXiv:2512.09570v1 Announce Type: cross 
Abstract: This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows</title>
<link>https://arxiv.org/abs/2512.09572</link>
<guid>https://arxiv.org/abs/2512.09572</guid>
<content:encoded><![CDATA[
arXiv:2512.09572v1 Announce Type: cross 
Abstract: Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\!\propto\!|k|^{-\lambda}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $\beta(\tau)\!\propto\!\tau^\gamma$ that preserve fine-scale structure deeper into diffusion time, along with \emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that na\"ive Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation</title>
<link>https://arxiv.org/abs/2512.09577</link>
<guid>https://arxiv.org/abs/2512.09577</guid>
<content:encoded><![CDATA[
arXiv:2512.09577v1 Announce Type: cross 
Abstract: We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title>
<link>https://arxiv.org/abs/2512.09579</link>
<guid>https://arxiv.org/abs/2512.09579</guid>
<content:encoded><![CDATA[
arXiv:2512.09579v1 Announce Type: cross 
Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates</title>
<link>https://arxiv.org/abs/2512.09586</link>
<guid>https://arxiv.org/abs/2512.09586</guid>
<content:encoded><![CDATA[
arXiv:2512.09586v1 Announce Type: cross 
Abstract: Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models</title>
<link>https://arxiv.org/abs/2512.09591</link>
<guid>https://arxiv.org/abs/2512.09591</guid>
<content:encoded><![CDATA[
arXiv:2512.09591v1 Announce Type: cross 
Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation</title>
<link>https://arxiv.org/abs/2512.09610</link>
<guid>https://arxiv.org/abs/2512.09610</guid>
<content:encoded><![CDATA[
arXiv:2512.09610v1 Announce Type: cross 
Abstract: People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Chain-of-Thought Reasoning for Videos</title>
<link>https://arxiv.org/abs/2512.09616</link>
<guid>https://arxiv.org/abs/2512.09616</guid>
<content:encoded><![CDATA[
arXiv:2512.09616v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection</title>
<link>https://arxiv.org/abs/2512.09662</link>
<guid>https://arxiv.org/abs/2512.09662</guid>
<content:encoded><![CDATA[
arXiv:2512.09662v1 Announce Type: cross 
Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $\kappa$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power</title>
<link>https://arxiv.org/abs/2512.09673</link>
<guid>https://arxiv.org/abs/2512.09673</guid>
<content:encoded><![CDATA[
arXiv:2512.09673v1 Announce Type: cross 
Abstract: Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization</title>
<link>https://arxiv.org/abs/2512.09678</link>
<guid>https://arxiv.org/abs/2512.09678</guid>
<content:encoded><![CDATA[
arXiv:2512.09678v1 Announce Type: cross 
Abstract: In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies</title>
<link>https://arxiv.org/abs/2512.09682</link>
<guid>https://arxiv.org/abs/2512.09682</guid>
<content:encoded><![CDATA[
arXiv:2512.09682v1 Announce Type: cross 
Abstract: This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method</title>
<link>https://arxiv.org/abs/2512.09729</link>
<guid>https://arxiv.org/abs/2512.09729</guid>
<content:encoded><![CDATA[
arXiv:2512.09729v1 Announce Type: cross 
Abstract: We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs</title>
<link>https://arxiv.org/abs/2512.09742</link>
<guid>https://arxiv.org/abs/2512.09742</guid>
<content:encoded><![CDATA[
arXiv:2512.09742v1 Announce Type: cross 
Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Circuits, Features, and Heuristics in Molecular Transformers</title>
<link>https://arxiv.org/abs/2512.09757</link>
<guid>https://arxiv.org/abs/2512.09757</guid>
<content:encoded><![CDATA[
arXiv:2512.09757v1 Announce Type: cross 
Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.09775</link>
<guid>https://arxiv.org/abs/2512.09775</guid>
<content:encoded><![CDATA[
arXiv:2512.09775v1 Announce Type: cross 
Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation</title>
<link>https://arxiv.org/abs/2512.09779</link>
<guid>https://arxiv.org/abs/2512.09779</guid>
<content:encoded><![CDATA[
arXiv:2512.09779v1 Announce Type: cross 
Abstract: Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&amp;Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing</title>
<link>https://arxiv.org/abs/2512.09806</link>
<guid>https://arxiv.org/abs/2512.09806</guid>
<content:encoded><![CDATA[
arXiv:2512.09806v1 Announce Type: cross 
Abstract: U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composing Concepts from Images and Videos via Concept-prompt Binding</title>
<link>https://arxiv.org/abs/2512.09824</link>
<guid>https://arxiv.org/abs/2512.09824</guid>
<content:encoded><![CDATA[
arXiv:2512.09824v1 Announce Type: cross 
Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs in Interpreting Legal Documents</title>
<link>https://arxiv.org/abs/2512.09830</link>
<guid>https://arxiv.org/abs/2512.09830</guid>
<content:encoded><![CDATA[
arXiv:2512.09830v1 Announce Type: cross 
Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI</title>
<link>https://arxiv.org/abs/2512.09867</link>
<guid>https://arxiv.org/abs/2512.09867</guid>
<content:encoded><![CDATA[
arXiv:2512.09867v1 Announce Type: cross 
Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09872</link>
<guid>https://arxiv.org/abs/2512.09872</guid>
<content:encoded><![CDATA[
arXiv:2512.09872v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Learning from Modern Language Models via Low Logit Rank</title>
<link>https://arxiv.org/abs/2512.09892</link>
<guid>https://arxiv.org/abs/2512.09892</guid>
<content:encoded><![CDATA[
arXiv:2512.09892v1 Announce Type: cross 
Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Heading Prediction for Autonomous Aerial Vehicles</title>
<link>https://arxiv.org/abs/2512.09898</link>
<guid>https://arxiv.org/abs/2512.09898</guid>
<content:encoded><![CDATA[
arXiv:2512.09898v1 Announce Type: cross 
Abstract: The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506{\deg} and a root mean squared error of 0.1957{\deg}, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STACHE: Local Black-Box Explanations for Reinforcement Learning Policies</title>
<link>https://arxiv.org/abs/2512.09909</link>
<guid>https://arxiv.org/abs/2512.09909</guid>
<content:encoded><![CDATA[
arXiv:2512.09909v1 Announce Type: cross 
Abstract: Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach</title>
<link>https://arxiv.org/abs/2512.09910</link>
<guid>https://arxiv.org/abs/2512.09910</guid>
<content:encoded><![CDATA[
arXiv:2512.09910v1 Announce Type: cross 
Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised learning pays attention</title>
<link>https://arxiv.org/abs/2512.09912</link>
<guid>https://arxiv.org/abs/2512.09912</guid>
<content:encoded><![CDATA[
arXiv:2512.09912v1 Announce Type: cross 
Abstract: In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.
  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FALCON: Few-step Accurate Likelihoods for Continuous Flows</title>
<link>https://arxiv.org/abs/2512.09914</link>
<guid>https://arxiv.org/abs/2512.09914</guid>
<content:encoded><![CDATA[
arXiv:2512.09914v1 Announce Type: cross 
Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating</title>
<link>https://arxiv.org/abs/2512.09920</link>
<guid>https://arxiv.org/abs/2512.09920</guid>
<content:encoded><![CDATA[
arXiv:2512.09920v1 Announce Type: cross 
Abstract: Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction</title>
<link>https://arxiv.org/abs/2509.11719</link>
<guid>https://arxiv.org/abs/2509.11719</guid>
<content:encoded><![CDATA[
arXiv:2509.11719v2 Announce Type: replace 
Abstract: Multi-agent trajectory prediction in autonomous driving requires a comprehensive understanding of complex social dynamics. Existing methods, however, often struggle to capture the full richness of these dynamics, particularly the co-existence of multi-scale interactions and the diverse behaviors of heterogeneous agents. To address these challenges, this paper introduces HeLoFusion, an efficient and scalable encoder for modeling heterogeneous and multi-scale agent interactions. Instead of relying on global context, HeLoFusion constructs local, multi-scale graphs centered on each agent, allowing it to effectively model both direct pairwise dependencies and complex group-wise interactions (\textit{e.g.}, platooning vehicles or pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of agent heterogeneity through an aggregation-decomposition message-passing scheme and type-specific feature networks, enabling it to learn nuanced, type-dependent interaction patterns. This locality-focused approach enables a principled representation of multi-level social context, yielding powerful and expressive agent embeddings. On the challenging Waymo Open Motion Dataset, HeLoFusion achieves state-of-the-art performance, setting new benchmarks for key metrics including Soft mAP and minADE. Our work demonstrates that a locality-grounded architecture, which explicitly models multi-scale and heterogeneous interactions, is a highly effective strategy for advancing motion forecasting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.23589</link>
<guid>https://arxiv.org/abs/2509.23589</guid>
<content:encoded><![CDATA[
arXiv:2509.23589v2 Announce Type: replace 
Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 7.72% over prior arts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows</title>
<link>https://arxiv.org/abs/2510.03506</link>
<guid>https://arxiv.org/abs/2510.03506</guid>
<content:encoded><![CDATA[
arXiv:2510.03506v3 Announce Type: replace 
Abstract: We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding</title>
<link>https://arxiv.org/abs/2510.15952</link>
<guid>https://arxiv.org/abs/2510.15952</guid>
<content:encoded><![CDATA[
arXiv:2510.15952v3 Announce Type: replace 
Abstract: Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking "what is intelligence?" (ontological), SCL asks "under what conditions does cognition emerge?" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling "executable epistemology" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking World-Model Learning</title>
<link>https://arxiv.org/abs/2510.19788</link>
<guid>https://arxiv.org/abs/2510.19788</guid>
<content:encoded><![CDATA[
arXiv:2510.19788v3 Announce Type: replace 
Abstract: Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended $\unicode{x2014}$ models should support many different tasks unknown ahead of time $\unicode{x2014}$ and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template $\unicode{x2014}$ reward-free exploration, derived tests, and behavior-based scoring $\unicode{x2014}$ to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Failures: Rethinking Foundation Models in Pathology</title>
<link>https://arxiv.org/abs/2510.23807</link>
<guid>https://arxiv.org/abs/2510.23807</guid>
<content:encoded><![CDATA[
arXiv:2510.23807v4 Announce Type: replace 
Abstract: Despite their successes in vision and language, foundation models have stumbled in pathology, revealing low accuracy, instability, and heavy computational demands. These shortcomings stem not from tuning problems but from deeper conceptual mismatches: dense embeddings cannot represent the combinatorial richness of tissue, and current architectures inherit flaws in self-supervision, patch design, and noise-fragile pretraining. Biological complexity and limited domain innovation further widen the gap. The evidence is clear-pathology requires models explicitly designed for biological images rather than adaptations of large-scale natural-image methods whose assumptions do not hold for tissue.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</title>
<link>https://arxiv.org/abs/2511.03092</link>
<guid>https://arxiv.org/abs/2511.03092</guid>
<content:encoded><![CDATA[
arXiv:2511.03092v5 Announce Type: replace 
Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning</title>
<link>https://arxiv.org/abs/2511.06316</link>
<guid>https://arxiv.org/abs/2511.06316</guid>
<content:encoded><![CDATA[
arXiv:2511.06316v2 Announce Type: replace 
Abstract: Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed language (e.g. Bangla-English) scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning), a vision-language framework that emulates human spatial reasoning to infer accident location coordinates directly from available textual and map-based cues. ALIGN integrates large language and vision-language model mechanisms within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data source, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district- and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</title>
<link>https://arxiv.org/abs/2511.09900</link>
<guid>https://arxiv.org/abs/2511.09900</guid>
<content:encoded><![CDATA[
arXiv:2511.09900v3 Announce Type: replace 
Abstract: Protein evolution through amino acid mutations is a cornerstone of life sciences. Recent advances in protein language models have shown rich evolutionary patterns, offering unprecedented potential for in-silicon directed evolution. However, existing directed evolution methods largely rely on heuristic evolution strategies and have yet to efficiently integrate the transformative protein language models with advanced optimization techniques, such as reinforcement learning, to learn optimal evolution policies. To bridge this gap, we propose AlphaDE, a novel framework that evolves protein sequences by harnessing the innovative paradigms of large language models, such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility of the interested protein family. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A case study further demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v2 Announce Type: replace 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2511.14256</link>
<guid>https://arxiv.org/abs/2511.14256</guid>
<content:encoded><![CDATA[
arXiv:2511.14256v2 Announce Type: replace 
Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference in Discrete State Spaces from First Principles</title>
<link>https://arxiv.org/abs/2511.20321</link>
<guid>https://arxiv.org/abs/2511.20321</guid>
<content:encoded><![CDATA[
arXiv:2511.20321v2 Announce Type: replace 
Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A survey on the impacts of recommender systems on users, items, and human-AI ecosystems</title>
<link>https://arxiv.org/abs/2407.01630</link>
<guid>https://arxiv.org/abs/2407.01630</guid>
<content:encoded><![CDATA[
arXiv:2407.01630v2 Announce Type: replace-cross 
Abstract: Recommendation systems and assistants (in short, recommenders) influence through online platforms most actions of our daily lives, suggesting items or providing solutions based on users' preferences or requests. This survey systematically reviews, categories, and discusses the impact of recommenders in four human-AI ecosystems -- social media, online retail, urban mapping and generative AI ecosystems. Its scope is to systematise a fast-growing field in which terminologies employed to classify methodologies and outcomes are fragmented and unsystematic. This is a crucial contribution to the literature because terminologies vary substantially across disciplines and ecosystems, hindering comparison and accumulation of knowledge in the field. We follow the customary steps of qualitative systematic review, gathering 154 articles from different disciplines to develop a parsimonious taxonomy of methodologies employed (empirical, simulation, observational, controlled), outcomes observed (concentration, content degradation, discrimination, diversity, echo chamber, filter bubble, homogenisation, polarisation, radicalisation, volume), and their level of analysis (individual, item, and ecosystem). We systematically discuss substantive and methodological commonalities across ecosystems, and highlight potential avenues for future research. The survey is addressed to scholars and practitioners interested in different human-AI ecosystems, policymakers and institutional stakeholders who want to understand better the measurable outcomes of recommenders, and tech companies who wish to obtain a systematic view of the impact of their recommenders.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Transportation by Orthogonal Coupling Dynamics</title>
<link>https://arxiv.org/abs/2410.08060</link>
<guid>https://arxiv.org/abs/2410.08060</guid>
<content:encoded><![CDATA[
arXiv:2410.08060v2 Announce Type: replace-cross 
Abstract: Many numerical and learning algorithms rely on the solution of the Monge-Kantorovich problem and Wasserstein distances, which provide appropriate distributional metrics. While the natural approach is to treat the problem as an infinite-dimensional linear programming, such a methodology limits the computational performance due to the polynomial scaling with respect to the sample size along with intensive memory requirements. We propose a novel alternative framework to address the Monge-Kantorovich problem based on a projection type gradient descent scheme. The dynamics builds on the notion of the conditional expectation, where the connection with the opinion dynamics is leveraged to devise efficient numerical schemes. We demonstrate that the resulting dynamics recovers random maps with favourable computational performance. Along with the theoretical insight, the proposed dynamics paves the way for innovative approaches to construct numerical schemes for computing optimal transport maps as well as Wasserstein distances.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning and Opportunistic Inference for Continuous Monitoring of Freezing of Gait in Parkinson's Disease</title>
<link>https://arxiv.org/abs/2410.21326</link>
<guid>https://arxiv.org/abs/2410.21326</guid>
<content:encoded><![CDATA[
arXiv:2410.21326v2 Announce Type: replace-cross 
Abstract: Parkinson's disease (PD) is a progressive neurological disorder that impacts the quality of life significantly, making in-home monitoring of motor symptoms such as Freezing of Gait (FoG) critical. However, existing symptom monitoring technologies are power-hungry, rely on extensive amounts of labeled data, and operate in controlled settings. These shortcomings limit real-world deployment of the technology. This work presents LIFT-PD, a computationally-efficient self-supervised learning framework for real-time FoG detection. Our method combines self-supervised pre-training on unlabeled data with a novel differential hopping windowing technique to learn from limited labeled instances. An opportunistic model activation module further minimizes power consumption by selectively activating the deep learning module only during active periods. Extensive experimental results show that LIFT-PD achieves a 7.25% increase in precision and 4.4% improvement in accuracy compared to supervised models while using as low as 40% of the labeled training data used for supervised learning. Additionally, the model activation module reduces inference time by up to 67% compared to continuous inference. LIFT-PD paves the way for practical, energy-efficient, and unobtrusive in-home monitoring of PD patients with minimal labeling requirements.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding World or Predicting Future? A Comprehensive Survey of World Models</title>
<link>https://arxiv.org/abs/2411.14499</link>
<guid>https://arxiv.org/abs/2411.14499</guid>
<content:encoded><![CDATA[
arXiv:2411.14499v4 Announce Type: replace-cross 
Abstract: The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection Strategy Doesn't Matter (Much)</title>
<link>https://arxiv.org/abs/2502.04499</link>
<guid>https://arxiv.org/abs/2502.04499</guid>
<content:encoded><![CDATA[
arXiv:2502.04499v2 Announce Type: replace-cross 
Abstract: Knowledge distillation (KD) is a popular method of transferring knowledge from a large "teacher" model to a small "student" model. Previous work has explored various layer-selection strategies (e.g., forward matching and in-order random matching) for intermediate-layer matching in KD, where a student layer is forced to resemble a certain teacher layer. In this work, we revisit such layer-selection strategies and observe an intriguing phenomenon that layer-selection strategy does not matter (much) in intermediate-layer matching -- even seemingly nonsensical matching strategies such as reverse matching still result in surprisingly good student performance. We provide an interpretation for this phenomenon by examining the angles between teacher layers viewed from the student's perspective. Our work sheds light on KD practice, as layer-selection strategies may not be the main focus of KD system design, and vanilla forward matching works well in most setups.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms</title>
<link>https://arxiv.org/abs/2502.17801</link>
<guid>https://arxiv.org/abs/2502.17801</guid>
<content:encoded><![CDATA[
arXiv:2502.17801v3 Announce Type: replace-cross 
Abstract: Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?</title>
<link>https://arxiv.org/abs/2503.05507</link>
<guid>https://arxiv.org/abs/2503.05507</guid>
<content:encoded><![CDATA[
arXiv:2503.05507v2 Announce Type: replace-cross 
Abstract: Grammar serves as a cornerstone in programming languages and software engineering, providing frameworks to define the syntactic space and program structure. Existing research demonstrates the effectiveness of grammar-based code representations in small-scale models, showing their ability to reduce syntax errors and enhance performance. However, as language models scale to the billion level or beyond, syntax-level errors become rare, making it unclear whether grammar information still provides performance benefits. To explore this, we develop a series of billion-scale GrammarCoder models, incorporating grammar rules in the code generation process. Experiments on HumanEval (+) and MBPP (+) demonstrate a notable improvement in code generation accuracy. Further analysis shows that grammar-based representations enhance LLMs' ability to discern subtle code differences, reducing semantic errors caused by minor variations. These findings suggest that grammar-based code representations remain valuable even in billion-scale models, not only by maintaining syntax correctness but also by improving semantic differentiation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v5 Announce Type: replace-cross 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Story of Two GPUs: Characterizing the Resilience of Hopper H100 and Ampere A100 GPUs</title>
<link>https://arxiv.org/abs/2503.11901</link>
<guid>https://arxiv.org/abs/2503.11901</guid>
<content:encoded><![CDATA[
arXiv:2503.11901v4 Announce Type: replace-cross 
Abstract: This study characterizes GPU resilience in Delta, a large-scale AI system that consists of 1,056 A100 and H100 GPUs, with over 1,300 petaflops of peak throughput. We used 2.5 years of operational data (11.7 million GPU hours) on GPU errors. Our major findings include: (i) H100 GPU memory resilience is worse than A100 GPU memory, with 3.2x lower per-GPU MTBE for memory errors, (ii) The GPU memory error-recovery mechanisms on H100 GPUs are insufficient to handle the increased memory capacity, (iii) H100 GPUs demonstrate significantly improved GPU hardware resilience over A100 GPUs with respect to critical hardware components, (iv) GPU errors on both A100 and H100 GPUs frequently result in job failures due to the lack of robust recovery mechanisms at the application level, and (v) We project the impact of GPU node availability on larger-scales and find that significant overprovisioning of 5% is necessary to handle GPU failures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset</title>
<link>https://arxiv.org/abs/2503.19804</link>
<guid>https://arxiv.org/abs/2503.19804</guid>
<content:encoded><![CDATA[
arXiv:2503.19804v2 Announce Type: replace-cross 
Abstract: Low-light image enhancement is crucial for a myriad of applications, from night vision and surveillance, to autonomous driving. However, due to the inherent limitations that come in hand with capturing images in low-illumination environments, the task of enhancing such scenes still presents a formidable challenge. To advance research in this field, we introduce our Low Exposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure benchmark dataset for low-light image enhancement comprising of over 230K frames showcasing 24K real-world indoor and outdoor, with-and without human, scenes. Captured using 3 different camera sensors, LENVIZ offers a wide range of lighting conditions, noise levels, and scene complexities, making it the largest publicly available up-to 4K resolution benchmark in the field. LENVIZ includes high quality human-generated ground truth, for which each multi-exposure low-light scene has been meticulously curated and edited by expert photographers to ensure optimal image quality. Furthermore, we also conduct a comprehensive analysis of current state-of-the-art low-light image enhancement techniques on our dataset and highlight potential areas of improvement.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sinusoidal Initialization, Time for a New Start</title>
<link>https://arxiv.org/abs/2505.12909</link>
<guid>https://arxiv.org/abs/2505.12909</guid>
<content:encoded><![CDATA[
arXiv:2505.12909v3 Announce Type: replace-cross 
Abstract: Initialization plays a critical role in Deep Neural Network training, directly influencing convergence, stability, and generalization. Common approaches such as Glorot and He initializations rely on randomness, which can produce uneven weight distributions across layer connections. In this paper, we introduce the Sinusoidal initialization, a novel deterministic method that employs sinusoidal functions to construct structured weight matrices expressly to improve the spread and balance of weights throughout the network while simultaneously fostering a more uniform, well-conditioned distribution of neuron activation states from the very first forward pass. Because Sinusoidal initialization begins with weights and activations that are already evenly and efficiently utilized, it delivers consistently faster convergence, greater training stability, and higher final accuracy across a wide range of models, including convolutional neural networks, vision transformers, and large language models. On average, our experiments show an increase of 4.9% in final validation accuracy and 20.9% in convergence speed. By replacing randomness with structure, this initialization provides a stronger and more reliable foundation for Deep Learning systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.13572</link>
<guid>https://arxiv.org/abs/2505.13572</guid>
<content:encoded><![CDATA[
arXiv:2505.13572v2 Announce Type: replace-cross 
Abstract: The SPARQL query language is the standard method to access knowledge graphs (KGs). However, formulating SPARQL queries is a significant challenge for non-expert users, and remains time-consuming for the experienced ones. Best practices recommend to document KGs with competency questions and example queries to contextualise the knowledge they contain and illustrate their potential applications. In practice, however, this is either not the case or the examples are provided in limited numbers. Large Language Models (LLMs) are being used in conversational agents and are proving to be an attractive solution with a wide range of applications, from simple question-answering about common knowledge to generating code in a targeted programming language. However, training and testing these models to produce high quality SPARQL queries from natural language questions requires substantial datasets of question-query pairs. In this paper, we present Q${}^2$Forge that addresses the challenge of generating new competency questions for a KG and corresponding SPARQL queries. It iteratively validates those queries with human feedback and LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular, meaning that the different modules of the application (CQ generation, query generation and query refinement) can be used separately, as an integrated pipeline, or replaced by alternative services. The result is a complete pipeline from competency question formulation to query evaluation, supporting the creation of reference query sets for any target KG.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking data encoding methods in Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2505.14295</link>
<guid>https://arxiv.org/abs/2505.14295</guid>
<content:encoded><![CDATA[
arXiv:2505.14295v2 Announce Type: replace-cross 
Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine Learning (QML). While classical approaches process data directly as vectors, QML may require transforming classical data into quantum states through encoding circuits, known as quantum feature maps or quantum embeddings. This step leverages the inherently high-dimensional and non-linear nature of Hilbert space, enabling more efficient data separation in complex feature spaces that may be inaccessible to classical methods. This encoding part significantly affects the performance of the QML model, so it is important to choose the right encoding method for the dataset to be encoded. However, this choice is generally arbitrary, since there is no "universal" rule for knowing which encoding to choose based on a specific set of data. There are currently a variety of encoding methods using different quantum logic gates. We studied the most commonly used types of encoding methods and benchmarked them using different datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm</title>
<link>https://arxiv.org/abs/2505.15138</link>
<guid>https://arxiv.org/abs/2505.15138</guid>
<content:encoded><![CDATA[
arXiv:2505.15138v2 Announce Type: replace-cross 
Abstract: This paper investigates infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general parametrization. We propose a Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints while ensuring a high convergence rate. In particular, our algorithm achieves global convergence and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge of $\tau_{\mathrm{mix}}$, the achievable rates change to $\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq \tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results match the theoretical lower bound for Markov Decision Processes and establish a new benchmark in the theoretical exploration of average reward CMDPs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
arXiv:2505.16056v3 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v3 Announce Type: replace-cross 
Abstract: We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $\epsilon$-optimal robust policy within $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Minimalist Optimizer Design for LLM Pretraining</title>
<link>https://arxiv.org/abs/2506.16659</link>
<guid>https://arxiv.org/abs/2506.16659</guid>
<content:encoded><![CDATA[
arXiv:2506.16659v2 Announce Type: replace-cross 
Abstract: Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which introduce extra operations and require significant more memory to maintain first- and second-order moments than SGD. While recent works such as GaLore, Fira and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What are the minimum modifications to plain SGD needed to match state-of-the-art pretraining performance? We systematically investigate this question using a bottom-up approach, and identify two simple yet highly (memory- and compute-) efficient techniques: (1) column-wise gradient normalization (normalizing the gradient along the output dimension), which boosts SGD performance without momentum; and (2) applying first-order momentum only to the output layer, where gradient variance is highest. Combining these two techniques lead to SCALE (Stochastic Column-normAlized Last-layer momEntum), a simple optimizer for memory efficient pretraining. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For LLaMA 7B model, SCALE outperforms the state-of-the-art memory-efficient methods APOLLO and Muon, in terms of both perplexity and memory consumption.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement</title>
<link>https://arxiv.org/abs/2507.00966</link>
<guid>https://arxiv.org/abs/2507.00966</guid>
<content:encoded><![CDATA[
arXiv:2507.00966v3 Announce Type: replace-cross 
Abstract: With new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform the state-of-the-art in single-channel speech enhancement and audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VB-DemandEx, a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, MambAttention significantly outperforms existing state-of-the-art discriminative LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 without reverberation and EARS-WHAM_v2. MambAttention also matches or outperforms generative diffusion models in generalization performance while being competitive with language model baselines. Ablation studies highlight the importance of weight sharing between time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. Yet, MambAttention remains superior for cross-corpus generalization across all reported evaluation metrics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation</title>
<link>https://arxiv.org/abs/2507.10580</link>
<guid>https://arxiv.org/abs/2507.10580</guid>
<content:encoded><![CDATA[
arXiv:2507.10580v2 Announce Type: replace-cross 
Abstract: Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have increasingly been used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solutions. To address these challenges, we propose EmoSApp (Emotional Support App): an entirely offline, smartphone-based conversational app designed to provide mental health and emotional support. EmoSApp leverages a language model, specifically the LLaMA-3.2-1B-Instruct, which is fine-tuned and quantized on a custom-curated ``Knowledge Dataset'' comprising 14,582 mental health QA pairs along with multi-turn conversational data, enabling robust domain expertise and fully on-device inference on resource-constrained smartphones.
  Through qualitative evaluation with students and mental health professionals, we demonstrate that EmoSApp has the ability to respond coherently and empathetically, provide relevant suggestions to user's mental health problems, and maintain interactive dialogue. Additionally, quantitative evaluations on nine commonsense and reasoning benchmarks, along with two mental health specific datasets, demonstrate EmoSApp's effectiveness in low-resource settings. By prioritizing on-device deployment and specialized domain-specific adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health support.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v2 Announce Type: replace-cross 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROPS: Progressively Private Self-alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.06783</link>
<guid>https://arxiv.org/abs/2508.06783</guid>
<content:encoded><![CDATA[
arXiv:2508.06783v2 Announce Type: replace-cross 
Abstract: Alignment is a key step in developing Large Language Models (LLMs) using human feedback to ensure adherence to human values and societal norms. Dependence on human feedback raises privacy concerns about how much a labeler's preferences may reveal about their personal values, beliefs, and personality traits. Existing approaches, such as Differentially Private SGD (DP-SGD), provide rigorous privacy guarantees by privatizing gradients during fine-tuning and alignment but can provide more privacy than necessary as human preferences are tied only to labels of (prompt, response) pairs and can degrade model utility. This work focuses on LLM alignment with preference-level privacy, which preserves the privacy of preference labels provided by humans. We propose PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving alignment framework where privately aligned models in previous stages can serve as labelers for supplementing training data in the subsequent stages of alignment. We present theoretical guarantees for PROPS as well as comprehensive validation using multiple models (Pythia and GPT) and datasets (AlpacaEval, Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over existing methods while still providing high privacy. For the same privacy budget, alignment via PROPS can achieve up to 3x higher win-rates compared to DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based alignment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</title>
<link>https://arxiv.org/abs/2508.06982</link>
<guid>https://arxiv.org/abs/2508.06982</guid>
<content:encoded><![CDATA[
arXiv:2508.06982v4 Announce Type: replace-cross 
Abstract: We present WeatherDiffusion, a diffusion-based framework for controllable weather editing in intrinsic space. Our framework includes two components based on diffusion priors: an inverse renderer that estimates material properties, scene geometry, and lighting as intrinsic maps from an input image, and a forward renderer that utilizes these geometry and material maps along with a text prompt that describes specific weather conditions to generate a final image. The intrinsic maps enhance controllability compared to traditional pixel-space editing approaches. We propose an intrinsic map-aware attention mechanism that improves spatial correspondence and decomposition quality in large outdoor scenes. For forward rendering, we leverage CLIP-space interpolation of weather prompts to achieve fine-grained weather control. We also introduce a synthetic and a real-world dataset, containing 38k and 18k images under various weather conditions, each with intrinsic map annotations. WeatherDiffusion outperforms state-of-the-art pixel-space editing approaches, weather restoration methods, and rendering-based methods, showing promise for downstream tasks such as autonomous driving, enhancing the robustness of detection and segmentation in challenging weather scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vevo2: A Unified and Controllable Framework for Speech and Singing Voice Generation</title>
<link>https://arxiv.org/abs/2508.16332</link>
<guid>https://arxiv.org/abs/2508.16332</guid>
<content:encoded><![CDATA[
arXiv:2508.16332v2 Announce Type: replace-cross 
Abstract: Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a unified music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a unified content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during the speech-singing joint training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the Vevo2's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.19366</link>
<guid>https://arxiv.org/abs/2508.19366</guid>
<content:encoded><![CDATA[
arXiv:2508.19366v4 Announce Type: replace-cross 
Abstract: Hallucinations in LLMs--especially in multimodal settings--undermine reliability. We present a rigorous information-geometric framework, grounded in diffusion dynamics, to quantify hallucinations in MLLMs where model outputs are embedded via spectral decompositions of multimodal graph Laplacians, and their gaps to a truth manifold define a semantic distortion metric. We derive Courant-Fischer bounds on a temperature-dependent hallucination profile and use RKHS eigenmodes to obtain modality-aware, interpretable measures that track evolution over prompts and time. This reframes hallucination as quantifiable and bounded, providing a principled basis for evaluation and mitigation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction</title>
<link>https://arxiv.org/abs/2509.15872</link>
<guid>https://arxiv.org/abs/2509.15872</guid>
<content:encoded><![CDATA[
arXiv:2509.15872v2 Announce Type: replace-cross 
Abstract: Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing 2 pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Noise-Curvature View of Loss of Trainability</title>
<link>https://arxiv.org/abs/2509.19698</link>
<guid>https://arxiv.org/abs/2509.19698</guid>
<content:encoded><![CDATA[
arXiv:2509.19698v3 Announce Type: replace-cross 
Abstract: Loss of trainability refers to a phenomenon in continual learning where parameter updates no longer make progress on the optimization objective, so accuracy stalls or degrades as the learning problem changes over time. In this paper, we analyze loss of trainability through an optimization lens and find that the phenomenon is not reliably predicted by existing individual indicators such as Hessian rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios, and unit-sign entropy. Motivated by our analysis, we introduce two complementary indicators: a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound. We then combine these two indicators into a per-layer adaptive noise threshold on the effective step-size that anticipates trainability behavior. Using this insight, we propose a step-size scheduler that keeps each layer's effective parameter update below this bound, thereby avoiding loss of trainability. We demonstrate that our scheduler can improve the accuracy maintained by previously proposed approaches, such as concatenated ReLU (CReLU), Wasserstein regularizer, and L2 weight decay. Surprisingly, our scheduler produces adaptive step-size trajectories that, without tuning, mirror the manually engineered step-size decay schedules.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Formal Theory on the Logical Limits of Symbol Grounding</title>
<link>https://arxiv.org/abs/2509.20409</link>
<guid>https://arxiv.org/abs/2509.20409</guid>
<content:encoded><![CDATA[
arXiv:2509.20409v4 Announce Type: replace-cross 
Abstract: This paper synthesizes a series of formal proofs to construct a unified theory on the logical limits of the Symbol Grounding Problem. We distinguish between internal meaning (sense), which formal systems can possess via axioms, and external grounding (reference), which is a necessary condition for connecting symbols to the world. We demonstrate through a four-stage argument that meaningful grounding within a formal system must arise from a process that is external, dynamic, and non-fixed algorithmic. First, we show that for a purely symbolic system, the impossibility of grounding is a direct consequence of its definition. Second, we extend this limitation to systems with any finite, static set of pre-established meanings (Semantic Axioms). By formally modeling the computationalist hypothesis-which equates grounding with internal derivation-we prove via G\"odelian arguments that such systems cannot consistently and completely define a "groundability predicate" for all truths. Third, we demonstrate that the "grounding act" for emergent meanings cannot be inferred from internal rules but requires an axiomatic, meta-level update. Drawing on Turing's concept of Oracle Machines and Piccinini's analysis of the mathematical objection, we identify this update as physical transduction. Finally, we prove that this process cannot be simulated by a fixed judgment algorithm, validating the logical necessity of embodied interaction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting the Future with Yesterday's Climate: Temperature Bias in AI Weather and Climate Models</title>
<link>https://arxiv.org/abs/2509.22359</link>
<guid>https://arxiv.org/abs/2509.22359</guid>
<content:encoded><![CDATA[
arXiv:2509.22359v2 Announce Type: replace-cross 
Abstract: AI-based climate and weather models have rapidly gained popularity, providing faster forecasts with skill that can match or even surpass that of traditional dynamical models. Despite this success, these models face a key challenge: predicting future climates while being trained only with historical data. In this study, we investigate this issue by analyzing boreal winter land temperature biases in AI weather and climate models. We examine two weather models, FourCastNet V2 Small (FourCastNet) and Pangu Weather (Pangu), evaluating their predictions for 2020-2025 and Ai2 Climate Emulator version 2 (ACE2) for 1996-2010. These time periods lie outside of the respective models' training sets and are significantly more recent than the bulk of their training data, allowing us to assess how well the models generalize to new, i.e. more modern, conditions. We find that all three models produce cold-biased mean temperatures, resembling climates from 15-20 years earlier than the period they are predicting. In some regions, like the Eastern U.S., the predictions resemble climates from as much as 20-30 years earlier. Further analysis shows that FourCastNet's and Pangu's cold bias is strongest in the hottest predicted temperatures, indicating limited training exposure to modern extreme heat events. In contrast, ACE2's bias is more evenly distributed but largest in regions, seasons, and parts of the temperature distribution where climate change has been most pronounced. These findings underscore the challenge of training AI models exclusively on historical data and highlight the need to account for such biases when applying them to future climate prediction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impossibility of Inverse Permutation Learning in Transformer Models</title>
<link>https://arxiv.org/abs/2509.24125</link>
<guid>https://arxiv.org/abs/2509.24125</guid>
<content:encoded><![CDATA[
arXiv:2509.24125v3 Announce Type: replace-cross 
Abstract: In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</title>
<link>https://arxiv.org/abs/2509.25270</link>
<guid>https://arxiv.org/abs/2509.25270</guid>
<content:encoded><![CDATA[
arXiv:2509.25270v3 Announce Type: replace-cross 
Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Three Regimes of Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01460</link>
<guid>https://arxiv.org/abs/2510.01460</guid>
<content:encoded><![CDATA[
arXiv:2510.01460v2 Announce Type: replace-cross 
Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online-fine tuning that work well in one setting can fail completely in another. We propose a stability--plasticity principle that can explain this inconsistency: we should preserve the knowledge of pretrained policy or offline dataset during online fine-tuning, whichever is better, while maintaining sufficient plasticity. This perspective identifies three regimes of online fine-tuning, each requiring distinct stability properties. We validate this framework through a large-scale empirical study, finding that the results strongly align with its predictions in 45 of 63 cases. This work provides a principled framework for guiding design choices in offline-to-online RL based on the relative performance of the offline dataset and the pretrained policy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual Speech Recognition Evaluation</title>
<link>https://arxiv.org/abs/2510.06961</link>
<guid>https://arxiv.org/abs/2510.06961</guid>
<content:encoded><![CDATA[
arXiv:2510.06961v3 Announce Type: replace-cross 
Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including a dedicated multilingual track. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</title>
<link>https://arxiv.org/abs/2510.08352</link>
<guid>https://arxiv.org/abs/2510.08352</guid>
<content:encoded><![CDATA[
arXiv:2510.08352v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not "shortsighted", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</title>
<link>https://arxiv.org/abs/2510.09660</link>
<guid>https://arxiv.org/abs/2510.09660</guid>
<content:encoded><![CDATA[
arXiv:2510.09660v4 Announce Type: replace-cross 
Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adoption Paradox for Veterinary Professionals in China: High Use of Artificial Intelligence Despite Low Familiarity</title>
<link>https://arxiv.org/abs/2510.11758</link>
<guid>https://arxiv.org/abs/2510.11758</guid>
<content:encoded><![CDATA[
arXiv:2510.11758v2 Announce Type: replace-cross 
Abstract: While the global integration of artificial intelligence (AI) into veterinary medicine is accelerating, its adoption dynamics in major markets such as China remain uncharacterized. This paper presents the first exploratory analysis of AI perception and adoption among veterinary professionals in China, based on a cross-sectional survey of 455 practitioners conducted in mid-2025. We identify a distinct "adoption paradox": although 71.0% of respondents have incorporated AI into their workflows, 44.6% of these active users report low familiarity with the technology. In contrast to the administrative-focused patterns observed in North America, adoption in China is practitioner-driven and centers on core clinical tasks, such as disease diagnosis (50.1%) and prescription calculation (44.8%). However, concerns regarding reliability and accuracy remain the primary barrier (54.3%), coexisting with a strong consensus (93.8%) for regulatory oversight. These findings suggest a unique "inside-out" integration model in China, characterized by high clinical utility but restricted by an "interpretability gap," underscoring the need for specialized tools and robust regulatory frameworks to safely harness AI's potential in this expanding market.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title>
<link>https://arxiv.org/abs/2510.13865</link>
<guid>https://arxiv.org/abs/2510.13865</guid>
<content:encoded><![CDATA[
arXiv:2510.13865v5 Announce Type: replace-cross 
Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Sinks in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.15731</link>
<guid>https://arxiv.org/abs/2510.15731</guid>
<content:encoded><![CDATA[
arXiv:2510.15731v2 Announce Type: replace-cross 
Abstract: Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework</title>
<link>https://arxiv.org/abs/2510.15843</link>
<guid>https://arxiv.org/abs/2510.15843</guid>
<content:encoded><![CDATA[
arXiv:2510.15843v2 Announce Type: replace-cross 
Abstract: Accurately detecting sentiment polarity and intensity in product reviews and social media posts remains challenging due to informal and domain-specific language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer framework that combines rule-based heuristics, contextual deep learning, and fuzzy logic to generate continuous sentiment scores reflecting both polarity and strength. The pipeline begins with VADER-based initial sentiment estimations, which are refined through a two-stage adjustment process. This involves leveraging confidence scores from DistilBERT, a lightweight transformer and applying fuzzy logic principles to mitigate excessive neutrality bias and enhance granularity. A custom fuzzy inference system then maps the refined scores onto a 0 to 1 continuum, producing expert)like judgments. The framework is rigorously evaluated on four domain-specific datasets. food delivery, e-commerce, tourism, and fashion. Results show improved alignment with user ratings, better identification of sentiment extremes, and reduced misclassifications. Both quantitative metrics (distributional alignment, confusion matrices) and qualitative insights (case studies, runtime analysis) affirm the models robustness and efficiency. This work demonstrates the value of integrating symbolic reasoning with neural models for interpretable, finegrained sentiment analysis in linguistically dynamic domains.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</title>
<link>https://arxiv.org/abs/2510.17281</link>
<guid>https://arxiv.org/abs/2510.17281</guid>
<content:encoded><![CDATA[
arXiv:2510.17281v3 Announce Type: replace-cross 
Abstract: Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Diversity Regularizes Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2510.20690</link>
<guid>https://arxiv.org/abs/2510.20690</guid>
<content:encoded><![CDATA[
arXiv:2510.20690v2 Announce Type: replace-cross 
Abstract: Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. While existing mitigation strategies largely target accuracy, we provide the first formal tail bounds for hallucination probability in ensembled language models, reframing it as a second-moment reliability problem and explaining 94.3% of empirical reliability variation seen across parallel configurations. We introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and reduce hallucinations by up to 25.6% (and 14.6% on average) while preserving general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational studies indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different optimal amounts of neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2510.25781</link>
<guid>https://arxiv.org/abs/2510.25781</guid>
<content:encoded><![CDATA[
arXiv:2510.25781v2 Announce Type: replace-cross 
Abstract: The so-called Kolmogorov-Arnold Networks (KANs), whose design is merely inspired, rather than dictated, by the Kolmogorov superposition theorem, have emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs). This review provides a systematic and comprehensive overview of the rapidly expanding KAN landscape. By collecting and categorizing a large set of open-source implementations, we map the vibrant ecosystem supporting modern KAN development. We organize the review around four core themes:
  (i) presenting a precise history of Kolmogorov's superposition theory toward neural-network formulations; (ii) establishing the formal equivalence between KANs and MLPs; (iii) analyzing the critical role of basis functions; and (iv) organizing recent advancements in accuracy, efficiency, regularization, and convergence.
  Finally, we provide a practical Choose-Your-KAN guide to assist practitioners in selecting appropriate architectures, and we close by identifying current research gaps and future directions. The associated GitHub repository (https://github.com/AmirNoori68/kan-review) complements this paper and serves as a structured reference for ongoing KAN research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
<link>https://arxiv.org/abs/2511.02241</link>
<guid>https://arxiv.org/abs/2511.02241</guid>
<content:encoded><![CDATA[
arXiv:2511.02241v3 Announce Type: replace-cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[
arXiv:2511.18828v2 Announce Type: replace-cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization</title>
<link>https://arxiv.org/abs/2511.19253</link>
<guid>https://arxiv.org/abs/2511.19253</guid>
<content:encoded><![CDATA[
arXiv:2511.19253v2 Announce Type: replace-cross 
Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion</title>
<link>https://arxiv.org/abs/2511.22048</link>
<guid>https://arxiv.org/abs/2511.22048</guid>
<content:encoded><![CDATA[
arXiv:2511.22048v2 Announce Type: replace-cross 
Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing the Plasticity-Stability Dilemma in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01034</link>
<guid>https://arxiv.org/abs/2512.01034</guid>
<content:encoded><![CDATA[
arXiv:2512.01034v2 Announce Type: replace-cross 
Abstract: Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Learnability Transition of Measurement-Induced Entanglement</title>
<link>https://arxiv.org/abs/2512.01317</link>
<guid>https://arxiv.org/abs/2512.01317</guid>
<content:encoded><![CDATA[
arXiv:2512.01317v2 Announce Type: replace-cross 
Abstract: Measurement-induced entanglement (MIE) captures how local measurements generate long-range quantum correlations and drive dynamical phase transitions in many-body systems. Yet estimating MIE experimentally remains challenging: direct evaluation requires extensive post-selection over measurement outcomes, raising the question of whether MIE is accessible with only polynomial resources. We address this challenge by reframing MIE detection as a data-driven learning problem that assumes no prior knowledge of state preparation. Using measurement records alone, we train a neural network in a self-supervised manner to predict the uncertainty metric for MIE--the gap between upper and lower bounds of the average post-measurement bipartite entanglement. Applied to random circuits with one-dimensional all-to-all connectivity and two-dimensional nearest-neighbor coupling, our method reveals a learnability transition with increasing circuit depth: below a threshold, the uncertainty is small and decreases with polynomial measurement data and model parameters, while above it the uncertainty remains large despite increasing resources. We further verify this transition experimentally on current noisy quantum devices, demonstrating its robustness to realistic noise. These results highlight the power of data-driven approaches for learning MIE and delineate the practical limits of its classical learnability.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach for Precise Wall Segmentation</title>
<link>https://arxiv.org/abs/2512.02413</link>
<guid>https://arxiv.org/abs/2512.02413</guid>
<content:encoded><![CDATA[
arXiv:2512.02413v2 Announce Type: replace-cross 
Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans necessitates high-precision semantic segmentation of structural elements, particularly walls. However, existing methods often struggle with detecting thin structures and maintaining geometric precision. This study introduces MitUNet, a hybrid neural network combining a Mix-Transformer encoder and a U-Net decoder enhanced with spatial and channel attention blocks. Our approach, optimized with the Tversky loss function, achieves a balance between precision and recall, ensuring accurate boundary recovery. Experiments on the CubiCasa5k dataset and a proprietary regional dataset demonstrate MitUNet's superiority in generating structurally correct masks with high boundary accuracy, outperforming standard models. This tool provides a robust foundation for automated 3D reconstruction pipelines. To ensure reproducibility and facilitate future research, the source code and the proprietary regional dataset are publicly available at https://github.com/aliasstudio/mitunet and https://doi.org/10.5281/zenodo.17871079 respectively.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporality for Sketch Representation Learning</title>
<link>https://arxiv.org/abs/2512.04007</link>
<guid>https://arxiv.org/abs/2512.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: sketch representation, temporal aspect, positional encoding, autoregressive decoder, sequence modeling  

<br /><br />Summary:  
This article explores the significance of the temporal aspect in sketch representation learning by questioning whether sketches should be modeled as sequences. The study evaluates different internal orders of sketches to identify which are most influential for representation quality. Results reveal that traditional positional encodings are effective in capturing the sequential nature of sketches. However, absolute coordinate encodings outperform relative coordinate encodings consistently across experiments. Additionally, non-autoregressive decoders demonstrate superior performance compared to autoregressive decoders when reconstructing or interpreting sketches. Importantly, the study finds that the importance of temporality varies depending on the specific sketch order used as input and the downstream task being performed. Overall, this investigation clarifies how temporal modeling benefits sketch representations and guides the design of learning architectures by emphasizing absolute positional information and non-autoregressive decoding methods for enhanced performance. <div>
arXiv:2512.04007v2 Announce Type: replace-cross 
Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs</title>
<link>https://arxiv.org/abs/2512.07841</link>
<guid>https://arxiv.org/abs/2512.07841</guid>
<content:encoded><![CDATA[
<div> Data Oriented Design, Object-Oriented Design, A* search algorithm, multi-threading, cache utilization<br /><br />Summary:<br /><br />This study analyzes the performance differences between Data Oriented Design (DOD) and traditional Object-Oriented Design (OOD), focusing on cache usage and efficiency in multi-threaded environments. Four versions of the A* search algorithm were developed and compared: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). Evaluations considered execution time, memory usage, and CPU cache misses. Results show that in multi-threaded scenarios, DOD achieved significant performance improvements with faster runtime, fewer system calls, and reduced cache misses. Although OOD occasionally exhibited slight advantages in memory consumption or percentage-based cache miss rates, DOD's efficiency in handling data-intensive tasks was more pronounced. The study also found that for a fine-grained algorithm like A*, the overhead caused by thread management led both paradigms’ single-threaded versions to outperform their multi-threaded equivalents. These findings underscore DOD's architectural superiority, suggesting its better suitability for optimizing hardware utilization in complex, large-scale AI and parallel computing applications despite subtle differences in simpler algorithms. <div>
arXiv:2512.07841v1 Announce Type: new 
Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI autonomously build, operate, and use the entire data stack?</title>
<link>https://arxiv.org/abs/2512.07926</link>
<guid>https://arxiv.org/abs/2512.07926</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous data estates, data lifecycle, AI agents, data management, modern data stack<br /><br />Summary:<br /><br />1. Enterprise data management involves complex processes such as data architecture, integration, quality, governance, and continuous improvement.<br /><br />2. Current AI assistants help specific roles like data engineers and stewards but do not achieve full automation in managing data systems.<br /><br />3. The paper argues for a paradigm shift from isolated AI applications in data components to a holistic, fully autonomous management of the entire data lifecycle.<br /><br />4. It explores how intelligent agents can autonomously manage each stage of the modern data stack, creating self-sufficient systems that serve both human users and AI itself.<br /><br />5. The authors discuss driving forces behind this shift, potential improvements from agent-based automation, and highlight open questions requiring further research to realize a more autonomous data future. <div>
arXiv:2512.07926v1 Announce Type: new 
Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07993</link>
<guid>https://arxiv.org/abs/2512.07993</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, KV cache eviction, chain-of-thought reasoning, SkipKV, sentence-level compression

<br /><br />Summary:  
The paper addresses the significant memory and throughput challenges posed by large reasoning models (LRMs) during chain-of-thought (CoT) reasoning, which cause linear growth in key-value (KV) cache size. Existing KV cache eviction methods are examined but found ineffective in multi-batch settings due to unstable token-wise scoring and reduced effective KV budget from padding tokens. These methods also generate longer sequences by causing repeated revalidation, as they do not consider semantic coherence. To overcome these challenges, the authors propose SkipKV, a training-free KV compression technique that performs selective eviction and generation at the sentence level rather than token level. SkipKV introduces a novel sentence-scoring metric to identify and remove highly similar sentences while preserving semantic coherence. Additionally, it dynamically adjusts a steering vector to modify hidden activation states during inference, reducing redundant generation and encouraging more concise outputs. Experimental results across multiple reasoning benchmarks demonstrate that SkipKV achieves up to 26.7% higher accuracy than alternative methods at equivalent compression budgets. It also reduces generation length by up to 1.6 times and improves throughput by up to 1.7 times compared to state-of-the-art techniques, making it a more efficient solution for CoT reasoning with LRMs. <div>
arXiv:2512.07993v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching</title>
<link>https://arxiv.org/abs/2512.08026</link>
<guid>https://arxiv.org/abs/2512.08026</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-augmented patient-trial matching, electronic health record integration, large language models, dynamic eligibility assessment, secure scalable system<br /><br />Summary:<br /><br />This paper presents a secure and scalable proof-of-concept system designed to automate and augment the process of screening patients for clinical trial eligibility, which is traditionally manual and resource-intensive. The system integrates heterogeneous electronic health record (EHR) data to ensure comprehensive patient information is utilized. It leverages open-source, reasoning-enabled large language models (LLMs) to go beyond simple binary classifications by generating structured eligibility assessments with interpretable reasoning chains. This supports human-in-the-loop expert review, ensuring transparency and reliability. The system treats eligibility as a dynamic state rather than a static decision, identifying immediate matches while also offering actionable recommendations that could make a patient eligible in the future. This approach broadens the range of trials considered for each patient and aims to reduce the burden on clinical coordinators. Additionally, the design guarantees rigorous security measures and comprehensive auditability of all AI-generated outputs, addressing critical implementation challenges in clinical settings. Overall, the solution promises improved efficiency, transparency, and flexibility in clinical trial patient matching workflows. <div>
arXiv:2512.08026v1 Announce Type: new 
Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Education and Research: An Empirical and User Survey-based Analysis</title>
<link>https://arxiv.org/abs/2512.08057</link>
<guid>https://arxiv.org/abs/2512.08057</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ChatGPT, DeepSeek, education, research<br /><br />Summary:<br /><br />This study evaluates two prominent Large Language Models (LLMs), ChatGPT and DeepSeek, focusing on their application in education and research domains. It begins with a background technology analysis to understand the models' underlying architectures and design goals. Empirical experiments benchmark their performance in various tasks, including text generation, programming, and specialized problem-solving such as mathematics and medicine. Results indicate that ChatGPT excels in general language understanding and natural language text generation, offering high accuracy and user-friendliness. Meanwhile, DeepSeek shows superior efficiency and stronger performance in programming-related tasks, attributed to its design emphasis on computational efficiency. Both models reliably produce medically accurate diagnostic outputs and effectively address complex mathematical problems, making them valuable tools for scientific and academic use. Additionally, the study incorporates a real-world user survey involving students, educators, and researchers, which provides qualitative insights into the practical benefits and limitations of both LLMs. The survey responses highlight how these models aid learning, research productivity, and problem-solving while identifying areas needing improvement. Overall, the research offers a comprehensive perspective on trade-offs between model accuracy, computational costs, and user experience, underscoring the growing role of LLMs in advancing educational and research practices. <div>
arXiv:2512.08057v1 Announce Type: new 
Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Back-End for an AI-Based Diabetes Prediction Application</title>
<link>https://arxiv.org/abs/2512.08147</link>
<guid>https://arxiv.org/abs/2512.08147</guid>
<content:encoded><![CDATA[
<div> Keywords: diabetes prediction, scalable back-end, horizontal scaling, RabbitMQ, performance evaluation<br /><br />Summary:<br /><br />The paper addresses the rising need for early diabetes detection by developing a scalable back-end architecture for a mobile diabetes prediction application. The system aims to maintain a failure rate below 5% and an average response latency under 1000 milliseconds. To achieve scalability and responsiveness, the architecture employs horizontal scaling techniques and database sharding, which distribute load and data effectively. Asynchronous communication using RabbitMQ is a key design element, enabling the system to handle computationally intensive prediction requests without overwhelming the back end, by queuing requests and minimizing data loss during peak loads. Performance evaluation results indicate that 83% of the system's features (20 out of 24 functions) met the established performance criteria. Critical features such as user profile management, activity tracking, and read-heavy prediction operations achieved the desired low latency and reliability. The system successfully supported up to 10,000 concurrent users, demonstrating its capability to scale and maintain stability under significant demand. This work highlights the importance of asynchronous messaging and distributed architecture for real-time AI prediction services in healthcare applications. <div>
arXiv:2512.08147v1 Announce Type: new 
Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions</title>
<link>https://arxiv.org/abs/2512.08230</link>
<guid>https://arxiv.org/abs/2512.08230</guid>
<content:encoded><![CDATA[
<div> Keywords: causal learning, empowerment, causal Bayes nets, intrinsic reward, human cognition<br /><br />Summary:<br />1. The paper addresses the challenge of understanding causal structure learning, a key aspect of human cognition, highlighting difficulties faced by large pretrained models using conventional deep learning techniques. <br />2. It emphasizes the role of causal Bayes nets, a formalism from computer science, that cognitive scientists have used to model human causal learning effectively. <br />3. The concept of "empowerment," an intrinsic reward signal from reinforcement learning that maximizes mutual information between actions and outcomes, is proposed as a critical link between Bayesian causal learning and reinforcement learning paradigms. <br />4. The authors argue that accurate causal world models inherently increase empowerment, and boosting empowerment, in turn, improves the accuracy of causal models. This bidirectional relationship might explain unique features in children's causal learning and offer a computationally feasible account of that process. <br />5. An empirical study is conducted comparing children and adults, systematically testing how they utilize cues related to empowerment to infer causal relationships and design effective interventions, supporting the theoretical claims of the framework. <div>
arXiv:2512.08230v1 Announce Type: new 
Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes</title>
<link>https://arxiv.org/abs/2512.08261</link>
<guid>https://arxiv.org/abs/2512.08261</guid>
<content:encoded><![CDATA[
<div> Keywords: disease prediction, knowledge graph, prototype learning, interpretability, large language models<br /><br />Summary: Predicting diseases from patient-reported information such as demographics and symptoms is important for early healthcare engagement and system efficiency. However, current methods often struggle with imbalanced disease distributions and lack interpretability, leading to biased or unreliable predictions. The proposed KPI framework addresses these challenges by integrating structured, trusted medical knowledge into a unified disease knowledge graph, which helps represent diseases comprehensively. KPI constructs clinically meaningful disease prototypes to better capture disease characteristics, especially benefiting prediction of rare or long-tailed diseases. In addition, it leverages contrastive learning techniques to improve predictive accuracy by effectively distinguishing between similar disease representations. To enhance the interpretability of predictions, KPI uses large language models to generate patient-specific, medically relevant explanations. These explanations align with patient narratives, increasing the reliability and clinical validity of the model outputs. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods both in accuracy and explanation quality. By combining knowledge integration, prototype learning, and advanced language models, KPI offers a practical, patient-centered approach for disease prediction that supports both accurate diagnosis and understandable medical insights. <div>
arXiv:2512.08261v1 Announce Type: new 
Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Models Ace the CFA Exams</title>
<link>https://arxiv.org/abs/2512.08270</link>
<guid>https://arxiv.org/abs/2512.08270</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, CFA exams, reasoning models, Gemini 3.0 Pro, GPT-5  

<br /><br />Summary:  
This study evaluates the performance of state-of-the-art large language models (LLMs) on mock Chartered Financial Analyst (CFA) exams comprising 980 questions from Levels I, II, and III. Contrary to earlier research showing LLM struggles on CFA exams, recent reasoning models demonstrate strong results across all three exam levels. Using established pass/fail criteria, most models successfully pass each level of the CFA exams. The top-performing models in order are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Gemini 3.0 Pro stands out with a record-breaking 97.6% score on Level I. On Level II, GPT-5 leads with a 94.3% score, indicating high proficiency in intermediate financial topics. For Level III, which emphasizes constructed-response questions, Gemini 2.5 Pro achieves the highest multiple-choice accuracy at 86.4%, while Gemini 3.0 Pro attains an impressive 92.0% on constructed responses. These results suggest that the newest LLMs have substantially improved reasoning capabilities relevant to professional finance exams, surpassing prior benchmarks and highlighting their potential utility in financial education and assessment. <div>
arXiv:2512.08270v1 Announce Type: new 
Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content</title>
<link>https://arxiv.org/abs/2512.08273</link>
<guid>https://arxiv.org/abs/2512.08273</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Agents, Large Language Models, Automated Evaluation, Content Quality, Business Content  

<br /><br />Summary:  
This research addresses the increasing challenges businesses face in creating and evaluating high-quality content, which is often time-consuming and costly due to human constraints and expensive extrinsic evaluation methods. It highlights the potential of Large Language Models (LLMs) to aid content creation while acknowledging persistent concerns about the quality of AI-generated outputs. To overcome the limitations of traditional human evaluations, the study introduces Generative Agents capable of simulating human judgment. These agents can rapidly and cost-effectively assess AI-generated content by rating key aspects such as coherence, interestingness, clarity, fairness, and relevance. The integration of such agents allows businesses to streamline their content generation workflows, ensuring consistent and high-quality results without the operational burden of human evaluators. Furthermore, the paper provides valuable insights into improving LLMs specifically for producing business-aligned content, marking a significant step forward in automated content generation and evaluation technologies. This approach promises to reduce costs and improve efficiency, offering a scalable solution for modern content challenges faced by enterprises. <div>
arXiv:2512.08273v1 Announce Type: new 
Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Science of Scaling Agent Systems</title>
<link>https://arxiv.org/abs/2512.08296</link>
<guid>https://arxiv.org/abs/2512.08296</guid>
<content:encoded><![CDATA[
<div> Agents, language models, coordination, performance scaling, multi-agent systems<br /><br />Summary:<br /><br />This paper investigates the principles that determine the performance of agent-based language model (LM) systems, which are critical for reasoning, planning, and acting in real-world AI applications. The authors conduct a large-scale empirical evaluation across four benchmarks (Finance-Agent, BrowseComp-Plus, PlanCraft, Workbench) using five architectures (Single, Independent, Centralized, Decentralized, Hybrid) and three LLM families, covering 180 configurations. They develop a predictive model based on coordination metrics—efficiency, overhead, error amplification, and redundancy—with a cross-validated R² of 0.513. The study identifies three core effects: (1) a trade-off between tool usage and coordination overhead where multi-agent overhead disproportionately impacts tool-heavy tasks under fixed computational budgets; (2) capability saturation, showing diminishing or negative returns on coordination once single-agent baseline performance surpasses roughly 45%; (3) error amplification varies by topology, with independent agents amplifying errors 17.2 times, while centralized coordination limits this to 4.4 times. Centralized coordination offers an 80.9% performance gain on parallelizable tasks like financial reasoning, whereas decentralized coordination performs better on dynamic web navigation (+9.2% vs. +0.2%). However, for sequential reasoning tasks, all multi-agent approaches degrade performance significantly (39-70%). Their framework predicts the optimal coordination strategy for 87% of unseen configurations, offering principled guidance for agentic system design based on task characteristics. <div>
arXiv:2512.08296v1 Announce Type: new 
Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection</title>
<link>https://arxiv.org/abs/2512.08300</link>
<guid>https://arxiv.org/abs/2512.08300</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning strategies, multi-agent reinforcement learning, planner, chain of thoughts<br /><br />Summary:<br /><br />This paper introduces a novel reinforced strategy injection mechanism (rSIM) designed to transform any large language model (LLM) into a Reasoning Language Model (RLM) by enabling advanced reasoning capabilities. The key innovation involves using a small planner (leader agent) that guides the LLM (follower agent) through the adaptive injection of reasoning strategies within chain of thoughts (CoTs). The planner and LLM are trained jointly via multi-agent reinforcement learning (MARL) following a leader-follower framework with simple rule-based rewards. Experiments demonstrate that rSIM significantly enhances the reasoning abilities of a smaller model Qwen2.5-0.5B, enabling it to outperform a much larger LLM, Qwen2.5-14B. Importantly, the planner component is generalizable and can be trained once to serve as a plug-in that improves reasoning capabilities across different existing LLMs without retraining them extensively. Furthermore, the planner supports continual learning, allowing it to evolve its planning skills over time and generalize effectively across a wide variety of tasks and problem types. This approach presents a promising direction for cost-efficiently upgrading LLMs’ reasoning skills through strategic planning and reinforcement learning. <div>
arXiv:2512.08300v1 Announce Type: new 
Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from T\"urkiye</title>
<link>https://arxiv.org/abs/2512.08340</link>
<guid>https://arxiv.org/abs/2512.08340</guid>
<content:encoded><![CDATA[
<div> Keywords: California Bearing Ratio, machine learning, random forest, geotechnical engineering, soil properties<br /><br />Summary:<br /><br />This study addresses the prediction of the California Bearing Ratio (CBR), a crucial measure of subgrade soil strength used in transportation infrastructure and foundation design. Traditional CBR tests, although accurate, are time-consuming and costly, limiting their practicality for large-scale soil assessments. To overcome these limitations, the authors developed a machine learning (ML) framework utilizing a dataset of 382 soil samples from various geoclimatic regions in Türkiye. The dataset incorporates multiple physicochemical soil properties to create a multidimensional feature set for supervised learning. Twelve ML algorithms were evaluated, including decision trees, random forest, gradient boosting, support vector regression, and stacking regressors. The random forest regressor achieved the best performance, with R² scores of 0.95 for training, 0.76 for validation, and 0.83 for testing, demonstrating strong generalization and robustness. The study highlights the random forest’s ability to model complex nonlinear relationships in soil data effectively. Ultimately, this research illustrates the potential of intelligent, data-driven approaches to serve as efficient alternatives to traditional CBR testing, facilitating faster, cost-effective geotechnical evaluations and promoting digital transformation in infrastructure analysis and design. <div>
arXiv:2512.08340v1 Announce Type: new 
Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in T\"urkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach</title>
<link>https://arxiv.org/abs/2512.08343</link>
<guid>https://arxiv.org/abs/2512.08343</guid>
<content:encoded><![CDATA[
<div> Keywords: Soil compaction, Optimum Moisture Content, Maximum Dry Density, AutoML, XGBoost<br /><br />Summary:<br /><br />Soil compaction is essential in construction engineering to ensure stable structures like road embankments and earth dams. Traditional approaches for determining Optimum Moisture Content (OMC) and Maximum Dry Density (MDD) rely heavily on time-consuming laboratory experiments and empirical regression models, which often lack accuracy and broad applicability. This study introduces an automated machine learning (AutoML) framework to predict OMC and MDD more efficiently. AutoML automates the selection of algorithms and hyperparameter tuning, aiming to enhance model accuracy and scalability across diverse soil types. Extensive experimentation showed that the Extreme Gradient Boosting (XGBoost) model outperformed others, achieving R-squared values of 80.4% for MDD prediction and 89.1% for OMC on an independent dataset. The results emphasize the significance of using heterogeneous datasets to improve generalization and prediction quality of machine learning models in soil parameter estimation. Ultimately, the study demonstrates how integrating AutoML can lead to more accurate, reliable, and efficient soil compaction parameter predictions, thereby supporting improved construction practices. This advancement has the potential to reduce reliance on labor-intensive testing while enhancing engineering decision-making for various soil conditions. <div>
arXiv:2512.08343v1 Announce Type: new 
Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions</title>
<link>https://arxiv.org/abs/2512.08344</link>
<guid>https://arxiv.org/abs/2512.08344</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Explainable AI, post-hoc explanations, interpretable models, graph structure analysis  

<br /><br />Summary:  
This thesis addresses the challenge of explaining Graph Neural Networks (GNNs), which are widely used for analyzing graph-structured data but whose decision-making processes are difficult to understand. Existing Explainable AI (XAI) techniques either rely on post-hoc explanations that analyze trained models after the fact or on interpretable models designed to provide intrinsic explanations. Post-hoc methods, while flexible and adaptable to different models, often demand significant computational resources and may lack reliability due to limited access to the model’s inner workings. Interpretable models offer immediate explanations but face challenges when applied broadly across diverse scenarios. Many current approaches focus predominantly on graph structure analysis to identify key patterns related to prediction outcomes, but often limit explanations to individual feature contributions. The proposed research aims to develop a novel XAI framework suited specifically for graph-based machine learning that can efficiently provide adaptable and computationally light explanations. This framework seeks to go beyond simplistic feature-based explanations by capturing the influence of graph structure on model predictions, thus improving interpretability in a way that balances reliability, efficiency, and generalizability. <div>
arXiv:2512.08344v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations</title>
<link>https://arxiv.org/abs/2512.08345</link>
<guid>https://arxiv.org/abs/2512.08345</guid>
<content:encoded><![CDATA[
<div> workplace toxicity, large language models, multi-agent systems, adversarial debates, operational efficiency<br /><br />Summary:  
This study addresses the challenge of quantifying the direct impact of workplace toxicity on operational efficiency, which is difficult to measure ethically and practically in human subjects. Using Large Language Model (LLM)-based multi-agent systems, the researchers simulate 1-on-1 adversarial debates to create a controlled "sociological sandbox" environment. They apply a Monte Carlo method to run hundreds of these simulated discussions, comparing a baseline control group against treatment groups where agents are prompted to behave toxically. The key metric analyzed is the convergence time, defined as the number of arguments required for the agents to reach a conclusion. Results show that conversations involving toxic agents take approximately 25% longer to reach conclusions, indicating increased social friction and inefficiency. The authors propose that this increased "latency of toxicity" could act as a proxy for financial losses in both corporate and academic contexts. Furthermore, their approach illustrates that agent-based modeling with LLMs provides a reproducible, ethical alternative to traditional human-subject research when studying the dynamics of social conflict and friction in organizations. <div>
arXiv:2512.08345v1 Announce Type: new 
Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making</title>
<link>https://arxiv.org/abs/2512.08366</link>
<guid>https://arxiv.org/abs/2512.08366</guid>
<content:encoded><![CDATA[
<div> Keywords: Dual-Strategy Agent, Large Language Models, Reflection Mechanism, ALFWorld, Co-adaptive Reasoning  

<br /><br />Summary:  
This paper introduces DuSAR (Dual-Strategy Agent with Reflecting), a novel demonstration-free framework designed for large language model (LLM) agents to improve reasoning and problem-solving abilities. DuSAR leverages two complementary strategies: a high-level holistic plan that directs overall problem-solving, and a context-grounded local policy focused on detailed, step-by-step action. These strategies are coordinated through a lightweight reflection mechanism, where the agent continuously evaluates its progress using a Strategy Fitness Score. This dynamic self-assessment allows the agent to revise its global plan when encountering obstacles or optimize it upon progress, mimicking human-like metacognitive behavior. Experimentally, DuSAR achieves state-of-the-art results on ALFWorld and Mind2Web benchmarks using open-source LLMs (7B-70B parameters). It reaches a 37.1% success rate on ALFWorld with Llama3.1-70B, more than doubling previous best results, and attains 4.02% success on Mind2Web, also outperforming prior baselines by over two times. Additionally, the method significantly reduces token usage per step by 3 to 9 times while maintaining strong performance. Ablation studies emphasize the importance of coordinating the dual strategies. Finally, DuSAR’s design supports the optional integration of expert demonstrations, further enhancing its adaptability and effectiveness. <div>
arXiv:2512.08366v1 Announce Type: new 
Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals</title>
<link>https://arxiv.org/abs/2512.08379</link>
<guid>https://arxiv.org/abs/2512.08379</guid>
<content:encoded><![CDATA[
<div> Keywords: wearable biosignals, feature extraction, large language models, machine learning, DeepFeature<br /><br />Summary:<br /><br />This paper addresses the challenges in feature extraction from biosignals collected via wearable devices, which are critical for healthcare machine learning applications. Traditional methods often lack task-specific context, struggle to optimize settings in high-dimensional spaces, and can suffer from automation errors. To overcome these issues, the authors propose DeepFeature, a novel framework empowered by large language models (LLMs) that generates context-aware features for wearable biosignals. DeepFeature uniquely integrates expert knowledge and task-specific parameters through a multi-source feature generation mechanism. It incorporates an iterative feature refinement process that uses feedback based on feature assessment to iteratively re-select and improve features. Moreover, it ensures robustness in feature-to-code translation by implementing multi-layer filtering and verification, preventing extraction functions from crashing. Experimental evaluation across eight diverse tasks demonstrates that DeepFeature improves the average area under the receiver operating characteristic curve (AUROC) by 4.21% to 9.67% compared to baseline methods. It outperforms existing state-of-the-art techniques on five tasks while maintaining comparable results on the others. This work represents a significant advancement in automated, context-aware feature extraction for biosignal-based machine learning in healthcare. <div>
arXiv:2512.08379v1 Announce Type: new 
Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems</title>
<link>https://arxiv.org/abs/2512.08411</link>
<guid>https://arxiv.org/abs/2512.08411</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid dynamics, model-based planning, Mixture-of-Experts, latent orthogonalization, trajectory optimization<br /><br />Summary:<br /><br />Model-based planning in robotics struggles with hybrid physical dynamics, where continuous motion interleaves with discrete events like contacts and impacts. Traditional latent world models use monolithic neural networks that assume global continuity, causing excessive smoothing of distinct dynamic modes such as sticking vs. sliding or flight vs. stance. This smoothing leads to large compounding errors over long-horizon predictions, undermining reliability in planning near physical boundaries. To tackle these challenges, the authors propose the Prismatic World Model (PRISM-WM), a structured architecture that breaks down complex hybrid dynamics into composable primitives. PRISM-WM employs a context-aware Mixture-of-Experts (MoE) framework, where a gating mechanism implicitly detects the current physical mode and routes to specialized experts predicting associated transitions. They also introduce a latent orthogonalization objective to promote expert diversity and avoid mode collapse. This approach accurately captures sharp mode transitions, substantially reducing rollout drift during planning. Experiments on complex continuous control benchmarks, including high-dimensional humanoids and multi-task settings, confirm that PRISM-WM offers a high-fidelity substrate for trajectory optimization algorithms like TD-MPC. The results suggest PRISM-WM as a promising foundational model for the next generation of model-based agents, enabling more reliable and accurate long-term planning in hybrid dynamical systems. <div>
arXiv:2512.08411v1 Announce Type: new 
Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change</title>
<link>https://arxiv.org/abs/2512.08449</link>
<guid>https://arxiv.org/abs/2512.08449</guid>
<content:encoded><![CDATA[
<div> Keywords: Impact-Driven AI Framework, Theory of Change, value alignment, adversarial debiasing, Assurance Layer<br /><br />Summary:<br /><br />This paper introduces the Impact-Driven AI Framework (IDAIF), a novel AI architectural methodology integrating Theory of Change (ToC) principles to address the AI alignment problem by ensuring AI behavior aligns with human values and intentions. It highlights the limitations of current approaches focused mainly on technical metrics, neglecting sociotechnical deployment aspects, and proposes a systematic mapping of ToC's five-stage model (Inputs, Activities, Outputs, Outcomes, Impact) to AI architecture layers (Data, Pipeline, Inference, Agentic, Normative). Each layer leverages rigorous theoretical foundations: multi-objective Pareto optimization for aligning values; hierarchical multi-agent orchestration to achieve outcomes; causal directed acyclic graphs (DAGs) to mitigate hallucinations; and adversarial debiasing combined with Reinforcement Learning from Human Feedback (RLHF) to ensure fairness. The framework also introduces an Assurance Layer employing guardian architectures to manage assumption failures and enhance system trustworthiness. Through formal mathematical formulations and three diverse case studies in healthcare, cybersecurity, and software engineering, IDAIF demonstrates a paradigm shift from model-centric to impact-centric AI development. This provides engineers with concrete architectural patterns for designing ethical, trustworthy, and socially beneficial AI systems capable of addressing sociotechnical challenges and improving real-world impacts. <div>
arXiv:2512.08449v1 Announce Type: new 
Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using reinforcement learning to probe the role of feedback in skill acquisition</title>
<link>https://arxiv.org/abs/2512.08463</link>
<guid>https://arxiv.org/abs/2512.08463</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, drag control, skill acquisition, flow feedback, physical system  

<br /><br />Summary:  
This study investigates skill acquisition in physical systems by bypassing human subjects and using a generalist reinforcement learning agent interfaced with a spinning cylinder in a tabletop circulating water channel. The goal is to maximize or minimize drag, presenting a real-world, chaotic flow environment that is difficult to model or simulate accurately. The task is straightforward in terms of objectives, as drag minimization or maximization can be directly translated into the agent’s reward, yet discovering effective strategies is challenging. The experimental setup is inexpensive, easy to reproduce, and supported by decades of prior open-loop policy studies. Results demonstrate that the agent, equipped with high-dimensional flow feedback, rapidly discovers high-performance drag-control strategies within minutes of real-world interaction. Furthermore, replaying the learned action sequences without any feedback yields nearly identical performance, indicating that flow feedback is essential for learning but not for executing the skill. Interestingly, training without flow feedback causes failure in the drag maximization task but still allows successful, though slower and less reliable, learning in drag minimization. The findings highlight that acquiring high-performance skills may require richer informational inputs than those needed for skill execution, and that the nature of the learning challenge depends on the goal rather than the complexity of dynamics or policy. <div>
arXiv:2512.08463v1 Announce Type: new 
Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance</title>
<link>https://arxiv.org/abs/2512.08492</link>
<guid>https://arxiv.org/abs/2512.08492</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Program Repair, Data Transformation Graph, Large Language Models, Neuro-symbolic reasoning, Autonomous Issue Resolver<br /><br />Summary:<br />1. The paper addresses the challenge of repository-scale Automated Program Repair (APR), highlighting the limitations of existing control-centric approaches that focus on navigating directory structures and control flow logic.<br />2. It proposes a novel paradigm shift from Code Property Graphs (CPGs) to Data Transformation Graphs (DTGs), where data states are modeled as nodes and functions as edges, allowing agents to track defects via data lineage rather than control flow.<br />3. A multi-agent framework is introduced that integrates data integrity navigation with control flow logic, improving the capability to resolve semantic errors typically trapped in standard Retrieval-Augmented Generation (RAG) systems.<br />4. The authors present Autonomous Issue Resolver (AIR), a self-improvement system using neuro-symbolic reasoning combined with the DTG structure to enable zero-touch code maintenance and scalable logic repair.<br />5. Experimental results demonstrate that AIR achieves a resolution rate of 87.1% on the SWE-Verified benchmark and effectively overcomes core limitations of current AI-assisted coding tools, offering a more robust foundation for reliable software maintenance in increasingly software-dependent environments. <div>
arXiv:2512.08492v1 Announce Type: new 
Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles</title>
<link>https://arxiv.org/abs/2512.08512</link>
<guid>https://arxiv.org/abs/2512.08512</guid>
<content:encoded><![CDATA[
<div> Keywords: state-of-health (SOH), transfer learning (TL), lithium-ion batteries, constructive incremental transfer learning (CITL), unmanned air vehicles (UAV)

<br /><br />Summary: Accurate and rapid state-of-health (SOH) monitoring is crucial for lithium-ion batteries in portable mobile devices to provide reliable energy information. Traditional transfer learning (TL) techniques help reduce the amount of training data needed for SOH monitoring under varying working conditions by leveraging knowledge from data-rich source domains. However, conventional TL approaches demand significant computational resources during training, which is impractical for portable devices due to their limited endurance. To overcome these challenges, the paper proposes a lightweight TL-based SOH monitoring framework called Constructive Incremental Transfer Learning (CITL). CITL incorporates a semi-supervised TL mechanism that effectively utilizes unlabeled data in the target domain by iteratively adding network nodes, thus minimizing monitoring residuals constructively. The approach ensures cross-domain learning capability of added nodes by combining structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Theoretical convergence analysis guarantees both the efficiency of TL and network compactness. The proposed CITL method is validated on an unmanned air vehicle (UAV) battery dataset collected from multiple flight missions, demonstrating superior SOH estimation accuracy. Compared with existing methods—SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM—CITL achieves reduction in root mean square error by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, confirming its effectiveness and practical applicability. <div>
arXiv:2512.08512v1 Announce Type: new 
Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans</title>
<link>https://arxiv.org/abs/2512.08536</link>
<guid>https://arxiv.org/abs/2512.08536</guid>
<content:encoded><![CDATA[
<div> Ethical planning, Large Language Model, automated planning, human-LLM collaboration, principle-grounded rules<br /><br />Summary: Principles2Plan is an innovative interactive prototype designed to enhance ethical awareness in robotic systems operating within human environments by integrating human expertise with the capabilities of Large Language Models (LLMs). The system addresses the limitations of current automated planning tools, which often lack mechanisms to incorporate ethical considerations effectively. It reduces the manual effort and context-specific challenges typically involved in specifying ethical rules by enabling a collaborative process between a domain expert and an LLM. The domain expert inputs the planning domain, problem specifics, and overarching ethical principles such as beneficence and privacy. Based on this input, Principles2Plan generates operational ethical rules consistent with these principles. Users can then review, prioritize, and refine these generated rules. The finalized ethical guidelines are fed into an automated planner to produce plans that are informed by ethical considerations. This approach is novel in providing principle-grounded ethical rule generation tailored for classical planning contexts. Overall, Principles2Plan demonstrates the potential for human-LLM collaboration to make ethical automated planning more practical, adaptable, and context-sensitive, marking an important step towards embedding ethical reasoning in autonomous systems. <div>
arXiv:2512.08536v1 Announce Type: new 
Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SMART+ Framework for AI Systems</title>
<link>https://arxiv.org/abs/2512.08592</link>
<guid>https://arxiv.org/abs/2512.08592</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, SMART+ Framework, AI Governance, Risk Mitigation, Clinical Research<br /><br />Summary:<br /><br />1. Artificial Intelligence (AI) is increasingly embedded across multiple industries, including healthcare, finance, and manufacturing, improving operational workflows such as clinical trial adverse event detection, fraud detection, and predictive maintenance.<br /><br />2. Although AI enhances efficiency, it raises significant concerns related to safety, accountability, and regulatory compliance that necessitate structured governance.<br /><br />3. The paper introduces the SMART+ Framework, a comprehensive model designed to govern AI systems based on core pillars: Safety, Monitoring, Accountability, Reliability, and Transparency.<br /><br />4. The framework is further strengthened by incorporating Privacy & Security, Data Governance, Fairness & Bias mitigation, and operational Guardrails, addressing broader challenges of ethical and regulatory AI deployment.<br /><br />5. SMART+ aligns with evolving regulatory guidance and best practices to provide mechanisms for risk mitigation, trust-building, auditability, and compliance readiness, particularly emphasizing its application in clinical research.<br /><br />6. By adopting SMART+, organizations can ensure responsible AI use, enhance oversight procedures, and integrate effective safeguards for trustworthy AI adoption across diverse sectors. <div>
arXiv:2512.08592v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models</title>
<link>https://arxiv.org/abs/2512.08609</link>
<guid>https://arxiv.org/abs/2512.08609</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Heuristic Design, Large Language Models, Monte Carlo Tree Search, Cognitive Guidance, Heuristic Optimization  

<br /><br />Summary:  
This paper addresses the limitations of existing large language model (LLM)-based evolutionary methods for Automatic Heuristic Design (AHD), which often suffer from local optima due to reliance on population strategies. By integrating LLMs with Monte Carlo Tree Search (MCTS), the trade-off between exploration and exploitation improves; however, current approaches lack multi-round cognitive integration and have limited search diversity. To tackle these challenges, the authors propose CogMCTS, a novel framework that tightly couples the cognitive guidance mechanism from LLMs with MCTS for enhanced automated heuristic optimization. CogMCTS incorporates multi-round cognitive feedback that leverages historical experience, node information, and negative outcomes to dynamically refine heuristic generation. It employs dual-track node expansion alongside elite heuristic management to balance the need for exploring diverse heuristics and exploiting high-quality experiences. Furthermore, strategic mutation techniques adjust heuristic forms and parameters to boost solution diversity and overall optimization performance. Experimental evaluations demonstrate that CogMCTS surpasses existing LLM-based AHD methods in terms of solution stability, optimization efficiency, and solution quality, validating the effectiveness of the proposed cognitive-guided MCTS framework. <div>
arXiv:2512.08609v1 Announce Type: new 
Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protein Secondary Structure Prediction Using Transformers</title>
<link>https://arxiv.org/abs/2512.08613</link>
<guid>https://arxiv.org/abs/2512.08613</guid>
<content:encoded><![CDATA[
<div> Keywords: protein secondary structure, transformer model, attention mechanism, data augmentation, CB513 dataset<br /><br />Summary: This paper introduces a transformer-based model designed to predict protein secondary structures—alpha helices, beta sheets, and coils—directly from amino acid sequences. Leveraging the attention mechanism inherent in transformers, the model effectively captures both local and long-range interactions between residues, which are critical for accurate structural motif identification. To enhance the training process, a sliding-window data augmentation technique is applied to the CB513 dataset, significantly expanding the number of training samples and improving model robustness. The approach is particularly notable for its ability to generalize across sequences of varying lengths, a common challenge in protein structure prediction. The results demonstrate that the transformer model outperforms traditional methods by efficiently modeling complex residue relationships without losing contextual information. This work highlights the potential of transformer architectures in computational biology, especially for tasks involving sequences with intricate dependencies. Overall, the paper contributes a novel methodology that combines advanced sequence modeling with practical data augmentation strategies to improve secondary structure prediction accuracy. <div>
arXiv:2512.08613v1 Announce Type: new 
Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>